{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rancher KB Alt website","text":"<p>This website provide another look and feel and a better search to the SUSE Rancher KB articles.</p>"},{"location":"000020067/","title":"Logs not forwarded by Rancher Logging in Rancher v2.x when Docker daemon logging driver is not set to json-file","text":"<p>This document (000020067) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020067/#situation","title":"Situation","text":""},{"location":"000020067/#issue","title":"Issue","text":"<p>The Rancher v2.x Logging feature enables you to configure log forwarding for Pods, as well as system component containers, in a cluster to a logging endpoint such as Elasticsearch or Splunk.</p> <p>This feature works by deploying a workload to each node in the cluster that mounts the container log directory from the host to parse the Docker container json log files. This is dependent upon use of the json-file Docker logging driver. In the event that the Docker daemon is configured with an alternative logging driver, the logging feature will be unable to parse the logs and will not forward these.</p> <p>In CentOS and RHEL packaged Docker 1.13.1, the default log driver configured is journald, which will prevent log forwarding functioning. Meanwhile, whilst json-file is the default log driver in the upstream Docker packages, if an alternative has been configured on nodes this will also prevent the correct functioning of the log forwarding.</p> <p>You can verify the currently configured Docker logging driver on a node by running <code>docker info | grep Logging</code>, which will show output of the following format: <code>Logging Driver: journald</code>.</p> <p>In the event that json-file is not the configured logging driver, the output of <code>ls -la /var/log/containers/</code> on the node should also be empty. With json-file configured this would display symoblic links to paths under <code>/var/log/pods</code>, containing symbolic links which in turn point to the Docker container json log files.</p>"},{"location":"000020067/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.x managed cluster with Rancher logging enabled</li> </ul>"},{"location":"000020067/#resolution","title":"Resolution","text":""},{"location":"000020067/#centos-or-rhel-packaged-docker","title":"CentOS or RHEL packaged Docker","text":"<ol> <li>Update <code>/etc/sysconfig/docker</code>, as shown in the screenshot below, to set <code>--log-driver=json-file</code> instead of <code>journald</code>.</li> </ol> <ol> <li> <p>Restart the Docker daemon: <code>systemctl restart docker</code></p> </li> <li> <p>You should now see symlinked logs created under <code>/var/log/containers</code></p> </li> </ol>"},{"location":"000020067/#upstream-docker","title":"Upstream Docker","text":"<ol> <li>Configure the json-file Docker logging driver in <code>/etc/docker/daemon.json</code> per the Docker documentation</li> <li>Restart the Docker daemon: <code>systemctl restart docker</code></li> <li>You should now see symlinked logs created under <code>/var/log/containers</code></li> </ol>"},{"location":"000020067/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020068/","title":"\"ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend\" error messages logged by the Docker daemon upon daemon startup","text":"<p>This document (000020068) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020068/#situation","title":"Situation","text":""},{"location":"000020068/#issue","title":"Issue","text":"<p>During startup of the Docker daemon, an error message of the following format is present in the system logs:</p> <pre><code>Jun  13 13:55:47 hostname container-storage-setup: ERROR: XFS filesystem  at /var has ftype=0, cannot use overlay backend; consider different  driver or separate volume or OS reprovision\n</code></pre>"},{"location":"000020068/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Docker daemon with the <code>overlay</code> or <code>overlay2</code> storage driver</li> </ul>"},{"location":"000020068/#resolution","title":"Resolution","text":"<p>An <code>xfs</code> formatted filesystem is only supported as backing for the <code>overlay</code> or <code>overlay2</code> Docker storage drivers if formatted with <code>d_type</code> set to <code>true</code>.</p> <p>The <code>d_type</code> value of an <code>xfs</code> filesystem can be verified with the <code>xfs_info</code> utility. Example output for this command can be found in the <code>xfs_info</code> man pages. If <code>ftype=1</code> the filesystem was formatted with <code>d_type</code> <code>true</code> and the filesystem is suitable for use as backing for the <code>overlay</code> or <code>overlay2</code> storage drivers. If the value is set to <code>0</code> the filesystem is not suitable for use with the <code>overlay</code> or <code>overlay2</code> storage drivers, and would need to be reformated with the flag <code>-n ftype=1</code>.</p> <p>Per the Docker documentation: \"Running on XFS without d_type support now causes Docker to skip the attempt to use the <code>overlay</code> or <code>overlay2</code> driver. Existing installs will continue to run, but produce an error. This is to allow users to migrate their data. In a future version, this will be a fatal error, which will prevent Docker from starting.\"</p>"},{"location":"000020068/#further-reading","title":"Further reading","text":"<p>Docker documentation on the <code>overlay</code> and <code>overlay2</code> storage drivers</p>"},{"location":"000020068/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020069/","title":"[JP] How to troubleshoot using the namespace of a container","text":"<p>This document (000020069) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020069/#situation","title":"Situation","text":""},{"location":"000020069/#_1","title":"\u80cc\u666f","text":"<p>\u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u5834\u5408\u3001\u767a\u751f\u3057\u305f\u554f\u984c\u3068\u4e00\u81f4\u3059\u308b\u518d\u73fe\u74b0\u5883\u304c\u5fc5\u8981\u3067\u3059\u3002\u305f\u3060\u3057\u30b3\u30f3\u30c6\u30ca\u30fc\u74b0\u5883\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3067\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u30fc\u5185\u3067\u30c4\u30fc\u30eb\u3068\u30b7\u30a7\u30eb\u74b0\u5883\u304c\u7c21\u5358\u306b\u5229\u7528\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u518d\u73fe\u74b0\u5883\u306e\u69cb\u7bc9\u304c\u56f0\u96e3\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"000020069/#_2","title":"\u624b\u9806","text":"<p>\u4e0a\u8a18\u306e\u8ab2\u984c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u4ee5\u4e0b\u4e8c\u3064\u306e\u624b\u9806\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"000020069/#sidecar-container","title":"Sidecar container\u00a0\u3092\u5229\u7528","text":"<p>\u554f\u984c\u3092\u6301\u3064\u30b3\u30f3\u30c6\u30ca\u30fc\u304c\u5c5e\u3059\u308bNamespace\u3067\u65b0\u3057\u3044sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u3053\u306esidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u7528\u3044\u3066\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u306f\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3057\u306a\u304c\u3089\u3001\u540c\u3058Network\u3068PID\u306eNamespace\u3092\u4f7f\u7528\u3057\u3066\u8d77\u52d5\u3067\u304d\u307e\u3059\u3002</p> <ul> <li>\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>\u540c\u3058Network\u3001PID\u306eNamespace\u3068Volume\u3092\u4f7f\u7528\u3057sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</li> </ul> <p><code>docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh</code></p> <ul> <li>\u3053\u308c\u304b\u3089\u554f\u984c\u304c\u3042\u308bcontainer\u3084Pod\u3068\u540c\u3058\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5185\u3067\u3001alpine\u30b3\u30f3\u30c6\u30ca\u30fc\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul> <p>\u4f8b\u3048\u3070\u3001Pod\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001Sidecar Container\u306b\u5165\u308a\u3001\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u306a\u304c\u3089\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u74b0\u5883\u306e\u8a2d\u5b9a\u3092\u78ba\u8a8d\u3057\u305f\u308a\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u3001alpine container\u3092\u5225\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u7f6e\u304d\u63db\u3048\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3067\u304d\u307e\u3059\u304c\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u306eRead\uff0fWrite\u30ec\u30a4\u30e4\u306e\u30a2\u30af\u30bb\u30b9\u306f\u3067\u304d\u307e\u305b\u3093\u3002\u540c\u3058\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044\u5834\u5408\u306f\u4e0b\u8a18\u306ensenter\u306e\u4f8b\u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"000020069/#nsenter","title":"nsenter \u3092\u5229\u7528","text":"<p><code>nsenter</code> \u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30ce\u30fc\u30c9\u4e0a\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 <code>nsenter</code> \u30b3\u30de\u30f3\u30c9\u306f\u3001\u307b\u3068\u3093\u3069\u306eLinux\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u7684\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001Ubuntu\u3067\u306f\u3001util-linux\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u3063\u3066\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>Container\u5185\u306e\u521d\u3081\u3066\u306e\u30d7\u30ed\u30bb\u30b9(PID 1)\u306e\u756a\u53f7\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</li> </ul> <p><code>PID=$(docker inspect --format '{{ .State.Pid }}' $ID)</code></p> <ul> <li>nsenter\u3092\u4f7f\u3063\u3066Container/Pod\u306e\u5168\u7a2e\u985e\u306eNamespace\u3067\u3001\u30ce\u30fc\u30c9\u4e0a\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul> <p><code>nsenter -a -t $PID &lt;command&gt;</code></p> <p>\u4f8b\u3048\u3070\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u6642\u3001\u30ce\u30fc\u30c9\u4e0a\u306b\u3042\u308b\u30b3\u30de\u30f3\u30c9\u3001tcpdump\u3001curl\u3001dig\u3084mtr\u306a\u3069\u304c\u4f7f\u7528\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> <p><code>-a</code> \u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u6700\u8fd1\u306e <code>nsenter</code> \u3067\u5229\u7528\u53ef\u80fd\u3060\u304c\u3001\u3082\u3057\u3053\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u4f7f\u3048\u306a\u304b\u3063\u305f\u3089\u3001\u5358\u72ec\u306eNamespace\u306b\u5165\u308b\u3088\u3046\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3057\u3066\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f <code>nsenter --help</code> \u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"000020069/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020070/","title":"How to enable support for use-forwarded-headers in ingress-nginx","text":"<p>This document (000020070) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020070/#situation","title":"Situation","text":""},{"location":"000020070/#task","title":"Task","text":"<p>Per the [ingress-nginx documentation], the <code>use-forwarded-headers</code> configuration option enables passing \"the incoming X-Forwarded-* headers to upstreams. Use this option when NGINX is behind another L7 proxy / load balancer that is setting these headers.\"</p> <p>This article details how to enable the <code>use-forwarded-headers</code> option in the ingress-nginx instance of Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"000020070/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced</li> <li>For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"000020070/#resolution","title":"Resolution","text":""},{"location":"000020070/#configuration-for-rke-cli-provisioned-clusters","title":"Configuration for RKE CLI provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>use-forwarded-headers: true</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     options:\n       use-forwarded-headers: true\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Verify the new configuration:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"000020070/#configuration-for-rancher-v2x-provisioned-clusters","title":"Configuration for Rancher v2.x provisioned clusters","text":"<ol> <li>Log in to the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name.</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the <code>use-forwarded-headers</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     options:\n       use-forwarded-headers: true\n</code></pre> <ol> <li> <p>Click \"Save\" at the bottom of the page.</p> </li> <li> <p>Wait for cluster to finish upgrading.</p> </li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"000020070/#further-reading","title":"Further reading","text":"<ul> <li>ingress-nginx ConfigMap configuration documentation</li> </ul>"},{"location":"000020070/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020071/","title":"How to enable container log rotation with k3s or containerd","text":"<p>This document (000020071) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020071/#situation","title":"Situation","text":""},{"location":"000020071/#task","title":"Task","text":"<p>In a Kubernetes cluster running an alternative container runtime, such as containerd, instead of Docker, the kubelet manages container logs. The kubelet default values in relation to log rotation can be found in the upstream kubelet | Kubernetes\u00a0documentation,-%2D%2Dcontainer%2Druntime%20string). These values can be adjusted by adding flags to the kubelet process.</p>"},{"location":"000020071/#pre-requisites","title":"Pre-requisites","text":"<p>These steps have been validated for a k3s cluster using the default containerd runtime, in theory these same flags should work for any Kubernetes cluster which does not use Docker as the container runtime.</p>"},{"location":"000020071/#resolution","title":"Resolution","text":"<p>Two kubelet flags need to be added to configure log rotation, the flags will take effect only at start time.</p> <p>In the case of k3s, passing the needed flags can be done a number of ways, the most common perhaps is with the <code>INSTALL_K3S_EXEC</code> environment variable when installing k3s as a service. These same flags can be added to a previous install command to update the service configuration of an existing install of k3s.</p> <p>Note When updating an existing k3s install, the following command will restart k3s.</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--kubelet-arg \"container-log-max-files=4\" --kubelet-arg \"container-log-max-size=50Mi\"\" sh -\n</code></pre> <p>However, flags can also be supplied to the k3s binary directly if a service is not being used. A restart of k3s is required, using the updated flags.</p> <pre><code>k3s server --kubelet-arg container-log-max-files=4 --kubelet-arg container-log-max-size=50Mi\n</code></pre> <p>Note please adjust the values to suit your needs, for demonstration purposes the above commands used 4 log files of 50MB, allowing for 200MB of total space to be retained per container.</p>"},{"location":"000020071/#further-reading","title":"Further reading","text":"<p>Please reference the k3s and kubelet documentation pages to find more information on these flags.</p>"},{"location":"000020071/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020072/","title":"How to create an RKE template and template revision using the Rancher2 Terraform Provider","text":"<p>This document (000020072) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020072/#situation","title":"Situation","text":""},{"location":"000020072/#task","title":"Task","text":"<p>This article details how to create an RKE cluster template revision using the Rancher2 Terraform provider.</p>"},{"location":"000020072/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, from v2.3.0 and above</li> <li>Terraform and the Rancher2 Terraform Provider, authenticated with a Rancher user who has permission to create RKE Templates and RKE Template Revisions</li> </ul>"},{"location":"000020072/#resolution","title":"Resolution","text":"<p>RKE cluster templates can be created using the Rancher2 Terraform Provider per the documentation on the <code>rancher2_cluster_template</code> resource.</p> <p>An example of this resource can be found below:</p> <pre><code>resource \"rancher2_cluster_template\" \"foo\" {\nname = \"foo\"\nmembers {\n    access_type = \"owner\"\n    user_principal_id = \"local://user-XXXXX\"\n}\ntemplate_revisions {\n    name = \"V1\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"6h\"\n            retention = \"24h\"\n          }\n        }\n      }\n    }\n    default = true\n}\ndescription = \"Terraform cluster template foo\"\n}\n</code></pre> <p>Having configured the Rancher2 Terraform Provider and added the above example resource, adjusting as desired and replacing <code>local://user-XXXXX</code> with a valid user prinical ID, run <code>terraform apply</code> to create the RKE template.</p> <p>N.B. the <code>default = true</code> flag, which will specify this <code>V1</code> revision as the the default revision.</p> <p>To add additional revisions, each one will be nested as a new <code>template_revisions</code> block for that resource. Here is an example <code>V2</code> revision:</p> <pre><code>template_revisions {\nname = \"V2\"\ncluster_config {\n    rke_config {\n      network {\n        plugin = \"canal\"\n      }\n      services {\n        etcd {\n          creation = \"3h\"\n          retention = \"12h\"\n        }\n      }\n    }\n}\n}\n</code></pre> <p>So, the full resource block would now look like this:</p> <pre><code>resource \"rancher2_cluster_template\" \"foo\" {\nname = \"foo\"\nmembers {\n    access_type = \"owner\"\n    user_principal_id = \"local://user-XXXXX\"\n}\ntemplate_revisions {\n    name = \"V1\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"6h\"\n            retention = \"24h\"\n          }\n        }\n      }\n    }\n    default = true\n\n}\ntemplate_revisions {\n    name = \"V2\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"3h\"\n            retention = \"12h\"\n          }\n        }\n      }\n    }\n}\ndescription = \"Terraform cluster template foo\"\n}\n</code></pre> <p>Run <code>terraform apply</code> and observe this second <code>V2</code> revision created for the RKE template.</p> <p>N.B. the <code>default</code> revision is still set to <code>V1</code>; this can be changed as needed.</p>"},{"location":"000020072/#further-reading","title":"Further reading","text":"<ul> <li>Rancher RKE Template documentation.</li> <li>Rancher2 Terraform Provider <code>rancher2_cluster_template</code> resource documentation.</li> </ul>"},{"location":"000020072/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020073/","title":"How to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed cluster","text":"<p>This document (000020073) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020073/#situation","title":"Situation","text":""},{"location":"000020073/#task","title":"Task","text":"<p>This article details how to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed Kubernetes cluster.</p>"},{"location":"000020073/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster</li> </ul>"},{"location":"000020073/#resolution","title":"Resolution","text":"<p>In Rancher v2.x you can create a custom Project Role that provides the permissions to enable a user to view Pods, Pod logs and to exec into Pods. You can then grant this role to users on Projects to provide them this access where necessary.</p> <p></p> <ol> <li> <p>Navigate to Security -&gt; Roles from the Global namespace.</p> </li> <li> <p>From the Projects tab, select Add Project Role.</p> </li> <li> <p>Provide a name for the role.</p> </li> <li> <p>Under Grant Resources, select Add Resource and fill in the information for each of the following: Permission(s)ResourceCreatepods/execGet, ListpodsGet, Listpods/log</p> </li> <li>Select Create at the bottom.</li> </ol>"},{"location":"000020073/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Docs: Project Administration</li> <li>Rancher Docs: Cluster and Project Roles - Defining Custom Roles</li> </ul>"},{"location":"000020073/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020074/","title":"How to troubleshoot HTTP 400 response codes from ingress-nginx ingresses","text":"<p>This document (000020074) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020074/#situation","title":"Situation","text":""},{"location":"000020074/#task","title":"Task","text":"<p>This article details how to troubleshoot failing requests, with a HTTP 400 response code, when using an ingress to access a service in a Kubernetes cluster.</p> <p>This error message is typically returned due to a bad request, or as a result of an issue with the request headers or cookies.</p> <p>This article is not intended to be exhaustive as there are a wide variety of causes, however some possible issues are covered.</p>"},{"location":"000020074/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"000020074/#resolution","title":"Resolution","text":""},{"location":"000020074/#large-request-headers-exceeding-header-buffer","title":"Large request headers exceeding header buffer","text":"<p>Nginx has a client header buffer configuration of 4x 8KB by default. Per the Nginx documentation:</p> <ul> <li>\"A request line cannot exceed the size of one buffer, or the 414 (Request-URI Too Large) error is returned to the client.\"</li> <li>\"A request header field cannot exceed the size of one buffer as well, or the 400 (Bad Request) error is returned to the client.\"</li> </ul> <p>The buffer sizes can be extended by defining the <code>large-client-header-buffers</code> option in the <code>nginx-configuration</code> ConfigMap (see below).</p>"},{"location":"000020074/#large-uri-is-duplicated-into-a-new-header-passed-to-the-backend","title":"Large URI is duplicated into a new header passed to the backend","text":"<p>This issue is commonly seen where the app itself responds when queried directly with a large path, but returns a 400 error when queried through an ingress.</p> <p>By default, when ingress-nginx receives a request, it adds the original request's URI to the <code>X-Original-Uri</code> header that it passes on to the backend. This can result in the app being unable to handle the large sized headers, in addition to the long path.</p> <p>This behaviour can be disabled by setting the <code>proxy-add-original-uri-header</code> option to false in your <code>nginx-configuration</code> ConfigMap (see below).</p>"},{"location":"000020074/#adding-options-to-the-ingress-nginx-nginx-configuration-configmap","title":"Adding options to the ingress-nginx nginx-configuration ConfigMap","text":"<p>This configuration map is populated by RKE from configuration defined in the cluster config:</p>"},{"location":"000020074/#if-the-cluster-is-provisioned-by-rancher-v2x","title":"If the cluster is provisioned by Rancher v2.x:","text":"<ol> <li>Edit the cluster (navigate to the cluster within Rancher, select the triple-dot button and then \"Edit\")</li> <li>Select \"Edit as YAML\" to open the cluster configuration as YAML, instead of a form.</li> <li>Add the desired configuration within the ingress block in the following format:</li> </ol> <pre><code>     ingress:\n       provider: nginx\n       options:\n         name: value\n</code></pre> <ol> <li>Save the cluster configuration changes. RKE will go through and apply the config defined during its update process.</li> </ol>"},{"location":"000020074/#if-the-cluster-is-provisioned-by-the-rke-cli","title":"If the cluster is provisioned by the RKE CLI:","text":"<p>The process is largely the same as the Rancher process above, but the configuration is defined in the cluster.yml for this cluster:</p> <ol> <li>Open the cluster configuration yaml with your editor and add the ingress.options block:</li> </ol> <p><code>yaml     ingress:        provider: nginx        options:          name: value</code></p> <p></p> <ol> <li>Apply this config with <code>rke up --config &lt;cluster configuration yaml&gt;</code></li> </ol> <p>Ensure you have an up-to-date cluster.rkestate within the same directory before running <code>rke up</code></p>"},{"location":"000020074/#further-reading","title":"Further reading","text":"<ul> <li>ingress-nginx ConfigMap documentation</li> <li>RKE documentation on ingress-nginx configuration</li> </ul>"},{"location":"000020074/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020075/","title":"How to increase the log level for Canal components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020075) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020075/#situation","title":"Situation","text":""},{"location":"000020075/#task","title":"Task","text":"<p>During network troubleshooting it may be useful to increase the log level of the Canal components. This article details how to set verbose debug-level Canal component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"000020075/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Canal Network Provider</li> </ul>"},{"location":"000020075/#resolution","title":"Resolution","text":"<p>N.B. As these instructions involve editing the Canal DaemonSet directly, the change will not persist cluster update events, i.e. invocations of <code>rke up</code> for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting.</p>"},{"location":"000020075/#via-the-rancher-ui","title":"Via the Rancher UI","text":"<p>For a Rancher v2.x managed cluster, the Canal component log level can be adjusted via the Rancher UI, per the following process:</p> <ol> <li>Navigate to the <code>System</code> project of the relevant cluster within the Rancher UI.</li> <li>Locate the canal DaemonSet workload within the <code>kube-system</code> namespace, click the vertical elipses ( <code>\u22ee</code>) and select Edit.</li> <li>Click to Edit the <code>calico-node</code> container.</li> <li>Add <code>CALICO_STARTUP_LOGLEVEL = DEBUG</code> in the Environment Variables section and click <code>Save</code>.</li> <li>Click Edit for the canal DaemonSet again.</li> <li>This time click to Edit the <code>kube-flannel</code> container.</li> <li>Click <code>Show advanced options</code>.</li> <li>In the Command section add <code>--v=10</code> to the <code>Entrypoint</code> e.g.: <code>/opt/bin/flanneld --ip-masq --kube-subnet-mgr --v=10</code>, and click <code>Save</code>.</li> </ol>"},{"location":"000020075/#via-kubectl","title":"Via kubectl","text":"<p>With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Canal component log level can be adjusted via kubectl, per the following process:</p> <ol> <li>Run <code>kubectl -n kube-system edit daemonset canal</code>.</li> <li>In the <code>env</code> definition for the <code>calico-node</code> container add an environment variable with the name <code>CALICO_STARTUP_LOGLEVEL</code> and value <code>DEBUG</code>, e.g.:</li> </ol> <pre><code>[...]\n         containers:\n      - env:\n        [...]\n        - name: CALICO_STARTUP_LOGLEVEL\n          value: DEBUG\n[...]\n</code></pre> <ol> <li>In the <code>command</code> definition for the <code>kube-flannel</code> container add <code>--v=10</code> to the command, e.g.:</li> </ol> <pre><code>[...]\n      - commmand:\n        - /opt/bin/flanneld\n        - --ip-masq\n        - --kube-subnet-mgr\n        - --v=10\n[...]\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"000020075/#further-reading","title":"Further reading","text":"<ul> <li>Calico configuration documentation</li> <li>Flannel troubleshooting documentation</li> </ul>"},{"location":"000020075/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020076/","title":"How to enable legacy TLS versions for ingress-nginx in Rancher Kubernetes Engine (RKE) CLI and Rancher v2.x provisioned Kubernetes clusters","text":"<p>This document (000020076) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020076/#situation","title":"Situation","text":""},{"location":"000020076/#task","title":"Task","text":"<p>This article details how to enable TLS 1.1 on the ingress-nginx controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"000020076/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced</li> <li>For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"000020076/#resolution","title":"Resolution","text":""},{"location":"000020076/#configuration-for-rke-provisioned-clusters","title":"Configuration for RKE provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>ssl-protocols</code> option for the ingress, as follows:</li> </ol> <pre><code>     ingress:\n       provider: nginx\n       options:\n         ssl-protocols: \"TLSv1.1 TLSv1.2\"\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Verify the new configuration:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"000020076/#configuration-for-rancher-provisioned-clusters","title":"Configuration for Rancher provisioned clusters","text":"<ol> <li>Login into the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the <code>ssl-protocols</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     options:\n       ssl-protocols: \"TLSv1.1 TLSv1.2\"\n</code></pre> <ol> <li> <p>Click \"Save\" at the bottom of the page.</p> </li> <li> <p>Wait for cluster to finish upgrading.</p> </li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"000020076/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020077/","title":"How to disable Grafana usage analytics reporting for cluster and project monitoring in Rancher v2.2.x - v2.4.x","text":"<p>This document (000020077) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020077/#situation","title":"Situation","text":""},{"location":"000020077/#task","title":"Task","text":"<p>By default, Grafana will report usage analytics to the endpoint at stats.grafana.org. In an air-gapped environment this can result in many connection timeout or proxy certificate errors in the Grafana pod logs of cluster and project monitoring, as in the following example:</p> <pre><code>lvl=eror msg=\"Failed to send usage stats\" logger=metrics err=\"Post https://stats.grafana.org/grafana-usage-report: x509: certificate signed by unknown authority\"\n</code></pre> <p>This article outlines how to disable this usage analytics reporting in the Grafana instance of cluster and project monitoring for Rancher v2.2.x - v2.4.x.</p>"},{"location":"000020077/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.2.x - v2.4.x</li> <li>Cluster or project monitoring enabled</li> </ul>"},{"location":"000020077/#resolution","title":"Resolution","text":"<ol> <li>Navigate to either the main cluster page or a project for which you have configured monitoring.</li> <li>Click \"Tools\" in the top menu bar and then \"Monitoring\".</li> <li>Click \"Show advanced options\" at the bottom of the page to reveal the \"Answers\" fields and add the following two answers:</li> </ol> <pre><code>grafana.extraVars[0].name=GF_ANALYTICS_REPORTING_ENABLED\ngrafana.extraVars[0].value='false'\n</code></pre> <ol> <li> <p>Click \"Save\".</p> </li> <li> <p>You can verify this by checking the logs for the Grafana Pod, which should show the following near the top of the logs at container startup:</p> </li> </ol> <pre><code>lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_ANALYTICS_REPORTING_ENABLED='false'\"\n</code></pre>"},{"location":"000020077/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Cluster Monitoring documentation</li> <li>Grafana analytics configuration documentation</li> </ul>"},{"location":"000020077/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020078/","title":"How to confirm a version upgrade of Rancher v2.x is completed successfully","text":"<p>This document (000020078) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020078/#situation","title":"Situation","text":""},{"location":"000020078/#task","title":"Task","text":"<p>This article details how to confirm that a Rancher version upgrade has successfully completed.</p>"},{"location":"000020078/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, either a single Docker container or a Highly Available (HA) installation in Kubernetes</li> <li>A Rancher version upgrade performed per the Rancher upgrade documentation</li> </ul>"},{"location":"000020078/#resolution","title":"Resolution","text":"<p>The following can be verified to confirm that the Rancher component containers have all been successfully upgrade to the newer version:</p> <ul> <li>Within the Rancher UI, confirm the version in the bottom-left corner displays the newer version.</li> <li>For a HA installation, confirm the rancher Deployment Pods within the cattle-system namespace of the Rancher cluster have all been updated to the newer version.</li> <li>Confirm that the Rancher agent workloads (the cattle-node-agent DaemonSet and cattle-cluster-agent Deployment in the cattle-system namespace) in all of the Rancher managed clusters have been updated to the newer version.</li> </ul>"},{"location":"000020078/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020079/","title":"How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile","text":"<p>This document (000020079) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020079/#situation","title":"Situation","text":""},{"location":"000020079/#question","title":"Question","text":"<p>How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile</p>"},{"location":"000020079/#answer","title":"Answer","text":"<p>About the parameters;</p> <p>worker_processes</p> <p>This parameter determines the number of Nginx worker processes to spawn during startup.</p> <p>worker_rlimit_nofile</p> <p>This parameter controls the open file limit per worker process.</p> <p>More details can be found on Nginx documentation</p> <p>Both <code>worker_processes</code> and <code>worker_rlimit_nofile</code> are calculated dynamically by Nginx Ingress during startup.</p> <p>Based on the source code of Ingress Nginx;</p> <pre><code>worker_processes = Number of CPUs ($ grep -c processor /proc/cpuinfo)\nworker_rlimit_nofile = ( RLIMIT_NOFILE / worker_processes ) - 1024\n</code></pre> <p>where RLIMIT_NOFILE is the maximum allowed open files by the process ( <code>ulimit -n</code> )</p> <p>From Nginx Ingress shell, you can verify the same.</p> <pre><code># kubectl exec -it  -n ingress-nginx nginx-ingress-controller-8ln2b -- bash\nbash-5.0$ ulimit -n\n1048576\nbash-5.0$\nbash-5.0$ grep -c processor /proc/cpuinfo\n2        &lt;&lt;---- worker_processes\nbash-5.0$\nbash-5.0$ echo $(((1048576/2)-1024))\n523264    &lt;&lt;--- worker_rlimit_nofile\nbash-5.0$\nbash-5.0$ egrep \"worker_processes|worker_rlimit_nofile\" /etc/nginx/nginx.conf\nworker_processes 2;\nworker_rlimit_nofile 523264;\nbash-5.0$\n</code></pre>"},{"location":"000020079/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020080/","title":"How can I tell whether my app is installed with Helm v2 or Helm v3?","text":"<p>This document (000020080) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020080/#situation","title":"Situation","text":""},{"location":"000020080/#question","title":"Question","text":"<p>How can I tell whether my app was installed with Helm v2 or Helm v3?</p>"},{"location":"000020080/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>kubectl access to the cluster the app is deployed in</li> </ul>"},{"location":"000020080/#answer","title":"Answer","text":"<p>The easiest way is to check what version of Helm was used to deploy resources is to look at the <code>heritage</code> label. For example, to check whether Rancher was installed via Helm v2 or v3, run:</p> <pre><code>kubectl get deployment -n cattle-system rancher -o yaml | grep heritage\n</code></pre> <p>The heritage version defines what version of helm was used to install this chart.</p> <p><code>heritage: Tiller</code> - This is a Helm v2 resource</p> <p><code>heritage: Helm</code> - This is a Helm v3 resource</p>"},{"location":"000020080/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020081/","title":"Troubleshooting - Nodes wont join cluster or show unavailable","text":"<p>This document (000020081) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020081/#situation","title":"Situation","text":""},{"location":"000020081/#issue-nodes-are-not-added-to-rancher-or-are-not-provisioned-correctly","title":"Issue - Nodes are not added to Rancher or are not provisioned correctly","text":"<p>The following article should help empower Rancher administrators diagnose and troubleshoot when a node is not added to Rancher or when a node is not provisioned correctly. We'll outline the process nodes undergo when they are added to a cluster.</p>"},{"location":"000020081/#scope","title":"Scope","text":"<p>We'll kick off by scoping what cluster types this document might pertain to. We're speaking specifically about custom clusters and clusters launched with a node driver. Mention of node driver will be synonymous with 'With RKE and new nodes in an infrastructure provider' in the Rancher UI.</p>"},{"location":"000020081/#tracing-the-steps-during-the-bootstrapping-of-a-node","title":"Tracing the steps during the bootstrapping of a node.","text":"<p>Whether you're selecting custom clusters or clusters launched with a node driver, the way to add nodes to the cluster is by executing a docker run command generated for the created cluster. In case of a custom cluster, the command will be generated and displayed on the final step of cluster creation. In case of a cluster launched with a node driver, the command is generated and executed as final command after creating the node and installing Docker.</p> <p>Note: not all roles may be present in the generated command, depending on what role(s) is/are selected.</p> <pre><code>sudo docker run -d \\\n --privileged \\\n --restart=unless-stopped \\\n --net=host \\\n -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:&lt;version&gt; --server https://&lt;server_url&gt; \\\n --token &lt;token&gt; \\\n --ca-checksum &lt;checksum_value&gt; \\\n --etcd \\\n --controlplane \\\n --worker\n</code></pre> <p>What happens next: 1. The <code>docker run</code> command launches a bootstrap agent container. It will be identified with a randomly generated name - The entrypoint is a shell script which parses the flags and runs some validation tests on said flags and their provided values - A token is then used to authenticate against your Rancher server in order to interact with it. - The agent retrieves the CA certificate from the Rancher server and places it in /etc/kubernetes/ssl/certs/serverca, then the checksum is used to validate if the CA certificate retrieved from Rancher matches. This only applies when a self signed certificate is in use. - Runs an agent binary and connects to Rancher using a WebSocket connection - Agent then checks in with the Rancher server to see if the node is unique, and gets a node plan - Agent executes the node plan provided by the Rancher server - Docker run command will create the path <code>/etc/kubernetes</code> if it doesn't exist - Rancher will run cluster provisioning/reconcile based on the desired role for the node being added (etcd and control plane nodes only). This process will copy certificates down from the server via the built in rke cluster provisioning. - On worker nodes, the process is slightly different. The agent requests a node plan from the Rancher server. The Rancher server generates the node config then sends it back down to the agent. The agent then executes the plan contained in the node config. This involves; certificate generation for the Kubernetes components, and the container create commands to create the following services; kubelet, kube-proxy, and nginx-proxy. - The Rancher agent uses the node plan to write out a cloud-config to configure cloud provider settings.</p> <ol> <li> <p>If provisioning of the node succeeds, the node will be registering to the Kubernetes cluster and cattle-node-agent DaemonSet pods will be scheduled to the node, and the pod will remove and replace the agent container that was created via the Docker run command</p> </li> <li> <p>The <code>share-mnt</code> binary (aka bootstrap phase 2) - The share-mnt container runs the share-root.sh which creates filesystem resources that other container end up using. Certificate folders, configuration files, etc... - The container spings up another container that runs a share mount binary. This container makes sure /var/lib/kubelet or /var/lib/rancher have the right share permissions for systems like boot2docker.</p> </li> </ol> <p>Note: All Kubernetes control plane components talk directly with the Kubernetes API server that's housed on the same node. This proxy is configured to front all k8s API servers within the cluster. It's nginx.conf should reflect that.</p> <ol> <li>If all goes well, the share-mnt bootstrap and share-root container exit and the share-root container gets removed. The kubelet starts, registers with Kubernetes, and cattle-node-agent <code>DaemonSet</code> schedules a pod. The pod should then take over the websocket connection to the rancher server. This should end our provisioning journey and hopefully lead to a functional, happy cluster.</li> </ol>"},{"location":"000020081/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020082/","title":"How to deploy Nginx instead of Traefik as your ingress controller on K3s","text":"<p>This document (000020082) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020082/#situation","title":"Situation","text":""},{"location":"000020082/#task","title":"Task","text":"<p>This knowledge base article will provide the directions for deploying NGINX instead of Traefik as your Kubernetes ingress controller on K3s. Please note that Traefik is the support ingress controller for K3s and NGINX is not officially supported by SUSE Rancher support.</p>"},{"location":"000020082/#requirements","title":"Requirements","text":"<ul> <li>K3s 1.17+ (may apply to other versions)</li> </ul>"},{"location":"000020082/#background","title":"Background","text":"<p>By default, K3s uses Traefik as the ingress controller for your cluster. The decision to use Traefik over NGINX was based on multi-architecture support across x86 and ARM based platforms. Normally Traefik meets the needs of most Kubernetes clusters. However, there are unique use cases where NGINX may be required or preferred. If you don't think you need NGINX, it's recommended to stick with Traefik.</p>"},{"location":"000020082/#solution","title":"Solution","text":"<p>The first step to using NGINX or any alternative ingress controller is to tell K3s that you do not want to deploy Traefik. When installing K3s add the following <code>--no-deploy traefik</code> flag to the <code>INSTALL_K3S_EXEC</code> environment variable:</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--no-deploy traefik\" sh -s -\n</code></pre> <p>If you have already downloaded the k3s.sh install script, you can run the following:</p> <pre><code>INSTALL_K3S_EXEC=\"--no-deploy traefik\" k3s.sh\n</code></pre> <p>This will install the K3s server and form a single node cluster. You can confirm the cluster is operational (\"Ready\") by running:</p> <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES    AGE    VERSION\nip-10-0-0-100       Ready    master   1m     v1.18.4+k3s1\n</code></pre> <p>Note, if you already had the kubectl binary installed on your host and it is not configured correctly, you may need to run <code>k3s kubectl</code> instead of <code>kubectl</code>.</p> <p>Next, confirm your out-of-box pods are running and Traefik is not running:</p> <pre><code>$ kubectl get pods -A\nNAMESPACE                   NAME                                                      READY   STATUS      RESTARTS   AGE\nkube-system                 local-path-provisioner-58fb86bdfd-vt57d                   1/1     Running     0          1m\nkube-system                 metrics-server-6d684c7b5-qmlcn                            1/1     Running     0          1m\nkube-system                 coredns-d798c9dd-72qrq                                    1/1     Running     0          1m\n</code></pre> <p>K3s has a nice feature that allows you to deploy Helm Charts by placing a <code>HelmChart</code> YAML in <code>/var/lib/rancher/k3s/server/manifests</code>. Create this file by running:</p> <pre><code>cat &gt;/var/lib/rancher/k3s/server/manifests/ingress-nginx.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\nname: ingress-nginx\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\nname: ingress-nginx\nnamespace: kube-system\nspec:\nchart: ingress-nginx\nrepo: https://kubernetes.github.io/ingress-nginx\ntargetNamespace: ingress-nginx\nversion: v3.29.0\nset:\nvaluesContent: |-\n    fullnameOverride: ingress-nginx\n    controller:\n      kind: DaemonSet\n      hostNetwork: true\n\u00a0\u00a0\u00a0\u00a0\u00a0 hostPort:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 enabled: true\n      service:\n        enabled: false\n      publishService:\n        enabled: false\n      metrics:\n        enabled: true\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 serviceMonitor:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 enabled: true\n      config:\n        use-forwarded-headers: \"true\"\nEOF\n</code></pre> <p>K3s periodically polls the manifests folder and applies the YAML in these files. After about a minute, you should see new pods running, including the NGINX Ingress Controller and default backend:</p> <pre><code>$ kubectl get pods -A\nNAMESPACE                   NAME                                                      READY   STATUS      RESTARTS   AGE\nkube-system                 local-path-provisioner-58fb86bdfd-vt57d                   1/1     Running     0          2m\nkube-system                 metrics-server-6d684c7b5-qmlcn                            1/1     Running     0          2m\nkube-system                 coredns-d798c9dd-72qrq                                    1/1     Running     0          2m\nkube-system                 helm-install-ingress-nginx-s99ct                          0/1     Completed   0          1m\ningress-nginx               ingress-nginx-default-backend-7fb8995f4d-h6rkb            1/1     Running     0          1m\ningress-nginx               ingress-nginx-controller-c8mkg                            1/1     Running     0          1m\n</code></pre> <p>You'll also see a <code>helm-install-ingress-nginx</code> pod in your environment. K3s uses this pod to deploy the Helm Chart and it's normal for it to be in a READY=0/1 and STATUS=Completed state once the Helm Chart has been successfully deployed. In the event your Helm Chart failed to deploy, you can view the logs of this pod to troubleshoot further.</p>"},{"location":"000020082/#reference","title":"Reference","text":"<ul> <li>K3s documentation</li> <li>NGINX Ingress Controller</li> </ul>"},{"location":"000020082/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020083/","title":"How to deploy the AWS EBS CSI driver on K3s","text":"<p>This document (000020083) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020083/#situation","title":"Situation","text":""},{"location":"000020083/#task","title":"Task","text":"<p>This knowledge base article will provide the directions for deploying and testing the AWS EBS CSI driver and storage class on K3s.</p>"},{"location":"000020083/#requirements","title":"Requirements","text":"<ul> <li>K3s 1.18+ (may apply to other versions)</li> <li>Amazon Web Services (AWS) account with privileges to launch EC2 instances and create IAM policies.</li> </ul>"},{"location":"000020083/#background","title":"Background","text":"<p>K3s has all in-tree storage providers removed since Kubernetes is shifting to out of tree providers for Container Storage Interface (CSI) and Cloud Provider Interface (CPI). While in-tree providers are convenient, they add a lot of bloat to Kubernetes and will eventually be removed from upstream Kubernetes, possibly in 2021.</p> <p>This how-to guide will instruct you on installing and configuring the AWS EBS CSI driver and storage class. This will allow you to dynamically provision and attach an EBS volume to your pod without having to manually create a persistent volume (PV) and EBS volume in advance. In the event that your node crashes and your pod is re-launched on another node, your pod will be reattached to the volume assuming that node is running in the same availability zone used by the defunct node.</p>"},{"location":"000020083/#solution","title":"Solution","text":"<p>Assuming you want the CSI and storage class automatically deployed by K3s, copy the following YAML to a file in your manifests folder on one or all of your K3s servers. For example, <code>/var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml</code>:</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\nname: aws-ebs-csi-driver\nnamespace: kube-system\nspec:\nchart: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/v0.5.0/helm-chart.tgz\nversion: v0.5.0\ntargetNamespace: kube-system\nvaluesContent: |-\n    enableVolumeScheduling: true\n    enableVolumeResizing: true\n    enableVolumeSnapshot: true\n    extraVolumeTags:\n      Name: k3s-ebs\n      anothertag: anothervalue\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\nname: ebs-storageclass\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>First, note at the time of this writing, v0.5.0 is the latest version of the driver. If there is a newer version available, you can replace this in the chart and version tags. See the AWS EBS CSI readme for documentation on the versions currently available. Second, you can customize the <code>enableVolumeScheduling</code>, <code>enableVolumeResizing</code>, <code>enableVolumeSnaphost</code>, and <code>extraVolumeTags</code> based on your needs. These parameters and others are documented in the Helm chart.</p> <p>Next, you need to give the driver IAM permissions to manage EBS volumes. This can be done one of two ways. You can either feed your AWS access key and secret key as a Kubernetes secret, or use an AWS instance profile. Since the first option involves passing sensitive keys in clear text and storing them directly in Kubernetes, the second option is usually preferred. I will go over both options. For either option, make sure your access keys or instance profile has the following permissions set in IAM:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AttachVolume\",\n        \"ec2:CreateSnapshot\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVolume\",\n        \"ec2:DeleteSnapshot\",\n        \"ec2:DeleteTags\",\n        \"ec2:DeleteVolume\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeSnapshots\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeVolumes\",\n        \"ec2:DescribeVolumesModifications\",\n        \"ec2:DetachVolume\",\n        \"ec2:ModifyVolume\"\n      ],\n      \"Resource\": \"*\"\n    }\n]\n}\n</code></pre> <p>Reference: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/example-iam-policy.json</p>"},{"location":"000020083/#option-1-kubernetes-secret","title":"Option 1: Kubernetes Secret","text":"<p>You can place your AWS access key and secret key into a Kubernetes secret. Create a YAML file with the following contents and run a kubectl apply. You can also place this inside your <code>/var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml</code> file. Keep in mind this is not a terribly secure option and anyone with access to these files or secrets in the kube-system namespace will be able to obtain your AWS access keys.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: aws-secret\nnamespace: kube-system\nstringData:\nkey_id: \"AKI**********\"\naccess_key: \"**********\"\n</code></pre>"},{"location":"000020083/#option-2-instance-profile","title":"Option 2: Instance Profile","text":"<p>This option to more secure and should not expose your keys in clear text or in a Kubernetes secret object. You'll need to make sure when your EC2 instances are launched, you've attached an instance profile that has the permissions defined above in the JSON block.</p>"},{"location":"000020083/#verifying-and-testing","title":"Verifying and Testing","text":"<p>You can now check your pods to see if the CSI pods are running. You should see something like this:</p> <pre><code># kubectl get pods -n kube-system | grep ebs\nebs-snapshot-controller-0                1/1     Running   0          15m\nebs-csi-node-k2gh5                       3/3     Running   0          15m\nebs-csi-node-xdcvn                       3/3     Running   0          15m\nebs-csi-controller-6f799b5548-46jqr      6/6     Running   0          15m\nebs-csi-controller-6f799b5548-h4nbb      6/6     Running   0          15m\n</code></pre> <p>Time to test things out. The following command can be run that should provision a 1GB EBS and attach it to your pod:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: myclaim\nspec:\naccessModes:\n    - ReadWriteOnce\nstorageClassName: ebs-storageclass\nresources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: storage-test\nspec:\ncontainers:\n  - name: \"storage-test\"\n    image: \"ubuntu:latest\"\n    command: [\"/bin/sleep\"]\n    args: [\"infinity\"]\n    volumeMounts:\n      - name: myebs\n        mountPath: /mnt/test\nvolumes:\n  - name: myebs\n    persistentVolumeClaim:\n      claimName: myclaim\nEOF\n</code></pre> <p>In your AWS console, you should see a new EBS volume has been created. After about a minute, you should be able to exec into your pod and see the volume mounted in your pod:</p> <pre><code># kubectl exec storage-test -- df -h\nFilesystem      Size  Used Avail Use% Mounted on\noverlay          31G  6.2G   25G  20% /\ntmpfs            64M     0   64M   0% /dev\ntmpfs           3.8G     0  3.8G   0% /sys/fs/cgroup\n/dev/nvme2n1    976M  2.6M  958M   1% /mnt/test\n/dev/root        31G  6.2G   25G  20% /etc/hosts\nshm              64M     0   64M   0% /dev/shm\ntmpfs           3.8G   12K  3.8G   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs           3.8G     0  3.8G   0% /proc/acpi\ntmpfs           3.8G     0  3.8G   0% /proc/scsi\ntmpfs           3.8G     0  3.8G   0% /sys/firmware\n</code></pre>"},{"location":"000020083/#cleaning-up","title":"Cleaning Up","text":"<p>Remove the test pod by running the following:</p> <pre><code>kubectl delete pod storage-test\n</code></pre> <p>Remove the PVC by running:</p> <pre><code>kubectl delete pvc myclaim\n</code></pre> <p>Check the AWS console and you should see your EBS volume has been removed automatically by the AWS EBS CSI driver.</p>"},{"location":"000020083/#reference","title":"Reference","text":"<ul> <li>K3s documentation</li> <li>AWS EBS CSI documentation</li> </ul>"},{"location":"000020083/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020084/","title":"How to perform a rolling change to nodes","text":"<p>This document (000020084) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020084/#situation","title":"Situation","text":""},{"location":"000020084/#task","title":"Task","text":"<p>In a Kubernetes cluster nodes can be treated as ephemeral building blocks providing the resources necessary for all workloads. Managing nodes in an immutable way is particularly common in a cloud environment.</p> <p>In an on premise environment however, nodes can be recycled and updated, in general it's typical that nodes have a longer lifecycle.</p> <p>There may be significant changes to nodes over time, for example: IP addresses, storage/filesystems migration to other hypervisors, data centers, large OS updates, or even migration between clusters.</p> <p>To perform large changes like this, this article aims to provide example steps to apply large changes like this safely in a rolling fashion.</p>"},{"location":"000020084/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A custom or imported cluster managed by Rancher, or an RKE/k3s cluster</li> <li>Access to the nodes in the cluster with sudo/root</li> <li>Permission to perform drain and delete actions on the nodes</li> </ul> <p>If there are any single replica workloads, whenever possible it is ideal to ensure at least 2 replicas are configured for availablity during rolling changes. These are best scheduled on separate nodes, a preferred anti-affinity can help with this.</p>"},{"location":"000020084/#steps","title":"Steps","text":"<p>While performing a rolling change to nodes you will need to determine a batch size, effectively how many nodes you wish to take out of service at a time. Initially, it is recommended to perform the change on one node as a canary first, and testing the change has the desired outcome before doing more at once.</p> <ol> <li> <p>If you wish to maintain the number of nodes in the cluster while performing the rolling change, at this point you may wish to add new nodes, this ensures that when nodes are out of service the cluster maintains at least the original number of available nodes.</p> </li> <li> <p>Drain the node, this can be done with <code>kubectl drain &lt;node&gt;</code>, or in the Rancher UI.</p> </li> </ol> <p>This is particularly important to avoid disruptions to services, by draining first, service endpoints are updated to remove the pods from services, stopped, started on a new node in the cluster, and added back to the service safely.</p> <p>If there are pods using local storage (commonly <code>emptyDir</code> volumes), and these should be drained, the <code>--delete-local-data=true</code> will be needed, beware: the data will be lost.</p> <ol> <li>Optional Delete the node(s) from the cluster, this can be done with <code>kubectl delete &lt;node&gt;</code>. This is needed for changes that cannot be performed on existing nodes, such as IP address, hostnames, moving nodes to another cluster, and large configuration updates. Any pods and Kubernetes components running on the nodes will be removed.</li> </ol> <p>Note: if this is an <code>etcd</code> node, ensure that the cluster has quorum and at least two remaining <code>etcd</code> nodes to maintain HA before performing this step.</p> <ul> <li>For an imported cluster, there is no automated cleanup so at this point you would remove the node from the cluster configuration<ul> <li>RKE, remove the node from the cluster.yaml file followed by an <code>rke up</code></li> <li>k3s, stop the k3s service and uninstall k3s using the script</li> </ul> </li> <li> <p>Optional If the node has been deleted in step 3, cleaning the node is important\u00a0to ensure all previous history of the cluster, CNI devices, volumes, and containers are removed. This is especially important if the node is to be re-used in another cluster.</p> </li> <li> <p>Perform the changes to the node, this could be automated with configuration management, scripted or manual steps.</p> </li> <li> <p>Once step 5 is complete, add the node back to the desired cluster.</p> </li> <li>In a custom cluster this can be done with the <code>docker run</code> command supplied in the Rancher UI</li> <li>For an imported cluster the steps are different<ul> <li>RKE, you would add this node to the cluster by configuring it in the cluster.yaml file, followed with an <code>rke up</code></li> <li>k3s, re-install k3s using the correct flags/variables</li> </ul> </li> <li> <p>Test the nodes with running workloads, and monitor before proceeding with the next node, or a larger batch size of nodes.</p> </li> <li> <p>If additional nodes were added in step 1, these can be removed from the cluster at this point by following steps 2, 3, and 4.</p> </li> </ul>"},{"location":"000020084/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020085/","title":"How is my SUSE Rancher Hosted environment monitored?","text":"<p>This document (000020085) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020085/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is monitored by multiple systems which will trigger an email/Slack/SMS notification to a SUSE Rancher Hosted DevOps on-call engineer in the event there's a problem with your environment. Prometheus and Grafana are used to monitor the health of the VMs running SUSE Rancher Hosted and look at CPU, load, memory, and disk metrics. CloudWatch is used to monitor response times and database health inside the cloud infrastructure. Pingdom and Datadog are used to monitor uptime and availability from multiple geographies. If at any time you notice a performance or availability problem with SUSE Rancher Hosted, please open a support case on our support portal.</p>"},{"location":"000020085/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020086/","title":"How often is maintenance performed on SUSE Rancher Hosted?","text":"<p>This document (000020086) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020086/#resolution","title":"Resolution","text":"<p>Paid customers with an SLA are given the choice of a one hour weekly maintenance window, so maintenance is done at most on a weekly basis with the exception of emergency maintenance to address an outage or high severity issue. Maintenance typically involves upgrading the underlying Kubernetes cluster or operating system patches and updates. Most maintenance can be done with little or no interruption to service.</p>"},{"location":"000020086/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020087/","title":"Is it possible to have alpha, beta, or release candidate (RC) available on SUSE Rancher Hosted?","text":"<p>This document (000020087) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020087/#situation","title":"Situation","text":""},{"location":"000020087/#resolution","title":"Resolution","text":"<p>While it may be technically possible to run an alpha, beta, or release candidate version of Rancher on SUSE Rancher Hosted, we don't typically offer it so that we can deliver our 99.9% uptime SLA. If you want to test a version of Rancher that is not GA, it's recommended that you use your own on-premise or cloud infrastructure.</p>"},{"location":"000020087/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020088/","title":"Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted environment?","text":"<p>This document (000020088) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020088/#resolution","title":"Resolution","text":"<p>Yes, if you find yourself locked out of the admin account on Hosted Rancher, the Rancher operations team can reset it for you using the method defined in our documentation. To initiate this request, please file a support case through the Rancher Support Portal.</p>"},{"location":"000020088/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020089/","title":"Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters?","text":"<p>This document (000020089) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020089/#resolution","title":"Resolution","text":"<p>Customers are responsible for upgrading all downstream clusters that SUSE Rancher Hosted manages. RKE clusters can be easily upgraded and rolled back by using the UI or API. See Rancher docs for more details. K3s, RKE2, EKS, AKS, and GKE clusters can also be easily upgraded in the UI.</p>"},{"location":"000020089/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020090/","title":"Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers?","text":"<p>This document (000020090) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020090/#resolution","title":"Resolution","text":"<p>Yes, there are a variety of ways you can accomplish this. You can provision your VMs or bare metal servers ahead of time and use the Custom Cluster option when creating your Kubernetes cluster. A shell command will be provided which you can run on each server to join the cluster. Your on-premise servers will only require outbound (egress) access to SUSE Rancher Hosted.</p> <p>If you want to use the vSphere node driver to have SUSE Rancher Hosted provision your infrastructure, SUSE Rancher Hosted will need inbound (ingress) access to your on-premise infrastructure. This can be accomplished one of three ways:</p> <ol> <li>Open firewall rules on your corporate network.</li> <li>Establish a VPC peering connection between SUSE Rancher Hosted and your AWS cloud account. This requires that your AWS cloud account is connected to your on-premise infrastructure through Direct Connect or VPN.</li> <li>Establish a VPN connection between SUSE Rancher Hosted and your on-premise network.</li> </ol> <p>More details can be provided on each of these three options. See also SUSE Rancher Hosted Whitepaper.</p>"},{"location":"000020090/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020091/","title":"Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage?","text":"<p>This document (000020091) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020091/#situation","title":"Situation","text":""},{"location":"000020091/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted can manage up to 2,000 downstream clusters and a total of 20,000 nodes across all clusters. Check with your account executive on the node and cluster limits for your support contract.</p>"},{"location":"000020091/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020092/","title":"How often is SUSE Rancher Hosted upgraded?","text":"<p>This document (000020092) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020092/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is upgraded normally within two weeks after a stable release. There are typically one or two Rancher releases a quarter. SUSE will contact you to schedule the upgrade. In the future, we plan to have upgrades self-service by letting customers trigger an upgrade through the UI.</p>"},{"location":"000020092/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020093/","title":"How is SUSE Rancher Hosted different than the open-source Rancher I can download for free?","text":"<p>This document (000020093) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020093/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is built on the same Rancher open-source software that can be downloaded for free. SUSE Rancher Hosted's value proposition is that SUSE installs, upgrades, backs up, monitors, and completely manages the software for you.</p>"},{"location":"000020093/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020094/","title":"Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP based directory service?","text":"<p>This document (000020094) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020094/#resolution","title":"Resolution","text":"<p>Yes, the option to do authentication integration is available in SUSE Rancher Hosted and you can find all the options and directions in our documentation. Integration with external authentication services such as Okta or Azure Active Directory are fairly trivial. For integration with a private or on-premise directory service, you may need to open ports in your firewall or use SUSE Rancher Hosted's network peering or VPN capabilities. SUSE can guide you through this setup if needed.</p>"},{"location":"000020094/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020095/","title":"Does SUSE Rancher Hosted support multi-factor authentication (MFA)?","text":"<p>This document (000020095) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020095/#resolution","title":"Resolution","text":"<p>Local user accounts in SUSE Rancher Hosted authenticate only using a login and password, so multi-factor authentication (MFA) is not supported. However, SUSE Rancher Hosted can be integrated with many authentication providers that do support MFA, such as Microsoft Azure Active Directory. For a full list of authentication providers, see the Rancher 2.x Authentication Documentation.</p>"},{"location":"000020095/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020096/","title":"Do I have access to the \"local\" cluster in the management UI for my SUSE Rancher Hosted environment?","text":"<p>This document (000020096) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020096/#resolution","title":"Resolution","text":"<p>No, your SUSE Rancher Hosted environment will not display the \"local\" cluster in the UI and it is not accessible through the API. The local cluster is the Kubernetes cluster that is running the Rancher server workloads and is fully managed by the SUSE Rancher Hosted DevOps team.</p>"},{"location":"000020096/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020097/","title":"What are the \"-promoted\" Cluster Roles in Rancher?","text":"<p>This document (000020097) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020097/#situation","title":"Situation","text":""},{"location":"000020097/#question","title":"Question","text":"<p>When I query for Cluster Roles via kubectl, I see some entries with \"-promoted\" appended to them. What are these and why is Rancher creating them?</p>"},{"location":"000020097/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher server with RKE clusters added</li> <li>Users added to a Project</li> </ul>"},{"location":"000020097/#answer","title":"Answer","text":"<p>The ClusterRole with \"-promoted\" at the end, is created if the Project role given to a Project member contains any of these resources: storageClass, persistentVolumes, and apiServices.</p> <p>These resources are not scoped to a namespace. They do not belong to any Project but the entire Cluster. That is why Rancher creates an additional ClusterRole.</p>"},{"location":"000020097/#further-reading","title":"Further Reading","text":"<ul> <li>https://rancher.com/docs/rancher/v2.x/en/admin-settings/rbac/</li> </ul>"},{"location":"000020097/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020098/","title":"Can Rancher migrate my helm2 app to helm3?","text":"<p>This document (000020098) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020098/#situation","title":"Situation","text":""},{"location":"000020098/#question","title":"Question","text":"<p>Can I use Rancher to migrate a Rancher app I deployed from a Helm v2 catalog to Helm v3?</p>"},{"location":"000020098/#answer","title":"Answer","text":"<p>No, Rancher currently does not support migrating an app from Helm v2 to Helm v3. To migrate an app from Helm v2 to Helm v3, you would need to delete the app, re-add the catalog as a helm_v3 catalog and re-install the app.</p>"},{"location":"000020098/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020099/","title":"Loading the new Rancher Dashboard in an airgapped environment redirects to /fail-whale","text":"<p>This document (000020099) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020099/#situation","title":"Situation","text":""},{"location":"000020099/#issue","title":"Issue","text":"<p>When attempting to view the new Rancher dashboard in an airgapped environment, or one that requires a proxy to access the internet, the dashboard eventually times out and the user is redirected to https://rancher_server/fail-whale</p> <p></p>"},{"location":"000020099/#root-cause","title":"Root cause","text":"<p>As the dashboard is currently in beta testing, the code for it resides in our CDN instead of being included in our images. The service responsible for pulling this code currently does not support proxy configuration.</p>"},{"location":"000020099/#resolution","title":"Resolution","text":"<p>Until the dashboard goes into a General Release status, there is a requirement for internet connectivity to https://releases.rancher.com and https://github.com from both the Rancher cluster and also downstream controlplane nodes for this functionality to work.</p>"},{"location":"000020099/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020100/","title":"Slow etcd performance (performance testing and optimization)","text":"<p>This document (000020100) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020100/#situation","title":"Situation","text":""},{"location":"000020100/#issue","title":"Issue","text":"<p>If your etcd logs start showing messages like the following, your storage might be too slow for etcd or the server might be doing too much for etcd to operate properly:</p> <pre><code>2019-08-11 23:27:04.344948 W | etcdserver: read-only range request \"key:\\\"/registry/services/specs/default/kubernetes\\\" \" with result \"range_response_count:1 size:293\" took too long (1.530802357s) to execute\n</code></pre> <p>If your storage is really slow you will even see it throwing alerts in your monitoring system. What can you do to the verify the performance of your storage? If the storage is is not performing correctly, how can you fix it? After researching this I found an IBM article that went over this extensively. Their findings on how to test were very helpful. The biggest factor is your storage latency. If it is not well below 10ms in the 99th percentile, you will see warnings in the etcd logs. We can test this with a tool called fio which I will outline below.</p>"},{"location":"000020100/#testing-etcd-performance","title":"Testing etcd performance","text":"<ol> <li>Download and install the latest version of fio. This is important because older versions do not provide storage latency. I have a very simple script below to download and install this.</li> </ol> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/instant-fio-master/instant-fio-master.sh\nbash instant-fio-master.sh\n</code></pre> <ol> <li>Test the storage, create a directory on the device you want to test then run the fio command as shown below.</li> </ol> <pre><code>export PATH=/usr/local/bin:$PATH\nmkdir test-data\nfio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest\n</code></pre> <ol> <li>Below is an example output from an etcd,controlplane,worker node of a Rancher installation cluster running on an AWS ec2 instance type of t2.large.</li> </ol> <pre><code>[root@ip-172-31-14-184 ~]# fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest\nmytest: (g=0): rw=write, bs=(R) 2300B-2300B, (W) 2300B-2300B, (T) 2300B-2300B, ioengine=sync, iodepth=1\nfio-3.15-23-g937e\nStarting 1 process\nmytest: Laying out IO file (1 file / 100MiB)\nJobs: 1 (f=1): [W(1)][100.0%][w=2684KiB/s][w=1195 IOPS][eta 00m:00s]\nmytest: (groupid=0, jobs=1): err= 0: pid=21203: Sun Aug 11 23:47:30 2019\n     write: IOPS=1196, BW=2687KiB/s (2752kB/s)(99.0MiB/38105msec)\n       clat (nsec): min=2840, max=99026, avg=8551.56, stdev=3187.53\n         lat (nsec): min=3337, max=99664, avg=9191.92, stdev=3285.92\n       clat percentiles (nsec):\n         |  1.00th=[ 4640],  5.00th=[ 5536], 10.00th=[ 5728], 20.00th=[ 6176],\n         | 30.00th=[ 6624], 40.00th=[ 7264], 50.00th=[ 7968], 60.00th=[ 8768],\n         | 70.00th=[ 9408], 80.00th=[10304], 90.00th=[11840], 95.00th=[13760],\n         | 99.00th=[19328], 99.50th=[23168], 99.90th=[35584], 99.95th=[44288],\n         | 99.99th=[63744]\n       bw (  KiB/s): min= 2398, max= 2852, per=99.95%, avg=2685.79, stdev=104.84, samples=76\n       iops        : min= 1068, max= 1270, avg=1195.96, stdev=46.66, samples=76\n     lat (usec)   : 4=0.52%, 10=76.28%, 20=22.34%, 50=0.82%, 100=0.04%\n     fsync/fdatasync/sync_file_range:\n       sync (usec): min=352, max=21253, avg=822.36, stdev=652.94\n       sync percentiles (usec):\n         |  1.00th=[  400],  5.00th=[  420], 10.00th=[  437], 20.00th=[  457],\n         | 30.00th=[  478], 40.00th=[  529], 50.00th=[  906], 60.00th=[  947],\n         | 70.00th=[  988], 80.00th=[ 1020], 90.00th=[ 1090], 95.00th=[ 1156],\n         | 99.00th=[ 2245], 99.50th=[ 5932], 99.90th=[ 8717], 99.95th=[11600],\n         | 99.99th=[16581]\n     cpu          : usr=0.79%, sys=7.38%, ctx=119920, majf=0, minf=35\n     IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n         submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         issued rwts: total=0,45590,0,0 short=45590,0,0,0 dropped=0,0,0,0\n         latency   : target=0, window=0, percentile=100.00%, depth=1\nRun status group 0 (all jobs):\n     WRITE: bw=2687KiB/s (2752kB/s), 2687KiB/s-2687KiB/s (2752kB/s-2752kB/s), io=99.0MiB (105MB), run=38105-38105msec\nDisk stats (read/write):\n     xvda: ios=0/96829, merge=0/3, ticks=0/47440, in_queue=47432, util=92.25%\n</code></pre> <p>In the fsync data section you can see that the 99th percentile is 2245 or about 2.2ms of latency. This storage is well suited for an etcd node. The etcd documentation suggests that for storage to be fast enough, the 99th percentile of fdatasync invocations when writing to the WAL file must be less than 10ms.</p>"},{"location":"000020100/#resolution","title":"Resolution","text":"<p>What if your node's storage isn't fast enough? The simple solution is to upgrade the storage but that isn't always an option. If you are on the cusp of acceptable, there are things you can do to optimize your storage so that etcd is happy.</p> <ol> <li> <p>Don't run etcd on a node with other roles. A general rule of thumb is to never have the worker role on the same node as etcd. However many environments have etcd and controlplane roles on the same node and run just fine. If this is the case for your environment then you should consider separating etcd and controlplane nodes.</p> </li> <li> <p>If you've separated etcd and the controlplane node and are still having issues, you can mount a separate volume for etcd so that read write operations for everything else on the node do not impact etcd's performance. This is mostly applicable to Cloud hosted nodes since each volume mounted has its own allocated set of resources.</p> </li> <li> <p>If you are on a dedicated server and would like to separate etcd read write operations from the rest of the server, you should install a new storage device for etcd mounts.</p> </li> <li> <p>Always use SSD's for your etcd nodes, whether it is dedicated or in the cloud.</p> </li> <li> <p>Set the priority of the etcd container so that it is higher than other processes but not too high that it overwhelms the server.</p> </li> </ol> <pre><code>ionice -c2 -n0 -p `pgrep -x etcd`\n</code></pre>"},{"location":"000020100/#further-reading","title":"Further reading","text":"<p>Below is a list of links that I used for my research. I highly recommend reading these as they contain more information than I've posted in this article.</p> <ul> <li>IBM blog post on use of fio to test etcd storage performance</li> <li>etcd performance documentation</li> <li>etcd documentation on node sizing examples</li> <li>etcd metrics documentation</li> <li>etcd tuning documentation</li> <li>AWS blog post on the difference between burst and baseline performance in EC2 storage</li> </ul>"},{"location":"000020100/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020101/","title":"How to create docker goroutine, and memory heap, dumps","text":"<p>This document (000020101) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020101/#situation","title":"Situation","text":""},{"location":"000020101/#task","title":"Task","text":"<p>It's important to observe Docker as it operates to help drive troubleshooting an issue. Here are some commands to generate memory heap and goroutine dumps without killing the Docker process.</p>"},{"location":"000020101/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Docker with an exposed socket (typically found at <code>/var/run/docker.sock</code>)</li> </ul>"},{"location":"000020101/#collecting-dumps","title":"Collecting dumps","text":""},{"location":"000020101/#heap-dump","title":"Heap dump","text":"<p>Heap dumps report a sampling of memory allocations of live objects.</p> <pre><code>curl --unix-socket /var/run/docker.sock http://./debug/pprof/heap?debug=2\n</code></pre>"},{"location":"000020101/#goroutine-dump","title":"Goroutine dump","text":"<p>The goroutine dump reports stack traces of all current goroutines for the docker process.</p> <pre><code>curl --unix-socket /var/run/docker.sock http://./debug/pprof/goroutine?debug=2\n</code></pre> <p>The output normally is output to <code>stdout</code>, where it can be redirected to a file.</p> <p>Depending on how Docker is configured, and where its configured to log to, the traces could end up in the <code>docker.log</code> file or with the system logs (syslog, journalctl, kern.log, messages, etc...).</p>"},{"location":"000020101/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020102/","title":"Are API audit logs enabled in SUSE Rancher Hosted?","text":"<p>This document (000020102) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020102/#resolution","title":"Resolution","text":"<p>Yes, API audit logs are enabled in SUSE Rancher Hosted at level 2. Level 2 includes log event metadata and request body, but does not include response metadata and response body. Rancher APIs typically do not contain personally identifiable information (PII). One exception is the user API which can contain a user's full name. More details on Rancher's API audit logging can be found in the Rancher Documentation . Audit logs are stored in the same region as your SUSE Rancher Hosted environment and retained for 1 month. Only the SUSE Rancher team has access to these logs for troubleshooting purposes. Customers may request logs by filing a support case on SCC and providing a date and time range in UTC.</p>"},{"location":"000020102/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020103/","title":"What information is stored in SUSE Rancher Hosted and where is it stored?","text":"<p>This document (000020103) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020103/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted stores the following information:</p>"},{"location":"000020103/#user-data","title":"User Data","text":"<ul> <li>First and last name of users (aka Display Name)</li> <li>Login id and password. Password is stored using one-way encryption and transported using TLS.</li> <li>Other user information from GitHub, Okta, Microsoft Active Directory, etc. if authentication integration is enabled.</li> </ul>"},{"location":"000020103/#cloud-provider-credentials-if-provided","title":"Cloud Provider Credentials (if provided)","text":"<ul> <li>Amazon Web Services Access Key and Secret Key</li> <li>Microsoft Azure Subscription ID, Client ID, Client Secret</li> <li>DigitalOcean Access Token</li> <li>Linode Access Token</li> <li>VMWare vSphere endpoint, Username, and Password</li> <li>Similar types of keys, tokens, or credentials for other cloud providers that are enabled by the customer.</li> </ul>"},{"location":"000020103/#other-application-data","title":"Other Application Data","text":"<ul> <li>Catalogs and Helm Charts</li> <li>CIS Scan Results</li> <li>Cluster Monitoring Metrics (if turned on)</li> <li>Cluster infrastructure, including node roles, node hardware specs, node software versions, workload metadata, workload logs.</li> <li>Anything else entered by the end-user in the Rancher user interface, API, or CLI which could change from version to version.</li> </ul> <p>Data is stored in our third-party cloud service provider on virtual machines managed by the SUSE Rancher Hosted operations team in the region/country selected by the customer.</p>"},{"location":"000020103/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020104/","title":"Filesystem actions in containers fail with `Too many levels of symbolic links`","text":"<p>This document (000020104) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020104/#situation","title":"Situation","text":""},{"location":"000020104/#issue","title":"Issue","text":"<p>When attempting to perform a filesystem action inside a container with a volume located on an autofs directory, the error <code>Too many levels of symbolic links</code> is thrown and the action fails.</p> <pre><code>bash: cd: /data: Too many levels of symbolic links\n</code></pre>"},{"location":"000020104/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Docker container or Kubernetes Pod with a volume defined that is mounted on the host with autofs, typically backed by NFS</li> </ul>"},{"location":"000020104/#root-cause","title":"Root cause","text":"<p>As the share that backs the autofs volume isn't mounted until the directory specified is accessed, it is typically not mounted when a container is run.</p> <p>With the default Docker bind-mount propagation of <code>rprivate</code>, containers do not receive mount changes for volumes from the host.</p>"},{"location":"000020104/#resolution","title":"Resolution","text":"<p>Docker - Mount the volume in question with the flag <code>slave</code>, <code>rslave</code>, <code>shared</code>, or <code>rshared</code> to ensure that mount changes are propagated to the container.</p> <p>Example:</p> <p><code>docker run -d -v /path/to/autofs:/data:shared ubuntu</code></p> <p>See the links at the bottom of this article for info on what each of these flags does</p> <p>Kubernetes - Define mountPropagation for the volume in question as either <code>HostToContainer</code> (same as Docker's <code>rslave</code>) or <code>Bidirectional</code> (same as Docker's <code>rshared</code>):</p> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\nname: test-app\nspec:\ncontainers:\n    - name: test\n      image: busybox\n      volumeMounts:\n      - mountPath: \"/data\"\n        name: test-app-vol\n        mountPropagation: HostToContainer\nvolumes:\n    - name: test-app-vol\n      hostPath:\n        path: /data\n\n</code></pre>"},{"location":"000020104/#further-reading","title":"Further reading","text":"<p>Docker bind propagation - https://docs.docker.com/storage/bind-mounts/#configure-bind-propagation</p> <p>Kubernetes mountPropagation - https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation</p> <p>Linux Kernel Shared Subtree - https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt</p>"},{"location":"000020104/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020105/","title":"Best Practices Rancher","text":"<p>This document (000020105) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020105/#situation","title":"Situation","text":"<p>This article aims to provide a number of checks that can be evaluated to ensure best practices are in place when planning, building or preparing a Rancher 2.x and Kubernetes environment.</p>"},{"location":"000020105/#1-architecture","title":"1. Architecture","text":""},{"location":"000020105/#11-nodes","title":"1.1 Nodes","text":"<p>Understanding workload resource needs in downstream clusters upfront can help choose an appropriate node configuration; some nodes may need different configurations; however, all nodes of the same role are generally configured the same.</p> <p>Checks</p> <p>Standardize on supported versions and ensure minimum requirements are met:</p> <ul> <li>Confirm the OS is covered in the supported versions</li> <li>Resource needs can vary based on cluster size and workload, however, in general, no less than 8GB of memory and 2 vCPUs is recommended</li> <li>SSD storage is recommended, especially for nodes with the <code>etcd</code> role</li> <li>Firewall rules allow connectivity for nodes ( k3s,\u00a0RKE)</li> <li>A static IP for all nodes is required, if using DHCP, all nodes should have a reserved address</li> <li>Swap is disabled on the nodes</li> <li>NTP is enabled on the nodes</li> </ul>"},{"location":"000020105/#12-separation-of-concerns","title":"1.2 Separation of concerns","text":"<p>The Rancher management cluster should be dedicated to running the Rancher deployment, additional workloads added to the cluster can contend for resources and impact the performance and predictability of Rancher.</p> <p>This is also important to consider in downstream clusters, the etcd\u00a0and control plane\u00a0nodes (RKE), and server nodes (k3s) should be dedicated to the purpose. When possible, it is recommended that each node have a single role, for example, separate nodes for the etcd and control plane roles.</p> <p>Checks</p> <p>Using the following commands on each cluster, check and confirm for any unexpected workloads running on the Rancher management cluster, or running on the server or etcd/control plane nodes of a downstream cluster.</p>"},{"location":"000020105/#rancher-management-cluster","title":"Rancher management cluster","text":"<ul> <li>Check for any unexpected pods running in the cluster: <code>kubectl get pods --all-namespaces</code></li> <li>Check for any single points of failure or discrepancies in OS, kernel and CRI version: <code>kubectl get nodes -o wide</code></li> </ul>"},{"location":"000020105/#downstream-cluster","title":"Downstream cluster","text":"<p>k3sRKE</p> <ul> <li>Check for any unexpected pods running on server nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/master=true --no-headers | cut -d \" \" -f1)\ndo\n    kubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre> <ul> <li>Check for any unexpected pods running on etcd nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/etcd=true --no-headers | cut -d \" \" -f1)\ndo\n    kubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre> <ul> <li>Check for any unexpected pods running on control plane nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/controlplane=true --no-headers | cut -d \" \" -f1)\ndo\n    kubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n\n</code></pre>"},{"location":"000020105/#13-high-availability","title":"1.3 High Availability","text":"<p>Ensure nodes within a cluster are spread across separate failure boundaries as much as possible. This could mean VMs running on separate physical hosts, data centres, switches, storage pools, etc. If running in a cloud environment, instances in separate availability zones.</p> <p>For High Availability in Rancher, a Kubernetes install is required.</p> <p>Checks</p> <ul> <li>When deploying the Rancher management cluster it is recommended to use the following configuration:</li> </ul> <p>DistributionRecommendationk3s2 server nodesRKE3 nodes with all roles</p> <ul> <li>Confirm the components of all clusters and external datastores (k3s) are satisfying minimum HA requirements:</li> </ul> <p>k3sRKE</p> <p>ComponentMinimumRecommendedNotesexternal datastore22 or greaterThe external datastore should provide failover to a standby using the datastore-endpointserver nodes22 or greaterAllow tolerance for at least 1 server node failureagent nodes2N/AAllow tolerance for at least 1 agent node failure, scale up to meet the workload needs</p> <p>ComponentMinimumRecommendedNotesetcd nodes33To maintain quorum it is important to have an uneven # of nodes to provide tolerance for at least 1 node failurecontrol plane nodes22Allow tolerance for at least 1 node failureworker nodes2N/AAllow tolerance for at least 1 worker node failure, scale up to meet the workload needs</p>"},{"location":"000020105/#cloud-provider","title":"Cloud provider","text":"<p>The following commands can also be used with clusters configured with a cloud provider to review the instance type and availability zones of each node.</p> <ul> <li>Kubernetes v1.17 or earlier: <code>kubectl get nodes -L beta.kubernetes.io/instance-type -L failure-domain.beta.kubernetes.io/zone</code></li> <li>Kubernetes v1.17 or greater: <code>kubectl get nodes -L node.kubernetes.io/instance-type -L topology.kubernetes.io/zone</code></li> </ul> <p>These labels may not be available on all cloud providers.</p>"},{"location":"000020105/#14-load-balancer","title":"1.4 Load balancer","text":"<p>To provide a consistent endpoint for the Rancher management cluster, a load balancer is highly recommended to ensure the Rancher agents, UI, and API connectivity can effectively reach the Rancher deployment.</p> <p>Checks</p> <p>The load balancer is configured:</p> <ul> <li>Within close proximity of the Rancher management cluster to reduce latency</li> <li>For high availability, with all Rancher management nodes configured as upstream targets</li> <li>With a health check to the following path:</li> </ul> <p>DistributionHealth check pathk3s<code>/ping</code>RKE<code>/healthz</code></p> <p>A health check interval is generally recommended at 30 seconds or less</p>"},{"location":"000020105/#15-proximity-and-latency","title":"1.5 Proximity and latency","text":"<p>For performance reasons, it is recommended to avoid spreading cluster nodes over long distances and unreliable networks. For example, nodes could be in separate AZs in the same region, the same datacenter, or separate nearby data centres.</p> <p>This is particularly important for etcd nodes\u00a0which are sensitive to network latency, the RTT between etcd nodes in the cluster will determine the minimum time to complete a commit .</p> <p>Checks</p> <ul> <li>Network latency and bandwidth is adequate between locations that the cluster nodes will be provisioned</li> </ul> <p>A tool like <code>mtr</code> to gather connectivity statistics between locations over a long sample period can be useful to report on the packet loss and latency.</p> <p>Generally latency between etcd nodes is recommended at 5s or less</p>"},{"location":"000020105/#16-datastore","title":"1.6 Datastore","text":"<p>It is important to ensure that the chosen datastore is capable of handling requests inline with the workload of the cluster.</p> <p>Allocation of resources, storage performance, and tuning of the datastore may be needed over time, this could be due to an increase in churn in a cluster, downstream clusters growing in size, or the number of downstream clusters Rancher is managing increases.</p> <p>Checks</p> <p>Confirm the recommended options are met for the distribution in use:</p> <p>k3sRKE</p> <p>With an external datastore the general performance requirements include:</p> <ul> <li>SSD or similar storage providing 1,000 IOPs or greater performance</li> <li>Datastore servers are assigned 2 vCPUs and 4GB memory or greater</li> <li>A low latency connection to the datastore endpoint from all k3s server nodes</li> </ul> <p>MySQL 5.7 is recommended . If running in a cloud provider, you may wish to utilise a managed database service .</p> <p>To confirm the storage performance of etcd nodes is capable of handling the workload, a benchmark tool like <code>fio</code> can be used.</p> <ul> <li>Nodes with the <code>etcd</code> role have SSD or similar storage providing high IOPs and low latency</li> </ul> <p>On large downstream or Rancher environments, tuning etcd may be needed, including adding dedicated disk for etcd.</p>"},{"location":"000020105/#17-cidr-selection","title":"1.7 CIDR selection","text":"<p>The cluster and service CIDRs cannot be changed once a cluster is provisioned.</p> <p>For this reason, it is important to future proof by changing the ranges to avoid routing overlaps with other areas of the network and potential cluster IP exhaustion if the defaults are not suitable.</p> <p>Checks</p> <ul> <li>The default CIDR ranges do not overlap with any area of the network</li> </ul> <p>The default CIDRs are below which often don't need to be changed, to ensure the are no issues with routing from or two pods you may wish to adjust these when creating clusters ( RKE, k3s).</p> <p>NetworkDefault CIDRCluster10.42.0.0/16Service10.43.0.0/16</p> <p>Reducing the CIDR sizes can lower the number of IPs available and therefore total number of pods and services in the cluster. In a large cluster, the CIDR ranges may need to be increased .</p>"},{"location":"000020105/#18-authorized-cluster-endpoint","title":"1.8 Authorized cluster endpoint","text":"<p>At times connecting directly to a downstream cluster may be desired, this could be to reduce latency, avoid interruption if Rancher is unavailable, or that a high frequency of external API calls occur, for example, external monitoring, or a CI/CD pipeline.</p> <p>Checks</p> <ul> <li>Check for any use cases where an authorized cluster endpoint is needed</li> </ul> <p>Access directly to the downstream cluster kube-apiserver can be configured using the secondary context in the kubeconfig file.</p>"},{"location":"000020105/#2-best-practices","title":"2. Best Practices","text":""},{"location":"000020105/#21-installing-rancher","title":"2.1 Installing Rancher","text":"<p>It is highly encouraged to install Rancher on a Kubernetes cluster in an HA configuration .</p> <p>If starting with small resource requirements, at the very minimum always install on a Kubernetes cluster with a single node, this provides a future path to adding nodes at a later date.</p> <p>The design of the single node Docker install is for short-lived testing environments, migration from a Docker to a Kubernetes install is not possible.</p> <p>Checks</p> <ul> <li>Rancher is installed on a Kubernetes cluster, even if that is a single node cluster</li> </ul>"},{"location":"000020105/#22-rancher-resources","title":"2.2 Rancher Resources","text":"<p>The minimum resource requirements for nodes in the Rancher management cluster need to scale to match the number of downstream clusters and nodes; this may change over time and need reviewing as changes occur in the environment.</p> <p>Checks</p> <ul> <li>Verify that nodes in the Rancher management cluster meet at least the minimum requirements: ResourceRequirementsCPU/MemoryRancher v2.4.0 and greaterCPU/MemoryRancher v2.4.0 and earlierNetworkPort requirements</li> </ul>"},{"location":"000020105/#23-chart-options","title":"2.3 Chart options","text":"<p>When installing the Rancher helm chart, the default options may not always be the best fit for specific environments.</p> <p>Checks</p> <ul> <li> <p>The Rancher helm chart is installed with the desired options</p> </li> <li> <p><code>replicas</code> - the default number of Rancher replicas ( <code>3</code>) may not suit your cluster, for example, a k3s cluster with 2 x server nodes using a <code>replicas</code> value of <code>2</code> will ensure only one Rancher pod is running per node.</p> </li> <li><code>antiAffinity</code> - the default <code>preferred</code> scheduling can mean Rancher pods become imbalanced during the lifetime of a cluster, using <code>required</code> can ensure Rancher is always scheduled on unique nodes</li> </ul> <p>To confirm the options provided on an existing Rancher install with helm v3, the following command can be used <code>helm get values rancher -n cattle-system</code></p>"},{"location":"000020105/#24-supported-versions","title":"2.4 Supported versions","text":"<p>When choosing or maintaining the components for Rancher and Kubernetes clusters the product lifecycle and support matrix can be used to ensure the versions and OS configurations are certified and maintained.</p> <p>Checks</p> <ul> <li>All Rancher and Kubernetes cluster versions are under maintenance and certified</li> </ul> <p>As versions are a moving target, checking the current stable releases and planning for future upgrades on a schedule is recommended.</p>"},{"location":"000020105/#25-recurring-snapshots-and-backups","title":"2.5 Recurring snapshots and backups","text":"<p>It is important to configure snapshots on a recurring schedule and store these externally to the cluster for disaster recovery.</p> <p>Checks</p> <ul> <li>Recurring snapshots are configured for the distribution in use</li> </ul> <p>DistributionConfigurationk3sConfigure snapshots and backups on the external datastore, this can differ depending on the chosen databaseRKEConfigure recurring snapshots of etcd, with an S3 compatible endpoint for off-node copies</p> <p>In addition to a recurring schedule, it's important to take one-time snapshots of etcd (RKE) , or datastore (k3s) before and after significant changes.</p> <p>The Rancher backup operator can also be used on any\u00a0distribution to backup the related objects that Rancher needs to function, this can be used to migrate Rancher between clusters.</p>"},{"location":"000020105/#26-provisioning","title":"2.6 Provisioning","text":"<p>Provisioning nodes and resources for Rancher and downstream clusters in a repeatable and automated way will greatly improve the supportability of Rancher and Kubernetes. This allows nodes to be replaced in a cluster easily, and new clusters created in a consistent way.</p> <p>Checks</p> <p>The below points can help prepare the Rancher and Kubernetes environment with integrations and modern approaches to managing resources, such as infrastructure as code, CI/CD, immutable infrastructure, and configuration management:</p> <ul> <li>Manifests and configuration data are stored in source control, treated as the source of truth for containerized applications</li> <li>Automated build, deployment and/or configuration management</li> </ul> <p>The rancher2 terraform provider and pulumi package can be used to manage clusters and resources as code.</p>"},{"location":"000020105/#27-managing-node-lifecycle","title":"2.7 Managing node lifecycle","text":"<p>When making significant planned changes it is important to drain nodes that are being affected to avoid disrupting in-flight connections, such as restarting Docker, patching, shutting down or removing nodes.</p> <p>For example, the <code>kube-proxy</code> component manages iptables rules on nodes to manage service endpoints, if a node is suddenly shutdown, stale endpoints and orphaned pods can be left in place for a period of time causing connectivity issues.</p> <p>In some cases during an unplanned issue, draining can be automated, such as when a node may be terminated, restarted, or shutdown.</p> <p>Checks</p> <ul> <li>A process is in place to drain before planned disruptive changes are performed on a node</li> <li>Where possible, node draining during the shutdown sequence is automated, for example, with a systemd or similar service</li> </ul>"},{"location":"000020105/#3-operating-kubernetes","title":"3. Operating Kubernetes","text":""},{"location":"000020105/#31-capacity-planning-and-monitoring","title":"3.1 Capacity planning and Monitoring","text":"<p>It is recommended to measure resource usage of all clusters by enabling monitoring in Rancher, or your chosen solution. It is recommended to alert on resource thresholds and events in the cluster.</p> <p>On supported platforms, using Cluster Autoscaler can be used to ensure the number of nodes is right-sized for the pod workload. Combining this with Horizontal Pod Autoscaler provides both application and infrastructure scaling capabilities.</p> <p>Checks</p> <ul> <li>Monitoring is enabled for the Rancher and downstream clusters</li> <li>Alert notifiers are configured to stay informed if an alarm or event occurs</li> <li>A process for adding/removing nodes is established, automated if possible</li> </ul>"},{"location":"000020105/#32-probes","title":"3.2 Probes","text":"<p>In the defence against service and pod related failures, liveness and readiness probes are very useful; these can be in the form of HTTP requests, commands, or TCP connections.</p> <p>Checks</p> <ul> <li>Liveness and Readiness probes are configured where necessary</li> <li>Probes do not rely on the success of upstream dependencies, only the running application in the pod</li> </ul>"},{"location":"000020105/#33-resources","title":"3.3 Resources","text":"<p>Assigning resource requests to pods allows the <code>kube-scheduler</code> to make more informed placement decisions, avoiding the \"bin packing\" of pods onto nodes and resource contention.</p> <p>Limits also offer value in the form of a safety net against pods consuming an undesired amount of resources.</p> <p>In addition to defining requests and limits for pods, it can also be useful to\u00a0reserve capacity\u00a0on nodes to prevent allocating resources that may be consumed by the kubelet and other system daemons, like Docker.</p> <p>Checks</p> <ul> <li>All pods define resource requests and have limits configured where necessary</li> <li>Nodes have system and daemon reservations where necessary</li> </ul> <p>When Rancher Monitoring is enabled, the graphs in Grafana\u00a0can be used to find a baseline of CPU and Memory for resource requests</p>"},{"location":"000020105/#34-os-limits","title":"3.4 OS Limits","text":"<p>Containerized applications can consume high amounts of OS resources, such as open files, connections, processes, filesystem space and inodes.</p> <p>Often the defaults are adequate; however, establishing a standardized image for all nodes can help establish a baseline for all configuration and tuning.</p> <p>Checks</p> <p>In general, the below can be used to confirm the OS limits allow for adequate headroom for the workloads</p> <ul> <li>File descriptor usage: <code>cat /proc/sys/fs/file-nr</code></li> <li> <p>User ulimits: <code>ulimit -a</code> Or, a particular process can be checked: <code>cat /proc/PID/limits</code></p> </li> <li> <p>Conntrack limits:</p> </li> </ul> <p><code>cat /proc/sys/net/netfilter/nf_conntrack_max</code></p> <p><code>cat /proc/sys/net/netfilter/nf_conntrack_count</code></p> <ul> <li>Filesystem space and inode usage: <code>df -h</code> and <code>df -ih</code></li> </ul> <p>Requirements for Linux can differ slightly depending on the distribution, refer to the Linux Requirements for more information.</p>"},{"location":"000020105/#35-log-rotation","title":"3.5 Log rotation","text":"<p>To prevent large log files from accumulating, and apply a desired retention period it is recommended to rotate OS, pod log files, and configure an external log service to stream logs off the nodes for a longer-term lifecycle and easier searching.</p> <p>Checks</p>"},{"location":"000020105/#containers","title":"Containers","text":"<p>k3sRKE</p> <ul> <li>Log rotation is configured for the container logs</li> <li>An external logging service is configured as needed</li> </ul> <p>The below arguments for the <code>INSTALL_K3S_EXEC</code> environment variable can be used as an example to rotate container logs:</p> <p><code>INSTALL_K3S_EXEC=\"--kubelet-arg container-log-max-files=5 --kubelet-arg container-log-max-size=100Mi\"</code></p> <ul> <li>Log rotation is configured for the container logs</li> <li>An external logging service is configured as needed</li> </ul> <p>Rotating container logs can be accomplished by configuring logrotate or the <code>/etc/daemon.json</code> file with a size and retention configuration.</p>"},{"location":"000020105/#os","title":"OS","text":"<p>Rotation of log files on nodes is also important, especially if a long node lifecycle is expected.</p>"},{"location":"000020105/#36-dns-scalability","title":"3.6 DNS scalability","text":"<p>DNS is a critical service running within the cluster. DNS queries are distributed throughout the cluster, where the availability depends on the accessibility of the CoreDNS pods in the service.</p> <p>The Nodelocal DNS cache is a redesign on the architecture and is recommended for clusters that may experience high DNS workload or issues.</p> <p>Checks</p> <p>If a cluster has experienced a DNS issue, or high DNS workload is expected:</p> <ul> <li>Check the output of <code>conntrack -S</code> on related nodes.</li> </ul> <p>High amounts of the <code>insert_failed</code> counter can be indicative of a conntrack race condition, Nodelocal DNS cache is recommended to mitigate this.</p>"},{"location":"000020105/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"000020105/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020106/","title":"What permissions are required to grant access to manage Cluster Logging in Rancher v2.x","text":"<p>This document (000020106) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020106/#situation","title":"Situation","text":""},{"location":"000020106/#question","title":"Question","text":"<p>By default, only Global Admins or Cluster Owners have access to configure and manage Cluster Logging in a Rancher v2.x managed cluster. This article details the permissions required to grant this access to other users.</p>"},{"location":"000020106/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster managed by Rancher v2.x</li> </ul>"},{"location":"000020106/#answer","title":"Answer","text":"<p>Cluster Logging configuration is managed by the ClusterLoggings Custom Resource in the management.cattle.io API Group. In order to create a role that grants permission to manage the logging configuration for a cluster, you should therefore grant all verbs on the CluserLoggings Resource in the management.cattle.io API group.</p> <p>You can define a custom Cluster Role via the Rancher UI, by navigating to the Global view, and selecting Security -&gt; Roles -&gt; Cluster, creating a custom role with these permissions. Granting this custom role on a cluster to a user or group will then provide access to manage the Cluster Logging configuration for that cluster.</p>"},{"location":"000020106/#further-reading","title":"Further Reading","text":"<ul> <li>Rancher v2.x Cluster Logging Documentation</li> <li>Rancher v2.x Role-Based Access Control (RBAC) Documentation</li> </ul>"},{"location":"000020106/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020107/","title":"How to pull the logs from the rancher-wins service on a Windows node in a Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020107) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020107/#situation","title":"Situation","text":""},{"location":"000020107/#task","title":"Task","text":"<p>In Windows Kubernetes clusters, available in Rancher v2.3.0 and above, the <code>rancher-wins</code> service provides a method for Rancher to operate the Windows host. Whilst troubleshooting a Windows cluster issue it may be necessary to pull the logs from this service, as documented in this article.</p>"},{"location":"000020107/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Windows Kubernetes cluster provisioned by Rancher v2.3.0 and above</li> </ul>"},{"location":"000020107/#steps","title":"Steps","text":"<p>To pull the logs from the <code>rancher-wins</code> service, execute the following command in a Powershell session on the node:</p> <pre><code>Get-EventLog -LogName Application -Source rancher-wins &gt; wins.log\n</code></pre> <p>This will write the logs to the file <code>wins.log</code> in the working directory, which you can then provide in your Rancher Support Ticket, for analysis.</p>"},{"location":"000020107/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020108/","title":"How to enable CoreDNS query logging in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020108) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020108/#situation","title":"Situation","text":""},{"location":"000020108/#task","title":"Task","text":"<p>By default, DNS query logging is disabled in CoreDNS, this article details the steps to enable query logging for CoreDNS in a Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x.</p>"},{"location":"000020108/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS dns add-on.</li> </ul>"},{"location":"000020108/#steps","title":"Steps","text":"<p>To enable DNS query logging the log plugin needs to be configured, by addition of <code>log</code> to the Corefile in the coredns ConfigMap of the kube-system Namespace.</p> <p>For example, to use the default log plugin configuration and log all queries, the Corefile definition would be updated as follows:</p> <pre><code>.:53 {\n    log\n    errors\n    health\n    ready\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n      pods insecure\n      fallthrough in-addr.arpa ip6.arpa\n    }\n    prometheus :9153\n    forward . \"/etc/resolv.conf\" {\n      policy random\n    }\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n</code></pre> <p>Steps to update the CoreDNS ConfigMap and persist these changes can be found in the article \"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster\".</p> <p>For the full list of available options when configuring the log plugin refer to the plugin documentation.</p>"},{"location":"000020108/#further-reading","title":"Further reading","text":"<ul> <li>CoreDNS log plugin documentation</li> </ul>"},{"location":"000020108/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020109/","title":"How to use External TLS Termination with AWS","text":"<p>This document (000020109) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020109/#situation","title":"Situation","text":""},{"location":"000020109/#task","title":"Task","text":"<p>This document covers setting up Rancher using an AWS SSL certificate and an ALB (Application Load Balancer).</p>"},{"location":"000020109/#requirements","title":"Requirements","text":"<ul> <li>Running Rancher management servers on AWS</li> </ul>"},{"location":"000020109/#resolution","title":"Resolution","text":""},{"location":"000020109/#configure-the-ssl-certificate","title":"Configure the SSL certificate","text":"<ul> <li>If you are using your own certificate follow the AWS documentation to import the certificate.</li> <li>If you are using an AWS certificate following the AWS documentation to request a public ACM certificate.</li> </ul>"},{"location":"000020109/#create-the-target-group","title":"Create the Target Group","text":"<ol> <li>Log into the AWS Console to get started.</li> <li>Use Create a Target Group to create a Target group using the data in the tables below to complete the procedure:</li> </ol> <p>- Target Group Name: rancher-http-80     - Protocol: http     - Port: 80     - Target type: instance     - VPC: Choose your VPC     - Protocol (Health Check): http     - Path (Health Check): /healthz</p> <ol> <li>Use Register Targets to Rancher management servers making sure to use the port 80.</li> </ol>"},{"location":"000020109/#create-the-alb","title":"Create the ALB","text":"<ol> <li>From your web browser, navigate to the Amazon EC2 Console.</li> <li>From the navigation pane, choose LOAD BALANCING &gt; Load Balancers.</li> <li>Click Create Load Balancer.</li> <li>Choose Application Load Balancer.</li> <li> <p>Complete the Step 1: Configure Load Balancer form:</p> <p>- Basic Configuration  - Name: rancher-http  - Scheme: internet-facing  - IP address type: ipv4  - Listeners  - Add the Load Balancer Protocols and Load Balancer Ports below.  - HTTP: 80  - HTTPS: 443  - Availability Zones  - Select Your VPC and Availability Zones.</p> </li> <li> <p>Complete the Step 2: Configure Security Settings form.</p> <p>- Configure the certificate you want to use for SSL termination.</p> </li> <li> <p>Complete the Step 3: Configure Security Groups form.</p> </li> <li> <p>Complete the Step 4: Configure Routing form.</p> <p>- From the Target Group drop-down, choose Existing target group.  - Add target group rancher-http-80.</p> </li> <li> <p>Complete Step 5: Register Targets. Since you registered your targets earlier, all you have to do it click Next: Review.</p> </li> <li> <p>Complete Step 6: Review. Look over the load balancer details and click Create when you\u2019re satisfied.</p> </li> <li>After AWS creates the ALB, click Close.</li> </ol>"},{"location":"000020109/#configure-external-tls-termination-for-rancher","title":"Configure External TLS Termination for Rancher","text":"<p>You need to add the option <code>--set tls=external</code> to your Rancher install, per the following example: <code>helm install rancher rancher-latest/rancher --namespace cattle-system --set hostname=mmattox-example.support.rancher.space --version 2.3.6 --set tls=external</code></p>"},{"location":"000020109/#verification","title":"Verification","text":"<p>Run the following command to verify new certificate:</p> <pre><code>curl --insecure -v https://&lt;&lt;Rancher Hostname&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n</code></pre> <p>Example output:</p> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host mmattox-example.support.rancher.space left intact\n</code></pre> <p>NOTE: Some browsers will cache the certificate. Details on how to clear the SSL state in a browser can be found here.</p>"},{"location":"000020109/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020110/","title":"Can I Use Rancher 2.4 Dashboard Feature in Air-Gapped environment ?","text":"<p>This document (000020110) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020110/#situation","title":"Situation","text":""},{"location":"000020110/#issue","title":"Issue","text":"<p>In an Air-Gapped environment, when I try to access the Dashboard Feature, it is not working.</p> <p>I keep having an <code>error 500</code> from Rancher.</p>"},{"location":"000020110/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher 2.4.x</li> <li>Air-Gapped environment</li> <li>Experimental Dashboard Feature enabled.</li> </ul>"},{"location":"000020110/#resolution","title":"Resolution","text":"<p>The Dashboard feature is still experimental and is currently not bundled in the Rancher releases.</p> <p>It is currently hosted on <code>https://releases.rancher.com/dashboard/latest/</code></p> <p>This allows our Engineers to do some changes outside of the regular release cycle.</p> <p>The same rules apply if the Kubernetes cluster is using a proxy.</p> <p>The proxy should allow external access to this URL.</p>"},{"location":"000020110/#further-reading","title":"Further Reading","text":"<p>https://rancher.com/docs/rancher/v2.x/en/installation/options/feature-flags/</p>"},{"location":"000020110/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020111/","title":"Kernel crash after \"unregister_netdevice: waiting for lo to become free. Usage count\"","text":"<p>This document (000020111) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020111/#situation","title":"Situation","text":""},{"location":"000020111/#issue","title":"Issue","text":"<p>In a Linux Kubernetes cluster that has frequent pod creations and deletions along with large amounts of pod network traffic, the following error may be logged to the host system logs:</p> <p><code>\"unregister_netdevice: waiting for lo to become free. Usage count = 1\"</code></p> <p>The kernel will typically be in a semi-hung state after this, causing major system instability.</p>"},{"location":"000020111/#resolution","title":"Resolution","text":"<p>This issue is fixed upstream in the Linux Kernel by this commit which was released in version 4.4.0.</p> <p>We recommend upgrading to the latest linux kernel available in your distribution.</p> <p>We have seen cases where certain kernel modules can cause this issue while loaded, even on a kernel that includes the fix above. If you are running a kernel higher than 4.4.0 and still seeing this issue, try disabling any third-party kernel modules to test.</p>"},{"location":"000020111/#projectos-specific-bugs","title":"Project/OS Specific bugs:","text":"<p>Docker - https://github.com/moby/moby/issues/5618</p> <p>Ubuntu - https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1403152</p> <p>- https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1711407</p> <p>RedHat - https://access.redhat.com/solutions/3105941 - https://access.redhat.com/solutions/3659011</p> <p>Centos - https://bugs.centos.org/view.php?id=12711</p>"},{"location":"000020111/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020112/","title":"Experimental Dashboard feature causes memory leak in rancher server and cattle-cluster-agent processes in Rancher v2.4.0 - v2.4.2","text":"<p>This document (000020112) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020112/#situation","title":"Situation","text":""},{"location":"000020112/#issue","title":"Issue","text":"<p>With the experimental Dashboard feature enabled in Rancher v2.4.0 through v2.4.2, the rancher server and cattle-cluster-agent processes will leak memory. As a result the memory usage of these Pods will grow over time, until they restart due to an Out of Memory (OOM) condition.</p>"},{"location":"000020112/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher install with either v2.4.0 or v2.4.2</li> </ul>"},{"location":"000020112/#workaround","title":"Workaround","text":"<p>Disable the experimental Dashboard feature within the Rancher UI:</p> <ol> <li>Navigate to the Global View -&gt; Settings -&gt; Feature Flags.</li> <li>Click the elipses for the <code>dashboard</code> entry and click <code>Deactivate</code>.</li> </ol>"},{"location":"000020112/#resolution","title":"Resolution","text":"<p>Upgrade to a newer version of Rancher v2.4.3+.</p>"},{"location":"000020112/#further-reading","title":"Further Reading","text":"<p>GitHub issue #26577 GitHub issue #26633</p>"},{"location":"000020112/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020113/","title":"Logging integration doesn't work if Docker Root is not default /var/lib/docker","text":"<p>This document (000020113) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020113/#situation","title":"Situation","text":""},{"location":"000020113/#issue","title":"Issue","text":"<p>As of the time of this writing, Rancher Logging is broken when the Docker root is configured to something other than <code>/var/lib/docker</code>.</p> <p>This issue is tracked in GitHub issue #21112.</p>"},{"location":"000020113/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher 2.x managed/imported cluster with logging enabled.</li> <li>Docker root configured to something other than <code>/var/lib/docker</code> on the nodes (confirmed with <code>docker info | grep Root</code>).</li> </ul>"},{"location":"000020113/#workaround","title":"Workaround","text":"<p>These steps will assume you have the Docker data root set to <code>/other-docker-root</code>. Change <code>/other-docker-root</code> to whatever your custom path is:</p> <ol> <li> <p>Rancher UI -&gt; Cluster -&gt; System Project -&gt; Workloads -&gt; cattle-logging Namespace</p> </li> <li> <p>Find workload rancher-logging-fluentd-linux</p> </li> <li> <p>Edit YAML</p> </li> <li> <p>Edit volume dockerroot</p> </li> <li> <p>Change \"Path on the Node\" from <code>/var/lib/docker</code> to <code>/other-docker-root</code></p> </li> <li> <p>Add volume (with the following details):</p> </li> </ol> <pre><code>Volume Name: dockerrootcustom\nType: bind-mount\nPath on the Node: /other-docker-root\nMount Point: /other-docker-root\n</code></pre> <ol> <li>Click Save</li> </ol> <p>At this point logging should be working with your non-default Docker root directory. You should be able to verify this on your logging target. Keep in mind it may take a few minutes for logs to show up there as fluentd is configured to clear its buffer every 60 seconds by default.</p>"},{"location":"000020113/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020114/","title":"Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge ' for InstanceType.","text":"<p>This document (000020114) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020114/#situation","title":"Situation","text":""},{"location":"000020114/#issue","title":"Issue","text":"<p>When trying to deploy a node in AWS EC2 on Rancher v1.6.x, you recieve an error like <code>Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge\\t' for InstanceType.</code></p>"},{"location":"000020114/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher UI v1.6.50 or earlier (included by default in Rancher v1.6.28 or earlier)</li> <li>Trying to deploy an AWS EC2 node of size <code>r5.12xlarge</code>, <code>r5.24xlarge</code>, <code>r5a.12xlarge</code>, or <code>r5a.24xlarge</code></li> </ul>"},{"location":"000020114/#resolution","title":"Resolution","text":"<p>The fix is in rancher ui v1.6.51 which is introduced in Rancher v1.6.29. Upgrade to v1.6.29 or later to deploy EC2 nodes of the affected sizes. It is advised to upgrade to the latest stable Rancher v1.6.x and migrate to latest stable Rancher v2.x</p>"},{"location":"000020114/#further-reading","title":"Further reading","text":"<p>The issue is a trailing tab in the name of the instance type. The commit that solves the issue can be seen here: https://github.com/rancher/ui/commit/5709e997aea949f41e299db8f519dc046d731cb9 rancher/ui v1.6.51: https://github.com/rancher/ui/tree/v1.6.51/app/components/machine rancher/rancher v1.6.29: https://github.com/rancher/rancher/releases/tag/v1.6.29</p>"},{"location":"000020114/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020115/","title":"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020115) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020115/#situation","title":"Situation","text":""},{"location":"000020115/#task","title":"Task","text":"<p>You might wish to update the Corefile configuration of CoreDNS, defined via the coredns ConfigMap in the kube-system Namespace, for example, in order to enable query logging or update the resolver policy. This article details how to update this ConfigMap and persist changes in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster.</p>"},{"location":"000020115/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS add-on.</li> <li>kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user.</li> </ul>"},{"location":"000020115/#steps","title":"Steps","text":"<ol> <li>Capture the current CoreDNS ConfigMap definition, with the following <code>kubectl</code> command:</li> </ol> <pre><code>kubectl -n kube-system get configmap coredns -o go-template={{.data.Corefile}}\n</code></pre> <p>The output should look like the following:</p> <pre><code>.:53 {\n       errors\n       health\n       ready\n       kubernetes cluster.local in-addr.arpa ip6.arpa {\n         pods insecure\n         fallthrough in-addr.arpa ip6.arpa\n       }\n       prometheus :9153\n       forward . \"/etc/resolv.conf\" {\n         policy random\n       }\n       cache 30\n       loop\n       reload\n       loadbalance\n}\n</code></pre> <ol> <li>Edit the cluster configuration YAML, to define a custom add-on containing the CoreDNS ConfigMap, with your desired changes. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click <code>Edit as YAML</code>.</li> </ol> <p>Create the add-on with the content below, replacing the Corefile definition with the existing configuration retrieved in step 1. Then make the desired changes, in this example the resolver policy is updated from random, in the existing configuration, to sequential.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n     name: coredns\n     namespace: kube-system\ndata:\n     Corefile: |\n       .:53 {\n           errors\n           health\n           ready\n           kubernetes cluster.local in-addr.arpa ip6.arpa {\n             pods insecure\n             fallthrough in-addr.arpa ip6.arpa\n           }\n           prometheus :9153\n           forward . \"/etc/resolv.conf\" {\n             policy sequential\n           }\n           cache 30\n           loop\n           reload\n           loadbalance\n       }\n</code></pre> <ol> <li>Update the cluster with the new configuration. For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>). For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol>"},{"location":"000020115/#further-reading","title":"Further reading","text":"<ul> <li>How to update CoreDNS's resolver policy</li> </ul>"},{"location":"000020115/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020116/","title":"How to run workloads on etcd or controlplane nodes, without the worker role, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020116) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020116/#situation","title":"Situation","text":""},{"location":"000020116/#task","title":"Task","text":"<p>Although it is normally not advised to run workloads on your controlplane and etcd nodes, there are occasionally scenarios when this is necessary. A few common examples are virus scanning, monitoring, and log collection workloads.</p>"},{"location":"000020116/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"000020116/#steps","title":"Steps","text":"<p>Both the controlplane and etcd nodes, which are not additionaly designated the worker role, have taints. When RKE or Rancher provisions these nodes, it adds these taints automatically. Workloads that need to run on these nodes require tolerations for these taints. For Rancher managed clusters you can see these taints within the Rancher UI on the cluster node view. The following kubectl command will also list the taints for each node.</p> <pre><code>$ kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\nNAME           TAINTS\nip-10-0-2-10   [map[effect:NoExecute key:node-role.kubernetes.io/etcd value:true]]\nip-10-0-2-11   [map[effect:NoSchedule key:node-role.kubernetes.io/controlplane value:true]]\nip-10-0-2-12   &lt;none&gt;\n</code></pre> <p>Per this output, each etcd node has the <code>NoExecute</code> taint <code>node-role.kubernetes.io/etcd=true</code> and each controlplane node has the <code>NoSchedule</code> taint <code>node-role.kubernetes.io/controlplane=true</code>.</p> <p>The Rancher UI does not have fields for adding tolerations, so you will need to specify the tolerations directly in the workload's YAML manifest. You can use the <code>Import YAML</code> button to deploy your workload and make sure to add the following tolerations block in your manifest:</p> <pre><code>spec:\n...\ntemplate:\n...\n    spec:\n...\n      tolerations:\n      - operator: Exists\n...\n</code></pre> <p>If you have an existing workload, you can also select the <code>View/Edit YAML</code> option for the workload and apply the above change. This toleration will allow you to run the workload on any nodes with taints, so use with caution. If you are using Helm charts, you can also specify the same YAML in your Helm chart.</p>"},{"location":"000020116/#further-reading","title":"Further Reading","text":"<p>For more information on how taints and tolerations work in Kubernetes, see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</p>"},{"location":"000020116/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020117/","title":"How to override DNS results served by CoreDNS","text":"<p>This document (000020117) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020117/#situation","title":"Situation","text":""},{"location":"000020117/#task","title":"Task","text":"<p>By default, DNS requests for pods using CoreDNS will be made directly to the upstream nameservers configured in /etc/resolv.conf on the node.</p> <p>At times, it may not be possible to easily update records on the upstream nameservers, or specific records for the cluster may be needed. In these cases it's useful to override the results that CoreDNS will serve pods.</p>"},{"location":"000020117/#pre-requisites","title":"Pre-requisites","text":"<p>These steps should work for any cluster running CoreDNS where the <code>coredns</code> ConfigMap is used.</p>"},{"location":"000020117/#steps","title":"Steps","text":"<p>There are two approaches to achieve this, please read through both to understand which is best for your environment.</p> <p>Both approaches require editting the <code>coredns</code> ConfigMap, specifically the <code>Corefile</code> key. This can be done in the UI by clicking View/Edit YAML, Edit, or on the command line with kubectl.</p> <p>Along with these options, both plugins covered provide other features, like adjusting the TTL for records, see the documentation links for more information.</p>"},{"location":"000020117/#rewrite","title":"Rewrite","text":"<p>The rewrite plugin will perform a rewritten query to the upstream nameserver, and respond to the query with the results. The outcome would be similar to configuring a CNAME for the domain.</p> <pre><code>data:\nCorefile: |\n    .:53 {\n        [...]\n        rewrite name archive.ubuntu.com internal-mirror.ubuntu.local\n    }\n</code></pre> <p>In this example, pods configured with the default Ubuntu mirror are now resolving to the internal mirror without any custom configuration.</p> <p>The benefit of this approach is that the upstream nameserver remains the source of truth for the results.</p>"},{"location":"000020117/#hosts","title":"Hosts","text":"<p>The hosts plugin provides the ability to define a list of IPs and domains in the form of /etc/hosts to respond as query results.</p> <pre><code>data:\nCorefile: |\n    .:53 {\n        [...]\n        hosts {\n          10.0.0.1 archive.ubuntu.com\n          10.0.0.2 testing.com\n          fallthrough\n        }\n    }\n</code></pre> <p>A similar example, the internal IPs listed are provided as results.</p> <p>A downside to this approach is that the ConfigMap becomes a source of truth for these results, if changes in the environment are not reflected these entries could become stale. However, it does provide the most flexibility without needing to depend on any upstream nameserver to serve results.</p>"},{"location":"000020117/#persist-the-changes","title":"Persist the changes","text":"<p>In an RKE or Rancher environment, during cluster or addon upgrades, it's possible that changes to the <code>coredns</code> ConfigMap are updated to use the provided version.</p> <p>To persist the changes made to the ConfigMap, add the changes as a user-defined addon. The steps to do this are documented under How To Update CoreDNS's Resolver Policy article.</p>"},{"location":"000020117/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020118/","title":"How to monitor NTP on Linux nodes with Cluster Monitoring in Rancher v2.2.x+","text":"<p>This document (000020118) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020118/#situation","title":"Situation","text":""},{"location":"000020118/#task","title":"Task","text":"<p>Time drift between nodes in a Kubernetes cluster can create a range of issues, from a difficulty to correlate application log message timestamps across nodes, to a loss of etcd quorum (given the time sensitive nature of the consensus algorithm used in etcd).</p> <p>Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution.</p> <p>This article details how to monitor time drift, via the Network Time Protocol (NTP), on Linux nodes within Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters.</p>"},{"location":"000020118/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, starting at v2.2.0 and above</li> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with Cluster Monitoring enabled, with Monitoring Version 0.2.0+</li> <li>ntp configured on Linux nodes in the cluster (refer to the documentation for your Linux distribution on enabling and configuring ntp)</li> </ul>"},{"location":"000020118/#steps","title":"Steps","text":""},{"location":"000020118/#enable-the-ntp-collector-on-the-node-exporter-daemonset","title":"Enable the NTP collector on the Node Exporter DaemonSet","text":"<ol> <li>Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -&gt; Monitoring</li> <li>In the bottom-right corner of the form, click <code>Show advanced options</code></li> <li>Click <code>Add Answer</code></li> <li>Configure the variable <code>exporter-node.collectors.ntp.enabled</code> with value <code>true</code></li> <li>Click <code>Save</code></li> </ol>"},{"location":"000020118/#configure-an-alert-for-ntp-time-drift","title":"Configure an alert for NTP time drift","text":"<ol> <li>Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -&gt; Alerts</li> <li>On the <code>A set of alerts for node</code> Alert Group click <code>Add Alert Rule</code></li> <li>Set Name to <code>Node NTP time drift equal to or greater than 1 second</code></li> <li>Select <code>Expression</code> and enter <code>node_ntp_offset_seconds</code></li> <li>Click <code>Create</code></li> <li>Configure a Notifier for the <code>A set of alerts for node</code> Alert Group, by clicking the elipses for this Alert Group, and configuring the desired notifier in the <code>Alert</code> section at the bottom of the form.</li> </ol>"},{"location":"000020118/#further-reading","title":"Further Reading","text":"<ul> <li>Rancher Cluster Monitoring Documentation</li> <li>Prometheus Node Exporter README</li> </ul>"},{"location":"000020118/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020119/","title":"How to enable NGINX support for HTTP headers with underscores","text":"<p>This document (000020119) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020119/#situation","title":"Situation","text":""},{"location":"000020119/#task","title":"Task","text":"<p>This article details how to enable HTTP headers with underscores on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"000020119/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced.</li> <li>For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"000020119/#resolution","title":"Resolution","text":""},{"location":"000020119/#configuration-for-rke-provisioned-clusters","title":"Configuration for RKE provisioned clusters","text":"<ul> <li>Edit the cluster configuration YAML file to include the <code>enable-underscores-in-headers: true</code> option for the ingress, as follows:</li> </ul> <pre><code>ingress:\n    provider: nginx\n    options:\n      enable-underscores-in-headers: true\n</code></pre> <ul> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ul> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ul> <li>Recycle the nginx pods in-order to pick up new argument:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <ul> <li>Verify the new configuration:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"000020119/#configuration-for-rancher-provisioned-clusters","title":"Configuration for Rancher provisioned clusters","text":"<ul> <li>Login into the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the <code>enable-underscores-in-headers</code> option for the ingress, as follows:</li> </ul> <pre><code>ingress:\n    provider: nginx\n    options:\n      enable-underscores-in-headers: true\n</code></pre> <ul> <li>Click \"Save\" at the bottom of the page.</li> <li>Wait for cluster to finish upgrading.</li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <p>- Run the following inside the kubectl CLI to verify the new argument:</p> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"000020119/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020120/","title":"How to enable antiAffinity for Rancher v2.x server pods","text":"<p>This document (000020120) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020120/#situation","title":"Situation","text":""},{"location":"000020120/#task","title":"Task","text":"<p>By default the Rancher server pods are deployed without podAntiAffinity rules. As a result of this multiple Rancher pods may be scheduled onto a single node, potentially leading to temporary service disruption if the node is unavailable or gets rebooted.</p>"},{"location":"000020120/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> <li>kubectl access to the cluster</li> <li>Rancher Kubernetes Engine (RKE) to be installed, with access to the cluster.yml and correspoding cluster.rkestate file see the RKE documentation for more information</li> <li>helm v3</li> </ul>"},{"location":"000020120/#steps","title":"Steps","text":"<p>You need to add the option <code>--set-string antiAffinity=required</code> to your Rancher install. Details on how to add this option to both new installations of Rancher, as well as existing deployment are provided below.</p>"},{"location":"000020120/#new-rancher-installation","title":"New Rancher installation","text":"<p>For new installations of Rancher, add the antiAffinity option to the <code>helm install</code> command, per the following example:</p> <pre><code>helm install rancher rancher-latest/rancher \\\n--namespace cattle-system \\\n--set hostname=mmattox-example.support.rancher.space \\\n--version 2.3.6 \\\n--set-string antiAffinity=required\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade.</p>"},{"location":"000020120/#update-existing-rancher-deployments","title":"Update existing Rancher deployments","text":"<p>To add the antiAffinity option to an existing deployment of Rancher, follow the Rancher upgrade documentation, using the <code>--version</code> flag to pin to the running Rancher version, preventing a version upgrade.</p> <ol> <li>Run <code>helm get values rancher</code> to get the current Rancher helm chart values, which will be used to generate the <code>helm upgrade</code> command with matching values.</li> <li>Generate and run the <code>helm upgrade</code> command with the chart values, including the pinned version and antiAffinity option, per the following example:</li> </ol> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n   --namespace cattle-system \\\n   --set hostname=mmattox-example.support.rancher.space \\\n   --version 2.3.6 \\\n   --set-string antiAffinity=required\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade. NOTE: We recommend saving this command for future Rancher upgrades to save time.</p>"},{"location":"000020120/#verification","title":"Verification","text":"<p>Run the command <code>kubectl get deployment -n cattle-system rancher -o yaml</code> and verify the following <code>podAntiAffinity</code> spec has been added:</p> <pre><code>[...]\nspec:\naffinity:\npodAntiAffinity:\n     requiredDuringSchedulingIgnoredDuringExecution:\n     - labelSelector:\n         matchExpressions:\n         - key: app\n           operator: In\n           values:\n           - rancher\n       topologyKey: kubernetes.io/hostname\n[...]\n</code></pre>"},{"location":"000020120/#rollback","title":"Rollback","text":"<p>To remove the antiAffnitiy configuration you should remove the <code>--set-string antiAffinity=required</code> option from the <code>helm upgrade</code> command and re-run this, per the following example:</p> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n--namespace cattle-system \\\n--set hostname=mmattox-example.support.rancher.space \\\n--version 2.3.6\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade.</p>"},{"location":"000020120/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020121/","title":"How to create a cluster in Rancher v2.x using the Rancher CLI or v3 API","text":"<p>This document (000020121) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020121/#situation","title":"Situation","text":""},{"location":"000020121/#task","title":"Task","text":"<p>The process for creating Kubernetes clusters via the Rancher v2.x UI is documented in \"Setting up Kubernetes Clusters in Rancher\".</p> <p>This article details the process for creating Kubernetes clusters in Rancher v2.x via the Rancher CLI or v3 API interfaces.</p>"},{"location":"000020121/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>The Rancher CLI installed for CLI cluster creation (this can be downloaded from the Rancher UI, via the <code>Download CLI</code> link in the lower-right corner)</li> <li>curl installed to make Rancher v3 API requests for API cluster creation</li> <li>A Rancher API Key for a user with cluster creation permissions</li> <li>A Rancher Kubernetes Engine (RKE) cluster config file in YAML or JSON format (optional)</li> </ul>"},{"location":"000020121/#steps","title":"Steps","text":"<p>The cluster creation process is detailed below for both the Rancher CLI and v3 API.</p>"},{"location":"000020121/#cluster-creation-via-the-rancher-cli","title":"Cluster creation via the Rancher CLI","text":"<ol> <li>Log in to your Rancher Server:</li> </ol> <pre><code>rancher login &lt;server_url&gt; --token &lt;token&gt;\n</code></pre> <ol> <li>Create the cluster:</li> </ol> <p>To create a cluster with the default cluster configuration:</p> <pre><code>rancher cluster create &lt;new_cluster_name&gt;\n</code></pre> <p>If you are passing in an RKE cluster config file, do so as follows:</p> <pre><code>rancher cluster create --rke-config &lt;rke_config_file&gt; &lt;new_cluster_name&gt;\n</code></pre>"},{"location":"000020121/#cluster-creation-via-the-rancher-v3-api","title":"Cluster creation via the Rancher v3 API","text":"<ol> <li>Create a Rancher API Key, and save the access key and secret key as environment variables ( <code>export CATTLE_ACCESS_KEY=&lt;access_key&gt; &amp;&amp; export CATTLE_SECRET_KEY=&lt;secret_key&gt;</code>). Alternatively you can pass these directly into the curl request in place of the <code>${CATTLE_ACCESS_KEY}</code> and <code>${CATTLE_SECRET_KEY}</code> variables in the examples below.</li> <li>Send a POST request to the <code>/v3/clusters</code> API endpoint of your Rancher server: To create a cluster with the default cluster configuration:</li> </ol> <pre><code>curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n   -X POST \\\n   -H 'Accept: application/json' \\\n   -H 'Content-Type: application/json' \\\n   -d '{\"name\":\"test-cluster\"}' \\\n'https://&lt;rancher_server&gt;/v3/clusters'\n</code></pre> <p>If you are passing in an RKE cluster config file, do so as follows:</p> <pre><code>curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n   -X POST \\\n   -H 'Accept: application/json' \\\n   -H 'Content-Type: application/json' \\\n   -d @&lt;rke_config_file&gt; \\\n'https://&lt;rancher_server&gt;/v3/clusters'\n</code></pre>"},{"location":"000020121/#additional-reading","title":"Additional Reading","text":"<ul> <li>The Rancher v2.x API Documentation</li> <li>The Rancher v2.x API Specification</li> </ul>"},{"location":"000020121/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020122/","title":"How to edit the upstream nameservers used by CoreDNS or kube-dns, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020122) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020122/#situation","title":"Situation","text":""},{"location":"000020122/#task","title":"Task","text":"<p>By default, CoreDNS and kube-dns Pods will inherit the nameserver configuration from the node. In certain circumstances it might be desired to override this, and use a specific set of nameservers for external queries.</p> <p>Note: These steps update the nameservers only for Pods that use either the <code>ClusterFirst</code> (default) or <code>ClusterFirstWithHostNet</code> DNS policy. Nameserver configuration for nodes and other Pods will not be affected.</p>"},{"location":"000020122/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, with the CoreDNS or kube-dns dns addon enabled.</li> </ul> <p>Note: New clusters can also be created using the same steps.</p>"},{"location":"000020122/#steps","title":"Steps","text":""},{"location":"000020122/#option-a-update-the-clusteryaml","title":"Option A: Update the cluster.yaml","text":"<p>The cluster configuration YAML provides the <code>upstreamnameservers</code> option, to configure a list of upstream nameservers, per the example below:</p> <ol> <li>Add the <code>upstreamnameservers</code> option, with the list of nameservers, to the cluster configuration YAML.     For RKE provisioned clusters, add this into the cluster.yml file.     For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click <code>Edit as YAML</code>.</li> </ol> <pre><code>dns:\n     provider: coredns\n     upstreamnameservers:\n  - 1.1.1.1\n  - 8.8.8.8\n</code></pre> <ol> <li>Update the cluster with the new configuration.     For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>).     For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol> <p>Note: This option is recommended as it requires minimal change, see the RKE add-ons documentation for more information.</p>"},{"location":"000020122/#option-b-update-the-kubelet-resolvconf","title":"Option B: Update the kubelet resolv.conf","text":"<p>By default, the kubelet will refer to the <code>/etc/resolv.conf</code> file as the source for nameserver configuration.</p> <p>It is possible to override this by adding an <code>extra_args</code> option to the <code>kubelet</code> service, and this is also accomplished in the cluster configuration YAML.</p> <p>A custom resolv.conf file can then be used by the kubelet instead, per the example below:</p> <ol> <li>On each of the nodes in the cluster create the custom nameserver configuration file:</li> </ol> <pre><code>echo \"nameserver 8.8.8.8\" &gt; /etc/k8s-resolv.conf\n</code></pre> <ol> <li>Add <code>resolv-conf</code>, referencing the custom nameserver configuration file, to the <code>extra_args</code> option for the kubelet service, in the cluster configuration YAML.     For RKE provisioned clusters, add this into the cluster.yml file.     For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click <code>Edit as YAML</code>.</li> </ol> <pre><code>services:\n     kubelet:\n       extra_args:\n         resolv-conf: /host/etc/k8s-resolv.conf\n</code></pre> <ol> <li>Update the cluster with the new configuration.     For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>).     For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol> <p>See the RKE services documentation for more information.</p> <p>Note: kubelet flags are being updated, as such a restart of the kubelet component will occur on each node.</p>"},{"location":"000020122/#option-c-update-the-node-resolvconf","title":"Option C: Update the node resolv.conf","text":"<p>If the nameserver configuration should be consistent between the OS and Kubernetes Pods, updating the node <code>/etc/resolv.conf</code> file is recommended.</p> <p>This could be because nameservers are changing or that the caching configuration (for example systemd-resolved) is not desired.</p> <p>Changes to a systemd managed resolv.conf can be dependent on the Linux distribution and you should refer to the documentation for the distribution used in the cluster.</p> <p>Note: The kubelet component caches the <code>/etc/resolv.conf</code> file at start time, as such a restart of the kubelet component needs to occur on each node manually, after updating the configuration.</p> <p>This can be accomplished a number of ways:</p> <ul> <li><code>docker restart kubelet</code> on each node</li> <li>A drain and restart of each node</li> <li>Replacing nodes in the cluster with the updated configuration</li> </ul>"},{"location":"000020122/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020123/","title":"How to change etcd cipher suite","text":"<p>This document (000020123) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020123/#situation","title":"Situation","text":""},{"location":"000020123/#hardening-etcd-cluster-communication","title":"Hardening ETCD cluster communication","text":""},{"location":"000020123/#synopsis","title":"Synopsis:","text":"<p>This article will walk Rancher administrators through hardening the cluster communication between etcd nodes. We'll go over configuring etcd to use specific ciphers which enable stronger encryption for securing intra-cluster etcd traffic.</p>"},{"location":"000020123/#configuring-etcd-rke-and-rancher-ui","title":"Configuring etcd (rke and Rancher UI):","text":"<p>To make the modifications we'll be configuring our rke cluster YAML spec. This setting would be defined, then applied at the command line with the rke CLI, or alternately via the Rancher UI. From within the Rancher UI, navigate to the cluster you're looking to modify, and click edit under the 3 dot menu. From there, you should see a button labeled 'Edit as Yaml'. At the cluster YAML spec view we define the cipher-suites parameter under the etcd service definition. We recommend testing this out in a non-vital cluster before rolling out on important clusters to become familiar with the process.</p> <pre><code>services:\netcd:\n    extra_args:\n      cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"\n      election-timeout: \"5000\"\n      heartbeat-interval: \"500\"\n</code></pre>"},{"location":"000020123/#note","title":"Note:","text":"<p>The cipher suites defined in the example could trade off speed for stronger encryption. Consider the level of ciphers in use and how they could impact the performance of an etcd cluster. Testing should be done to factor the spec of your hosts (cpu, memory, disk, network, etc...) and the typical types of interacting with kubernetes as well as the amount of resources under management within the k8s cluster.</p>"},{"location":"000020123/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020124/","title":"How to block external connectivity with Calico","text":"<p>This document (000020124) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020124/#situation","title":"Situation","text":""},{"location":"000020124/#task","title":"Task","text":"<p>In cases where it is desired to control external connectivity from the cluster, such as to deny or allow specific IP addresses or ports from Pods using the CNI network, a <code>GlobalNetworkPolicy</code> object can be used to control the rules applied to all nodes in the cluster.</p> <p>The <code>GlobalNetworkPolicy</code> is provided by the Calico CRD deployed on RKE clusters.</p>"},{"location":"000020124/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE cluster configured with the Canal or Calico CNI</li> </ul>"},{"location":"000020124/#steps","title":"Steps","text":"<p>Configure a YAML manifest the desired rules, using the <code>nets</code> and/or <code>ports</code> keys, the Calico documentation provides some more information on each field.</p> <p>In the below example the EC2 metadata is being denied to prevent Pods from accessing the IAM profile credentials of the instance.</p> <pre><code>apiVersion: crd.projectcalico.org/v1\nkind: GlobalNetworkPolicy\nmetadata:\nname: deny-ec2-metadata\nspec:\ntypes:\n  - Egress\negress:\n  - action: Deny\n    destination:\n      nets:\n      - 169.254.169.254/32\n  - action: Allow\n    destination:\n      nets:\n      - 0.0.0.0/0\n</code></pre> <p>Deny 80/TCP connectivity external to the cluster</p> <pre><code>apiVersion: crd.projectcalico.org/v1\nkind: GlobalNetworkPolicy\nmetadata:\nname: deny-http\nspec:\ntypes:\n  - Egress\negress:\n  - action: Deny\n    protocol: TCP\n    destination:\n      ports:\n      - 80\n  - action: Allow\n    destination:\n      nets:\n      - 0.0.0.0/0\n</code></pre> <p>Apply the YAML file created and test connectivity from a Pod running within the cluster on the CNI network.</p> <p>Note: Pods running with <code>hostnetwork: true</code> will not be effected by the <code>GlobalNetworkPolicy</code> as these Pods do not use the CNI network.</p>"},{"location":"000020124/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020125/","title":"How to Enable Pod Presets","text":"<p>This document (000020125) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020125/#situation","title":"Situation","text":""},{"location":"000020125/#task","title":"Task","text":"<p>This how-to article outlines how to enable pod presets on your cluster. This is done by enabling the <code>PodPreset</code> admission plugin and the <code>settings.k8s.io/v1alpha1</code> API for the kube-apiserver.</p>"},{"location":"000020125/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes version 1.10 and above</li> <li>Access to edit the cluster in yaml or the cluster.yaml file you used with RKE.</li> </ul>"},{"location":"000020125/#resolution","title":"Resolution","text":"<p>Get to the cluster yaml in Rancher by editing the cluster and selecting \"edit as yaml\" or by opening the RKE cluster.yml file. Modify the kube-api section to resemble the following and hit save or running <code>rke up</code>:</p> <pre><code>services:\nkube-api:\n    extra_args:\n      runtime-config: authorization.k8s.io/v1beta1=true,settings.k8s.io/v1alpha1=true\n      enable-admission-plugins: PodPreset,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize\n</code></pre> <p>Notice that <code>settings.k8s.io/v1alpha1/podpreset</code> and <code>PodPreset</code> is added to the runtime-config and admission plugins.</p>"},{"location":"000020125/#further-reading","title":"Further reading","text":"<p>You can test the ability to use pod presets with this guide.</p> <p>More details can be found in the kubernetes docs on pod presets.</p>"},{"location":"000020125/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020126/","title":"How to use nginx /dbg","text":"<p>This document (000020126) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020126/#situation","title":"Situation","text":""},{"location":"000020126/#what-is-the-dbg-command","title":"What is the /dbg command","text":"<p>/dbg is a program included in the ingress-nginx container image that can be used to show information about the nginx environment and the resulting nginx configuration, which can be helpful when debugging ingress issues in Kubernetes.</p>"},{"location":"000020126/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster that has ingress enabled with ingress-nginx as the ingress controller</li> <li>A cluster with Linux nodes, nginx will not run on Windows</li> <li>kubectl configured</li> </ul>"},{"location":"000020126/#using-dbg","title":"Using /dbg","text":"<p>This command needs to be run from inside one of the ingress-nginx pods, so first determine the pod to run it in.</p> <pre><code>&gt; kubectl get pods -n ingress-nginx\nNAME                                    READY   STATUS    RESTARTS   AGE\ndefault-http-backend-67cf578fc4-54jlz   1/1     Running   0          5d\nnginx-ingress-controller-56nss          1/1     Running   0          5d\nnginx-ingress-controller-hscfg          1/1     Running   0          4d21h\nnginx-ingress-controller-n4p22          1/1     Running   0          5d\n</code></pre> <pre><code>&gt; export NGINX_POD=nginx-ingress-controller-n4p22\n</code></pre> <p>If you are diagnosing specific connection issues, you can determine which controller is receiving the traffic by looking through the logs of each.</p>"},{"location":"000020126/#viewing-ingress-controller-status","title":"Viewing ingress-controller status","text":"<p>/dbg general will show the count of running controllers.</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg  general\n{\n\"controllerPodsCount\": 3\n}\n</code></pre>"},{"location":"000020126/#viewing-backend-configuration","title":"Viewing backend configuration","text":"<p>/dbg backends list will list the discovered backends:</p> <p>```</p> <p>kubectl exec -n ingress-nginx $NGINX_POD /dbg backends list cattle-system-rancher-80 upstream-default-backend ```</p> <p>/dbg backends get will show the configuration for the named backend:</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg backends get cattle-system-rancher-80\n</code></pre>"},{"location":"000020126/#viewing-ingress-certificate-data","title":"Viewing ingress certificate data","text":"<p>/dbg certs will dump the x509 cert and key for a certificate that nginx has discovered from k8s secrets for the given hostname:</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg certs get &lt;fqdn&gt;\n</code></pre>"},{"location":"000020126/#viewing-dynamically-generated-nginx-configuration","title":"Viewing dynamically generated nginx configuration","text":"<p>/dbg conf will dump the dynamically generated nginx configuration. To view the configuration for a specific ingress hostname, you could run /dbg conf and then grep for the server_name:</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg conf | grep \"server_name example.com\" -B2 -A20\n</code></pre>"},{"location":"000020126/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020127/","title":"Where is SUSE Rancher Hosted hosted?","text":"<p>This document (000020127) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020127/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is hosted in the Cloud. You can have your choice of regions available in North America, EMEA, and APAC geographies.</p>"},{"location":"000020127/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020128/","title":"How is uptime measured for my SUSE Rancher Hosted environment?","text":"<p>This document (000020128) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020128/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted currently uses Pingdom for measuring uptime. Your Hosted Rancher endpoint is tested every one minute and is considered down if a response is not returned within five seconds. Pingdom tests endpoints from over 100 locations across the world. Uptime for the month is calculated by dividing the number of uptime minutes by the total number of minutes in the month. For example, in the month of May, there are 44,640 minutes (60 X 24 X 31). If there were 5 minutes of downtime and 44,635 minutes of uptime, the uptime measurement would be 44,635 / 44,640 = 99.989%. Please also refer to your Service Agreement for the legal definitions for uptime.</p>"},{"location":"000020128/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020129/","title":"Does SUSE Rancher Hosted offer an uptime SLA?","text":"<p>This document (000020129) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020129/#resolution","title":"Resolution","text":"<p>Yes, Hosted Rancher offers a 99.9% uptime Service Level Agreement (SLA). For the details on our SLA, check your master service agreement or contact your Account Executive.</p>"},{"location":"000020129/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020130/","title":"Can I have more than one SUSE Rancher Hosted environment?","text":"<p>This document (000020130) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020130/#resolution","title":"Resolution","text":"<p>Yes, you can have multiple SUSE Rancher Hosted environments. Check with your Account Executive for pricing details. There are several use cases where this is preferred, such as having separate environments for development, quality assurance, and production. You will also want multiple SUSE Rancher Hosted environments if you need to manage Kubernetes clusters in separate geographies, such as North America, Europe, and Asia.</p>"},{"location":"000020130/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020131/","title":"Does SUSE Rancher Hosted offer a support SLA?","text":"<p>This document (000020131) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020131/#resolution","title":"Resolution","text":"<p>Yes, SUSE Rancher Hosted offers the same support Service Level Agreement (SLA) to both SUSE Rancher Hosted and customers who manage their own Rancher server instance. Support cases can be opened on the Support Portal. Details of the support offering can be found on the SUSE support page.</p>"},{"location":"000020131/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020132/","title":"How long are SUSE Rancher Hosted backups retained?","text":"<p>This document (000020132) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020132/#resolution","title":"Resolution","text":"<p>Backups on SUSE Rancher Rancher are taken hourly and retained for up to 1 year.</p>"},{"location":"000020132/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020133/","title":"How to configure the CoreDNS Autoscaler in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020133) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020133/#situation","title":"Situation","text":""},{"location":"000020133/#task","title":"Task","text":"<p>During the life of a cluster, you may need to adjust the scaling parameters for the kube-dns or CoreDNS autoscaler. The autoscaler runs as an independant Deployment in the cluster, using the cluster-proportional-autoscaler container to scale up and down the related kube-dns or CoreDNS Deployment, using a linear or ladder pattern.</p>"},{"location":"000020133/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> <li>The cluster is configured with either the kube-dns or coredns provider (enabled by default)</li> </ul> <p>Note When running <code>rke up</code> commands, ensure the <code>.rkestate</code> file for the cluster is present in the working directory as per the documentation here.</p>"},{"location":"000020133/#steps","title":"Steps","text":"<p>Four approaches are provided, depending on the Rancher or RKE version in use.</p> <p>Note: When making the changes, the <code>coredns-autoscaler</code> or kube-dns-autoscaler` pod will be restarted with updated command arguments, this will not cause any disruption to DNS resolution.</p> <p>Note: Check the logs of the <code>kube-dns-autoscaler</code>, or <code>coredns-autoscaler</code> pod after making changes to confirm they have taken effect.</p>"},{"location":"000020133/#a-rancher-provisioned-cluster-managed-by-rancher-versions-after-v24x","title":"A Rancher provisioned cluster managed by Rancher versions after v2.4.x","text":"<ol> <li>Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'.</li> <li>Click 'Edit as YAML'.</li> <li>Locate or add the <code>dns</code> field, using the below as an example to add the desired parameters below:</li> </ol> <pre><code>rancher_kubernetes_engine_config:\n     [...]\n     dns:\n       linear_autoscaler_params:\n         cores_per_replica: 128\n         max: 0\n         min: 1\n         nodes_per_replica: 4\n         prevent_single_point_failure: true\n</code></pre> <ol> <li>Click 'Save' to update the cluster with the new configuration.</li> </ol>"},{"location":"000020133/#a-rancher-provisioned-cluster-managed-by-rancher-versions-before-v24x","title":"A Rancher provisioned cluster managed by Rancher versions before v2.4.x","text":"<ol> <li>Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'.</li> <li>Click 'Edit as YAML'.</li> <li>Locate or add the <code>addons</code> field, using the below as an example to add the desired parameters:</li> </ol> <pre><code>rancher_kubernetes_engine_config:\n     [...]\n     addons: |-\n       apiVersion: v1\n       data:\n         linear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}'\n       kind: ConfigMap\n       metadata:\n         name: coredns-autoscaler\n         namespace: kube-system\n</code></pre> <ol> <li>Click 'Save' to update the cluster with the new configuration.</li> </ol>"},{"location":"000020133/#an-rke-provisioned-cluster-managed-by-rke-versions-after-v110","title":"An RKE provisioned cluster managed by RKE versions after v1.1.0","text":"<ol> <li>Edit the cluster configuration YAML file to configure the <code>dns</code> field, using the below as an example to add the desired parameters below:</li> </ol> <pre><code>dns:\n     linear_autoscaler_params:\n       cores_per_replica: 128\n       max: 0\n       min: 1\n       nodes_per_replica: 4\n       prevent_single_point_failure: true\n</code></pre> <ol> <li>Invoke <code>rke up --config &lt;cluster configuration YAML file&gt;</code> to update the cluster.</li> </ol>"},{"location":"000020133/#an-rke-provisioned-cluster-managed-by-rke-versions-before-v110","title":"An RKE provisioned cluster managed by RKE versions before v1.1.0","text":"<ol> <li>Edit the cluster configuration YAML file to configure the ConfigMap addon, using the below as an example to add the desired parameters below:</li> </ol> <pre><code>addons: |-\n     apiVersion: v1\n     data:\n       linear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}'\n     kind: ConfigMap\n     metadata:\n       name: coredns-autoscaler\n       namespace: kube-system\n</code></pre> <ol> <li>Invoke <code>rke up --config &lt;cluster configuration YAML file&gt;</code> to update the cluster.</li> </ol>"},{"location":"000020133/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020134/","title":"Do SUSE employees have a login account for my SUSE Rancher Hosted environment?","text":"<p>This document (000020134) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020134/#resolution","title":"Resolution","text":"<p>When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment. At your discretion, you can create an account for SUSE employees, with the permissions you feel comfortable with, to allow SUSE staff to perform troubleshooting activities. SUSE can also do a Teams or Zoom screen-sharing session to help troubleshoot any issues you have with SUSE Rancher Hosted.</p>"},{"location":"000020134/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020135/","title":"Do SUSE employees have the credentials to my \u201cadmin\u201d account?","text":"<p>This document (000020135) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020135/#resolution","title":"Resolution","text":"<p>When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment.</p>"},{"location":"000020135/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020136/","title":"What type of cluster is SUSE Rancher Hosted running on?","text":"<p>This document (000020136) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020136/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted runs on top of SUSE Rancher's k3s which is a fully compliant, lightweight Kubernetes distribution. The cluster consists of two k3s server nodes with a MySQL cluster backend for the datastore.</p>"},{"location":"000020136/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020137/","title":"Why does `kubectl get` show a different API group for a resource to the group originally applied in the resource specification?","text":"<p>This document (000020137) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020137/#situation","title":"Situation","text":""},{"location":"000020137/#question","title":"Question","text":"<p>The Kubernetes API group and version returned for a resource from the Kubernetes API (using for example the <code>kubectl</code> CLI) may show as different to the original group and version defined in the resource specification via the <code>apiVersion</code>. For example, when creating a Deployment resource in the v1 version of the apps API group, the output of <code>kubectl get deployment -o yaml</code> may show the Deployment resource in the v1beta1 version of the extensions API group, per the below:</p> <p>Original Deployment YAML:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\n[...]\n</code></pre> <p>Output of <code>kubectl get deployment -o yaml</code> for this Deployment resource post-creation:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\n[...]\n</code></pre> <p>This article explains the cause of this behaviour and how to ensure resources are returned by the API under a specific API group and version.</p>"},{"location":"000020137/#answer","title":"Answer","text":"<p>A Kubernetes resource type, such as Deployment, can exist within multiple API groups. Where this is the case, and no API group and version is specified in the command, <code>kubectl</code> will use the first group listed in the discovery docs published by the Kubernetes API server that you are querying. In the instance of the above example, the Deployment resource exists under boths the <code>apps/v1</code> and <code>extensions/v1beta1</code> API groups, but for backwards compatability the API server lists this first under the <code>extensions/v1beta1</code> group.</p> <p>To ensure that the resource retrieved is in a particular API group, you should fully qualify the resource type in the <code>kubectl</code> command, i.e. to query Deployment resources in the apps API group run <code>kubectl get deployments.apps -o yaml</code>. Additionally you can provide an explicit version of the API group, i.e. to query Deployment resources in the v1 version of the apps API group run <code>kubectl get deployments.v1.apps -o yaml</code>.</p>"},{"location":"000020137/#further-reading","title":"Further Reading","text":"<p>You can find a good discussion on this behaviour in the Kubernetes GitHub Issue #58131.</p> <p>The Kubernetes developer documentation on API resource versioning can be found here.</p>"},{"location":"000020137/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020138/","title":"How can I audit or examine RBAC Roles for different accounts within a Kubernetes cluster?","text":"<p>This document (000020138) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020138/#situation","title":"Situation","text":""},{"location":"000020138/#question","title":"Question","text":"<p>Access to different resources within Kubernetes is handled by role-based access control (RBAC).</p> <p>These resources are referenced by the resource name and API group, for example pods within the core/v1 Kubernetes API group or clusters within the management.cattle.io/v3 API group.</p> <p>A role can be applied (or bound) to different subjects, like a user, group or service account via role bindings, to grant varying degress of access to these resource types at a cluster or namespace level. The access a role grants on a particular resource type is defined by verbs, e.g. get, create, list, watch, delete, and patch etc.</p> <p>This article details methods by which you can audit or examine role-based access control (RBAC) roles for different accounts within a Kubernetes cluster.</p>"},{"location":"000020138/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster</li> <li>kubectl access to the cluster</li> </ul>"},{"location":"000020138/#answer","title":"Answer","text":"<p>To audit a specific account, the kubectl command can use the can-i option with the impersonation API to examine what verbs a user has access to, given a specific namespace.</p>"},{"location":"000020138/#basic-usage","title":"Basic Usage","text":"<p>Basic usage of the kubectl can-i option takes the following form:</p> <pre><code>kubectl auth can-i &lt;verb&gt; &lt;resource&gt; --as account --namespace=&lt;namespace&gt;\n</code></pre>"},{"location":"000020138/#can-my-user-perform-all-verbs-on-all-resources-am-i-an-admin","title":"Can my user perform all verbs on all resources? Am I an admin?","text":"<pre><code>kuboectl auth can-i \"*\" \"*\"\n</code></pre>"},{"location":"000020138/#can-the-helm-serviceaccount-delete-pods-in-the-current-namespace-or-cluster-wide","title":"Can the helm serviceaccount delete pods in the current namespace or cluster-wide?","text":"<pre><code>kubectl auth can-i delete pods --as helm\n</code></pre>"},{"location":"000020138/#is-user1234-an-admin-in-the-testing-namespace-can-they-perform-all-verbs-on-all-resources","title":"Is user1234 an admin in the \"testing\" namespace? Can they perform all verbs on all resources?","text":"<pre><code>kubectl auth can-i \"*\" \"*\" --namespace=testing --as user1234\n</code></pre>"},{"location":"000020138/#list-option-gives-insight-into-permissions-for-a-user-or-account","title":"List option gives insight into permissions for a user or account","text":"<pre><code>kubectl auth can-i --list --namespace=testing --as user1234\n</code></pre>"},{"location":"000020138/#additional-tools-for-querying-rbac","title":"Additional tools for querying RBAC","text":"<p>Other open-source third-party tools exist for auditing RBAC, many of which use the Krew plugin framework:</p> <ul> <li>access-matrix - output a CLI matrix of what users or roles have permissions</li> <li>rbac-lookup - perform lookups given subject queries</li> <li>who-can - see \"who-can\" perform a certain verb on a resource, like an opposite view of \"can-i\"</li> </ul> <p>Third-party tools also exist for creating visualizations of the RBAC configuration:</p> <ul> <li>RBack - parse the output from the kubectl commands as json, import into visualization in different formats</li> <li>RBAC-view - visualizing RBAC relationships via a dashboard interface</li> </ul>"},{"location":"000020138/#further-reading","title":"Further Reading","text":"<ul> <li>Offical Kubernetes RBAC documentation</li> <li>CNCF RBAC Blog post</li> <li>NCCGROUP Examples</li> <li>Krew Plugin Framework</li> <li>RBAC-View</li> <li>RBack</li> <li>who-can</li> <li>rakksess, acess-matrix plugin</li> <li>rbac-lookup</li> </ul>"},{"location":"000020138/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020139/","title":"What is the kube_config_cluster.yml file that is created after provisioning a cluster with the Rancher Kubernetes Engine (RKE) CLI?","text":"<p>This document (000020139) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020139/#situation","title":"Situation","text":""},{"location":"000020139/#question","title":"Question","text":"<p>The Rancher Kubernetes Engine (RKE) documentation references a file <code>kube_config_cluster.yml</code> that is generated after running <code>rke up</code>, this article explains what this file is and how to use it.</p>"},{"location":"000020139/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI provisioned Kubernetes cluster</li> <li>kubectl installed</li> </ul>"},{"location":"000020139/#answer","title":"Answer","text":"<p>When you provision a Kubernetes cluster using RKE, a kubeconfig file is automatically generated for your cluster.</p> <p>This file is created and saved as <code>kube_config_&lt;cluster&gt;.yml</code>, where <code>&lt;cluster&gt;</code> is the filename of your cluster configuration YAML file. This kubeconfig defines the connection and authentication details to interact with your cluster, using tools such as <code>kubectl</code>.</p> <p>By default, kubectl checks <code>~/.kube/config</code> for a kubeconfig file, but you can specify a different kubeconfig file using the --kubeconfig flag. For example:</p> <pre><code>kubectl --kubeconfig /custom/path/rke/kube_config_cluster.yml get pods\n</code></pre> <p>Or you can export the config path into the KUBECONFIG environment variable, removing the requirement to specify the --kubeconfig flag each time you run kubectl:</p> <pre><code>export KUBECONFIG=\"/custom/path/rke/kube_config_cluster.yml\"\n</code></pre>"},{"location":"000020139/#further-reading","title":"Further Reading","text":"<ul> <li> <p>RKE Documentation on the kubeconfig</p> </li> <li> <p>Kubernetes Documentation on kubeconfig files</p> </li> </ul>"},{"location":"000020139/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020140/","title":"What permissions are required for the API token when configuring the Rancher2 Terraform Provider?","text":"<p>This document (000020140) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020140/#situation","title":"Situation","text":""},{"location":"000020140/#question","title":"Question","text":"<p>When configuring the Rancher2 Terraform Provider, what permissions are required for the API token configured to authenticate with Rancher (as in the below example)?</p> <pre><code>provider \"rancher2\" {\napi_url    = \"https://rancher.my-domain.com\"\naccess_key = \"${var.rancher2_access_key}\"\nsecret_key = \"${var.rancher2_secret_key}\"\n}\n</code></pre>"},{"location":"000020140/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>The Rancher2 Terraform Provider</li> </ul>"},{"location":"000020140/#answer","title":"Answer","text":"<p>The user account for which you generate the API token, to configure the Terraform provider, will need permissions granted on any resources that you intend to configure and manage via Terraform.</p>"},{"location":"000020140/#further-reading","title":"Further Reading","text":"<p>Rancher2 Terraform Provider Documentation</p>"},{"location":"000020140/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020141/","title":"How to configure container log rotation for the Docker daemon","text":"<p>This document (000020141) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020141/#situation","title":"Situation","text":""},{"location":"000020141/#task","title":"Task","text":"<p>As the default setting on Docker is to log using the json-file log driver, without a container log limit, this can lead to disk-fill events on nodes. This article provides steps to configure any nodes running Docker to have a limited container log size and rotate out older container logs.</p>"},{"location":"000020141/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Node(s) running Docker, using the json-file log driver</li> <li>Permission to edit the <code>/etc/docker/daemon.json</code> and to restart the Docker daemon</li> </ul>"},{"location":"000020141/#warning","title":"Warning","text":"<ul> <li>You must restart the Docker daemon for the changes to take effect for newly created containers</li> <li>N.B. As the container logging configuration for pre-existing container is immutable, existing containers do not use the new logging configuration and would need to be redeployed to take on this new configuration.</li> </ul>"},{"location":"000020141/#resolution","title":"Resolution","text":"<ol> <li>Edit the Docker daemon configuration file:</li> </ol> <pre><code>$ vim /etc/docker/daemon.json\n</code></pre> <ol> <li>Add the following lines to the file, to configure a maximum container log file size of 10MB and maintain only 10 of these before deleting the oldest:</li> </ol> <pre><code>{\n     \"log-driver\": \"json-file\",\n     \"log-opts\": {\n       \"max-size\": \"10m\",\n       \"max-file\": \"10\"\n     }\n}\n</code></pre> <ol> <li>Restart the docker daemon to apply the settings to new containers (see Warnings above):</li> </ol> <pre><code>$ systemctl restart docker\n</code></pre>"},{"location":"000020141/#tips","title":"Tips","text":"<p>You could include this Docker daemon container log rotation configuration in your build/connfiguration management systems, to ensure this is automatically applied to nodes on provisioning, removing any requirement for manual configuration.</p>"},{"location":"000020141/#further-reading","title":"Further reading","text":"<p>Docker JSON file log driver documentation</p>"},{"location":"000020141/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020142/","title":"How to configure an internal Elastic Load Balancer (ELB) or Network Load Balancer (NLB) with an Istio Ingress Gateway in Rancher v2.3+","text":"<p>This document (000020142) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020142/#situation","title":"Situation","text":""},{"location":"000020142/#task","title":"Task","text":"<p>When configuring an Istio Ingress Gateway, a <code>LoadBalancer</code> type service is commonly configured to provide external access to the cluster.</p> <p>By default Kubernetes will provision an internet-facing Classic Load Balancer (CLB). The below steps provide guidance on the annotations needed to configure an internal CLB or Network Load Balancer (NLB) using private subnets.</p>"},{"location":"000020142/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.3+ managed Kubernetes cluster, runnning in AWS, with the AWS cloud provider configured</li> <li>Istio enabled in the cluster</li> <li>Tagging configured for the VPC and Subnets that will be used for the ELB or NLB</li> </ul> <p>Note: When using Load Balancers with the AWS cloud provider, it is important tag the private and public subnets in the VPC so that kube-controller-manager can correctly discover the specific subnets intended for use.</p> <p>For example the <code>kubernetes.io/role/internal-elb</code> and <code>kubernetes.io/role/elb</code> keys configured respectively, with the value of <code>1</code>.</p>"},{"location":"000020142/#steps","title":"Steps","text":""},{"location":"000020142/#enable-the-istio-ingress-gateway","title":"Enable the Istio Ingress Gateway","text":"<p>If the not already enabled, enable the Istio Ingress Gateway. In the drop down list for 'Service Type of Ingress Gateway', select <code>LoadBalancer</code>.</p>"},{"location":"000020142/#use-an-internal-load-balancer","title":"Use an internal Load Balancer","text":"<p>When editing the Istio Ingress Gateway, click the drop down for Custom Answers.</p> <p>Paste the below in the Variable field, this will automatically populate the value:</p> <pre><code>gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-internal\" = \"true\"\n</code></pre>"},{"location":"000020142/#use-an-nlb","title":"Use an NLB","text":"<p>To use an NLB, click 'Add Answer' and paste the below in the Variable field:</p> <pre><code>gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-type\" = nlb\n</code></pre> <p>Note: An NLB can be used as an internet-facing loadbancer by using only the above annotation, without adding the aws-load-balancer-internal annotation.</p>"},{"location":"000020142/#references","title":"References","text":"<p>Istio install options documentation</p> <p>Kubernetes load balancer documentation</p>"},{"location":"000020142/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020143/","title":"How to set server-tokens to false, to disable the the NGINX header in ingress-nginx responses, within a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster","text":"<p>This document (000020143) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020143/#situation","title":"Situation","text":""},{"location":"000020143/#task","title":"Task","text":"<p>The ingress-nginx server-tokens option controls display of the NGINX server header, including version information, in the response to ingress requests. By default this header is enabled; however, due to security concerns in exposing version information, a user might want to disable this on the nginx-ingress-controllers of their Kubernetes cluster(s). This article details how to disable the header, via the server-tokens option, in Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters.</p>"},{"location":"000020143/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"000020143/#resolution","title":"Resolution","text":""},{"location":"000020143/#rke-provisioned-clusters","title":"RKE provisioned clusters","text":"<ol> <li>Add the <code>server-tokens: \"false\"</code> option for nginx into the cluster configuration YAML file as follows:</li> </ol> <pre><code>ingress:\n       provider: nginx\n       options:\n         server-tokens: \"false\"\n</code></pre> <p>Example:</p> <pre><code>nodes:\n  - address: x.x.x.x\n    internal_address: x.x.x.x\n    user: ubuntu\n    role: [controlplane,worker,etcd]\ningress:\n    provider: nginx\n    options:\n      server-tokens: \"false\"\nservices:\netcd:\n    snapshot: true\n    creation: 6h\n    retention: 24h\n</code></pre> <ol> <li>Execute <code>rke up</code> to update the cluster with the new configuration. N.B. Ensure the <code>.rkestate</code> file for the cluster is present in the working directory when invoking <code>rke up</code> per the documentation here:</li> </ol> <pre><code>rke up --config &lt;cluster configuration YAML file&gt;\n</code></pre>"},{"location":"000020143/#rancher-v2x-provisioned-clusters","title":"Rancher v2.x provisioned clusters","text":"<ol> <li>Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'.</li> <li>Click 'Edit as YAML'.</li> <li>Add the <code>server-tokens: \"false\"</code> option for nginx into the cluster configuration YAML file as follows:</li> </ol> <pre><code>rancher_kubernetes_engine_config:\n[...]\n     ingress:\n         provider: nginx\n         options:\n           server-tokens: \"false\"\n</code></pre> <ol> <li>Click 'Save' to update the cluster with the new configuration.</li> </ol>"},{"location":"000020143/#further-reading","title":"Further reading","text":"<p>ingress-nginx documentation on the server-tokens options</p>"},{"location":"000020143/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020144/","title":"How to enable debug level logging for the Rancher Cluster/Project Alerting Alertmanager instance, in a Rancher v2.x managed cluster?","text":"<p>This document (000020144) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020144/#situation","title":"Situation","text":""},{"location":"000020144/#task","title":"Task","text":"<p>This article details how to enable debug level logging on the Alertmanager instance in a Rancher v2.x managed Kubernetes cluster, which may assist when troubleshooting cluster or project alerting.</p>"},{"location":"000020144/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster</li> <li>Cluster or project alerting configured</li> </ul>"},{"location":"000020144/#resolution","title":"Resolution","text":"<ol> <li>Within the Rancher UI navigate to the System Project of the relevant cluster and click on the Apps view.</li> <li>Click 'Upgrade' on the cluster-alerting app.</li> <li>In the Answers section click 'Add Answer' and add the variable <code>alertmanager.logLevel</code> with a value of <code>debug</code>.</li> <li>Click upgrade to save the change and update the Alertmanager instance with the debug log level.</li> <li>Navigate to the cattle-prometheus namespace within the System Project for the cluster, and view the logs of the alertmanager-cluster-alerting-0 Pod running for the alertmanager-cluster-alerting StatefulSet. You should see <code>level=debug</code> log messages, such as in the following example, confirming debug level logging has been successfully configured:</li> </ol> <p><code>plaintext     level=debug  ts=2019-07-09T15:03:37.511451301Z caller=dispatch.go:104  component=dispatcher msg=\"Received alert\" alert=[433a194][active]     level=debug  ts=2019-07-09T15:03:38.511774835Z caller=dispatch.go:430  component=dispatcher  aggrGroup=\"{}/{group_id=\\\"c-5h85q:event-alert\\\"}/{rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\"}:{event_message=\\\"Scaled  up replica set mynginx2-7994cd84ff to 1\\\",  resource_kind=\\\"Deployment\\\",  rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\",  target_name=\\\"mynginx2\\\", target_namespace=\\\"default\\\"}\" msg=flushing  alerts=[[433a194][active]]</code></p>"},{"location":"000020144/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020145/","title":"How to generate a Longhorn Support Bundle","text":"<p>This document (000020145) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020145/#situation","title":"Situation","text":""},{"location":"000020145/#task","title":"Task","text":"<p>When troubleshooting an issue with Longhorn, Rancher Support may request a Longhorn Support Bundle, which can be generated via the Longhorn UI, and contains system information and logs.</p>"},{"location":"000020145/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster with Longhorn deployed</li> </ul>"},{"location":"000020145/#steps","title":"Steps","text":""},{"location":"000020145/#generate-the-support-bundle","title":"Generate the Support Bundle","text":"<ol> <li>Log in into the Rancher UI.</li> <li>Select the cluster with Longhorn depoyed.</li> <li>Select the Project where Longhorn is deployed (typically under the System project).</li> <li>Click on \"Apps\" button.</li> <li> <p>Find the Longhorn system app and click on the index.html button</p> </li> <li> <p>Click on the \"Generate Support Bundle\" in the bottom left of the screen</p> </li> <li> <p>Type in a description and click generate (issue url is optional)</p> </li> </ol>"},{"location":"000020145/#upload-the-support-bundle","title":"Upload the Support Bundle","text":"<p>Generally Longhorn Support Bundles files are small in size; however, if the pack is too large to upload directly to the ticket, please request a temporary upload location.</p>"},{"location":"000020145/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020146/","title":"How to add additional scrape configs to a Rancher cluster or project monitoring prometheus","text":"<p>This document (000020146) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020146/#situation","title":"Situation","text":""},{"location":"000020146/#task","title":"Task","text":"<p>The Rancher cluster and project monitoring tools, allow you to monitor cluster components and nodes, as well as workloads and custom metrics from any HTTP or TCP/UDP metrics endpoint that these workloads expose.</p> <p>This article will detail how to manually define additional scrape configs for either the cluster or project monitoring prometheus instance, where you want to scrape other metrics.</p> <p>Whether to define the additional scrape config at the cluster or project level would depend on the desired scope for the metrics and possible alerts. If you wish to scope the metrics scraped, and thus possible alerts configured for these metrics, to a project, you could configure the additional scrape config at the project monitoring level. If you wish to scope the metrics at the cluster level, so only those with cluster admin access could see the metrics or configure alerts, you could configure the additional scrape config at the cluster monitoring level.</p>"},{"location":"000020146/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.2.x, v2.3.x or v2.4.x managed cluster, with cluster monitoring enabled (and optionally project monitoring enabled, if you wish to configure the additonal scrape config at the project scope).</li> </ul>"},{"location":"000020146/#resolution","title":"Resolution","text":"<p>For both cluster and project monitoring the additional scrape config(s) are defined in the Answers section of the Monitoring configuration. This can be found as follows:</p> <ul> <li>Cluster Monitoring: As a user with permissions to edit cluster monitoring (global admins and cluster owners by default), navigate to the cluster view and click Tools -&gt; Monitoring from the menu bar. Click 'Show advanced options' at the bottom right.</li> <li>Project Monitoring: As a user with permissions to edit project monitoring (global admins, cluster owners and project owners by default), navigate to the project and click Tools -&gt; Monitoring from the menu bar. Click 'Show advanced options' at the bottom right.</li> </ul> <p>You can add an array of prometheus.additionalScrapeConfigs in the Answers section here.</p> <p>For example to define a scrape job of the following:</p> <pre><code> - job_name: \"prometheus\"\nstatic_configs:\n   - targets:\n     - \"localhost:9090\"\n</code></pre> <p>You would add the following two definitions to the Answers section:</p> <p>prometheus.additionalScrapeConfigs[0].job_name = prometheus prometheus.additionalScrapeConfigs[0].static_configs[0].targets[0] = localhost:9090</p> <p>After adding the answers, click 'Save' and you should now be able to view the target and its status within the Prometheus UI under Status -&gt; Targets.</p>"},{"location":"000020146/#further-reading","title":"Further reading","text":"<p>Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here.</p>"},{"location":"000020146/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020147/","title":"How to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters","text":"<p>This document (000020147) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020147/#situation","title":"Situation","text":""},{"location":"000020147/#task","title":"Task","text":"<p>This article details how to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"000020147/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Enginer (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced.</li> <li>For Rancher v.2x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"000020147/#resolution","title":"Resolution","text":""},{"location":"000020147/#configuration-for-rke-provisioned-clusters","title":"Configuration for RKE provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>enable-ssl-passthrough: true</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     extra_args:\n       enable-ssl-passthrough: true\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Recycle the nginx pods in-order to pick up new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <ol> <li>Verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo \"Good\" || echo \"Bad\"; done\n</code></pre> <ol> <li>Edit the ingress to include the new annotations:</li> </ol> <pre><code>kubectl -n default edit ingress hello-world-lb\n</code></pre> <p>Example:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n     annotations:\n       nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n     name: hello-world-lb\n     namespace: default\n</code></pre>"},{"location":"000020147/#configuration-for-rancher-provisioned-clusters","title":"Configuration for Rancher provisioned clusters","text":"<ol> <li>Login into the Rancher UI.</li> <li> <p>Go to Global -&gt; Clusters -&gt; &lt;&gt;.</p> </li> <li> <p>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</p> </li> <li>Click \"Edit as YAML\".</li> <li> <p>Enclude the <code>enable-ssl-passthrough: true</code> option for the ingress, as follows:</p> <p><code>yaml ingress:   provider: nginx   extra_args:     enable-ssl-passthrough: true</code></p> </li> <li> <p>Click \"Save\" at the bottom of the page.</p> </li> <li> <p>Wait for cluster to finish upgrading.</p> </li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li> <p>Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument:</p> <p><code>bash for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done</code></p> <p>9. Run the following inside the kubectl CLI to verify the new argument:</p> <p><code>bash for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo \"Good\" || echo \"Bad\"; done</code></p> </li> <li> <p>Browse to the ingress in question and click edit.</p> </li> <li> <p>Expand \"Labels &amp; Annotations\".</p> </li> <li>Click \"Add annotation\" and add <code>nginx.ingress.kubernetes.io/ssl-passthrough=true</code> under \"Annotations\".</li> <li>Click \"Save\".</li> </ol>"},{"location":"000020147/#verification-steps","title":"Verification Steps","text":"<p>Run the following command to verify new certificate:</p> <pre><code>```bash\ncurl --insecure -v https://&lt;&lt;APP URL&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n```\n\n</code></pre> <p>Example output:</p> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host lab.rancher.tools left intact\n</code></pre> <p>N.B. Some browsers will cache the certificate, as a result you might need to close and re-open the browser in order to get the new certificate. How to clear the SSL state in a browser.</p>"},{"location":"000020147/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020148/","title":"Istio fails to deploy with restricted PodSecurityPolicy in Rancher v2.3 and v2.4","text":"<p>This document (000020148) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020148/#situation","title":"Situation","text":""},{"location":"000020148/#issue","title":"Issue","text":"<p>Attempting to enable Istio in a Rancher v2.3 or v2.4 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the istio-galley, istio-pilot, istio-policy, istio-sidecar-injector and istio-telemtry Deployments in a CrashLoopBackOff, with log messages of the formats below:</p> <pre><code>fatal   validation  admission webhook ListenAndServeTLS failed: listen tcp :443: bind: permission denied\n</code></pre> <p>or</p> <pre><code>nginx: [emerg] chown(\"/tmp/nginx\", 101) failed (1: Operation not permitted)\n</code></pre> <p>In addition in namespaces with Istio sidecar auto injection enabled, an error of the following format will show for Pods upon scheduling:</p> <pre><code>Pods \"nginx-7f4c54479d-\" is forbidden: unable to validate against any pod security policy: [spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_ADMIN\": capability may not be added spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_RAW\": capability may not be added]\n</code></pre> <p>This is a result of the system capabilities required by the Istio system components ( <code>CHOWN</code> and <code>NET_BIND_SERVICE</code>), as well as the Istio sidecar containers ( <code>NET_ADMIN</code> and <code>NET_RAW</code>), in the default Istio configuration and which are blocked by the restricted PSP.</p>"},{"location":"000020148/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.3.x or v2.4.x with a restricted PSP configured as the default and Istio enabled</li> </ul>"},{"location":"000020148/#resolution","title":"Resolution","text":"<p>The steps to configure Istio in a cluster with restrictive Pod Security Policies enabled can be found in the Rancher documentation \"Enable Istio with Pod Security Policies\".</p>"},{"location":"000020148/#futher-reading","title":"Futher Reading","text":"<ul> <li> <p>Rancher Documentation on Istio</p> </li> <li> <p>Kubernetes Documentation on PSP Capabilities</p> </li> <li> <p>Istio Requirements Documentation</p> </li> </ul>"},{"location":"000020148/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020149/","title":"Resolving conntrack table full error messages: 'nf_conntrack: table full, dropping packets'","text":"<p>This document (000020149) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020149/#situation","title":"Situation","text":""},{"location":"000020149/#issue","title":"Issue","text":"<p>When investigating a connectivity issue, you may experience errors like the below in the system logs:</p> <pre><code>nf_conntrack: table full, dropping packets\n</code></pre> <p>This error indicates the connection tracking table size has been exhausted. This can manifest with different symptoms, such as intermittent or consistent network timeouts.</p> <p>The conntrack table keeps state on open connections that the kernel is translating. This occurs often in a Kubernetes cluster when pods access an external endpoint, or another service within the cluster. These scenarios use NAT and stateful firewall rules which are maintained as entries in the conntrack table.</p>"},{"location":"000020149/#investigation","title":"Investigation","text":"<p>By default, the table size is calculated based on the memory allocated to the node. This does not fit all workloads demands, for example in a microservice environment typically a higher number of inter-service connections could be expected without consuming a high amount of memory.</p> <p>To output the current max table size:</p> <pre><code>cat /proc/sys/net/netfilter/nf_conntrack_max\n</code></pre> <p>To get a point in time count of the current entries in the table:</p> <pre><code>cat /proc/sys/net/netfilter/nf_conntrack_count\n</code></pre> <p>Note: With the <code>conntrack</code> package installed, you can also use <code>conntrack -C</code></p> <p>If the <code>nf_conntrack_count</code> and <code>nf_conntrack_max</code> are close, it is indicating that the current workload requires a larger table size.</p> <p>If the current number of entries are not approaching the table size, this could indicate that a burst of workload was experienced historically, in a containerized environment this can be common. For example, if the high-traffic Pods may now running on different nodes.</p>"},{"location":"000020149/#resolution","title":"Resolution","text":"<p>Increasing the conntrack table size is achieved with <code>sysctl</code>.</p> <p>Calculate a higher value, this can be applied to the node immediately with:</p> <pre><code>sysctl -w net.netfilter.nf_conntrack_max=&lt;value&gt;\n</code></pre> <p>To persist through reboot, add the tunable to either <code>/etc/sysctl.conf</code>, or a specific config file in <code>/etc/sysctl.d</code>.</p> <p>For example, if your Linux distribution follows the /etc/sysctl.d/ directory structure:</p> <pre><code>echo \"net.netfilter.nf_conntrack_max=&lt;value&gt;\" &gt; /etc/sysctl.d/10-conntrack-max.conf\nsysctl -p /etc/sysctl.d/10-conntrack-max.conf\n</code></pre> <p>This creates a new config file to set the table size at each boot.</p> <p>Additionally, if you configure nodes with configuration management, UserData, or build custom images etc., you may wish to add this to your usual approach to configure this for future nodes.</p>"},{"location":"000020149/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020150/","title":"Does SUSE Rancher Hosted provide downstream clusters?","text":"<p>This document (000020150) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020150/#resolution","title":"Resolution","text":"<p>No, currently our SUSE Rancher Hosted service only includes the Rancher multi-cluster management software. Downstream clusters are not provided as part of the service. You will either need to use Rancher to provision Kubernetes clusters on-premise or in a cloud provider account that you own. You can also import existing Kubernetes clusters into SUSE Rancher Hosted.</p>"},{"location":"000020150/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020152/","title":"Updating SSL cert in Rancher v2.x with the same CA","text":"<p>This document (000020152) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020152/#situation","title":"Situation","text":""},{"location":"000020152/#task","title":"Task","text":"<p>How do I renew my SSL/TLS certificate for Rancher?</p>"},{"location":"000020152/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> <li>Rancher on a Kubernetes Cluster see documentation for more information</li> <li>The new certificate must have the same root CA as the current certificate.</li> <li>Used the option \"Bring your own certificate\" when installing Rancher Doc</li> <li>A copy of the certificate and private key in Base64 format Doc</li> <li>A copy of the root and intermediate CA certificate (Sometimes called the certificate chain).</li> </ul>"},{"location":"000020152/#assumptions","title":"Assumptions","text":"<ul> <li>kubectl access to the Rancher local cluster</li> <li>The certificate is stored as server.crt</li> <li>The private key is stored as tls.key</li> <li>The root CA is stored as root-ca.crt</li> <li>The intermediate CA is stored as intermediate-ca.crt</li> </ul>"},{"location":"000020152/#resolution","title":"Resolution","text":""},{"location":"000020152/#install-steps","title":"Install Steps","text":"<ol> <li>Verify private key doesn't have a passphrase using the command listed below. If the following command asks for a passphrase then it is password protected and this must be removed.</li> </ol> <pre><code>openssl rsa -in tls.key -noout\n</code></pre> <ol> <li>Remove the passphrase (skip this step if the previous command didn't ask for a passphrase):</li> </ol> <pre><code>mv tls.key tls-pass.key\nopenssl rsa -in tls-pass.key -out tls.key\nEnter your passphrase here\n</code></pre> <ol> <li>Create the certificate chain. If you have additional intermediate certs please add them at this step.</li> </ol> <p>NB: Order is important!</p> <pre><code>cat server.crt intermediate-ca.crt root-ca.crt &gt; tls.crt\n</code></pre> <ol> <li>Backup the current certificate:</li> </ol> <pre><code>kubectl -n cattle-system get secret tls-rancher-ingress -o yaml &gt; tls-rancher-ingress-bk.yaml\n</code></pre> <ol> <li>Remove the current certificate:</li> </ol> <pre><code>kubectl -n cattle-system delete secret tls-rancher-ingress\n</code></pre> <ol> <li>Install the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n   --cert=tls.crt \\\n   --key=tls.key\n</code></pre>"},{"location":"000020152/#verification-steps","title":"Verification Steps","text":"<ul> <li>Run the following command to verify the new certificate. (Replace Rancher with your Rancher URL):</li> </ul> <pre><code>curl --insecure -v https://&lt;&lt;Rancher&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n</code></pre> <ul> <li>Example output:</li> </ul> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host lab.rancher.tools left intact\n</code></pre> <ul> <li>NOTE: Some browsers will cache the certificate. So you might to close the browser and reopen in order to get the new certificate. How to clear the SSL state in a browser.</li> </ul>"},{"location":"000020152/#rollback-steps","title":"Rollback Steps","text":"<ol> <li>Backup the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system get secret tls-rancher-ingress -o yaml &gt; tls-rancher-ingress-new.yaml\n</code></pre> <ol> <li>Remove the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system delete secret tls-rancher-ingress\n</code></pre> <ol> <li>Re-install the old certificate:</li> </ol> <pre><code>kubectl -n cattle-system apply -f tls-rancher-ingress-bk.yaml\n</code></pre>"},{"location":"000020152/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020154/","title":"How to troubleshoot SNI enabled endpoints with curl and openssl","text":"<p>This document (000020154) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020154/#situation","title":"Situation","text":""},{"location":"000020154/#issue","title":"Issue","text":"<p>A modern webserver hosting or proxying to multiple backend domain names will often be configured to use SNI (Server Name Indication).</p> <p>SNI allows multiple SSL-protected domains to be hosted on the same IP address, and is commonly used in Kubernetes with ingress controllers, for example, the nginx ingress controller.</p> <p>As the SNI extension requires a slight change to the conversation between client and server - the hostname must be provided in the <code>Hello</code> message to correctly access the associated domain name.</p> <p>This can present an issue when troubleshooting a node or pod directly, where an IP address is used.</p>"},{"location":"000020154/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>The <code>curl</code> and/or <code>openssl</code> command installed</li> <li>Network access to the endpoint you wish to troubleshoot</li> </ul>"},{"location":"000020154/#steps","title":"Steps","text":"<p>To perform an SNI-compliant request using an IP address, use the following commands replacing the domain name and IP address.</p> <ul> <li>Using the <code>curl</code> command:</li> </ul> <pre><code>curl -v --resolve domain.com:443:&lt;ip address&gt; https://domain.com\n</code></pre> <ul> <li>Using <code>openssl</code> can be useful to obtain details about the certificate configured:</li> </ul> <pre><code>openssl s_client -showcerts -servername domain.com -connect &lt;ip address&gt;:443\n</code></pre>"},{"location":"000020154/#further-reading","title":"Further reading","text":"<p>More information on SNI can be found here.</p>"},{"location":"000020154/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020155/","title":"What is SUSE Rancher Hosted?","text":"<p>This document (000020155) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020155/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is a software-as-a-service offering from SUSE. As the name implies, Rancher is completely hosted for you in the cloud. SUSE takes care of the installation, upgrade, and day-to-day operations of your Rancher control plane. Using Rancher, you can then add your own cloud-based, on-premise, AKS, GKE, or EKS clusters. For more information, see the Rancher v2.4 announcement, blog announcement, or SUSE Rancher Hosted product page .</p>"},{"location":"000020155/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020156/","title":"Can I move an existing Kubernetes cluster to SUSE Rancher Hosted?","text":"<p>This document (000020156) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020156/#resolution","title":"Resolution","text":"<p>Existing Kubernetes clusters can be imported into SUSE Rancher Hosted. However, you cannot currently move a cluster that is already managed by Rancher to SUSE Rancher Hosted. We are currently looking to enhance our management capabilities to allow users to move clusters between Rancher clusters. See GitHub issue 16471. As a workaround, you can redeploy workloads running in an existing cluster over to a new SUSE Rancher Hosted managed cluster.</p>"},{"location":"000020156/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020157/","title":"Where are the links for Prometheus and Grafana in Rancher v2.x Cluster and Project Monitoring?","text":"<p>This document (000020157) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020157/#situation","title":"Situation","text":""},{"location":"000020157/#question","title":"Question","text":"<p>Using Rancher v2.x, from v2.2.4 and above, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through the use of the built-in Rancher cluster and project monitoring. Rancher monitoring deploys the open-source Grafana and Prometheus projects, and this article details how to access the UI for these components, so you can view the monitoring dashboards and query metrics.</p>"},{"location":"000020157/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster managed by Rancher v2.x, from v2.2.4 and above</li> <li>Cluster monitoring enabled: Go to your Cluster --&gt; Tools --&gt; Monitoring --&gt; Enable</li> <li>(Optionally) Project monitoring enabled: Go to your Project --&gt; Tools --&gt; Monitoring --&gt; Enable</li> </ul>"},{"location":"000020157/#answer","title":"Answer","text":""},{"location":"000020157/#cluster-level","title":"Cluster level","text":""},{"location":"000020157/#grafana","title":"Grafana","text":"<p>To access the Grafana UI for Rancher cluster monitoring, from the main dashboard for your cluster, click the three-dot option menu in the top-right and click the button \"Go to Grafana\".</p> <p></p> <p>You should also see Grafana logos next to the system components on the main dashboard for your cluster. Click on any of them to take you to the Grafana dashboard for that particular component.</p> <p></p>"},{"location":"000020157/#prometheus","title":"Prometheus","text":"<p>To access the Prometheus UI for the cluster monitoring, navigate to the Apps page in the System project for your cluster. You will see an app called \"cluster-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus.</p> <p></p>"},{"location":"000020157/#project-level","title":"Project level","text":""},{"location":"000020157/#grafana-and-prometheus","title":"Grafana and Prometheus","text":"<p>To access the Grafana and Prometheus UIs, for a project with project monitoring enabled, navigate to the Apps page within the project. You will see an app called \"project-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus.</p> <p></p>"},{"location":"000020157/#further-reading","title":"Further Reading","text":"<p>Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here.</p>"},{"location":"000020157/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020158/","title":"How to use the calicoctl CLI in an RKE or Rancher provisioned Kubernetes cluster","text":"<p>This document (000020158) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020158/#situation","title":"Situation","text":""},{"location":"000020158/#task","title":"Task","text":"<p>The <code>calicoctl</code> CLI provides an interface for managing calico network and security policy.</p> <p>In Kubernetes clusters provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, and which use the Calico or Canal Container Networking Interface (CNI) Plugin, <code>calicoctl</code> can be used to configure Calico GlobalNetworkPolicy and NetworkPolicy resources.</p>"},{"location":"000020158/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned with Rancher Kubernetes Engine (RKE) v0.x.x or v1.x.x, or Rancher v2.x.x</li> <li>The Calico or Canal Container Networking Interface (CNI) Plugin (Canal is the default in both RKE and Rancher provisioned clusters).</li> <li>A cluster-admin level kube config sourced via $KUBECONFIG on a host running Docker</li> </ul>"},{"location":"000020158/#resolution","title":"Resolution","text":"<p>N.B. The commands in this section should be run from a host running Docker, with a cluster-admin level kube config sourced.</p> <p>For the purpose of this example, we will demonstrate creating an empty GlobalNetworkPolicy resource via <code>calicoctl</code>.</p>"},{"location":"000020158/#set-kubeconfig-environment-variable-to-the-cluster-admin-kube-config","title":"Set $KUBECONFIG environment variable to the cluster-admin kube config","text":"<p>With the cluster-admin level kube config file present on the host, execute <code>export KUBECONFIG=&lt;full path to cluster-admin kube config&gt;</code> replacing with the full path of the kube config.</p>"},{"location":"000020158/#create-the-desired-resource-in-the-working-directory","title":"Create the desired resource in the working directory","text":"<p>Create a YAML file in the working directory with the NetworkPolicy resource definition(s) you want to apply to the cluster.</p> <p>For this example create a file named <code>globalpolicy.yaml</code> in the working directory with the following contents:</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: GlobalNetworkPolicy\nmetadata:\nname: allow-tcp-port-6379\n</code></pre>"},{"location":"000020158/#determine-the-calico-node-version-of-the-cluster","title":"Determine the calico-node version of the cluster","text":"<p>First get the version of the <code>calico-node</code> container running in the cluster.</p> <p>In a cluster with the Canal CNI Network Provider, run the following, with the admin kube config sourced:</p> <pre><code>CALICOVERSION=`kubectl -n kube-system get daemonset canal -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3`\necho $CALICOVERSION\n</code></pre> <p>In a cluster with the Calico CNI Network Provider, run the following, with the admin kube config sourced:</p> <pre><code>CALICOVERSION=`kubectl -n kube-system get daemonset calico-node -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3`\necho $CALICOVERSION\n</code></pre>"},{"location":"000020158/#run-calicoctl","title":"Run <code>calicoctl</code>","text":"<p>With the <code>calico-node</code> version determined and now set in the variable <code>$CALICOVERSION</code>, <code>calicoctl</code> can be invoked. This is done by running the <code>calico/ctl</code> image, with the version matching the <code>calico-node</code>. The kube config file is mounted into the container, as is the present working directory (at the path <code>/host</code>), so that the desired resource (in this example in the file globalpolicy.yaml) is available.</p> <p>To execute <code>calicoctl</code> run the following command, altering the filename as applicable to the resource you have created in the working directory:</p> <pre><code>docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION apply -f /host/globalpolicy.yaml\n</code></pre> <p>We can now view the GlobalNetworkPolicy resource by using <code>calicoctl get</code> as follows:</p> <pre><code>docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION get globalnetworkpolicy allow-tcp-port-6379 -o yaml\n</code></pre> <p>This should return output similar to the following:</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: GlobalNetworkPolicy\nmetadata:\ncreationTimestamp: \"2020-04-08T15:12:45Z\"\nname: allow-tcp-port-6379\nresourceVersion: \"9033\"\nuid: df2875a6-1142-4fe0-9f0c-5dc1372bd2c5\nspec:\ntypes:\n  - Ingress\n</code></pre>"},{"location":"000020158/#further-reading","title":"Further reading","text":"<ul> <li>Calico's Get started with Calico network policy.</li> <li>The <code>calicoctl</code> user reference documentation.</li> </ul>"},{"location":"000020158/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020159/","title":"How to switch from private mirror to bundled system-charts in Rancher v2.3 and v2.4","text":"<p>This document (000020159) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020159/#situation","title":"Situation","text":""},{"location":"000020159/#task","title":"Task","text":"<p>The Rancher system-charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS.</p> <p>In air gapped installations prior to Rancher v2.3.0 it was necessary to host a private mirror of this repository. However, since v2.3.0 a copy of these charts has been bundled with the Rancher image.</p> <p>This article outlines how to switch from using a private mirror to the bundled system-charts that are included in the Rancher v2.3 and v2.4 images, removing the requirement to host a private mirror.</p>"},{"location":"000020159/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.3.x or v2.4.x instance, provisioned as a Highly Available (HA) install on a Kubernetes cluster, or running as a Single Node install using Docker.</li> </ul>"},{"location":"000020159/#resolution","title":"Resolution","text":""},{"location":"000020159/#rancher-ha-install","title":"Rancher HA Install","text":"<p>For a Rancher HA install this follows the same steps as the upgrade documentation.</p> <ol> <li>Get your current helm deployment values with <code>helm get values rancher</code>. Example output:</li> </ol> <pre><code>helm get values rancher\n\nhostname: rancher.my.org\n</code></pre> <ol> <li>Append <code>--set useBundledSystemChart=true</code> to your values, set the <code>--version</code> value to your current Rancher version, e.g. 2.4.2, (to pin the version and prevent an actual upgrade) and run the <code>helm upgrade</code> command. Example:</li> </ol> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n   --namespace cattle-system \\\n   --set hostname=rancher.my.org \\\n   --set useBundledSystemChart=true \\\n   --version 2.4.2\n</code></pre> <p>At this point your HA Rancher should be using the bundled charts for the system-charts.</p>"},{"location":"000020159/#rancher-single-node-install-using-docker","title":"Rancher Single Node Install Using Docker","text":"<p>To accomplish this for single node installations using Docker, you will be re-creating the Rancher container, with its current data, to add the <code>CATTLE_SYSTEM_CATALOG=bundled</code> environment variable. This closely follows the upgrade documentation.</p> <ol> <li>Create a copy of the data from your Rancher server container. <code>&lt;RANCHER_CONTAINER_NAME&gt;</code> is the name of your container as shown with <code>docker ps</code> and the <code>&lt;RANCHER_CONTAINER_TAG&gt;</code> is the version of Rancher ( <code>v2.3.0</code> for example):</li> </ol> <pre><code>docker stop &lt;RANCHER_CONTAINER_NAME&gt;\ndocker create --volumes-from &lt;RANCHER_CONTAINER_NAME&gt; --name rancher-data rancher/rancher:&lt;RANCHER_CONTAINER_TAG&gt;\n</code></pre> <ol> <li>Create a backup tarball:</li> </ol> <pre><code>docker run --volumes-from rancher-data -v $PWD:/backup busybox tar zcvf /backup rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;.tar.gz /var/lib/rancher\n</code></pre> <ol> <li>Start a new Rancher container with the added environment variable. The important thing to note here is that you use all of the same flags as when you initially started Rancher and append <code>-e CATTLE_SYSTEM_CATALOG=bundled</code> before the Rancher image. Example:</li> </ol> <pre><code>docker run -d --volumes-from rancher-data \\\n   --restart=unless-stopped \\\n   -p 80:80 -p 443:443 \\\n   -e CATTLE_SYSTEM_CATALOG=bundled \\\nrancher/rancher:&lt;RANCHER_VERSION_TAG&gt;\n</code></pre> <p>At this point your single node Rancher installation using Docker should be using the bundled charts for the system-charts.</p>"},{"location":"000020159/#further-reading","title":"Further reading","text":"<p>Documentation on Setting up Local System Charts for Air Gapped Installations</p>"},{"location":"000020159/#documentation-for-rancher-ha-installs","title":"Documentation for Rancher HA Installs","text":"<p>Rancher HA Upgrade Documentation</p> <p>Rancher HA Helm Chart Options Documentation</p>"},{"location":"000020159/#documentation-for-single-node-installs","title":"Documentation for Single Node Installs","text":"<p>Rancher Single Node Upgrade Documentation</p>"},{"location":"000020159/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020160/","title":"How to run multiple ingress controllers","text":"<p>This document (000020160) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020160/#situation","title":"Situation","text":""},{"location":"000020160/#why-use-multiple-ingress-controllers","title":"Why use multiple ingress controllers?","text":"<p>At large numbers of ingresses and related workloads, a single ingress-controller can be a bottleneck in both throughput and reliability. It is recommended to shard ingresses across multiple ingress controllers in these scenarios.</p>"},{"location":"000020160/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster created by Rancher v2.x or RKE</li> <li>A Linux cluster, Windows is currently not supported</li> <li>Helm installed and configured</li> </ul>"},{"location":"000020160/#overview","title":"Overview","text":"<p>At a high level, the process for sharding ingresses is to build out one or more extra ingress controllers and logically separate your ingresses to evenly split the load between your ingress controllers. This separation is handled through annotations on the ingresses. When an nginx-ingress-controller pod starts up with an ingressClass set, it will only try to satisfy ingresses that are annotated with the same ingressClass. This allows you to run as many ingress-controllers as needed to satisfy your ingress needs.</p>"},{"location":"000020160/#creating-extra-nginx-ingress-controller-charts","title":"Creating extra nginx-ingress-controller charts","text":"<p>It is recommended to use the community nginx-ingress helm chart to install the extra ingress-controllers with NodePort services.</p> <p>This deployment method allows you to run multiple ingress controllers on a single node, as there are no conflicting ports. You are required to route traffic to the correct ingress controller ports through an external load balancer.</p> <p>Deploy a second default backend and ingress-controller from the nginx-ingress helm chart with the following values: <code>controller.ingressClass</code> - unique name of the ingress class, such as <code>ingress-nginx-2</code> <code>controller.service.type=NodePort</code></p> <p><code>controller.service.nodePorts.http</code> - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned</p> <p><code>controller.service.nodePorts.https</code> - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned</p> <p><code>controller.kind=DaemonSet</code></p> <p>For more configuration options, see the chart readme.</p> <p>An example daemonset install would be:</p> <pre><code>helm repo add stable https://kubernetes-charts.storage.googleapis.com\nhelm install nginx-ingress-second -n ingress-nginx stable/nginx-ingress --set controller.ingressClass=\"ingress-class-2\" --set controller.service.type=NodePort --set controller.kind=DaemonSet\n</code></pre> <p>This will create an ingress-nginx daemonset and service. This ingress controller will handle any ingress routed to it tagged with the annotation <code>kubernetes.io/ingress.class: ingress-class-2</code></p>"},{"location":"000020160/#sharding-ingresses","title":"Sharding Ingresses","text":"<p>It is recommended to shard (split) your ingresses in a way that evenly splits load and configuration size between ingress controllers.</p> <p>Sharding in this way does mean changing dns and ingress hosts so that traffic for ingresses is sent to the correct ingress controllers, typically through an external load balancer.</p> <p>The process for sharding ingresses is to tag each ingress with the ingressClass for the ingress controller you want to route them through. For example:</p> <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\nname: app_1_ingress\nannotations:\n    kubernetes.io/ingress.class: \"ingress-class-2\"\nspec:\n</code></pre> <p>Once annotated with an ingressClass, these ingresses are now only handled by the ingress-controller that has that ingressClass.</p> <p>In the default configuration, the Rancher-provided nginx-ingress-controller will only handle ingresses that either have the default ingress.class annotation of <code>nginx</code> or do not have an ingress.class annotation at all.</p>"},{"location":"000020160/#next-steps","title":"Next steps","text":"<p>From here it is just a matter of ensuring that the traffic for each ingress is routed to the correct nodePort on the nodes that the daemonset is targeted against.</p> <p>If you did not specify a nodePort when deploying the chart, you can determine the nodePort that was assigned by checking the service created:</p> <pre><code>$ kubectl describe svc -n ingress-nginx nginx-ingress-second\nName:                     nginx-ingress-second-controller\nNamespace:                ingress-nginx\nLabels:                   app=nginx-ingress\n                          chart=nginx-ingress-1.35.0\n                          component=controller\n                          heritage=Helm\n                          release=nginx-ingress-second\nAnnotations:              field.cattle.io/publicEndpoints:\n                            [{\"addresses\":[\"13.210.157.241\"],\"port\":30155,\"protocol\":\"TCP\",\"serviceName\":\"ingress-nginx:nginx-ingress-second-controller\",\"allNodes\":tr...\nSelector:                 app.kubernetes.io/component=controller,app=nginx-ingress,release=nginx-ingress-second\nType:                     NodePort\nIP:                       10.43.139.23\nPort:                     http  80/TCP\nTargetPort:               http/TCP\nNodePort:                 http  30155/TCP\nEndpoints:                &lt;none&gt;\nPort:                     https  443/TCP\nTargetPort:               https/TCP\nNodePort:                 https  30636/TCP\nEndpoints:                &lt;none&gt;\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n</code></pre> <p>In this example, the service is exposed on every node on ports 30155 for http and 30636 for https</p>"},{"location":"000020160/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020161/","title":"Why does the kubelet certificate still show as expired after performing a cluster certificate rotation in an Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster?","text":"<p>This document (000020161) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020161/#situation","title":"Situation","text":""},{"location":"000020161/#question","title":"Question","text":"<p>Why is Kubelet certificate still indicating expired after performing a cluster certificate rotation?</p>"},{"location":"000020161/#pre-requisite","title":"Pre-requisite","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"000020161/#answer","title":"Answer","text":"<p>Before Rancher v2.3.3 and RKE v1.0.0, cluster provisioning did not supply the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> arguments to the Kubelet container. As a result, the kubelet automatically generates the <code>kubelet.crt</code>, and <code>kubelet.key</code> files under the <code>/var/lib/kubelet/pki</code> \u200bdirectory and the certificate is not rotated during the certificate rotation.</p>"},{"location":"000020161/#how-to-verify-the-kubelet-certificate","title":"How to verify the Kubelet certificate","text":"<ul> <li> <p><code>openssl s_client -connect &lt;NODE IP&gt;:10250 | openssl x509 -text</code></p> </li> <li> <p><code>curl -vk https://&lt;NODE IP&gt;:10250</code></p> </li> </ul>"},{"location":"000020161/#resolution","title":"Resolution","text":"<p>You can rotate the kubelet certificate in RKE and Rancher provisioned clusters as follows:</p>"},{"location":"000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v220-v230-and-rke-v020-v032-provisioned-clusters","title":"How to rotate the kubelet certificate in Rancher v2.2.0 - v2.3.0 and RKE v0.2.0 - v0.3.2 provisioned clusters","text":"<p>For clusters provisioned and managed by Rancher prior to v2.3.3 or RKE prior to v1.0.0, you will need to manually delete the <code>kubelet.crt</code> and <code>kubelet.key</code> in <code>/var/lib/kubelet/pki</code> and restart the Kubelet container:</p> <pre><code>docker exec kubelet rm /var/lib/kubelet/pki/kubelet.crt\ndocker exec kubelet rm /var/lib/kubelet/pki/kubelet.key\ndocker restart kubelet\n</code></pre>"},{"location":"000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v232-provisioned-clusters","title":"How to rotate the kubelet certificate in Rancher v2.3.2+ provisioned clusters","text":"<p>For Rancher provisioned clusters managed by Rancher v2.3.3 and above, you can set the <code>generate_serving_certificate</code> kubelet option to <code>true</code> in the cluster configuration YAML to rotate the kubelet certificate.</p> <p>N.B. If <code>hostname_override</code> is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding.</p> <ol> <li> <p>For the affected cluster click 'Edit Cluster' from within the Rancher UI cluster view.</p> </li> <li> <p>Click 'Edit as YAML'.</p> </li> <li> <p>Set the <code>generate_serving_certificate</code> option to true for the kubelet, per the below:</p> </li> </ol> <pre><code>services:\n     kubelet:\n       generate_serving_certificate: true\n</code></pre> <ol> <li>Click 'Save' to intitate a cluster reconciliation and trigger rotation of the kubelet certificate.</li> </ol>"},{"location":"000020161/#how-to-rotate-the-kubelet-certificate-in-rke-v100-provisioned-clusters","title":"How to rotate the kubelet certificate in RKE v1.0.0+ provisioned clusters","text":"<p>For clusters managed by RKE v1.0.0 and above, you can set the <code>generate_serving_certificate</code> kubelet option to <code>true</code> in the cluster configuration YAML and invoke <code>rke up</code> to rotate the kubelet certificate.</p> <p>N.B. If <code>hostname_override</code> is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding.</p> <ol> <li>Set the <code>generate_serving_certificate</code> option to true for the kubelet, within the cluster configuration YAML file, per the below:</li> </ol> <pre><code>services:\n     kubelet:\n       generate_serving_certificate: true\n</code></pre> <ol> <li>Invoke <code>rke up --config &lt;cluster configuration yaml&gt;</code> to update the cluster configuration with the new kubelet option and trigger rotation of the kubelet certificate.</li> </ol>"},{"location":"000020161/#further-reading","title":"Further Reading","text":"<p>RKE Certificate Rotation Documentation. Rancher v2.x Certificate Rotation Documentation. Kubelet Service Certificate Requirements Documentation.</p>"},{"location":"000020161/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020162/","title":"How to clean a Rancher 2.x node","text":"<p>This document (000020162) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020162/#situation","title":"Situation","text":""},{"location":"000020162/#task","title":"Task","text":"<p>At times a node may need to be cleaned of all state to ensure it is consistent for further use in a cluster. This article and script are for Rancher 2.x.</p> <p>Please note, this script will delete all containers, volumes, images, network interfaces, and directories that relate to Rancher and Kubernetes. It can also optionally flush all iptables rules and delete container images. It is important to perform pre-checks, and backup the node as needed before proceeding with any steps below.</p>"},{"location":"000020162/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A node provisioned with the RKE distribution using Rancher or the RKE CLI.</li> <li>The node should no longer be a member of any cluster.</li> <li>A copy of the cleanup script, and root/sudo access.</li> <li>Check the running containers or Pods, these will be forcefully deleted in the following steps.</li> <li>Confirm you are on the correct node and are ready to proceed with cleaning all containers and all data specific to Kubernetes and Rancher/RKE.</li> </ul> <p>Note, for RKE2 and K3s use the uninstall script deployed on the node during install.</p>"},{"location":"000020162/#resolution","title":"Resolution","text":"<p>The below steps use a script to automate the clean of a node, the commands used can be run manually as needed, follow the steps below cleaning a node that has been used previously in a cluster.</p> <ul> <li>Login to the node and download the cleanup script:</li> </ul> <p><code>curl -sLO https://github.com/rancherlabs/support-tools/raw/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh</code></p> <p>You should now have a copy of the script in the current directory.</p> <ul> <li>Run the script:</li> </ul> <p><code>sudo bash extended-cleanup-rancher2.sh</code></p> <p>If desired, the optional -f and -i flags can be used together or individually to flush iptables (-f) and delete container images (-i).</p> <p><code>sudo bash extended-cleanup-rancher2.sh -f -i</code></p> <ul> <li>Restart the node</li> </ul> <p>The node is now in a clean consistent state to be reused in a cluster.</p>"},{"location":"000020162/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020163/","title":"How to troubleshoot using the namespace of a container","text":"<p>This document (000020163) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020163/#situation","title":"Situation","text":""},{"location":"000020163/#task","title":"Task","text":"<p>When troubleshooting an issue, often a faithful reproduction and exact environment are needed. This can be a challenge in a containerized environment, where tools and a shell environment may not be easily available within containers of a Pod.</p>"},{"location":"000020163/#steps","title":"Steps","text":"<p>There are two approaches that can be taken:</p>"},{"location":"000020163/#sidecar-container","title":"Sidecar container","text":"<p>By running a container in the same namespaces as another, it's possible to use that container for troubleshooting.</p> <p>The sidecar container can be started using the same network and PID namespaces while attaching the same volumes:</p> <ul> <li>Set the ID or name of the container you wish to troubleshoot:</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>Run the sidecar container using the network, PID and volumes</li> </ul> <p><code>docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh</code></p> <ul> <li>It is now possible to troubleshoot with commands from the alpine container, within the context of the container or Pod with the issue.</li> </ul> <p>For example, if you were experiencing a network issue from this Pod, it is now possible to use tools available in the sidecar container to simulate the connection, view the network configuration and troubleshoot interactively.</p> <p>Substitute the alpine container as needed with an image of your choice.</p> <p>Note, this will attach the same volumes as the parent container, but the parent container read/write layers will not be accesible - to access the same container filesystem, see the nsenter example below.</p>"},{"location":"000020163/#use-the-host-tools-with-nsenter","title":"Use the host tools with nsenter","text":"<p>Alternatively you can use tools available on the host for the same usecase with the <code>nsenter</code> command. The <code>nsenter</code> command is standard on most Linux distributions, for example on Ubuntu it is provided by the util-linux package.</p> <ul> <li>Set the ID or name of the container you wish to troubleshoot:</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>Obtain the first process in the container (PID 1):</li> </ul> <p><code>PID=$(docker inspect --format '{{ .State.Pid }}' $ID)</code></p> <ul> <li>Run commands available on the node within the context of all of the container/Pod namespaces with nsenter:</li> </ul> <p><code>nsenter -a -t $PID &lt;command&gt;</code></p> <p>For example, if troubleshooting a network issue, tools from the node like tcpdump, curl, dig and mtr can be used to troubleshoot the issue interactively.</p> <p>Note, the <code>-a</code> flag is available in recent versions of <code>nsenter</code>, if this does not succeed, use a flag for a specific namespace, check the <code>nsenter --help</code> output.</p>"},{"location":"000020163/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020164/","title":"How to troubleshoot HTTP request performance with curl statistics","text":"<p>This document (000020164) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020164/#situation","title":"Situation","text":""},{"location":"000020164/#task","title":"Task","text":"<p>When troubleshooting a performance issue with a web-based endpoint, it's important to have metrics that assist in understanding what areas are related.</p> <p>This is where using a lightweight tool like curl, and it's ability to write out the statistics of a request can be very useful.</p>"},{"location":"000020164/#pre-requisites","title":"Pre-requisites","text":"<p>You will just need curl installed and available from the location performing the test.</p>"},{"location":"000020164/#steps","title":"Steps","text":"<p>Download the format file to use with curl:</p> <p><code>curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/files/curl-format.txt</code></p> <p>You should now have a <code>curl-format.txt</code> file locally in the current directory.</p> <p>Using the file and the <code>-w</code> flag, perform the desired request to the service, the example below displays the headers and statistics.</p> <p><code>curl -I -w \"@curl-format.txt\" https://rancher.com</code></p> <p>Timing statistics will be output with each run of the command, measurements are recorded in seconds.</p> <p>Note: run the command from a location that provides an accurate reproduction of the issue, to simulate the issue as closely as possible. Using the same request parameters are important - like the path and headers that might be used by client applications.</p>"},{"location":"000020164/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020165/","title":"How to recover an RKE v0.2.x, v0.3.x or v1.x.x cluster after restoration with an incorrect or missing rkestate file","text":"<p>This document (000020165) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020165/#situation","title":"Situation","text":""},{"location":"000020165/#issue","title":"Issue","text":"<p>When using RKE (Rancher Kubernetes Engine) v0.2.x, v0.3.x, v1.0.x or v1.1.0, if you have restored a cluster with the incorrect or missing rkestate file you will end up in a state where your infrastructure pods will not start. This includes all pods in the kube-system, cattle-system and ingress-nginx namespaces. As a result of these stopped infrastructure pods, workload pods will not function correctly. If you find yourself in this situation you can use the directions below to fix the cluster. For more information about the cluster state file, please see RKE documentation on Kubernetes Cluster State.</p>"},{"location":"000020165/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>RKE v0.2.x, v0.3.x, v1.0.x or v1.1.0</li> <li>A cluster restoration performed with the incorrect or missing rkestate file</li> </ul>"},{"location":"000020165/#workaround","title":"Workaround","text":"<ol> <li>Delete all service-account-token secrets in kube-system, cattle-system and ingress-nginx namespaces:</li> </ol> <pre><code>kubectl get secret -n cattle-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cattle-system delete secret \" $1) }'\nkubectl get secret -n kube-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n kube-system delete secret \" $1) }'\nkubectl get secret -n ingress-nginx | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n ingress-nginx delete secret \" $1) }'\nkubectl get secret -n cert-manager | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cert-manager delete secret \" $1) }'\n</code></pre> <ol> <li>Restart Docker on all nodes in the cluster currently:</li> </ol> <pre><code>systemctl restart docker\n</code></pre> <ol> <li>Force delete all pods stuck in a CrashLoopBackOff, Terminating, Error and Evicted state:</li> </ol> <pre><code>kubectl get po --all-namespaces | awk '{ if ($4 ==\"CrashLoopBackOff\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\nkubectl get po --all-namespaces | awk '{ if ($4 ==\"Terminating\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\nkubectl get po --all-namespaces | awk '{ if ($4 ==\"Error\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\nkubectl get po --all-namespaces | awk '{ if ($4 ==\"Evicted\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\n</code></pre> <ol> <li>Once your force delete has finished, restart Docker again to clear out any stale containers from the above force delete command:</li> </ol> <pre><code>systemctl restart docker\n</code></pre> <ol> <li>You may have to delete service account tokens more than once or delete pods more than once. After you go through the guide once, monitor pod statuses with a watch command in one terminal as shown below.</li> </ol> <pre><code>watch -n1 'kubectl get po --all-namespaces | grep -i  \"cattle-system\\|kube-system\\|ingress-nginx\\|cert-manager\"'\n</code></pre> <p>If you see any pods still in an error state, you can describe them to get idea of what is wrong. Most likely you'll see an error like the following which indicates that you need to delete its service account tokens again.</p> <pre><code>Warning  FailedMount  7m23s (x126 over 4h7m)  kubelet, 18.219.82.148  MountVolume.SetUp failed for volume \"rancher-token-tksxr\" : secret \"rancher-token-tksxr\" not found\nWarning  FailedMount  114s (x119 over 4h5m)   kubelet, 18.219.82.148  Unable to attach or mount volumes: unmounted volumes=[rancher-token-tksxr], unattached volumes=[rancher-token-tksxr]: timed out waiting for the condition\n</code></pre> <p>Delete the service account tokens again for that one namespace so that pods in other namespaces don't have to be disturbed if they are good. Once the service account tokens are deleted, run a delete pod command for just the namespace with pods still in an error state. cattle-node-agent and cattle-cluster-agent depend on the Rancher pod to be online, so you can ignore those until the very end. Once Rancher pods are stable, go back in and delete all the agents again to get them to restart more quickly.</p>"},{"location":"000020165/#resolution","title":"Resolution","text":"<p>An update to enable successful restoration of an RKE provisioned cluster without the correct rkestate file is targetted for an RKE v1.1.x patch release. For more information please see RKE GitHub issue #1336.</p>"},{"location":"000020165/#further-reading","title":"Further Reading","text":"<ul> <li>RKE documentation on Kubernetes Cluster State</li> <li>RKE documentation on etcd snapshots</li> </ul>"},{"location":"000020165/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020166/","title":"Pod network connectivity non-functional as a result of sysctl net.ipv4.ip_forward=0","text":"<p>This document (000020166) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020166/#situation","title":"Situation","text":""},{"location":"000020166/#issue","title":"Issue","text":"<p>If the sysctl <code>net.ipv4.ip_forward</code> is set to 0 (disabled) on a Linux host, then IPv4 packet forwarding is disabled.</p> <p>As a result, on a Kubernetes nodes this will prevent Pod networking from functioning.</p> <p>You can confirm the current value of this sysctl on a Linux host, if you are experiencing a network issue, with the following:</p> <p><code>sysctl net.ipv4.ip_forward</code></p> <p>The output should show 1, for enabled.</p>"},{"location":"000020166/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster with a CNI (Container Network Interface) plugin configure, e.g. an RKE (Rancher Kubernetes Engine) or Rancher launched cluster.</li> <li>The systctl net.ipv4.ip_forward set to 0 (disabled) on the cluster hosts.</li> </ul>"},{"location":"000020166/#resolution","title":"Resolution","text":"<p>Check if the kernel parameter <code>net.ipv4.ip_forward</code> is set to 1 with:</p> <p><code>sysctl net.ipv4.ip_forward</code></p> <p>If the current value of net.ipv4.ip_forward is 0, then set to this to 1 with the following:</p> <p><code>sysctl net.ipv4.ip_forward=1</code></p> <p>To make it permanent across reboot, add the following line in <code>/etc/sysctl.conf</code>:</p> <p><code>net.ipv4.ip_forward=1</code></p> <p>With this sysctl correctly enabled, Pod ingress and egress will be able to function as expected.</p>"},{"location":"000020166/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020167/","title":"How to setup your network CIDR for a large cluster","text":"<p>This document (000020167) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020167/#situation","title":"Situation","text":""},{"location":"000020167/#task","title":"Task","text":"<p>If you are expecting to use Rancher to deploy a Kubernetes cluster with more than 256 nodes, you'll need to make sure you adjust the default cluster CIDR settings. The default settings only allows clusters of 256 nodes or less.</p>"},{"location":"000020167/#requirements","title":"Requirements","text":"<ul> <li>Rancher v2.x</li> <li>A lot of hardware or VMs!</li> </ul>"},{"location":"000020167/#background","title":"Background","text":"<p>Kubernetes provides each pod with an IP address and each node with a block of IP addresses. Each cluster is also provided a block of IP addresses that is distributed to each node.</p> <p>This is controlled by two settings, the <code>cluster_cidr</code> block and <code>node-cidr-mask-size</code>. By default, the <code>cluster_cidr</code> block is 10.42.0.0/16 and the <code>node-cidr-mask-size</code> is 24. This gives the cluster 256 blocks of /24 networks to distribute out to the pool of nodes. For example, node1 will get 10.24.0.0/24, node2 will get 10.42.1.0/24, node3 will get 10.42.2.0/24 and so on.</p>"},{"location":"000020167/#solution","title":"Solution","text":"<p>To support more than 256 nodes, you will need to use a larger cluster_cidr block, a smaller node-cidr-mask-size, or adjust both. For example, if you want to support up to 512 nodes you can set:</p> <ul> <li><code>cluster_cidr</code> to 10.40.0.0/15</li> <li><code>node-cidr-mask-size</code> to 24</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.42.0.0/16</li> <li><code>node-cidr-mask-size</code> to 25</li> </ul> <p>To support up to 1024 nodes, you can use a larger <code>cluster_cidr</code>, smaller <code>node-cidr-mask-size</code>, or combination of both:</p> <ul> <li><code>cluster_cidr</code> to 10.38.0.0/14</li> <li><code>node-cidr-mask-size</code> to 24</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.42.0.0/16</li> <li><code>node-cidr-mask-size</code> to 26</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.40.0.0/15</li> <li><code>node-cidr-mask-size</code> to 25</li> </ul> <p>You should be aware of the following caveats when specifying your <code>cluster_cidr</code> and <code>node-cidr-mask-size</code> settings:</p> <ul> <li>Make sure you don't set your <code>cluster_cidr</code> to overlap with the default cluster service network of 10.43.0.0/16. That's why the examples above used 10.40.0.0/15 and 10.38.0.0/14. A CIDR of 10.42.0.0/15 will clash with the default cluster service CIDR.</li> <li>Make sure you don't set your <code>cluster_cidr</code> to overlap with IP address ranges already used in your enterprise infrastructure such as your node IPs, firewalls, load balancers, DNS, or other internal networks.</li> <li>Make sure your <code>node-cidr-mask-size</code> is large enough to accommodate the number of pods you want to run on each node. A size of 24 will give enough IP addresses for about 250 pods per node, which is well above the 110 maximum. However a size of 26 will only give you about 60 IPs, which is below the 110 maximum. If you plan to raise the default pod per node limit beyond 110, make sure sure your <code>node-cidr-mask-size</code> is large enough to support it. Note that pods that have <code>hostNetwork: true</code> do not count toward this total.</li> <li>Set it right the first time! Once your cluster has been deployed, these values cannot change. You'll need to decommission your cluster and start over again if you don't set it right.</li> <li>As of v1.17, Kubernetes supports clusters up to 5000 nodes. If you plan to go beyond this, you're venturing into unknown territory. For the latest large cluster best practices, see https://kubernetes.io/docs/setup/best-practices/cluster-large/</li> </ul> <p>Setting these values can be done when first creating the cluster. You'll need to click on the <code>Edit as YAML</code> button and merge in the following YAML:</p> <pre><code>rancher_kubernetes_engine_config:\nservices:\n    kube-controller:\n      cluster_cidr: 10.40.0.0/15\n      extra_args:\n        node-cidr-mask-size: 25\n</code></pre> <p>The above configuration should allow you to have about 120 pods per node and 1024 nodes in your cluster. That's over 100,000 pods, wow!</p>"},{"location":"000020167/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020168/","title":"Update self signed certificate on single install of Rancher 2.x","text":"<p>This document (000020168) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020168/#situation","title":"Situation","text":""},{"location":"000020168/#task","title":"Task","text":"<p>Update/renew self signed certificates to ten year expiration on Single Server Install of Rancher 2.x</p>"},{"location":"000020168/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>rancher-single-tool</li> </ul>"},{"location":"000020168/#resolution","title":"Resolution","text":"<ol> <li>Download Rancher single tool on the server that is running your Rancher container:</li> </ol> <pre><code>curl -LO https://github.com/patrick0057/rancher-single-tool/raw/master/rancher-single-tool.sh\n</code></pre> <ol> <li>Run script so that it upgrades your installation (you can upgrade to the same version) and pass flags to indicate that you want to regenerate your self signed certificate. The most reliable way is to just specify all of your options on the command line but the script does have an easy to use automated system as well as shown in option b.</li> </ol> <p>a. Specify all flags on command line, including any rancher options you had and docker options. Option -s is required for generating new 10 year self signed SSL certificates.</p> <pre><code>bash rancher-single-tool.sh -f -c'&lt;container_id&gt;' -t'upgrade' -v'&lt;rancher_version&gt;' -d'&lt;docker_options&gt;' -r'&lt;rancher_options&gt;' -s'&lt;self_signed_ssl_hostname&gt;'\n</code></pre> <p>For example:</p> <pre><code>bash rancher-single-tool.sh -f -c'984f2fe62f6a' -t'upgrade' -v'v2.2.4' -d'-d --restart=unless-stopped -p 80:80 -p 443:443' -r'none' -s'company.domain.com'\n</code></pre> <p>b. Let the script prompt you for answers and autodetect docker and rancher options when asked to.</p> <pre><code>bash rancher-single-tool.sh -s'&lt;self_signed_ssl_hostname&gt;'\n</code></pre> <p>For example:</p> <pre><code>bash rancher-single-tool.sh -s'company.domain.com'\n</code></pre> <ol> <li>In order to see the new SSL you need to completely quit your browser and start it back up, otherwise it might still show you the old certificate. Alternatively you can consistently check this using openssl instead of using your browser.</li> </ol> <pre><code>openssl s_client -connect company.domain.com:443 | openssl x509 -noout -text -startdate -enddate\n</code></pre> <ol> <li>If you have any downstream clusters attached to this Rancher installation you will need to update their Rancher agent deployment which will be covered in https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool</li> </ol>"},{"location":"000020168/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020169/","title":"How to troubleshoot IPsec stability issues in a Rancher v1.6 cluster","text":"<p>This document (000020169) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020169/#situation","title":"Situation","text":""},{"location":"000020169/#troubleshooting","title":"Troubleshooting","text":"<p>If you are experiencing issues with containers communicating to each other in your Rancher 1.6 environment, your ipsec might be having some issues. In this article I will go over common troubleshooting steps and procedures to correct the problem.</p> <ul> <li>exec into one of your ipsec-router containers and run the following ipsec test</li> </ul> <pre><code>for i in `curl -s rancher-metadata/latest/self/service/containers/| cut -f1 -d=` ; do ping -c2 `curl -s curl rancher-metadata/latest/self/service/containers/$i/primary_ip` ; done\n</code></pre> <p>If all containers or a majority are not responding then there is likely an issue with ipsec that needs to be addressed. Usually when there are ipsec issues, it is because metadata is having issues getting in sync. To confirm this, check your metadata logs (Infrastructure stacks&gt; network-services&gt; metadata&gt;) and look at the \"Download and reload in\" time. If it is hovering around 10 seconds or greater then this is most likely your problem. We generally want this value to be 1-2 seconds. Below is a sample of what this looks like. </p>"},{"location":"000020169/#information","title":"Information","text":"<p>The metadata container is a database that runs on every host in an environment. Infrastructure containers on each host rely on their local metadata database for information that allows them to run correctly. The data that is retrieved by metadata is serialized, so if it detects that it is out of date it will grab the data again until it is in sync. On a system that downloads and reloads in 10 seconds, the metadata container will be stuck in a perpetual loop of not having the correct data. This will result in infrastructure containers on that host to not work as expected.</p>"},{"location":"000020169/#repair","title":"Repair","text":"<p>IPsec usually has issues when there are more than 50 hosts in an environment. Rancher's official recommendation is that you have no more than 50 hosts in an environment. If you need more, we recommend scaling your hosts vertically or creating a separate environment. If you are still having issues or cannot for some reason scale down your environment right away then you can try increasing the CPU allowance to the metadata stack.</p> <p>To check metadata CPU usage, we need to go to infrastructure stacks then click on network-services. In network-services click \"Up to date\" in the top right corner. Then select the latest template version in the drop down menu to reveal the settings. You should see settings similar to the screenshot below.</p> <p></p> <p>The number on the left is the CPU Period which indicates a number that represents a full CPU core. The number on the right is the CPU quota which indicates how much CPU we want to allow metadata to use. By default we only allow metadata to use 1/2 of a core. In larger environments you can increase this value to correct ipsec issues.</p> <p>To increase 1/2 core to 2 cores for example, you could change the above CPU Quota number from 200000 to 800000. Once you save changes the containers will go through a rolling upgrade, this can take a while depending on how overloaded your environment is and how many hosts are in it. Once the rolling update is complete, test your ipsec connectivity again to ensure that it is working as expected.</p>"},{"location":"000020169/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020170/","title":"How to upgrade Docker using Rancher's install script","text":"<p>This document (000020170) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020170/#situation","title":"Situation","text":""},{"location":"000020170/#task","title":"Task","text":"<p>Rancher provides quick scripts for installing Docker, which are available for the most recent versions of Docker https://rancher.com/docs/rancher/v2.x/en/installation/requirements/installing-docker/ Upgrading Docker on your machine using these scripts is equally as simple</p>"},{"location":"000020170/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A supported node with a version of Docker needing to be upgraded</li> <li>Curl or Wget installed</li> </ul>"},{"location":"000020170/#resolution","title":"Resolution","text":"<p>Just run the script with the version number you are trying to upgrade to. Let's say you're running 18.09 and want to upgrade to 19.03. Simply provide the version number as the name of the script to run. For example:</p> <p><code>curl https://releases.rancher.com/install-docker/19.03.sh | sh</code></p> <p>or</p> <p><code>wget -O- https://releases.rancher.com/install-docker/19.03.sh | sh</code></p> <p>This will throw a warning that Docker is already installed, stop the running Docker engine, and upgrade your version. Note that restarting Docker will also stop any running container or workloads running on this host.</p> <p>This procedure does not apply to RancherOS</p>"},{"location":"000020170/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020171/","title":"Information to provide when logging a support case","text":"<p>This document (000020171) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020171/#situation","title":"Situation","text":""},{"location":"000020171/#context","title":"Context","text":"<p>To allow us to provide quick and efficient support, we ask that customers provide as much information as possible when logging a ticket.</p> <p>Below are some things that we find we typically need from customers when diagnosing.</p>"},{"location":"000020171/#assumptions","title":"Assumptions","text":"<p>You have ssh access to the affected nodes and access to the Rancher UI</p>"},{"location":"000020171/#information-to-provide","title":"Information to provide","text":"<ul> <li>When did you first notice the issue?</li> <li>Is the issue related to Rancher or a downstream cluster managed by Rancher?</li> <li>What is the version of Rancher server and the affected cluster?</li> <li>The Rancher version can be found at the bottom left of the Rancher UI or by inspecting the image version of the rancher container.</li> <li>The k8s version can be found by navigating to the relevant cluster in the UI and looking for the Kubernetes Version string or can be retrieved with RKE: <code>rke version --config &lt;path to cluster.yml&gt;</code></li> <li>If the issue is related to an upgrade of Rancher or k8s, what versions did you come from?</li> <li>If the issue is related to a downstream cluster, how was the cluster built? rke, hosted provider, imported or custom?</li> <li>Are there any Github issues or previous support tickets you think are related?</li> <li>Are there any events that you think may correlate with the issue? e.g., infrastructure outage, host reboot, os upgrade, docker upgrade, network changes, configuration changes?</li> <li>Have you taken any corrective action and if so, what?</li> <li>Logs to assist in debugging:</li> <li>System logs are almost always required when diagnosing an issue, you can generate these using the Rancher log collector scripts.</li> <li> <p>For some issues it can help to capture traffic from the browser to the Rancher UI, as per the process found here .</p> <p>If any of the files above are too big to be uploaded to Zendesk, we can provide an alternate location</p> </li> </ul>"},{"location":"000020171/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020172/","title":"How to setup Rancher 2.x with Active Directory external authentication","text":"<p>This document (000020172) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020172/#situation","title":"Situation","text":""},{"location":"000020172/#overview-and-intention","title":"Overview and Intention","text":"<p>This is a quick guide aiming to get Rancher v2.x using external authentication via Active Directory with the least amount of effort. Of course there is much more to consider and configure in a production enterprise environment. For more detail on this function please refer to this Rancher article and fine tune as required. Configuring Active Directory</p>"},{"location":"000020172/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher v2.x, either a single node instance or High Availability (HA) cluster.</li> <li>Local account to log onto the Rancher Server (usually admin)</li> <li>A Windows Server running Active Directory</li> <li>Name of the domain you wish to join</li> <li>A restricted account that Rancher can use to bind and query the Directory with (Security Recommendations at the bottom of article)</li> <li>A standard user account that will be used to test and enable the authentication (i.e your domain account)</li> <li>Knowledge of where the users are in the Active Directory OU (Organisational Unit) structure</li> <li>Network connectivity from the Rancher worker nodes to the Active Directory Servers (There is probably more than one, run nslookup on the domain name)</li> <li>Also good to test ports 389 or 636 (TLS) as these need to be allowed</li> </ul>"},{"location":"000020172/#steps-on-how-to-get-rancher-talking-to-ad-quickly","title":"Steps on How To Get Rancher Talking to AD Quickly","text":""},{"location":"000020172/#tested-with-ad-running-windows-server-20162019","title":"(Tested with AD running Windows Server 2016/2019)","text":"<p>In this example I have used the below examples (yours will be different):</p> <ul> <li>my domain is 'rancher.local'</li> <li>All or my users are located under the Users OU in AD</li> <li>my bind account is 'svc-rancher'</li> </ul> <p>For more detail refer to Configuring Active Directory:</p> <ol> <li>Log into the Rancher UI using the initial local admin account.</li> <li>From the Global view, navigate to Security &gt; Authentication</li> <li>Select Active Directory. The Configure an AD server form will be displayed.</li> <li>Add in the Hostname or IP address into the Hostname field</li> <li>Add 'rancher/svc-rancher' to the Service Account Username field</li> <li>Add 'cn=users,dc=rancher,dc=local' to the User Search Base</li> <li>Goto Section 3 add your domain account username and password</li> <li>Click 'Authenticate with Active Directory'</li> </ol>"},{"location":"000020172/#security-tips-and-best-practices","title":"Security tips and Best Practices","text":"<p>WARNING: Once enabled all users in the Search base will be able to log into Rancher.</p> <ol> <li>Once auth is configured in Rancher change the relaxed default setting from 'Allow any valid Users' to login to 'only allow members of Cluster, Projects' to login. Access must now be specified instead of allowing any User onto the cluster.</li> <li>Under 'Global, Security, Roles' It is best to drop 'New User Default' setting from 'User' to 'User Base' which provide less privleges to new users and must increased as required not as a default.</li> <li>The bind account is critical for ongoing authentication so locking the account will break functionality.</li> <li>If this account gets locked or the password changes your AD authentication will be broken. Setting the account and the password not to expire and removing lockout policies prevent disruption.</li> <li>Remove interactive logon abilites as this account doesn't need to logon to a server and control it</li> </ol>"},{"location":"000020172/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020173/","title":"How to change Rancher 2.x server-url","text":"<p>This document (000020173) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020173/#situation","title":"Situation","text":""},{"location":"000020173/#task","title":"Task","text":"<p>Changing the server URL on Rancher 2.x.</p>"},{"location":"000020173/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>rancher-single-tool for Single Server Rancher Installations</li> <li>cluster-agent-tool for both HA and Single Server Rancher Installtions</li> </ul>"},{"location":"000020173/#resolution","title":"Resolution","text":""},{"location":"000020173/#single-server-installation","title":"Single Server Installation","text":"<p>During this tutorial it is recommended to use the rancher-single-tool for Rancher single server installations. It isn't required but it makes the process much easier. As a result this guide will be based on using that tool.</p> <ol> <li>Download the rancher-single-tool to the node that is running your rancher server container.</li> </ol> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh\nwget https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh\n</code></pre> <ol> <li>Backup your Rancher installation.</li> </ol> <pre><code>bash rancher-single-tool.sh -t'backup'\n</code></pre> <ol> <li> <p>Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url.</p> </li> <li> <p>Now we need to upgrade your Rancher container to reflect new certs. This is required in most cases with the exception of already using a wildcard that also encompasses the new server-url.</p> </li> </ol> <p>a. To generate a new self signed certificate for your new URL use the following upgrade command. Follow the prompts to finish the upgrade.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -s'newhostname.company.com'\n</code></pre> <p>b. To generate a new Let's Encrypt certificate you will need to change the Rancher server options to reflect this. You could do this with the following command.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -r'--acme-domain newhostname.company.com'\n</code></pre> <p>c. If you were using certificates signed by a recognized CA before and just need to replace them, you should modify the docker options to reflect this change. Keep in mind that if you just replaced the cert files on the host path and the filenames didn't change, you can just restart the docker container. However if the filenames did change, I'm providing the example below of how you would do upgrade the container to see this change.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem'\n</code></pre> <p>d. If you were using certificates signed by a private CA or you want to use your own self signed certifiactes (certificates not created by rancher-single-tool option -s). Below is an example of how you would do that. The same rule applies from option c. If the filenames have not changed you don't need to upgrade, you can just restart the container.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem --volume=/etc/rancherssl/certs/ca.pem:/etc/rancher/ssl/cacerts.pem'\n</code></pre> <ol> <li>Once your Rancher container is backup and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool</li> </ol>"},{"location":"000020173/#ha-installation","title":"HA Installation","text":"<ol> <li> <p>Ensure that you have current etcd backups for your local rancher cluster.</p> </li> <li> <p>Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url.</p> </li> <li> <p>Log into a box where you have helm and kubectl installed. You will need your local Rancher cluster kubeconfig, ensure that it is set to the default config by either placing it in ~/.kube/config or by setting your KUBECONFIG environment variable.</p> </li> <li> <p>Check current helm chart options:</p> </li> </ol> <pre><code>helm get values rancher -n cattle-system\nhostname: rancher.company.com\nrancherImageTag: v2.3.5\n</code></pre> <ol> <li>Craft an upgrade command based on the values provided in the previous step and then modify the hostname to match the new server hostname/url.</li> </ol> <pre><code>helm upgrade rancher-stable/rancher --name rancher --namespace cattle-system --set hostname=newrancher.company.com --set rancherImageTag=v2.3.5\n</code></pre> <ol> <li>Run the upgrade command then wait for rollout to complete.</li> </ol> <pre><code>kubectl -n cattle-system  rollout status deploy/rancher\n</code></pre> <ol> <li>Once your Rancher deployment is back up and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. You also need to login to one of the controlplane nodes of your local Rancher cluster and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool</li> </ol>"},{"location":"000020173/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020174/","title":"How to setup Nodelocal DNS cache on Rancher 2.x","text":"<p>This document (000020174) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020174/#situation","title":"Situation","text":""},{"location":"000020174/#why-use-nodelocal-dns-cache","title":"Why use Nodelocal DNS cache?","text":"<p>Like many applications in a containerised architecture, CoreDNS or kube-dns runs in a distributed fashion. In certain circumstances, DNS reliability and latency can be impacted with this approach. The causes of this relate notably to conntrack race conditions or exhaustion, cloud provider limits, and the unreliable nature of the UDP protocol.</p> <p>A number of workarounds exist, however long term mitigation of these and other issues has resulted in a redesign of the Kubernetes DNS architecture, and the result being the Nodelocal DNS cache project.</p>"},{"location":"000020174/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster of v1.15 or greater created by Rancher v2.x or RKE</li> <li>A Linux cluster, Windows is currently not supported</li> <li>Access to the cluster</li> </ul>"},{"location":"000020174/#resolution","title":"Resolution","text":""},{"location":"000020174/#installing","title":"Installing","text":"<p>There are two installation approaches, both approaches should be non-invasive, pods that are currently running will not be modified. The DNS configuration will take effect for pods started after the install is complete.</p>"},{"location":"000020174/#rke1-using-a-rancher-version-after-v24x-or-rke-version-after-v110","title":"RKE1: Using a Rancher version after v2.4.x, or RKE version after v1.1.0","text":"<p>Update the cluster using 'Edit as YAML' in the Rancher UI. With RKE, edit the cluster.yaml file instead.</p> <p>Note: Updating the cluster using the below will create the <code>node-local-dns</code> Daemonset, and restart the <code>kubelet</code> container on each node.</p> <p>As in the documentation, update or add the <code>dns.nodelocal.ip_address</code> field using the following as an example:</p> <pre><code>dns:\n[..]\n    nodelocal:\n      ip_address: \"169.254.20.10\"\n</code></pre> <p>New pods created after the change will configure the node-local-dns link-local address as the nameserver in <code>/etc/resolv.conf</code>.</p> <p>Note: No further action is needed to use node-local-dns (as in the option A/B below), the changes to <code>/etc/resolv.conf</code> will take effect for pods started from this point onwards.</p>"},{"location":"000020174/#rke1-using-a-rancher-version-before-v24x-or-rke-version-before-v110","title":"RKE1: Using a Rancher version before v2.4.x, or RKE version before v1.1.0","text":"<p>Installing the YAML manifest by navigating to the cluster, and clicking the <code>Launch kubectl</code> button in the Rancher UI. This command can also be run from a terminal where a kubeconfig for the cluster is currently configured.</p> <p>Environment variables are replaced before applying the manifest, one assumption is that the cluster service discovery domain name is <code>cluster.local</code> (default), adjust the command if needed.</p> <pre><code>curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml \\\n| sed -e 's/__PILLAR__DNS__DOMAIN__/cluster.local/g' \\\n| sed -e \"s/__PILLAR__DNS__SERVER__/$(kubectl get service --namespace kube-system kube-dns -o jsonpath='{.spec.clusterIP}')/g\" \\\n| sed -e 's/__PILLAR__LOCAL__DNS__/169.254.20.10/g' \\\n| kubectl apply -f -\n</code></pre> <p>Ensure the <code>node-local-dns</code> pods start successfully, a pod should start on each control plane and worker node.</p> <pre><code>kubectl get -n kube-system pod -l k8s-app=node-local-dns\n\n</code></pre> <p>When deploying the YAML manifest there are two options to configure the cluster to use the new node-local-dns configuration, please choose from option A or B below.</p>"},{"location":"000020174/#option-a-configure-the-kubelet","title":"Option A - Configure the Kubelet","text":"<p>By default, the Kubelet will configure the <code>/etc/resolv.conf</code> of pods with the <code>kube-dns</code> Service ClusterIP as the nameserver. Configuring all new pods to query node-local-dns will require updating the Kubelet arguments.</p> <p>Note: Updating the arguments using the below will restart the <code>kubelet</code> container on each node.</p> <ul> <li> <p>If the cluster was provisioned by Rancher, edit the cluster in the UI and click on <code>Edit as YAML</code>.</p> </li> <li> <p>If the cluster was provisioned by RKE, edit the cluster.yml file directly.</p> </li> </ul> <p>Update the <code>kubelet</code> service with the <code>cluster-dns</code> argument and IP Address. Click save, or run an <code>rke up</code> to put this change into effect.</p> <pre><code>services:\nkubelet:\n    extra_args:\n      cluster-dns: \"169.254.20.10\"\n</code></pre> <p>New pods created after the change will configure the node-local-dns link-local address as the nameserver in <code>/etc/resolv.conf</code>.</p>"},{"location":"000020174/#option-b-configure-workloads","title":"Option B - Configure Workloads","text":"<p>Alternatively, node-local-dns can be configured on a per-workload basis by updating the workload with a <code>dnsConfig</code> and <code>dnsConfig</code> .</p> <ul> <li> <p>If using the Rancher UI, edit the workload, navigate to Show advanced options &gt; Networking &gt; DNS Nameservers and add <code>169.254.20.10</code>. Additionally, adjust the DNS Policy to <code>None</code>.</p> </li> <li> <p>If configuring by YAML, patch in the following to the pod spec to adjust the <code>dnsPolicy</code> and <code>dnsConfig</code>:</p> </li> </ul> <pre><code>    spec:\n      dnsPolicy: \"None\"\n      dnsConfig:\n        nameservers:\n        - 169.254.20.10\n</code></pre>"},{"location":"000020174/#rke2-using-any-rke2-kubernetes-version","title":"RKE2: Using any RKE2 Kubernetes version","text":"<p>Update the default HelmChart for CoreDNS, the nodelocal.enabled: true value will install node-local-dns in the cluster. Please see the documentation here for more details.</p>"},{"location":"000020174/#testing","title":"Testing","text":"<p>Once installed, start a new pod to test DNS queries.</p> <pre><code>kubectl run --restart=Never --rm -it --image=tutum/dnsutils dns-test -- dig google.com\n</code></pre> <p>Unless Option B was used to install node-local-dns, you should expect to see <code>169.254.20.10</code> as the server, and a successful answer to the query.</p> <p>To verify a pod or container is using node-local-dns by checking the <code>/etc/resolv.conf</code> file, for example:</p> <pre><code>kubectl exec -it &lt;pod name&gt; -- grep nameserver /etc/resolv.conf\nnameserver 169.254.20.10\n</code></pre>"},{"location":"000020174/#removing-nodelocal-dns-cache","title":"Removing Nodelocal DNS cache","text":"<p>To remove from a cluster, the reverse steps are needed.</p> <p>Note: Pods created with the node-local-dns nameserver in <code>/etc/resolv.conf</code> will need to be started again to use the kube-dns service as a nameserver again.</p>"},{"location":"000020174/#using-a-rancher-version-after-v24x-or-rke-version-after-v110","title":"Using a Rancher version after v2.4.x, or RKE version after v1.1.0","text":"<p>Remove the <code>dns.nodelocal</code> configuration from the cluster YAML</p>"},{"location":"000020174/#using-a-rancher-version-before-v24x-or-rke-version-before-v110","title":"Using a Rancher version before v2.4.x, or RKE version before v1.1.0","text":"<ol> <li> <p>Remove the Kubelet configuration (Option A), or remove the dnsConfig from workloads (Option B).</p> </li> <li> <p>If Option A was taken, delete any pods in workloads that were started since the Kubelet configuration change so that they are started with the kube-dns ClusterIP again.</p> </li> <li> <p>Remove the node-local-dns objects with the following command:</p> </li> </ol> <pre><code>curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml | kubectl delete -f -\n</code></pre> <p>Note: it is important to perform these steps in order, and only complete step 3 once the pods using node-local-dns have been started with the kube-dns ClusterIP configured in <code>/etc/resolv.conf</code> again.</p>"},{"location":"000020174/#additional-information","title":"Additional Information","text":""},{"location":"000020174/#troubleshooting","title":"Troubleshooting","text":"<p>Node-local-dns will perform external lookups on behalf of pods, this lookup occurs from the node-local-dns DaemonSet pod running on the same node as the pod.</p> <p>For internal lookups, CoreDNS will be used, node-local-dns will cache successful queries (30s), and negative queries (5s) by default. For an architecture overview please see the diagram here.</p> <p>In no specific order, the following can help understand a DNS issue further.</p>"},{"location":"000020174/#check-all-kube-dns-and-node-local-dns-objects","title":"Check all kube-dns and node-local-dns objects","text":"<p>Ensure there are no obvious issues with scheduling CoreDNS and node-local-dns pods in the cluster.</p> <pre><code>kubectl get all -n kube-system -l k8s-app=node-local-dns\nkubectl get all -n kube-system -l k8s-app=kube-dns\n</code></pre> <p>All node-local-dns and kube-dns pods should be ready and running, the kube-dns Service should exist. Check the events if needed to locate any warning or failed event messages.</p> <pre><code>kubectl describe ds -n kube-system -l k8s-app=node-local-dns\nkubectl describe rs -n kube-system -l k8s-app=kube-dns\n</code></pre>"},{"location":"000020174/#check-the-logs-and-configmap-of-kube-dns-and-node-local-dns-pods","title":"Check the logs and ConfigMap of kube-dns and node-local-dns pods","text":"<pre><code>kubectl logs -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=node-local-dns\n</code></pre> <pre><code>kubectl get configmap -n kube-system coredns -o yaml\nkubectl get configmap -n kube-system node-local-dns -o yaml\n</code></pre>"},{"location":"000020174/#enable-logging-and-perform-a-dns-test","title":"Enable logging and perform a DNS test","text":"<p>Note, query logging can increase the log output from CoreDNS, enabling this temporarily while investigating is suggested.</p> <ul> <li> <p>Enable query logging to understand the pattern from workloads</p> </li> <li> <p>Run a DaemonSet to perform queries from a pod running on each node in the cluster</p> </li> </ul>"},{"location":"000020174/#ask-questions-to-further-eliminate-the-issue","title":"Ask questions to further eliminate the issue","text":"<ul> <li>Is it only DNS that is affected, or is all connectivity affected?</li> <li>Are internal, external or all DNS queries failing?</li> <li>Are all nodes and workloads experiencing the issue, or a specific node or workload? * Nodes use the upstream DNS configured in <code>/etc/resolv.conf</code>, queries failing from a node could indicate the issue is with upstream DNS</li> <li>What is the error reported by applications? * If logs are aggregated, queries can be performed on the logs to identify timelines and impact</li> <li>Is the issue intermittent or constantly occuring? * If the issue is intermittent, configure monitoring or a loop to identify when the issue occurs, when it does - are internal, external or all queries affected?</li> </ul>"},{"location":"000020174/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020175/","title":"How to setup HAProxy for Rancher v2.x","text":"<p>This document (000020175) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020175/#situation","title":"Situation","text":""},{"location":"000020175/#task","title":"Task","text":"<p>Setup HAProxy as a frontend load balancer for Rancher v2.x.</p>"},{"location":"000020175/#overview","title":"Overview","text":""},{"location":"000020175/#install-haproxy","title":"Install HAProxy","text":""},{"location":"000020175/#ubuntu","title":"Ubuntu","text":"<pre><code>apt update\napt install -y haproxy\nsystemctl enable haproxy\nsystemctl start haproxy\n</code></pre>"},{"location":"000020175/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>yum update\nyum install haproxy -y\nsystemctl enable haproxy\nsystemctl start haproxy\n</code></pre>"},{"location":"000020175/#example-haproxy-config","title":"Example HAProxy Config","text":""},{"location":"000020175/#option-a-full-ssl","title":"Option A - Full SSL","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/</li> <li>Verify Rancher URL works when connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping\n</code></pre> <ul> <li>Copy cert and key into a single file called /etc/haproxy/cert.pem</li> <li>Add frontend to /etc/haproxy/haproxy.cfg:</li> </ul> <pre><code>frontend www-http\nbind *:80\nreqadd X-Forwarded-Proto:\\ http\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443 ssl crt /etc/haproxy/cert.pem\nreqadd X-Forwarded-Proto:\\ https\ndefault_backend rancher-https\n</code></pre> <ul> <li>Add backends to /etc/haproxy/haproxy.cfg:</li> </ul> <pre><code>backend rancher-http\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:80 check weight 1 maxconn 1024\nserver rancher02 192.168.1.104:80 check weight 1 maxconn 1024\nserver rancher03 192.168.1.105:80 check weight 1 maxconn 1024\n</code></pre> <pre><code>backend rancher-https\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:443 check weight 1 maxconn 1024 ssl verify none\nserver rancher02 192.168.1.104:443 check weight 1 maxconn 1024 ssl verify none\nserver rancher03 192.168.1.105:443 check weight 1 maxconn 1024 ssl verify none\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"000020175/#option-b-external-tls-termination","title":"Option B - External TLS Termination","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/options/chart-options/#external-tls-termination</li> <li>Verify Rancher URL works went connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl --header \"Host: rancher.example.com\" http://192.168.1.103/ping\n</code></pre> <ul> <li>Copy cert and key into a single file called /etc/haproxy/cert.pem</li> <li>Create frontends:</li> </ul> <pre><code>frontend www-http\nbind *:80\nreqadd X-Forwarded-Proto:\\ http\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443 ssl crt /etc/haproxy/cert.pem\nreqadd X-Forwarded-Proto:\\ https\ndefault_backend rancher-http\n</code></pre> <ul> <li>Create backends:</li> </ul> <pre><code>backend rancher-http\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:80 check weight 1 maxconn 1024\nserver rancher02 192.168.1.104:80 check weight 1 maxconn 1024\nserver rancher03 192.168.1.105:80 check weight 1 maxconn 1024\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"000020175/#option-c-tcp-pass-through","title":"Option C - TCP pass-through","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/</li> <li>Verify Rancher URL works when connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping\n</code></pre> <ul> <li>NOTE: The default gateway for all 3 Rancher nodes must be the load balancer. Doc: https://www.haproxy.com/blog/howto-transparent-proxying-and-binding-with-haproxy-and-aloha-load-balancer/</li> <li>Create frontends:</li> </ul> <pre><code>frontend www-http\nbind *:80\nmode tcp\noption tcplog\ntcp-request inspect-delay 5s\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443\nmode tcp\noption tcplog\ntcp-request inspect-delay 5s\ndefault_backend rancher-https\n</code></pre> <ul> <li>Create backends:</li> </ul> <pre><code>backend rancher-http\nmode tcp\nbalance roundrobin\nsource 0.0.0.0 usesrc client\nserver rancher01 192.168.1.103:80\nserver rancher02 192.168.1.104:80\nserver rancher03 192.168.1.105:80\n</code></pre> <pre><code>backend rancher-https\nmode tcp\nbalance roundrobin\nsource 0.0.0.0 usesrc client\nserver rancher01 192.168.1.103:443\nserver rancher02 192.168.1.104:443\nserver rancher03 192.168.1.105:443\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"000020175/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Add the following to /etc/haproxy/haproxy.cfg before the frontend section.</li> </ul> <pre><code>listen stats\nbind :9000\nmode http\nstats enable\nstats hide-version\nstats realm Haproxy\\ Statistics\nstats uri /\nstats auth admin:admin\n</code></pre> <ul> <li>Go to http://load01.example.com:9000/</li> <li>Username/Password: admin/admin</li> <li>If there are firewall rules blocking port 9000, use ssh tunneling to proxy the connection:</li> </ul> <pre><code>ssh -f -N -L 9000:127.0.0.1:9000 root@192.168.1.101\n</code></pre> <ul> <li>Go to http://localhost:9000/</li> </ul>"},{"location":"000020175/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020176/","title":"How to collect a trace and heap from nginx ingress","text":"<p>This document (000020176) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020176/#situation","title":"Situation","text":""},{"location":"000020176/#task","title":"Task","text":"<p>When troubleshooting an ingress-nginx issue, collecting the trace and heap dump from ingress-nginx Pods may be requested. This can assist with understanding issues like excessive memory consumption.</p>"},{"location":"000020176/#pre-requisites","title":"Pre-requisites","text":"<p>Access to the node(s) where the ingress-nginx Pods are experiencing the issue, or access to the node on <code>10254/TCP</code> from a workstation.</p> <p>To collect the output, the below commands use <code>curl</code>, you may need to install the package. If needed, <code>wget</code> could be used instead.</p> <p>The <code>date</code> command is used to provide a consistent timestamp for the files, this could be changed or removed if the <code>date</code> command on the node doesn't support these flags.</p> <p>The issue should be occurring at the time for the collection to be useful when investigating.</p>"},{"location":"000020176/#steps","title":"Steps","text":"<p>SSH to the node(s), use the following commands to collect the trace and heap dump. If the issue is intermittent or fluctuating, repeat the commands as necessary to capture the collection when the issue is ocurring.</p>"},{"location":"000020176/#heap","title":"Heap","text":"<pre><code>curl -s http://localhost:10254/debug/pprof/trace?seconds=5 --output /tmp/nginx-trace.$(date -u --iso-8601=seconds)\n</code></pre>"},{"location":"000020176/#trace","title":"Trace","text":"<pre><code>curl -s http://localhost:10254/debug/pprof/heap --output /tmp/nginx-heap.$(date -u --iso-8601=seconds)\n</code></pre> <p>Note: if accessing the node on <code>10254/TCP</code> instead, be sure to update <code>localhost</code> with the IP Address of the node.</p> <p>If the files are too large to upload to the ticket, please request or use the provided temporary upload location.</p>"},{"location":"000020176/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020177/","title":"How to generate a HAR file","text":"<p>This document (000020177) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020177/#situation","title":"Situation","text":""},{"location":"000020177/#task","title":"Task","text":"<p>When troubleshooting an issue that is reproducible in a browser, it is sometimes necessary to have additional information about the requests and responses. You may be requested to generate a HAR file recording to capture this and attach this to a ticket for analysis.</p> <p>Please note, the information collected in a HAR file can contain sensitive data like content, headers, and cookies. This is not always the case, and some information is transient only. However, please check and santise the information as necessary before uploading.</p>"},{"location":"000020177/#pre-requisites","title":"Pre-requisites","text":"<p>A browser that can reproduce the issue, we've covered Chrome and Firefox in this article.</p> <p>The issue should be occuring or reproducible at the time of the collection to contain an example of the issue.</p>"},{"location":"000020177/#steps","title":"Steps","text":"<p>Open your browser ready to reproduce the issue.</p>"},{"location":"000020177/#chrome","title":"Chrome","text":"<ul> <li>From the menu, select View &gt; Developer &gt; Developer Tools</li> <li>From the pane, click on the Network tab</li> <li>Locate the Preserve log setting in the upper left and ensure it is checked</li> <li>Locate the record button, it should be a red circle to indicate that it is currently recording, if it is grey, click it once to start recording</li> <li>Follow any steps needed to reproduce the issue during the recording</li> <li>Once the issue has occurred, right click in the pane and select Save as HAR with Content</li> </ul> <p>Firefox</p> <ul> <li>From the menu, select Tools &gt; Web Developer &gt; Network</li> <li>The recording to start automatically with any further navigation in the browser</li> <li>Follow any steps needed to reproduce the issue with the network pane open</li> <li>Once the issue has occurred, right click in the pane and select Save all as HAR</li> </ul>"},{"location":"000020177/#upload-the-har-file","title":"Upload the HAR file","text":"<p>Generally, HAR files are small in size, however if the file are too large to upload directly to the ticket, please request or use the provided temporary upload location.</p>"},{"location":"000020177/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020178/","title":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x","text":"<p>This document (000020178) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020178/#situation","title":"Situation","text":""},{"location":"000020178/#how-to-conduct-cis-hardening-benchmark-scanning-for-rancher-v23x_1","title":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x","text":"<p>CIS Benchmarks are best practices for the secure configuration of a target system. Available for more than 140 technologies, CIS Benchmarks are developed through a unique consensus-based process comprised of cybersecurity professionals and subject matter experts around the world. CIS Benchmarks are the only consensus-based, best-practice security configuration guides both developed and accepted by government, business, industry, and academia.</p> <p>This script is based on <code>CIS Benchmark Rancher Self-Assessment Guide v2.3</code> https://rancher.com/docs/rancher/v2.x/en/security/benchmark-2.3, which was derived from <code>CIS Kubernetes</code> <code>Benchmark v1.4.1</code>.</p>"},{"location":"000020178/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher version 2.3.x</li> <li>Kubernetes version 1.15</li> <li><code>jq</code>, <code>grep</code>, <code>awk</code> and <code>kubectl</code> installed on target node</li> </ul>"},{"location":"000020178/#steps","title":"Steps","text":"<ol> <li>Clone the script into the target node <code>git clone https://github.com/nickngch/rancher-hardening.git</code></li> <li>Access the folder <code>cd rancher-hardening</code></li> <li>Execute the script based on the node's role</li> <li>For Control Plane - <code>sudo bash ./master.sh 2.3 cp</code></li> <li>For Control Plane + ETCD - <code>sudo bash ./master.sh 2.3 all</code></li> <li>For ETCD - <code>sudo bash ./master.sh 2.3 etcd</code></li> <li>For worker node - <code>sudo ./worker.sh 2.3</code></li> </ol>"},{"location":"000020178/#limitation","title":"Limitation","text":"<ul> <li>Section 1.6 and 1.7 in master node require manual verification.</li> </ul>"},{"location":"000020178/#further-reading","title":"Further reading","text":"<p>https://www.cisecurity.org/cis-benchmarks/cis-benchmarks-faq/</p>"},{"location":"000020178/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020179/","title":"How to conduct performance testing with Clusterloader2","text":"<p>This document (000020179) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020179/#situation","title":"Situation","text":""},{"location":"000020179/#how-to-conduct-performance-testing-with-clusterloader2_1","title":"How to conduct performance testing with Clusterloader2","text":"<p>Clusterloader is an opensource performance testing tool to measure the performance metrics of your Kubernetes cluster.</p>"},{"location":"000020179/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Linux or Mac machine that has Golang and kubectl installed</li> <li>SSH key of the Kubernetes master node</li> <li>Kubeconfig file of the target cluster</li> </ul>"},{"location":"000020179/#steps","title":"Steps","text":"<ol> <li>Create a folder named k8s.io under <code>~/go/src/</code>:</li> </ol> <pre><code>mkdir ~/go/src/k8s.io\n</code></pre> <ol> <li>Clone the perf-test under k8s.io folder:</li> </ol> <pre><code>cd ~/go/src/k8s.io &amp;&amp; git clone https://github.com/galal-hussein/perf-tests.git\n</code></pre> <ol> <li>Navigate to the clusterloader2 directory:</li> </ol> <pre><code>cd ~/go/src/k8s.io/perf-tests/clusterloader2\n</code></pre> <ol> <li>Edit the testconfig according to the environment:</li> </ol> <pre><code>vim testing/load/config.yaml\n</code></pre> <ol> <li>Execute the clusterloader2 with appropriate options:</li> </ol> <pre><code>KUBE_SSH_USER=&lt;SSH USERNAME&gt; LOCAL_SSH_KEY=&lt;SSH KEY PATH&gt; go run cmd/clusterloader.go --nodes 3 --mastername=&lt;MASTER NODE NAME&gt; --kubeconfig=&lt;KUBECONFIG FILE PATH&gt; --provider=local --masterip=&lt;MASTER NODE IP ADDRESS&gt; --testconfig=testing/&lt;TESTING SUBJECT&gt;/config.yaml --report-dir=/tmp/reports 2&gt;&amp;1 | tee /tmp/tmp.log\n</code></pre> <ol> <li>The results of the testing will be stored in the <code>/tmp/reports</code> directory.</li> </ol>"},{"location":"000020179/#faq","title":"FAQ","text":"<pre><code>Errors: [config reading error: decoding failed: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal number -4611686018427388 into Go struct field Phase.Steps.Phases.ReplicasPerNamespace of type int32]\"\n</code></pre> <ul> <li>Change the value of <code>{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}}</code> to match the number of the nodes in the config.yaml file</li> </ul> <pre><code>level=warning msg=\"Got errors during step execution: [measurement call APIResponsiveness - APIResponsiveness error: unexpected response: \\\"# HELP aggregator_openapi_v2_regeneration_count [ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason.\n</code></pre> <ul> <li>Comment out the APIResponsiveness section in config.yaml:</li> </ul> <pre><code>measurements:\n- Identifier: APIResponsiveness\nMethod: APIResponsiveness\nParams:\n    action: reset\n</code></pre>"},{"location":"000020179/#further-reading","title":"Further reading","text":"<p>https://github.com/kubernetes/perf-tests</p>"},{"location":"000020179/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020180/","title":"How do I edit my cluster using RKE Templates?","text":"<p>This document (000020180) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020180/#situation","title":"Situation","text":""},{"location":"000020180/#question","title":"Question","text":"<p>After converting / managing a cluster using RKE Templates, when trying to make changes under \"Edit Cluster\" the 'Edit' button is gone and removed features such as the Kubernetes version dropdown menu. Where did this go?</p>"},{"location":"000020180/#pre-requisites","title":"Pre-requisites","text":"<p>Kubernetes clusters managed by the RKE Template feature</p>"},{"location":"000020180/#answer","title":"Answer","text":"<p>If your Kubernetes cluster now has an RKE Template attached, you now need to make changes to your cluster in the RKE Template section</p> <p>Navigate to Global --&gt; Tools ---&gt; RKE Tempates</p> <p>Click the three-dot menu to make a new revision</p> <p>Here is where you will make changes to the cluster configuration and save it as a new version. However it won't take effect immediately.</p> <p>After saving the revision, navigate back to your cluster, click Edit. Under \"Cluster Options\", there will be a drop down menu to select which version of your template you want to use. Select your new version and Save.</p>"},{"location":"000020180/#further-reading","title":"Further Reading","text":"<ul> <li>https://rancher.com/docs/rancher/v2.x/en/admin-settings/rke-templates/</li> </ul>"},{"location":"000020180/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020181/","title":"How to configure Okta Auth with Rancher HA","text":"<p>This document (000020181) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020181/#situation","title":"Situation","text":""},{"location":"000020181/#issue","title":"Issue","text":"<p>When configuring Okta Authentication using the Rancher Official Documentation in a Rancher HA environment you encounter 501 errors when trying to verify and enable the configuration.</p>"},{"location":"000020181/#cause","title":"Cause","text":"<p>For Rancher to fully enable Okta Authenication it requires a succesful test of your configuration to verify the information is correct. When the test request is sent from one of your Rancher Servers to Okta the returned verification is routed through a Load Balancer to a different Rancher Server in the cluster. As the recipient has not yet been configured to service Okta Authentication it will return a 501 for the request and the Rancher Server that acted as a requester will fail to enable as it could not complete the verification.</p>"},{"location":"000020181/#resolution","title":"Resolution","text":""},{"location":"000020181/#assumptions","title":"Assumptions","text":"<p>You have appropriately configured Okta Authentication according to the Rancher Official Documentation.</p>"},{"location":"000020181/#steps-to-resolve","title":"Steps to Resolve","text":"<ol> <li>Using the Nodes Tab in your Rancher Management Cluster cordon off the nodes you are not currently connected to, this will force traffic to be returned to the Requester.</li> <li>Run the test and enable procedure for Okta Configuration from Rancher and verify you can now login successfully.</li> <li>Uncordon the other Nodes and the settings will be synced across the cluster automatically.</li> <li>Verify the cluster is working as expected by logging in using an Okta sign-in.</li> </ol> <p>(Optional) To verify the settings have been synced to all nodes in the cluster you can cordon off all but another Node, not the one you used to configure, and attempt logging in using Okta. This process can be repeated for each node.</p>"},{"location":"000020181/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020182/","title":"Rancher pre v1.6.22 \"Hosts stuck Reconnecting\" Rancher server logs show 'Cursor returned more than one result'","text":"<p>This document (000020182) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020182/#situation","title":"Situation","text":""},{"location":"000020182/#issue","title":"Issue","text":"<p>Hosts getting stuck in either state Reconnecting or Finishing-Reconnect and Rancher server logs include errors like the following: <code>2019-02-26 12:05:55,265 ERROR [51e1303d-21b2-409f-ba2e-7542e8de4941:9663402] [healthcheckInstanceHostMap:445975] [healthcheckinstancehostmap.remove] [] [ecutorService-3] [c.p.e.p.i.DefaultProcessInstanceImpl] Unknown exception org.jooq.exception.InvalidResultException: Cursor returned more than one result</code></p>"},{"location":"000020182/#pre-requisites","title":"Pre-requisites","text":"<p>Rancher version lower than 1.6.22</p>"},{"location":"000020182/#workaround","title":"Workaround","text":"<ol> <li>In the Rancher MySQL database, find all the duplicates by checking column 3 for entries with more than a count of 1 in the return from the following query:</li> </ol> <pre><code>select host_id,healthcheck_instance_id,count(*) from healthcheck_instance_host_map where removed is null group by host_id,healthcheck_instance_id order by 3;\n</code></pre> <ol> <li>For each healthcheck_instance_id in any row with more than 1 in column 3, run the following command:</li> </ol> <pre><code>update healthcheck_instance_host_map set state='removed', removed=now(), remove_time=now() where healthcheck_instance_id='&lt;INSERT_HEALTHCHECK_ID&gt;';\n</code></pre> <ol> <li>Wait and watch the hosts view. The hosts should all finish reconnecting and instances should update.</li> </ol>"},{"location":"000020182/#resolution","title":"Resolution","text":"<p>Upgrade to 1.6.22+ or 2.x</p>"},{"location":"000020182/#further-reading","title":"Further reading","text":"<p>https://github.com/rancher/rancher/issues/15284</p>"},{"location":"000020182/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020183/","title":"Admins cannot edit or see node templates created by another user in Rancher v2.0.0-v2.3.2.","text":"<p>This document (000020183) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020183/#situation","title":"Situation","text":""},{"location":"000020183/#issue","title":"Issue","text":"<p>Admins cannot edit or see node templates created by another user in Rancher v2.0.0 to v2.3.2. As a result, they are also unable to edit a cluster with a user that did not create it, even if that user is an admin.</p>"},{"location":"000020183/#workaround","title":"Workaround","text":""},{"location":"000020183/#change-node-template-owner","title":"Change node template owner","text":"<p>This script will change your node template owner in Rancher 2.x. You can run this script as a Docker image or directly as a bash script. You'll need the cluster ID and the user ID you want to change the ownership to.</p> <ol> <li>To obtain the cluster ID in the Rancher user interface, Navigate to Global &gt; \"Your Cluster Name\", then grab the cluster ID from your address bar. I have listed an example of the URL and a cluster ID derrived from the URL below.</li> </ol> <p>- Example URL: <code>https://&lt;RANCHER URL&gt;/c/c-48x9z/monitoring</code>     - Derived cluster ID from above URL: c-48x9z</p> <ol> <li> <p>Now we need the user ID of the user to become the new node template owner, navigate to Global &gt; Users to find the ID.</p> </li> <li> <p>To run the script using a docker image, make sure your $KUBECONFIG is set to the full path of your Rancher local cluster kube config then run the following command.</p> </li> </ol> <pre><code>docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -c &lt;cluster-id&gt; -n &lt;user-id&gt;\n</code></pre> <ol> <li>To run the script directly, just download the change-nodetemplate-owner.sh script, make sure your $KUBECONFIG or ~/.kube/config is pointing to the correct Rancher local cluster then run the following command:</li> </ol> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh\nbash change-nodetemplate-owner.sh -c &lt;cluster-id&gt; -n &lt;user-id&gt;\n</code></pre>"},{"location":"000020183/#assign-a-node-template-to-a-clusters-node-pool","title":"Assign a node template to a cluster's node pool","text":"<p>Assign a node template to a cluster's node pool. This is useful for situations where the original owner of a cluster has been deleted which also deletes their node templates. To use this task successfully it is recommended that you create a new node template in the UI before using it. Make sure the node template matches the original ones as closely as possible. You will be shown options to choose from and prompted for confirmation.</p> <p>Run script with docker image:</p> <pre><code>docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -t changenodetemplate -c &lt;cluster-id&gt;\n</code></pre> <p>Run script from bash command line:</p> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh\nbash change-nodetemplate-owner.sh -t changenodetemplate -c &lt;cluster-id&gt;\n</code></pre>"},{"location":"000020183/#resolution","title":"Resolution","text":"<p>Upgrade to Rancher v2.3.3 or newer to receive the fix to this issue. More information on this bug can be found at the GitHub issue #12186.</p>"},{"location":"000020183/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020184/","title":"When will my cluster certificates expire?","text":"<p>This document (000020184) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020184/#situation","title":"Situation","text":""},{"location":"000020184/#question","title":"Question","text":"<p>I have a cluster which is displaying the message \"This cluster has certs that are expiring or have expired\". Is there any way to determine when the certs will expire?</p>"},{"location":"000020184/#pre-requisites","title":"Pre-requisites","text":"<p>Kubernetes clusters provisioned via the RKE, or Rancher launched Kubernetes clusters</p>"},{"location":"000020184/#answer","title":"Answer","text":"<p>Dates on when particular certificates will expire are located in the Rancher API.</p> <p>To view them, from the \"Clusters\" page, click the three-dot menu on your cluster and \"View in API\" Scroll down or search for the section called \"certificatesExpiration\" There you can see the expiration date for each kubernetes component</p> <p>FYI, as of Rancher 2.3.5, there is a known issue that old, removed nodes still appear in this list. This is cosmetic and won't affect your running cluster: https://github.com/rancher/rancher/issues/24333</p>"},{"location":"000020184/#further-reading","title":"Further Reading","text":"<ul> <li>https://rancher.com/blog/2019/kubernetes-certificate-expiry-and-rotation-in-rancher-kubernetes-clusters</li> </ul>"},{"location":"000020184/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020185/","title":"Why does a cluster or node show requested memory with a milli (m) unit?","text":"<p>This document (000020185) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020185/#situation","title":"Situation","text":""},{"location":"000020185/#question","title":"Question","text":"<p>Why does a cluster or node show requested memory with a milli (m) unit?</p>"},{"location":"000020185/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>provisioned Kubernetes cluster</li> </ul>"},{"location":"000020185/#answer","title":"Answer","text":"<p>CPU resources in Kubernetes are measured in millicpus, or 1/1000th of a CPU core, and 1 CPU Core = 1000m. The API will change any request for a decimal point into millicpus. For example 0.1 is converted into 100m. One hyperthread is considered one core, or 1000m.</p> <p>Memory resources in Kubernetes are mesured in bytes, and can be expressed as an integer with one of these suffixes: E, P, T, G, M, K - decimal suffixes, or Ei, Pi, Ti, Gi, Mi, Ki - binary suffixes (more commonly used for memory), or omit the suffix altogether. Lowercase \"m\" notation is not a recommended suffix for memory.</p> <p>The \"m\" notation for memory might indicate a misconfiguration, where CPU units are recommended to use that suffix (example: 200m), and Memory units are recommended to use \"Mi\" (example: 128Mi).</p>"},{"location":"000020185/#further-reading","title":"Further Reading","text":"<p>Explaination of Kubernetes CPU resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu ) Explaination of Kubernetes memory resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory )</p>"},{"location":"000020185/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020186/","title":"How To Update CoreDNS's Resolver Policy","text":"<p>This document (000020186) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020186/#situation","title":"Situation","text":""},{"location":"000020186/#task","title":"Task","text":"<p>This article outlines how to change CoreDNS's forward resolver policy.</p>"},{"location":"000020186/#pre-requisites","title":"Pre-requisites","text":"<p>A custom cluster provisioned by Rancher or RKE with CoreDNS.</p> <p>The CoreDNS docs explain the various configurations. In this case, we are concerned with the policy. Which defaults to <code>random</code>.</p>"},{"location":"000020186/#resolution","title":"Resolution","text":"<p>To change the policy to <code>sequential</code>, edit your clusters yaml. For RKE provisioned clusters this will be your <code>cluster.yaml</code> and for Rancher provisioned custom clusters this will be found by editing the cluster.</p> <p>For RKE add the following to the end of the file, but for Rancher provisioned clusters, nest this in the <code>rancher_kubernetes_engine_config</code> section.</p> <pre><code>addons: |-\n  ---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: coredns\n    namespace: kube-system\ndata:\n    Corefile: |\n      .:53 {\n          errors\n          health\n          ready\n          kubernetes cluster.local in-addr.arpa ip6.arpa {\n            pods insecure\n            fallthrough in-addr.arpa ip6.arpa\n          }\n          prometheus :9153\n          forward . \"/etc/resolv.conf\" {\n            policy sequential\n          }\n          cache 30\n          loop\n          reload\n          loadbalance\n      }\n</code></pre> <p>The lines of note here are the following, which are changed from just <code>forward . \"/etc/resolv.conf\"</code>.</p> <pre><code>          forward . \"/etc/resolv.conf\" {\n            policy sequential\n          }\n</code></pre> <p>At this point you should be able to just hit save in Rancher or run <code>rke up</code> and the change will be pushed to the cluster.</p>"},{"location":"000020186/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020187/","title":"How to update your etcd space alerts for better etcd monitoring","text":"<p>This document (000020187) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020187/#situation","title":"Situation","text":""},{"location":"000020187/#task","title":"Task","text":"<p>An alert you may have seen is <code>Database usage close to the quota 500M</code> in your Rancher 2.x cluster.</p> <p>This is a default etcd alert built into Rancher, you can find more info on the default alerts here: Rancher v2.x Default Alerts</p> <p>Upon further examination of the alert, you see the description of</p> <p>A warning alert is triggered when the size of etcd exceeds 500M.</p> <p>This alert is somewhat misleading as the default etcd size is 2GB. This alert is also at a severity level of Warning. Below, we suggest configuring your etcd alerts to better utilize the alert thresholds and to avoid the default alert constantly going off.</p>"},{"location":"000020187/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> </ul>"},{"location":"000020187/#resolution","title":"Resolution","text":"<p>You can access the alerts by going to <code>Tools -&gt; Alerts</code> at the cluster level. From here, clone the default alert three times. Once the 3 clones are created, disable the default alert so you always retain a clean reference alert.</p> <p>Alert for 1GB usage - INFO</p> <p>Alert for 1.5GB usage - WARNING</p> <p>Alert for 1.75GB usage - CRITICAL</p>"},{"location":"000020187/#if-you-are-running-out-of-space-please-reference-the-following-two-links-there-will-be-an-upcoming-kb-article-walking-through-the-resize-process-for-etcd","title":"If you are running out of space, please reference the following two links. There will be an upcoming KB article walking through the resize process for etcd.","text":"<p>Rancher v2.x etcd options</p> <p>etcd space quota documentation</p>"},{"location":"000020187/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020188/","title":"The RancherOS log collector script","text":"<p>This document (000020188) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020188/#situation","title":"Situation","text":""},{"location":"000020188/#rancheros-log-collection","title":"RancherOS log collection","text":"<p>Host logs and information can be collected from a node running RancherOS using the RancherOS log collector script.</p> <p>The script needs to be downloaded and run directly on the host using sudo, as follows:</p> <pre><code>wget https://raw.githubusercontent.com/rancher/os/master/scripts/tools/collect_rancheros_info.sh | sudo sh\n</code></pre> <p>The output will be written to <code>/tmp</code> to a tar file named <code>rancheros_export_&lt;datetime&gt;.tar</code></p>"},{"location":"000020188/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020189/","title":"How to test websocket connections to Rancher v2.x","text":"<p>This document (000020189) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020189/#situation","title":"Situation","text":""},{"location":"000020189/#task","title":"Task","text":"<p>Rancher depends heavily on websocket support for UI and CLI features within Rancher as well as managing and interacting with downstream clusters. This article provides a quick test to determine if websocket connections are working from a potential downstream node or client to the Rancher server cluster.</p>"},{"location":"000020189/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster.</li> </ul>"},{"location":"000020189/#executing-the-test","title":"Executing the test","text":"<p>First you will need to create an API token to authenticate against Rancher. Start by logging into the Rancher UI. Once logged in, navigate to the API &amp; Keys section by clicking the user icon in the top right of the pane, then click on the API &amp; Keys menu item. Generate a new key by clicking the Add Key button, providing a name for the token and clicking Create. Copy the bearer token to a safe location.</p> <p>In a Linux shell from the desired test node execute the following, substituting the bearer token and fully qualified domain name of your Rancher endpoint with these environmental variables:</p> <pre><code>export TOKEN=&lt;your token here&gt;\nexport FQDN=&lt;your Rancher fully qualified domain name here&gt;\n</code></pre> <p>Next execute the test using the following command:</p> <pre><code>curl -s -i -N \\\n  --http1.1 \\\n  -H \"Connection: Upgrade\" \\\n  -H \"Upgrade: websocket\" \\\n  -H \"Sec-WebSocket-Key: SGVsbG8sIHdvcmxkIQ==\" \\\n  -H \"Sec-WebSocket-Version: 13\" \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Host: $FQDN\" \\\n  -k https://$FQDN/v3/subscribe\n</code></pre> <p>If websockets work this will successfully connect to the Rancher server and print a steady stream of json output reflecting configuration items being sent from the server. In the event of a failed connection this should print a meaningful error you can act upon to get websockets working between your client and Rancher server.</p> <p>The below is an example of the output from the test upon a successfully established websocket:</p> <pre><code>HTTP/1.1 101 Switching Protocols\nDate: Tue, 21 Jan 2020 04:54:05 GMT\nConnection: upgrade\nServer: openresty/1.15.8.1\nUpgrade: websocket\nSec-WebSocket-Accept: qGEgH3En71di5rrssAZTmtRTyFk=\n\n{\"name\":\"resource.change\",\"data\":{\"baseType\":\"listenConfig\",\"created\":\"2020-01-04T22:34:26Z\",\"createdTS\":1578177266000,\"creatorId\":null,\"enabled\":true,\"generatedCerts\":{\"local/10.42.0.7\":\"*CERT_CONTENTS_REDACTED*\"},\"id\":\"cli-config\",\"keySize\":0,\"knownIps\":[\"10.42.0.7\",\"10.42.0.8\"],\"labels\":{\"cattle.io/creator\":\"norman\"},\"links\":{\"remove\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"self\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"update\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\"},\"mode\":\"https\",\"tos\":\"auto\",\"type\":\"listenConfig\",\"uuid\":\"511129ca-aa2c-4d16-a8e5-2d77cb171d61\",\"version\":0}\n</code></pre>"},{"location":"000020189/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020190/","title":"The Rancher v1.6 log collector script","text":"<p>This document (000020190) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020190/#situation","title":"Situation","text":""},{"location":"000020190/#rancher-v16-log-collection","title":"Rancher v1.6 log collection","text":"<p>Logs can be collected from a node within a Rancher v1.6 cluster using the Rancher v1.6 log collector script.</p> <p>The script needs to be downloaded and run directly on the host using the root user or using sudo, as follows:</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v1.6/logs-collector/rancher16_logs_collector.sh | sudo bash -s\n</code></pre> <p>The output will be written to <code>/tmp</code> to a gziped tar file named <code>&lt;hostname&gt;-&lt;datetime&gt;.tar.gz</code></p>"},{"location":"000020190/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020191/","title":"The Rancher v2.x Linux log collector script","text":"<p>This document (000020191) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020191/#situation","title":"Situation","text":""},{"location":"000020191/#rancher-v2x-linux-log-collector","title":"Rancher v2.x Linux log collector","text":"<p>Logs can be collected from a Linux node using the Rancher v2.x log collector script.</p> <p>Note: This script is intended to collect logs from Rancher Kubernetes Engine (RKE) CLI provisioned clusters, K3s clusters, RKE2 clusters, Rancher provisioned Custom, and Node Driver clusters.</p> <p>This script may not collect all necessary information when run on nodes in Hosted Kubernetes Provider clusters.</p> <p>The script needs to be downloaded and run directly on the node, using the root user or sudo.</p> <p>Output will be written to <code>/tmp</code> as a tar.gz archive named <code>&lt;hostname&gt;-&lt;date&gt;.tar.gz</code>, the default output directory can be changed with the <code>-d</code> flag.</p>"},{"location":"000020191/#download-and-run-the-script","title":"Download and run the script","text":"<ul> <li>Download the script as: <code>rancher2_logs_collector.sh</code></li> </ul> <p>Using <code>wget</code>:</p> <pre><code>wget https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh\n</code></pre> <p>Using <code>curl</code>:</p> <pre><code>curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh\n</code></pre> <ul> <li>Run the script:</li> </ul> <pre><code>sudo bash rancher2_logs_collector.sh\n</code></pre>"},{"location":"000020191/#optional-download-and-run-the-script-in-one-command","title":"Optional: Download and run the script in one command","text":"<pre><code>curl -Ls rnch.io/rancher2_logs | sudo bash\n</code></pre> <p>Note: This command requires curl to be installed, and internet access from the node.</p>"},{"location":"000020191/#options","title":"Options","text":"<p>The available flags that can be passed to the script can be found in the Rancher v2.x log collector script README.</p>"},{"location":"000020191/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020192/","title":"The Rancher v2.x systems summary script","text":"<p>This document (000020192) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020192/#situation","title":"Situation","text":""},{"location":"000020192/#rancher-v2x-systems-summary","title":"Rancher v2.x systems summary","text":"<p>Understanding your cluster/node distribution on an on-going basis assists Rancher in sending you any prescriptive advisories related to scale and performance.</p> <p>System information can be collected from a Rancher v2.x server node using the Rancher v2.x systems summary script.</p> <p>The script needs to be downloaded and run directly on a host running a Rancher server container, either as a single node install or a Rancher Pod as part of a High Availability install. The script needs to be run by a user with access to the Docker socket or using sudo, as follows:</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sudo bash -s\n</code></pre>"},{"location":"000020192/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020193/","title":"What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?","text":"<p>This document (000020193) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020193/#situation","title":"Situation","text":""},{"location":"000020193/#question","title":"Question","text":"<p>What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?</p>"},{"location":"000020193/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.0.x - v2.3.x. Note, Kubernetes upgrades will be changing in v2.4.x, see Further Reading below.</li> </ul> <p>OR</p> <ul> <li>RKE CLI v0.2.x+</li> </ul>"},{"location":"000020193/#answer","title":"Answer","text":"<p>Rancher, either through the UI or API, can be used to upgrade a Kubernetes cluster that was provisioned using the \"Custom\" option or on cloud infrastructure such as AWS EC2 or Azure. This can be accomplished by editing the cluster and selecting the desired Kubernetes version. Clusters provisioned with the RKE CLI can also be upgraded by editing the kubernetes_version key in the cluster YAML file. This will trigger an update of all the Kubernetes components in the order listed below:</p>"},{"location":"000020193/#etcd-plane","title":"Etcd plane","text":"<p>Each etcd container is updated, one node at a time. If the etcd version has not changed between versions of Kubernetes, no action is taken. The process consists of:</p> <ol> <li>Downloading etcd image</li> <li>Stopping and renaming old etcd container (backend datastore is preserved on host)</li> <li>Creating and starting new etcd container</li> <li>Running etcd health check</li> <li>Removing old etcd container</li> </ol> <p>For RKE CLI provisioned clusters, the etcd-rolling-snapshot container is also upgraded if a new version is available.</p>"},{"location":"000020193/#control-plane","title":"Control plane","text":"<p>Every Kubernetes update will require the control plane components to be updated. All control plane nodes are updated in parallel. The process consists of:</p> <ol> <li>Downloading hyperkube image, which is used by all control plane components.</li> <li>Stopping and renaming old kube-apiserver container</li> <li>Creating and starting new kube-apiserver container</li> <li>Running kube-apiserver health check</li> <li>Removing old kube-apiserver container</li> <li>Stopping and renaming old kube-controller-manager container</li> <li>Creating and starting new kube-controller-manager container</li> <li>Running kube-controller-manager health check</li> <li>Removing old kube-controller-manager container</li> <li>Stopping and renaming old kube-scheduler container</li> <li>Creating and starting new kube-scheduler container</li> <li>Running kube-scheduler health check</li> <li>Removing old kube-scheduler container</li> </ol>"},{"location":"000020193/#worker-plane","title":"Worker plane","text":"<p>Every Kubernetes update will require the worker components to be updated. These components run on all nodes, including the control plane and etcd. Nodes are updating in parallel. The process consists of:</p> <ol> <li>Downloading hyperkube image (if not already present)</li> <li>Stopping and renaming old kubelet container</li> <li>Creating and starting new kubelet container</li> <li>Running kubelet health check</li> <li>Removing old kubelet container</li> <li>Stopping and renaming old kube-proxy container</li> <li>Creating and starting new kube-proxy container</li> <li>Running kube-proxy health check</li> <li>Removing old kube-proxy container</li> </ol>"},{"location":"000020193/#addons-user-workloads","title":"Addons &amp; user workloads","text":"<p>Once Kubernetes etcd, control plane, and worker components have been updated, the latest manifests for addons are applied. This includes, but is not limited to KubeDNS/CoreDNS, Nginx Ingress, Metrics Server, and CNI plugin (Calico, Weave, Flannel, Canal). Depending on the manifest deltas and the upgrade strategy defined in the manifest, pods and their corresponding containers may or may not be removed and recreated. Please be aware that some of these addons are critical for your cluster to operator correctly and you may experience brief outages if these workloads are restarted. For example, when KubeDNS/CoreDNS is restarted, you could have issues resolving hostname to IP addresses. When the Nginx Ingress is restarted, layer 7 http/https traffic from outside your cluster to your workloads may get interrupted. When your CNI plugin is restarted on each node, the workloads running on the node may temporarily not be able to reach workloads running on other nodes. The best way to minimize outages or disruptions is to make sure you have proper fault tolerance in your cluster.</p> <p>The kubelet automatically destroys and recreates all user workload pods when the spec hash value is changed. This value will change for a pod if the Kubernetes upgrade involves any field changes in the pod manifest, such as a new field or the removal of a deprecated field. As a best practice, it's best to assume all your pods and containers will be destroyed and recreated during a Kubernetes upgrade. This is more likely to happen for major/minor releases and less likely for patch releases.</p>"},{"location":"000020193/#further-reading","title":"Further Reading","text":"<p>Upgrade refactor in v2.4: https://github.com/rancher/rancher/issues/23038</p> <p>Kubeadm upgrades: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p>"},{"location":"000020193/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020194/","title":"What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?","text":"<p>This document (000020194) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020194/#situation","title":"Situation","text":""},{"location":"000020194/#question","title":"Question","text":"<p>What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?</p>"},{"location":"000020194/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x HA deployed using Helm.</li> </ul>"},{"location":"000020194/#answer","title":"Answer","text":"<p>The bulk of the Rancher HA installation and upgrade are performed by using Helm. The core piece of the Rancher Helm Chart is the Rancher deployment. Please note the following characteristics of this Helm Chart:</p> <ul> <li>Deployment is set to a replica of 3. This means Kubernetes will attempt to run and maintain three rancher pods.</li> <li>Deployment is set to do a rolling update with a max surge of 25% and max unavailability of 25%. This means:</li> <li>During an upgrade, pods are updated in chunks, not all at once.</li> <li>During an update, no more than 4 pods will be running at once</li> <li>During an update, no fewer than 2 pods will be available at once</li> <li>Deployment has an anti-affinity for the node's hostname. This means Kubernetes will attempt to place each pod on a separate host. For three pods and three hosts, that means one pod on each host.</li> </ul> <p>Rancher will also apply two other important manifests to the Rancher HA cluster as well as all managed clusters. These are described below:</p>"},{"location":"000020194/#cattle-cluster-agent-deployment","title":"cattle-cluster-agent deployment","text":"<ul> <li>Deployment is set to a replica of 1</li> <li>Deployment is set to do a rolling update with a max surge of 25% and a max unavailability of 25%. See Rancher's deployment description above for the behavior of these settings.</li> </ul>"},{"location":"000020194/#cattle-node-agent-daemonset","title":"cattle-node-agent daemonset","text":"<ul> <li>Daemonset will deploy one agent per node</li> <li>Daemonset is set to a rolling update with max unavailable of 1 pod. That means during an update, one pod is updated at a time.</li> </ul> <p>Given the information above on how the manifests are defined, below is the expected sequence of events during a Rancher upgrade:</p>"},{"location":"000020194/#rancher-ha-cluster","title":"Rancher HA cluster","text":"<ol> <li>A new rancher pod is created</li> <li>An old rancher pod is terminated</li> <li>A new second rancher pod is created</li> <li>A second old rancher pod is terminated</li> <li>A new third rancher pod is created</li> <li>A third old rancher pod is terminated</li> <li>The latest versions of the cattle-cluster-agent and cattle-node-agent manifests are updated and deployed on the cluster. These deployments are triggered in parallel and will result in a new cattle-cluster-agent and new cattle-node-agents running on the cluster.</li> </ol>"},{"location":"000020194/#downstream-clusters","title":"Downstream clusters","text":"<p>Once Rancher is upgraded, Rancher will check each cluster it manages to make sure the cattle-cluster-agent and cattle-node-agents are up to date. If the cluster is not in a \"Provisioning\" state, meaning another cluster update is in progress, it will deploy the latest cattle-cluster-agent and cattle-node-agent manifests into the cluster. All managed clusters are updated in parallel and not sequentially.</p> <p>Other workloads running in the cluster should not be impacted.</p>"},{"location":"000020194/#further-reading","title":"Further Reading","text":"<p>Kubernetes deployments - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</p> <p>Kubernetes daemonsets - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</p>"},{"location":"000020194/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020195/","title":"How to rollback the Kubernetes version of a Rancher v2.x provisioned cluster","text":"<p>This document (000020195) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020195/#situation","title":"Situation","text":""},{"location":"000020195/#task","title":"Task","text":"<p>This article details how to rollback the Kubernetes version of a Rancher v2.x provisioned cluster.</p>"},{"location":"000020195/#important-note","title":"Important Note:","text":"<p>A Kubernetes Cluster Rollback will most definitely cause downtime in the cluster, as you are restoring a snapshot from before the upgrade and the cluster will have to reconcile state.</p>"},{"location":"000020195/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>In order to rollback your Kubernetes cluster version upgrade, you need to have first taken an etcd snapshot from before the upgrade. You should keep the reference to the snapshot name that was created as your \"pre-upgrade\" snapshot. In my cluster which has cluster ID: <code>c-q8st7</code> , my snapshot name was <code>c-q8st7-ml-qdxdh</code>. Our example upgrade is from <code>v1.14.9-rancher1-2</code> to <code>v1.15.7-rancher1-1</code>.</li> </ul>"},{"location":"000020195/#rollback-operation","title":"Rollback operation","text":"<p>In order to rollback, you must:</p> <ol> <li>Edit Cluster</li> <li>Edit as YAML</li> <li>Set <code>kubernetes_version</code> back to <code>v1.14.9-rancher1-2</code> (or whatever your desired restore version is)</li> <li>Find the <code>restore</code> key in the YAML.</li> </ol> <p>You will need to update the following configuration:</p> <pre><code>rancher_kubernetes_engine_config:\n       restore:\n       restore: false\n</code></pre> <p>You'll want to closely model the following:</p> <pre><code>rancher_kubernetes_engine_config:\n       restore:\n       restore: true\n       snapshot_name: \"c-q8st7:c-q8st7-ml-qdxdh\"\n</code></pre> <p>Note the <code>snapshot_name</code> has the cluster ID prefixed to it with a <code>:</code> .</p> <ol> <li>Finally, you can save the cluster, and observe the snapshot restore + K8s version rollback occur.</li> </ol>"},{"location":"000020195/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020196/","title":"HTTP 401 \"clusterID does not match\" error using cluster-scoped Rancher API token in Rancher v2.x","text":"<p>This document (000020196) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020196/#situation","title":"Situation","text":""},{"location":"000020196/#issue","title":"Issue","text":"<p>When attempting to perform operations against the Rancher v2.x API, with a cluster-scoped API token, you receive a HTTP 401 response code with a body of the following format:</p> <pre><code>{\n\"type\":\"error\",\n\"status\":\"401\",\n\"message\":\"clusterID does not match\"\n}\n</code></pre>"},{"location":"000020196/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>A cluster-scoped Rancher API token</li> </ul>"},{"location":"000020196/#root-cause","title":"Root cause","text":"<p>The primary purpose of cluster-scoped API tokens is to permit access to the Kubernetes API for a specific cluster via Rancher, i.e. via the endpoint <code>https://&lt;rancher_url&gt;/k8s/clusters/&lt;cluster_id&gt;</code> for the matching cluster. Cluster-scoped tokens can be used to interact directly with the Kubernetes API of clusters configured with an Authorized Cluster Endpoint.</p> <p>In addition, a cluster-scoped token also works for resources under the Rancher v3 API endpoint for that cluster, at <code>https://&lt;rancher_url&gt;/v3/clusters/&lt;cluster_id&gt;</code>.</p> <p>The token is not valid for the other available API endpoints, nor for other clusters. Attempts to perform API operations on other clusters or endpoints with a cluster-scoped token will result in the HTTP 401 <code>\"clusterID does not match\"</code> error.</p>"},{"location":"000020196/#resolution","title":"Resolution","text":"<p>Only use a cluster-scoped API token where you wish to restrict usage of the token to the Kubernetes API for that cluster, or the Rancher v3 cluster endpoint. To permit access to other API endpoints, or to use a token for API access to multiple clusters, create a Rancher API token that is not cluster-scoped.</p>"},{"location":"000020196/#further-reading","title":"Further reading","text":"<p>You can read more on the Rancher v2.x API within the API documentation.</p>"},{"location":"000020196/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020197/","title":"Provisioning of Kubernetes clusters in Rancher v2.x, prior to v2.3.3, using nodes in an infrastructure provider, does not respect NO_PROXY entries in CIDR format","text":"<p>This document (000020197) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020197/#situation","title":"Situation","text":""},{"location":"000020197/#issue","title":"Issue","text":"<p>Attempting to provision a Kubernetes cluster with the vSphere node-driver, in a Rancher v2.x environment, prior to v2.3.3, using a HTTP proxy configuration results in an error of the following format:</p> <pre><code>Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&amp;token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect\n</code></pre> <p>In addition, the Rancher logs show an error message of the following format:</p> <pre><code>...\n2019/12/06 10:23:51 [INFO] [node-controller-docker-machine] (vsphere-all1) Waiting for VMware Tools to come online...\n2019/12/06 10:25:49 [INFO] [node-controller-docker-machine] (vsphere-all1) Provisioning certs and ssh keys...\n2019/12/06 10:27:35 http: TLS handshake error from 127.0.0.1:41746: EOF\n2019/12/06 10:28:03 [INFO] [node-controller-docker-machine] The default lines below are for a sh/bash shell, you can specify the shell you're using, with the --shell flag.\n2019/12/06 10:28:03 [INFO] [node-controller-docker-machine]\n2019/12/06 10:28:04 [INFO] Generating and uploading node config vsphere-all1\n2019/12/06 10:28:04 [ERROR] NodeController c-f6xbs/m-fsl6t [node-controller] failed with : Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&amp;token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect\n...\n</code></pre>"},{"location":"000020197/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, prior to Rancher v2.3.3.</li> <li>A HTTP Proxy configured on Rancher, per the documentation for a single node or High Availability (HA) install of Rancher, in which the vSphere datacenter ESXi hosts are not reachable via the proxy.</li> <li>A Rancher provisioned Kubernetes cluster, using the the vSphere node-driver.</li> <li>The IP Range containing the ESXi hosts within the vSphere datacenter configured in CIDR notation within the Rancher NO_PROXY configuration.</li> </ul>"},{"location":"000020197/#root-cause","title":"Root cause","text":"<p>This issue was caused by the Go version used to build the docker-machine driver that provides the Rancher node driver capabilities, including the vSphere node driver.</p> <p>Support for NO_PROXY entries in CIDR notation was introduced in Go v1.10.x; however, the docker-machine version in Rancher v2.x, prior to v2.3.3, was built using an earlier version of Go.</p> <p>As a result, NO_PROXY entries in CIDR notation did not take effect during cluster provisioning via node drivers, even though these same NO_PROXY entries were observed by the Rancher server itself, built with a later version of Go.</p>"},{"location":"000020197/#workaround","title":"Workaround","text":"<p>To workaround this issue in Rancher v2.x versions before v2.3.3, you should ensure that the vSphere server address, and all ESXi hosts within the vSphere datacenter in which you are provisioning the cluster, are listed as individual IPs within the Rancher NO_PROXY configuration.</p>"},{"location":"000020197/#resolution","title":"Resolution","text":"<p>This issue was tracked in Rancher GitHub issue #21674 and a fix, bumping the Go version of the docker-machine driver to v1.12.9, was released in Rancher v2.3.3. Users can therefore upgrade to Rancher v2.3.3, or above, to resolve this issue.</p>"},{"location":"000020197/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020198/","title":"Launching kubectl for cluster within Rancher UI fails in a cluster after following the CIS Benchmark Hardening Guide for Kubernetes","text":"<p>This document (000020198) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020198/#situation","title":"Situation","text":""},{"location":"000020198/#issue","title":"Issue","text":"<p>Attempting to launch kubectl in the Rancher v2.x UI, for a cluster upon which the Rancher CIS Hardening Guide has been applied, results in a <code>Closed Code: 1006</code> message. Further, using the browser developer tools to inspect requests when opening this page reveals the API request to initiate the connection (https:///v3/clusters/?shell=true) receiving a HTTP 403 response.</p>"},{"location":"000020198/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE CLI or Rancher v2.x launched Kubernetes cluster, with the Rancher v2.1.x, v2.2.x or v2.3.x CIS Hardening Guide applied.</li> </ul>"},{"location":"000020198/#root-cause","title":"Root cause","text":"<p>This behaviour is caused by CIS Control 1.1.12, which specifies that the DenyEscalatingExec Admission Controller should be enabled on the Kubernetes API Server.</p> <p>The terminal for the Rancher UI is provided by exec'ing into a cattle-node-agent Pod, whilst Pods within this DaemonSet run in Privileged mode. As a result the exec to open the terminal session is denied by the DenyEscalatingExec Admission Controller.</p>"},{"location":"000020198/#workaround","title":"Workaround","text":"<p>You can workaround the issue by removing <code>DenyEscalatingExec</code> from the list of <code>enable-admission-plugins</code> in <code>extra_args</code> for the <code>kube-api</code> service.</p>"},{"location":"000020198/#resolution","title":"Resolution","text":"<p>This issue is tracked in the Rancher GitHub issue #19439.</p>"},{"location":"000020198/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020199/","title":"How to increase the log level of Kubernetes components in an RKE CLI or Rancher provisioned Kubernetes cluster","text":"<p>This document (000020199) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020199/#situation","title":"Situation","text":""},{"location":"000020199/#task","title":"Task","text":"<p>When troubleshooting an issue with an RKE CLI or Rancher provisioned Kubernetes cluster, it may be helpful to increase the verbosity of logging on one or more of the Kubernetes components, above the default level. This article details the process of increasing logging on both those components that use the Kubernetes hyperkube image (kubelet, kube-apiserver, kube-controller-manager, kube-scheduler, kube-proxy) as well as the etcd component.</p>"},{"location":"000020199/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the RKE CLI or Rancher v2.x</li> </ul>"},{"location":"000020199/#resolution","title":"Resolution","text":""},{"location":"000020199/#kubernetes-api-server-controller-manager-scheduler-kube-proxy-and-kubelet","title":"Kubernetes API Server, Controller Manager, Scheduler, Kube Proxy and Kubelet","text":"<p>The Kubernetes core components, which run using the Kubernetes hyperkube image, will log ERROR, WARNING and INFO messages. The verbosity of the INFO level log output is controlled by the <code>--v</code> flag, which is set to an integer from 0 to 9. In an RKE CLI or Rancher launched Kubernetes cluster, the <code>--v</code> flag is configured to <code>2</code> by default. At this level, the components will log <code>useful steady state information about the service and important log messages that may correlate to significant changes in the system.</code></p> <p>In order to troubleshoot an issue, it may be useful to increase the verbosity flag to one of the following:</p> <p>VerbosityDescription--v=3Extended information about changes.--v=4Debug level verbosity.--v=6Display requested resources.--v=7Display HTTP request headers.--v=8Display HTTP request contents.--v=9Display HTTP request contents without truncation of contents.</p>"},{"location":"000020199/#update-the-v-flag-in-an-rke-cli-launched-cluster","title":"Update the <code>--v</code> flag in an RKE CLI launched cluster","text":"<ol> <li>First set the <code>--v</code> flag for the desired components within the <code>cluster.yml</code>. For each of the services you wish to change the verbosity on, you should add an extra_args option with <code>v: \"&lt;value&gt;\"</code> in the services block, per the example below. The appropriate name for each service within this block can be found within the RKE documentation. N.B. Please see the separate section below for updating the log verbosity of the etcd component</li> </ol> <pre><code>     services:\n       kube-api:\n         extra_args:\n           v: '9'\n</code></pre> <ol> <li>Having set the flag in the cluster.yml, run <code>rke up --config cluster.yml</code> to update the cluster with the new configuration.</li> </ol>"},{"location":"000020199/#update-the-v-flag-in-a-rancher-launched-cluster","title":"Update the <code>--v</code> flag in a Rancher launched cluster","text":"<p>Navigate to the cluster within the Rancher UI and click <code>Edit Cluster</code>, then <code>Edit as YAML</code>. For each of the services you wish to change the verbosity on, you should add an extra_args option with <code>v: \"&lt;value&gt;\"</code> in the services block of the cluster, per the example below.</p> <p>N.B. Please see the separate section below for updating the log verbosity of the etcd component.</p> <pre><code>services:\n    kube-api:\n      extra_args:\n        v: '9'\n</code></pre> <p>The appropriate name for each service within this block can be found within the RKE documentation.</p> <p>Having set the verbosity flag, click <code>Save</code> at the bottom of the page, to update the cluster.</p>"},{"location":"000020199/#etcd","title":"etcd","text":"<p>The etcd component is configured to log at an INFO level by default, in an RKE CLI or Rancher launched Kubernetes cluster, but this can be set to DEBUG level by setting the <code>--debug=true</code> flag.</p>"},{"location":"000020199/#update-etcd-verbosity-in-an-rke-cli-launched-cluster","title":"Update etcd verbosity in an RKE CLI launched cluster","text":"<ol> <li>First set the <code>--debug=true</code> flag, within the <code>cluster.yml</code> cluster configuration file, under <code>extra_args</code> for the etcd service, per the following example:</li> </ol> <pre><code>     services:\n       etcd:\n         extra_args:\n           debug: 'true'\n</code></pre> <ol> <li>Having set the flag in the cluster.yml, run <code>rke up --config cluster.yml</code> to update the cluster with the new configuration.</li> </ol>"},{"location":"000020199/#update-etcd-verbosity-in-a-rancher-launched-cluster","title":"Update etcd verbosity in a Rancher launched cluster","text":"<p>Navigate to the cluster within the Rancher UI and click <code>Edit Cluster</code>, then <code>Edit as YAML</code>. Set the <code>--debug=true</code> flag under <code>extra_args</code>, for the etcd service, per the following example:</p> <pre><code>services:\n    etcd:\n      extra_args:\n        debug: 'true'\n</code></pre> <p>Having set the debug flag, click <code>Save</code> at the bottom of the page, to update the cluster.</p>"},{"location":"000020199/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020200/","title":"In Rancher v2.x, prior to v2.3, nodes in Rancher provisioned clusters deleted via Kubernetes, instead of via Rancher, remain present in Rancher in an 'unavailable' state","text":"<p>This document (000020200) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020200/#situation","title":"Situation","text":""},{"location":"000020200/#issue","title":"Issue","text":"<p>In Rancher v2.x, prior to v2.3, if a node is deleted via Kubernetes, rather than Rancher itself - i.e. via <code>kubectl delete node</code> or another process connecting to the Kubernetes API, such as the use of the cluster-autoscaler - the node will be removed from the Kubernetes cluster, but still be present according to Rancher, remaining in an 'unavailable' state.</p> <p>Kubernetes scheduling and workloads will perform as expected for the removal of the node, as the node is correctly removed from the Kubernetes cluster. However, the view in Rancher will continue to show the node as 'unavailable' until it is manually deleted from within Rancher too.</p>"},{"location":"000020200/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned Kubernetes cluster, prior to v2.3, using either custom nodes or nodes hosted in an infrastructure provider.</li> </ul>"},{"location":"000020200/#workaround","title":"Workaround","text":"<p>To remove nodes, in a Rancher v2.x provisioned cluster, that have been deleted in Kubernetes, and are no longer present in the output of <code>kubectl get nodes</code>, but remain in Rancher in an 'unavailable' state, you can delete these from within the node list for the cluster within the Rancher UI.</p>"},{"location":"000020200/#resolution","title":"Resolution","text":"<p>This was tracked in Rancher GitHub issue #14184 and has been resolved since the release of Rancher v2.3. Where a node is deleted via Kubernetes, in Rancher v2.3 and above, this is detected by Rancher and the cluster is reconciled by Rancher to reflect the removal.</p>"},{"location":"000020200/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020201/","title":"\"log unreadable. It is excluded and would be examined next time.\" warning messages, for kubelet and kube-proxy, in rancher-logging-fluentd Pod logs of worker nodes","text":"<p>This document (000020201) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020201/#situation","title":"Situation","text":""},{"location":"000020201/#issue","title":"Issue","text":"<p>In a Rancher v2.x provisioned Kubernetes cluster, with Rancher Cluster Logging configured, the <code>rancher-logging-fluentd</code> Pod logs on Linux worker role only nodes show warning messages of the following format:</p> <pre><code>2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kubelet_5c47838dd4af749a7a0d1c457b04a6d7b905e680157718063c8e5d9eb61268fa.log unreadable. It is excluded and would be examined next time.\n2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kube-proxy_d2beb2e667eefbd6d95355082af4bc61c367fc4c220d9f1d165d15a8c8be2ab1.log unreadable. It is excluded and would be examined next time.\n</code></pre> <p>The output of <code>ls /var/lib/rancher/rke/log/</code> on affected workers shows that these files referenced in the warning log messages are broken symlinks. In addition, <code>docker ps</code> output shows the container ID for the currently running <code>kubelet</code> and <code>kube-proxy</code> containers does not match the IDs in these filenames.</p>"},{"location":"000020201/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider.</li> <li>Rancher Cluster Logging configured on the cluster, with the 'Include System Log' option set.</li> <li>Worker role only nodes in the cluster.</li> </ul>"},{"location":"000020201/#root-cause","title":"Root cause","text":"<p>These warning level messages in the <code>rancher-logging-fluentd</code> Pod are the result of the issue tracked in Rancher GitHub issue #22549.</p> <p>The container log symlinks in <code>/var/lib/rancher/rke/log/</code> for cluster component containers ( <code>kubelet</code>, <code>kube-proxy</code>, <code>nginx-proxy</code>) on worker nodes in Rancher launched clusters are not cleaned up when these components are re-created, i.e. due to a Kubernetes version upgrade, or other configuration update for these components.</p> <p>As a result these broken symlinks persist and cause the <code>log unreadable</code> warning messages when the <code>rancher-logging-fluentd</code> Pod attempts to parse files in the <code>/var/lib/rancher/rke/log/</code> directory.</p> <p>This warning message itself is harmless and can be ignored.</p>"},{"location":"000020201/#resolution","title":"Resolution","text":"<p>The request to handle automatic clean-up of these log symlinks on worker nodes, in Rancher provisioned clusters, is tracked in Rancher GitHub issue #22549.</p>"},{"location":"000020201/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020202/","title":"Many rancher-agent containers running on Rancher v2.x provisioned Kubernetes cluster, where stopped containers are regularly deleted on hosts","text":"<p>This document (000020202) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020202/#situation","title":"Situation","text":""},{"location":"000020202/#issue","title":"Issue","text":"<p>On a Rancher v2.x provisioned cluster, a host shows a large number of containers running the <code>rancher-agent</code> image, per the following output of <code>docker ps | grep rancher-agent</code>:</p> <pre><code>$ docker ps | grep rancher-agent\n...\naeffe9725521        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   About a minute ago   Up About a minute                       sleepy_hopper\n130120f49b71        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   6 minutes ago        Up 6 minutes                            stoic_hypatia\n498b923d9b6e        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   11 minutes ago        Up 11 minutes                            laughing_elbakyan\n3453865e5f70        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   16 minutes ago        Up 16 minutes                            wonderful_gagarin\nf925209cd16a        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   21 minutes ago       Up 21 minutes                           silly_shannon\n7d7fb5d4bf04        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   26 minutes ago       Up 26 minutes                           gifted_elgamal\n...\n</code></pre> <p>A <code>docker inspect &lt;container_id&gt;</code> for these containers, shows the Path and Args are of the following format:</p> <pre><code>\"Path\": \"run.sh\",\n\"Args\": [\n    \"--server\",\n    \"https://167.172.96.240\",\n    \"--token\",\n    \"gwrp7zlnwvsnzh2nhbvwcgdw45ccv6cq9pztzdd92j6xlv69xxhvnp\",\n    \"--ca-checksum\",\n    \"bbc8c7ca05c87a7140154554fa1a516178852f2710538c57718f4c874c29533c\",\n    \"--no-register\",\n    \"--only-write-certs\"\n],\n</code></pre>"},{"location":"000020202/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider.</li> <li>Repeated deletion of stopped containers on hosts in the cluster, e.g. use of <code>docker system prune</code>, either manually or as part of an automated process such as a cronjob.</li> </ul>"},{"location":"000020202/#root-cause","title":"Root cause","text":"<p>This behaviour is a result of the issue tracked in Rancher GitHub issue #15364.</p> <p>The <code>share-mnt</code> container is created on a Rancher provisioned Kubernetes cluster, and exits upon completion, but is not removed such that it can be invoked again.</p> <p>Meanwhile, the Rancher <code>node-agent</code> Pod on a host will spawn a new <code>share-mnt</code> container, if the <code>share-mnt</code> is removed. Upon starting, the <code>share-mnt</code> process spawns a <code>rancher-agent</code> container to write certificates. This agent container will run indefinitely until the <code>node-agent</code> is triggered to reconnect to the Rancher server or the <code>node-agent</code> process is restarted.</p> <p>As a result, where the <code>share-mnt</code> container on a host is removed repeatedly, either manually or by an automated process, this will result in multiple running <code>rancher-agent</code> containers.</p>"},{"location":"000020202/#workaround","title":"Workaround","text":"<p>To trigger automatic removal of the <code>rancher-agent</code> containers, the <code>node-agent</code> container on the host can be restarted. Identifying the running agent container with <code>docker ps | grep k8s_agent_cattle-node</code> restart the container with <code>docker restart &lt;container_id&gt;</code>.</p> <p>In addition, you can prevent further creation of multiple <code>rancher-agent</code> container instances by removing whichever process is triggering the deletion of stopped containers.</p>"},{"location":"000020202/#resolution","title":"Resolution","text":"<p>An enhancement request, to prevent the creation of multiple long-running <code>rancher-agent</code> containers, in the event of repeated deletion of the <code>share-mnt</code> container, is tracked in Rancher GitHub issue #15364.</p>"},{"location":"000020202/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020203/","title":"Is it safe to update the Docker bridge IP range on hosts in an RKE or Rancher v2.x launched Kubernetes cluster?","text":"<p>This document (000020203) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020203/#situation","title":"Situation","text":""},{"location":"000020203/#question","title":"Question","text":"<p>The <code>docker0</code> bridge network has a default IP range of <code>172.17.0.0/16</code> (with an additional <code>docker-sys</code> bridge for system-docker using <code>172.18.0.0/16</code> by default on RancherOS). These ranges will be routed to these interfaces, per the below example of the <code>route</code> output. If the range(s) overlap with the internal IP space usage in your own network, the host will not be able to route packets to other hosts in your network that lie within these ranges. As a result you may wish to change the bridge range(s) to enable successful routing to hosts within these.</p> <pre><code>$ route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n...\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n...\n</code></pre>"},{"location":"000020203/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x, or Rancher v2.x</li> </ul>"},{"location":"000020203/#answer","title":"Answer","text":"<p>Updating the <code>docker0</code> bridge IP range (and <code>docker-sys</code> bridge IP range in RancherOS) is possible in an RKE or Rancher v2.x provisioned Kubernetes cluster, where no cluster containers are in fact running attached to the Docker bridge network. The only impact of the change should be some downtime, as you will be required to restart the Docker daemon for the change to take effect.</p> <p>On RancherOS the bridge IP range ( <code>bip</code>) can be updated for docker and system-docker per the RancherOS documentation on <code>Configuring Docker or System Docker</code>. You will need to reboot the host for the change to take effect after updating the settings.</p> <p>For other operating systems, where Docker is installed from the upstream Docker repositories, you should update the <code>bip</code> configuration in <code>/etc/docker/daemon.json</code> per the dockerd documentation.</p> <p>On CentOS 7, RHEL 7 and SLES 12 you should also check the configuration in /etc/sysconfig/docker to ensure <code>--bip</code> has not been configured there.</p>"},{"location":"000020203/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020204/","title":"Is it safe to disable inter-container connectivity (icc) on the Docker daemon in an RKE or Rancher v2.x launched Kubernetes cluster?","text":"<p>This document (000020204) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020204/#situation","title":"Situation","text":""},{"location":"000020204/#question","title":"Question","text":"<p>The Docker daemon provides a configuration option <code>icc</code> which permits a user to disable inter-container connectivity (icc) on the Docker bridge network. Is is safe to disable this Docker daemon option in an RKE or Rancher v2.x launched Kubernetes cluster?</p>"},{"location":"000020204/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x or Rancher v2.x</li> </ul>"},{"location":"000020204/#answer","title":"Answer","text":"<p>Setting <code>icc</code> to false in the docker daemon.json configuration, or as an argument to to dockerd, is possible but is unnecessary in an RKE or Rancher v2.x provisioned Kubernetes cluster, as containers are not run attached to the Docker bridge network. Therefore, whilst this step is often included in standard 'hardening Docker daemon' guides, it is not relevant to operating an RKE or Rancher launched Kubernetes cluster.</p>"},{"location":"000020204/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020205/","title":"Users assigned the Project Owner or Member role on a project are able to create namespaces on any project, in the same cluster, to which they have access","text":"<p>This document (000020205) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020205/#situation","title":"Situation","text":""},{"location":"000020205/#issue","title":"Issue","text":"<p>A user assigned the Project Owner or Member role on one project is able to create namespaces on any project, in the same cluster, to which they have access.</p> <p>For example, if a user has been granted the Project Member role on a Project named Dev in a cluster, and the Read-only role on a project named Test in that cluster, they will be able to create namespaces on both the Dev and Test projects.</p>"},{"location":"000020205/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.x</li> <li>A user granted the Project Member or Owner role on one project, and access e.g. the Read-only role, on another project</li> </ul>"},{"location":"000020205/#explanation","title":"Explanation","text":"<p>Per the caveat explanation in the Rancher v2.x documentation:</p> <p>Users assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole, meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the owner or member role for a project can create namespaces in other projects they\u2019re assigned to, even with only the Read Only role assigned.</p>"},{"location":"000020205/#further-reading","title":"Further Reading","text":"<p>Read more on Cluster and Project Roles in the Rancher v2.x. documentation.</p>"},{"location":"000020205/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020206/","title":"How does session management work in the Rancher v1.6 UI?","text":"<p>This document (000020206) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020206/#situation","title":"Situation","text":""},{"location":"000020206/#question","title":"Question","text":"<p>This article looks at how session management, and expiry, functions in the Rancher v1.6 UI.</p>"},{"location":"000020206/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This article is applicable to Rancher v1.6 instances</li> </ul>"},{"location":"000020206/#answer","title":"Answer","text":"<p>The Rancher user interface uses a token for session management. The token is originally obtained by the client by calling the <code>/v2-beta/token</code> API. This API is triggered by the end-user entering their username and password and clicking the \"Log In\" button. Below is an example request:</p> <p>URL: https://rancher.example.com/v2-beta/token</p> <p>Method: POST</p> <p>Request body (formatted for readability):</p> <pre><code>{\n\"code\":\"admin:&lt;password here&gt;\",\n\"authProvider\":\"localauthconfig\"\n}\n</code></pre> <p>Upon successful authentication, the server will generate a random 40 character token that is associated with the authenticated user. This token is provided back to the user interface in the <code>jwt</code> field in the JSON response. The token is valid for 16 hours from the time of creation. This expiration is enforced by the server. Below is a sample response (formatted for readability):</p> <pre><code>{\n\"id\":null,\n\"type\":\"token\",\n\"links\":{},\n\"baseType\":\"token\",\n\"actionLinks\":{},\n\"accountId\":\"1a1\",\n\"authProvider\":\"localAuthConfig\",\n\"code\":null,\n\"enabled\":true,\n\"jwt\":\"V1dMyPArix5nN1jxiA6DdzsqdZitDJhZuBR3vZNr\",\n\"originalLogin\":null,\n\"redirectUrl\":null,\n\"security\":true,\n\"user\":\"admin\",\n\"userIdentity\":\n{\n    \"externalId\":\"1a1\",\n    \"profilePicture\":null,\n    \"name\":\"admin\",\n    \"externalIdType\":\"rancher_id\",\n    \"profileUrl\":null,\n    \"login\":\"admin\",\n    \"role\":null,\n    \"projectId\":null,\n    \"user\":false,\n    \"all\":null,\n    \"id\":\"rancher_id:1a1\"\n},\n\"userType\":\"admin\"\n}\n</code></pre> <p>The user interface stores the token in a cookie called <code>token</code> and will send this cookie to all subsequent API requests to the server. In addition to a token, the server also sends a CSRF (Cross-Site Request Forgery) cookie which must be sent back on each request. This ensures the request came from the client and not a third party or malicious script. Below is a sequence diagram that demonstrates how a token is created and used.</p> <p></p> <p>Upon session expiration, the user interface will redirect the user back to the login page.</p> <p>Note, the session token expiration duration is not currently configurable. There is an enhancement request on GitHub to add this functionality, tracked in https://github.com/rancher/rancher/issues/16467</p>"},{"location":"000020206/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020207/","title":"Node labels and taints reset on reboot with AWS cloudprovider in Kubernetes lower than v1.12.0","text":"<p>This document (000020207) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020207/#situation","title":"Situation","text":""},{"location":"000020207/#issue","title":"Issue","text":"<p>In a Kubernetes cluster, running on AWS EC2 instances, with the AWS cloudprovider configured, labels and taints for a node are reset when the EC2 instance is rebooted.</p>"},{"location":"000020207/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes version lower than v1.12.0</li> <li>Cluster running on AWS EC2 instances, with the AWS cloudprovider configured</li> </ul>"},{"location":"000020207/#root-cause","title":"Root Cause","text":"<p>This behaviour is caused by the AWS cloudprovider in Kubernetes versions prior to v1.12.0, in which a stopped EC2 instance is deleted from the Kubernetes cluster, and then re-created when started again. As a result of this deletion and re-creation labels and taints on the node are lost during the reboot. Details of the issue and fix can be found in Kubernetes Pull Request #66835.</p>"},{"location":"000020207/#resolution","title":"Resolution","text":"<p>In order to resolve this issue, the cluster should be upgraded to Kubernetes version v1.12.0 or above.</p> <p>For clusters provisioned via the RKE CLI, users can upgrade the cluster to a Kubernetes version of v1.12.0 or higher with RKE v0.1.10 or above.</p> <p>For clusters provisioned via Rancher, users can upgrade the cluster to a Kubernetes version of v1.12.6 or higher with Rancher v2.1.7 or above.</p>"},{"location":"000020207/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020208/","title":"CronJobs fail to run in a Kubernetes v1.14 cluster, with more than 500 Job resources: \"expected type *batchv1.JobList, got type *internalversion.List\"","text":"<p>This document (000020208) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020208/#situation","title":"Situation","text":""},{"location":"000020208/#issue","title":"Issue","text":"<p>In a Kubernetes v1.14 cluster, CronJobs fail to run when there are more than 500 Job resources in the cluster. The Kubernetes controller manager logs show errors of the format <code>{\"log\":\"E0818 18:25:50.081946 1 cronjob_controller.go:117] expected type *batchv1.JobList, got type *internalversion.List\\n\",\"stream\":\"stderr\",\"time\":\"2019-08-18T18:25:50.082127727Z\"}</code>.</p>"},{"location":"000020208/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster, running Kubernetes v1.14, from v1.14.0 - v1.14.6</li> <li>More than 500 Job resources in the cluster</li> </ul>"},{"location":"000020208/#workaround","title":"Workaround","text":"<p>To mitigate the issue you should ensure that there are fewer than 500 Job resources in the cluster, you can view all Jobs with <code>kubectl get jobs --all-namespaces -o wide</code>.</p> <p>You should aim to delete completed Jobs to reduce the total number below 500. You can also check and adjust the configured job history limits for CronJobs to reduce the number of Jobs maintained for completed CronJobs per the Kubernetes documentation.</p>"},{"location":"000020208/#resolution","title":"Resolution","text":"<p>The issue was tracked in Kubernetes GitHub Issue #77465 and a fix was released in Kubernetes v1.14.7.</p> <p>A Kubernetes v1.14 patch release of v1.14.7 or above, including this fix, is available in Rancher v2.2, starting with v2.2.9 (v1.14.8), and v2.3, starting with v2.3.0 (v1.14.7). Similarly the fix is available via the RKE CLI in v0.2, starting with v0.2.9 (v1.14.8), and v0.3, starting with v0.3.0 (v1.14.7).</p>"},{"location":"000020208/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020209/","title":"How to configure iptables on RancherOS","text":"<p>This document (000020209) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020209/#situation","title":"Situation","text":""},{"location":"000020209/#task","title":"Task","text":"<p>How to configure firewall rules using iptables on RancherOS</p>"},{"location":"000020209/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A RancherOS v1.5.x host</li> </ul>"},{"location":"000020209/#resolution","title":"Resolution","text":"<p>The runcmd option in cloud-config can be used to run commands, such as iptables rules, to set firewall rules on a RancherOS host. For example the following can be used to disable SSH access on port 22.</p> <pre><code>#cloud-config\nruncmd:\n- \"iptables -A INPUT -p tcp --destination-port 22 -j DROP\"\n</code></pre> <p>The above snipet can be placed in /var/lib/rancher/conf/cloud-config.d/xxx.yaml, or added to the initial config while installing RancherOS. It will be executed every time RancherOS is booted.</p> <p>You can use the following iptables command to view the status of the rules:</p> <pre><code>$ iptables -t filter -nv -L INPUT\nChain INPUT (policy ACCEPT 321 packets, 41200 bytes)\npkts bytes target     prot opt in     out     source               destination\n    9     523 DROP       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:22\n</code></pre>"},{"location":"000020209/#further-reading","title":"Further reading","text":"<p>More information on running command on boot can be found here.</p>"},{"location":"000020209/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020210/","title":"RKE errors connecting to the Docker socket whilst updating clusters with the Aqua Enforcer deployed","text":"<p>This document (000020210) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020210/#situation","title":"Situation","text":""},{"location":"000020210/#issue","title":"Issue","text":"<p>During invocations of <code>rke up</code> via the RKE CLI or whilst modifying Rancher provisioned Kubernetes clusters, the process fails upon attempted creation of a Kubernetes component container with an error of the following format:</p> <pre><code>2019-04-30T15:19:17.9826528Z time=\"2019-04-30T15:19:17Z\" level=fatal msg=\"[etcd] Failed to bring up Etcd Plane: Failed to create [etcd] container on host [rancher.example.com]: Failed to create [etcd] container on host [rancher.example.com]: error during connect: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create?name=etcd: EOF\n</code></pre>"},{"location":"000020210/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned via the RKE CLI or Rancher</li> <li>The Aqua Enforcer workload deployed in the cluster, with AQUA_RUNC_INTERCEPTION environment variable set to 0</li> </ul>"},{"location":"000020210/#root-cause","title":"Root cause","text":"<p>The issue is caused by Aqua Enforcer's use of the Docker socket to perform runtime enforcement operations preventing RKE from successfully connecting to the Docker socket upon some requests.</p>"},{"location":"000020210/#resolution","title":"Resolution","text":"<p>To resolve this issue set the AQUA_RUNC_INTERCEPTION environment variable on the Aqua Enforcer daemonset to 1. With this setting the Aqua Enforcer will interact directly with runC to perform runtime enforcement operations, and not with the Docker daemon via the Docker socket. This is the default behaviour in new versions of the Aqua Enforcer, as it brings stability and performance benefits. More information on this setting can be found at https://docs.aquasec.com/docs/40-ga#section-new-aqua-enforcer-architecture-for-enhanced-stability-and-performance</p>"},{"location":"000020210/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020211/","title":"Editing Rancher launched Kubernetes cluster in infrastructure provider restricted to creating user","text":"<p>This document (000020211) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020211/#situation","title":"Situation","text":""},{"location":"000020211/#issue","title":"Issue","text":"<p>When attempting to edit a Rancher launched Kubernetes cluster, hosted on nodes in an infrastructure provider neither the <code>Cluster Options</code> nor <code>Node Pools</code> sections are available and configurable in the edit cluster view, if logged in as a different user to the cluster creator.</p>"},{"location":"000020211/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider</li> <li>Access to the Rancher UI as a user different to the cluster creator</li> </ul>"},{"location":"000020211/#root-cause","title":"Root cause","text":"<p>Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider.</p> <p>Node templates are user-scoped and, as a result, where <code>userA</code> creates a node template in Rancher it is not accessible by <code>userB</code>. This prevents Rancher launched Kubernetes clusters, provisioned on nodes in an infrastructure provider by <code>userA</code> from being edited by other users, as only <code>userA</code> has access to the node template configuration.</p>"},{"location":"000020211/#resolution","title":"Resolution","text":"<p>An enhancement request to enable users, other than the cluster creator, to edit Rancher launched Kubernetes clusters is tracked in Rancher GitHub Issue #12038.</p> <p>Where it is necessary for another user to edit the cluster, i.e. the original user who created the cluster has left the business, it is possible to re-associate the node template with a different user. If you encounter this situation, please open a ticket with Rancher Support for assistance.</p>"},{"location":"000020211/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020212/","title":"Blank provider listed for cluster when logged in as user who did not create cluster in Rancher v2.x","text":"<p>This document (000020212) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020212/#situation","title":"Situation","text":""},{"location":"000020212/#issue","title":"Issue","text":"<p>When viewing a Rancher launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider, as a user other than the cluster creator, the infrastructure provider name is blank.</p> <p></p>"},{"location":"000020212/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider</li> <li>Access to the Rancher UI as a user different to the cluster creator</li> </ul>"},{"location":"000020212/#root-cause","title":"Root cause","text":"<p>Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider.</p> <p>Node templates are user-scoped and, as a result, where <code>userA</code> creates a node template in Rancher it is not accessible by <code>userB</code>.</p> <p>Meanwhile, the Rancher v2.x UI determines the provider for a Rancher launched Kubernetes cluster by mapping <code>nodes</code> to <code>node templates</code> to <code>node drivers</code>.</p> <p>The user-scoping of node templates therefore prevents users, other than the creator, from viewing the provider of the cluster.</p>"},{"location":"000020212/#resolution","title":"Resolution","text":"<p>An enhancement request to enable all users to view the cluster provider is tracked in Rancher GitHub Issue #12038.</p>"},{"location":"000020212/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020213/","title":"How to recover after deleting the Calico CRDs from a cluster","text":"<p>This document (000020213) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020213/#situation","title":"Situation","text":""},{"location":"000020213/#issue","title":"Issue","text":"<p>Calico uses a number of Custom Resource Definitions (CRDs) in order to store configuration data in Custom Resources. In the event that these CRDs are accidentally deleted from a cluster by a user, the configuration data in these Custom Resources will be deleted, preventing successful programming of pod networking. This article documents how to recreate the CRDs and ensure the configuration data is also re-populated.</p>"},{"location":"000020213/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes v1.8.x - v1.16.x cluster provisioned by the RKE CLI or Rancher v2.x, running with the Canal or Calico network providers</li> </ul>"},{"location":"000020213/#resolution","title":"Resolution","text":""},{"location":"000020213/#1-re-create-the-crds","title":"1. Re-create the CRDs","text":"<p>The first step is to re-create the CRDs. The definitions are dependent upon the Kubernetes version running in the cluster, as well as whether the cluster is running the Canal or Calico network provider. Please refer to the matching network provider and Kubernetes version combination below:</p> <p>Canal Network Provider and Kubernetes version 1.8.x - 1.12.x</p> <p>Download the canal-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-8-to-1-12.yaml</code></p> <p>Canal Network Provider and Kubernetes version 1.13.x - 1.14.x</p> <p>Download the canal-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-13-to-1-14.yaml</code></p> <p>Canal Network Provider and Kubernetes version 1.15.x</p> <p>Download the canal-calico-crds-k8s-1-15.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-15.yaml</code></p> <p>Canal Network Provider and Kubernetes version 1.16.x</p> <p>Download the canal-calico-crds-k8s-1-16.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-16.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.8.x - 1.12.x</p> <p>Download the calico-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-8-to-1-12.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.13.x - 1.14.x</p> <p>Download the calico-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-13-to-1-14.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.15.x</p> <p>Download the calico-calico-crds-k8s-1-15.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-15.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.16.x</p> <p>Download the calico-calico-crds-k8s-1-16.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-16.yaml</code></p>"},{"location":"000020213/#2-delete-a-network-pod-to-trigger-re-creation-of-the-calico-custom-resources","title":"2. Delete a network pod to trigger re-creation of the Calico custom resources","text":"<p>Delete a network provider pod from a single node in the cluster, per the network provider specific instructions below. This will trigger creation of a new pod on that node, and the initialization of this will create the Calico custom resources containing Calico configuration. After this cluster networking should be fully restored.</p> <p>Canal Network Provider</p> <p>Delete one of the <code>canal</code> pods within the <code>kube-system</code> namespace.</p> <p>Calico Network Provider</p> <p>Delete one of the <code>calico-node</code> pods within the <code>kube-system</code> namespace.</p>"},{"location":"000020213/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020214/","title":"'Error: snapshot missing hash but --skip-hash-check=false' when performing `rke etcd snapshort-restore` with .zip extension included in snapshot name","text":"<p>This document (000020214) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020214/#situation","title":"Situation","text":""},{"location":"000020214/#issue","title":"Issue","text":"<p>When performing an etcd snapshot restore via RKE, including the <code>.zip</code> file extension in the snapshot name parameter, i.e. <code>rke etcd snapshot-restore --name snapshot.zip</code> and a snapshot filename of <code>snapshot.zip</code>, the restoration fails with an error of the following format:</p> <pre><code>FATA[0020] [etcd] Failed to restore etcd snapshot: Failed to run etcd restore container, exit status is: 128, container logs: Error: snapshot missing hash but --skip-hash-check=false\n</code></pre>"},{"location":"000020214/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This issue is applicable to RKE CLI v0.2.x, starting with v0.2.5, v0.3.x and v1.0.x</li> </ul>"},{"location":"000020214/#resolution","title":"Resolution","text":"<p>This issue is caused by the incorrect inclusion of the <code>.zip</code> file extension to the snapshot name parameter.</p> <p>The snapshot name parameter ( <code>--name</code>) should contain the snapshot name, excluding the file extension.</p> <p>In the example of a snapshot filename of <code>snapshot.zip</code> the correct name parameter is therefore just <code>snapshot</code>, i.e. <code>rke etcd snapshot-restore --name snapshot</code>.</p>"},{"location":"000020214/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020215/","title":"How to configure the Docker bridge IP range on RancherOS v1.5","text":"<p>This document (000020215) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020215/#situation","title":"Situation","text":""},{"location":"000020215/#task","title":"Task","text":"<p>In RancherOS v1.5.x the <code>docker0</code> bridge network has a default IP range of <code>172.17.0.0/16</code> and the <code>docker-sys</code> bridge for system-docker has a default range of <code>172.18.0.0/16</code>. This article details how to update these ranges.</p>"},{"location":"000020215/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A RancherOS v1.5.x host</li> </ul>"},{"location":"000020215/#resolution","title":"Resolution","text":"<p>The <code>docker0</code> bridge range is set by the <code>rancher.docker.bip</code> argument, whilst the <code>docker-sys</code> bridge is set by <code>rancher.system_docker.bip</code> argument.</p> <p>These can be configured in the cloud-config as follows:</p> <pre><code>rancher:\ndocker:\n    bip: 192.168.0.0/16\nsystem_docker:\n    bip: 172.19.0.0/16\n</code></pre> <p>These can also be customized after the host has started with the <code>ros config</code> command, and will take effect after a reboot:</p> <pre><code>ros config set rancher.docker.bip 192.168.0.0/16\nros config set rancher.system_docker.bip 172.19.0.0/16\n</code></pre>"},{"location":"000020215/#further-reading","title":"Further reading","text":"<p>You can read more on configuring Docker or System Docker within the RancherOS documentation.</p>"},{"location":"000020215/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020216/","title":"subPath does not work with hostPath volumes in Rancher v2.x or RKE CLI launched Kubernetes clusters","text":"<p>This document (000020216) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020216/#situation","title":"Situation","text":""},{"location":"000020216/#issue","title":"Issue","text":"<p>Attempting to use the <code>subPath</code> option with a <code>hostPath</code> volume in a Rancher Kubernetes Engine (RKE) CLI, or Rancher v2.x, launched Kubernetes fails. The particular failure type depends upon the value of the <code>type</code> field specified on the <code>hostPath</code> volume.</p>"},{"location":"000020216/#type-undefined","title":"<code>type</code> undefined","text":"<p>If no <code>type</code> is specified (defined as <code>Anything: do not check the target path</code> within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format <code>Error: lstat /site-data: no such file or directory</code></p> <pre><code>spec:\ncontainers:\n  - name: nginx\n    image: nginx:latest\n    volumeMounts:\n    - mountPath: /volume/nginx\n      name: site-data\n      subPath: nginx\nvolumes:\n  - name: site-data\n    hostPath:\n      path: /site-data\n</code></pre>"},{"location":"000020216/#type-directory","title":"<code>type: Directory</code>","text":"<p>If the <code>type</code> is specified as <code>Directory</code> (defined as <code>An existing directory</code> within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format <code>MountVolume.SetUp failed for volume \"site-data\" : hostPath type check failed: /site-data is not a directory</code></p> <pre><code>spec:\ncontainers:\n  - name: nginx\n    image: nginx:latest\n    volumeMounts:\n    - mountPath: /volume/nginx\n      name: site-data\n      subPath: nginx\nvolumes:\n  - name: site-data\n    hostPath:\n      path: /site-data\n      type: Directory\n</code></pre>"},{"location":"000020216/#type-directoryorcreate","title":"<code>type: DirectoryOrCreate</code>","text":"<p>If the <code>type</code> is specified as <code>DirectoryOrCreate</code> (defined as <code>A directory, or create if it does not exist</code> within the Rancher UI), per the example spec below, then the Pod will start successfully; however, an empty local volume will be mounted in the container at the <code>mountPath</code>, rather than this being bind-mounted to the path on the host as expected.</p> <pre><code>spec:\ncontainers:\n  - name: nginx\n    image: nginx:latest\n    volumeMounts:\n    - mountPath: /volume/nginx\n      name: site-data\n      subPath: nginx\nvolumes:\n  - name: site-data\n    hostPath:\n      path: /site-data\n      type: DirectoryOrCreate\n</code></pre>"},{"location":"000020216/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Any RKE CLI v0.1.x or v0.2.x, or Rancher v2.x launched Kubernetes cluster (except those launched in hosted Kubernetes providers)</li> </ul>"},{"location":"000020216/#root-cause","title":"Root Cause","text":"<p>This behavior is a result of the containerized kubelet process in RKE and Rancher launched Kubernetes clusters, and is tracked in GitHub issue https://github.com/rancher/rancher/issues/14836</p> <p>Pending resolution of this issue, you should avoid the use of the <code>subPath</code> option on <code>hostPath</code> volumes, to prevent encountering this.</p>"},{"location":"000020216/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020217/","title":"How to install or upgrade to a specific Rancher v2.x version","text":"<p>This document (000020217) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020217/#situation","title":"Situation","text":""},{"location":"000020217/#issue","title":"Issue","text":"<p>By default the installation and upgrade documentation references the installation of, or upgrade to, the most recently released latest or stable tagged version of Rancher. This article details how to install a specific version, both in a single node and high availability installation.</p> <p>For details on the difference between the latest and stable releases please see the documentation on 'Choosing a Version'.</p> <p>N.B. We strongly recommend you only run product releases tagged \u201cStable\u201d in your production and any other business-critical environments. Any product release with the \u201cLatest\u201d tag should only be used for testing the latest releases.\"</p>"},{"location":"000020217/#resolution","title":"Resolution","text":""},{"location":"000020217/#single-node-install","title":"Single Node Install","text":"<p>To install or upgrade to a specific Rancher version in a single node install, you can specify the exact version number of the image to run, <code>rancher/rancher:vX.X.X</code>, i.e.:</p> <pre><code>docker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\nrancher/rancher:v2.2.2\n</code></pre>"},{"location":"000020217/#high-availability-ha-install","title":"High Availability (HA) Install","text":"<p>To install or upgrade to a specific version in a High Availability install, you can specify the <code>--version X.X.X</code> parameter when running the <code>helm install</code> or <code>helm upgrade</code> command, i.e.:</p> <pre><code>helm install rancher-stable/rancher \\\n  --name rancher \\\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --version 2.2.2\n</code></pre>"},{"location":"000020217/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020218/","title":"Why are namespaces created via the kubectl CLI not assigned to a project?","text":"<p>This document (000020218) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020218/#situation","title":"Situation","text":""},{"location":"000020218/#question","title":"Question","text":"<p>When a user creates a Kubernetes namespace via the Rancher UI, API or CLI the namespace is created within a specified Rancher project in the cluster; however, when a user creates a namespace via the kubectl CLI ( <code>kubectl create ns &lt;namespace&gt;</code>) it is created outside of any project, why is this?</p>"},{"location":"000020218/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed via Rancher v2.x</li> </ul>"},{"location":"000020218/#answer","title":"Answer","text":"<p>It is expected behaviour that namespaces created via the kubectl CLI are created outside of a Project. Projects are Rancher abstractions and do not exist natively within the Kubernetes, as a result when you create a namespace via the kubectl CLI (or by otherwise POST'ing directly to the kube-apiserver) it is not associated with any project in Rancher.</p> <p>If you wish to create namespaces within a Rancher Project with a command-line tool, then you should use the Rancher CLI ( https://rancher.com/docs/rancher/v2.x/en/cli/ ).</p> <p>Where a namespace has been created outside a project via kubectl, a cluster admin can move the namespace into a project, within the 'Projects/Namespaces' view for the cluster.</p>"},{"location":"000020218/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020219/","title":"'SIGSEGV: segmentation violation' in prometheus container of the prometheus-project-monitoring-0 Pod when enabling Project monitoring on the System Project","text":"<p>This document (000020219) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020219/#situation","title":"Situation","text":""},{"location":"000020219/#issue","title":"Issue","text":"<p>Enabling project monitoring in a Rancher v2.2 cluster, in which cluster monitoring is enabled, fails with the Prometheus Pod in a CrashLoopBackOff.</p> <p>The <code>prometheus</code> container in the <code>prometheus-project-monitoring</code> StatefulSet fails with an error of the following format:</p> <pre><code>panic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x669c12]\ngoroutine 437 [running]:\nnet/http.(*Client).deadline(0x0, 0xc005381070, 0x40bb8f, 0xc0055e3600)\n/usr/local/go/src/net/http/client.go:187 +0x22\nnet/http.(*Client).do(0x0, 0xc005cdaa00, 0x0, 0x0, 0x0)\n/usr/local/go/src/net/http/client.go:527 +0xab\nnet/http.(*Client).Do(0x0, 0xc005cdaa00, 0x23, 0xc002802230, 0x9)\n/usr/local/go/src/net/http/client.go:509 +0x35\ngithub.com/prometheus/prometheus/scrape.(*targetScraper).scrape(0xc0060fa960, 0x1fd4a60, 0xc00010ec60, 0x1fb2760, 0xc0002eb110, 0x0, 0x0, 0x0, 0x0)\n/app/scrape/scrape.go:471 +0x111\ngithub.com/prometheus/prometheus/scrape.(*scrapeLoop).run(0xc00616a100, 0xdf8475800, 0x2540be400, 0x0)\n/app/scrape/scrape.go:813 +0x487\ncreated by github.com/prometheus/prometheus/scrape.(*scrapePool).sync\n/app/scrape/scrape.go:336 +0x45d\n</code></pre>"},{"location":"000020219/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>Cluster monitoring enabled and Project monitoring enabled on the System project</li> </ul>"},{"location":"000020219/#resolution","title":"Resolution","text":"<p>Project monitoring is not compatible with the Rancher System project and should not be enabled in the System project. Starting with Rancher v2.3.0 monitoring of the System project is performed by cluster monitoring, when this is enabled, and the UI prevents enabling of project monitoring on the System project.</p>"},{"location":"000020219/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020220/","title":"Is it possible to migrate a Rancher launched Kubernetes cluster between Rancher instances?","text":"<p>This document (000020220) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020220/#situation","title":"Situation","text":""},{"location":"000020220/#question","title":"Question","text":"<p>Is it possible to migrate a Rancher launched Kubernetes cluster from one Rancher server instance to another, e.g. to launch a custom cluster using one Rancher server, and then at a later time, to migrate this to be managed instead via a different Rancher instance?</p>"},{"location":"000020220/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched and managed by Rancher v2.x</li> </ul>"},{"location":"000020220/#answer","title":"Answer","text":"<p>No, it is not possible to migrate a cluster between Rancher server instances. A feature request for this is tracked in GitHub Issue #16471.</p> <p>Currently, if you launch a Kubernetes cluster in one Rancher instance, then later attempt to use the imported cluster feature to import this cluster into another Rancher instance, you will lose any ability to add or remove nodes from the cluster, perform etcd backups or disaster recovery, or to edit any of the cluster configuration. We would therefore strongly recommend against this. Instead, we recommend performing regular Rancher server backups, so that you can recover the Rancher server cluster in a disaster recovery scenario, ensuring successful on-going management of downstream clusters launched by the server.</p>"},{"location":"000020220/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020221/","title":"Is it possible to update the Cluster (Pod) CIDR or Service CIDR for an RKE CLI or Rancher launched cluster post-provisioning?","text":"<p>This document (000020221) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020221/#situation","title":"Situation","text":""},{"location":"000020221/#question","title":"Question","text":"<p>The Cluster (Pod) CIDR (default 10.42.0.0/16) and Service CIDR (default 10.43.0.0/16) ranges for a cluster can be specified in the cluster configuration YAML when launching a Kubernetes cluster via both the RKE CLI and Rancher v2.x. Is it possible to change these values after the cluster has been provisioned?</p>"},{"location":"000020221/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched by the RKE CLI, or Rancher v2.x</li> </ul>"},{"location":"000020221/#answer","title":"Answer","text":"<p>Updating either the Cluster (Pod) CIDR or Service CIDR after the cluster has been provisioned is not supported and you should be careful to set these as required when first configuring the cluster.</p>"},{"location":"000020221/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020222/","title":"\"fatal: could not read Username for 'http://host:port': No such device or address\" error for pipeline Publish Catalog Template step in Rancher v2.2 when Git URL contains a port","text":"<p>This document (000020222) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020222/#situation","title":"Situation","text":""},{"location":"000020222/#issue","title":"Issue","text":"<p>If a pipeline is configured in Rancher v2.2 with a Publish Catalog Template step, in which the specified Git URL contains a port, the step will fail to execute with an error of the format <code>fatal: could not read Username for 'http://host:port': No such device or address\"</code>.</p>"},{"location":"000020222/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>A pipeline configured with a Publish Catalog Template step, in which the Git URL contains a port</li> </ul>"},{"location":"000020222/#resolution","title":"Resolution","text":"<p>A patch was developed for this issue, and is available in Rancher v2.3.0 and above.</p>"},{"location":"000020222/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020223/","title":"Groups assigned Project roles do not display under the Edit Project view in the Rancher v2.2.4 UI","text":"<p>This document (000020223) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020223/#situation","title":"Situation","text":""},{"location":"000020223/#issue","title":"Issue","text":"<p>Despite being able to successfully assign user groups a project role, the groups are not listed under <code>Members</code> in the <code>Edit Project</code> view in the Rancher v2.2.4 UI.</p> <p>After adding a new group to the project as follows:</p> <p></p> <p>The newly added group does not display in the list:</p> <p></p> <p>However, querying the <code>projectRoleTemplateBindings</code> for the project via the Rancher API confirms it has been successfully added:</p> <pre><code>{\n\n    \"annotations\": {\n        \"lifecycle.cattle.io/create.cluster-prtb-sync_c-r5gcn\": \"true\",\n        \"lifecycle.cattle.io/create.mgmt-auth-prtb-controller\": \"true\"\n    },\n    \"baseType\": \"projectRoleTemplateBinding\",\n    \"created\": \"2019-09-19T10:19:08Z\",\n    \"createdTS\": 1568888348000,\n    \"creatorId\": \"user-2qtnq\",\n    \"groupId\": null,\n    \"groupPrincipalId\": \"freeipa_group://cn=developers,cn=groups,cn=accounts,dc=ipa,dc=example,dc=com\",\n    \"id\": \"p-nbhcs:prtb-nq2c9\",\n    \"labels\": {\n        \"cattle.io/creator\": \"norman\"\n    },\n    \"links\": {\n        \"remove\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\",\n        \"self\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\",\n        \"update\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\"\n    },\n    \"name\": \"prtb-nq2c9\",\n    \"namespaceId\": null,\n    \"projectId\": \"c-r5gcn:p-nbhcs\",\n    \"roleTemplateId\": \"project-member\",\n    \"type\": \"projectRoleTemplateBinding\",\n    \"userId\": null,\n    \"userPrincipalId\": null,\n    \"uuid\": \"ead04e5d-dac6-11e9-9a87-0242ac110002\"\n\n}\n</code></pre>"},{"location":"000020223/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This issue only affects Rancher v2.2.4</li> </ul>"},{"location":"000020223/#root-cause","title":"Root Cause","text":"<p>This issue is caused by a UI bug ( GitHub Issue #20760) in Rancher v2.2.4 that was patched in Rancher v2.2.5.</p>"},{"location":"000020223/#resolution","title":"Resolution","text":"<p>Users should upgrade Rancher to v2.2.5 or above to resolve the issue.</p>"},{"location":"000020223/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020224/","title":"Cluster Logging (log forwarding) fails to deploy with restricted PodSecurityPolicy in Rancher v2.2","text":"<p>This document (000020224) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020224/#situation","title":"Situation","text":""},{"location":"000020224/#issue","title":"Issue","text":"<p>Attempting to enable Cluster Logging (log forwarding) in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the <code>rancher-logging-fluentd</code> and <code>rancher-logging-log-aggregator</code> Deployments failing to create Pods.</p> <p>The <code>rancher-logging-fluentd</code> Deployment fails to validate against the restricted PSP, with an error of the following format in events:</p> <pre><code>Error creating: pods \"rancher-logging-fluentd-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[1]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[2]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[3]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[4]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[5]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[6]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[10]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used]\n</code></pre> <p>The <code>rancher-logging-log-aggregator</code> Deployment fails to validate against the restricted PSP, with and error of the following format in events:</p> <pre><code>Error creating: pods \"rancher-logging-log-aggregator-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed]\n</code></pre>"},{"location":"000020224/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>Cluster Logging enabled</li> <li>PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level</li> </ul>"},{"location":"000020224/#root-cause","title":"Root Cause","text":"<p>The logging system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, cluster logging will fail to deploy successfully.</p>"},{"location":"000020224/#workaround","title":"Workaround","text":"<p>To workaround this issue, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with <code>kubectl --config &lt;cluster kubeconfig&gt; apply -f &lt;file&gt;</code>.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: default-psp-role\nnamespace: cattle-logging\nrules:\n- apiGroups:\n  - extensions\nresourceNames:\n  - default-psp\nresources:\n  - podsecuritypolicies\nverbs:\n  - use\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: default-psp-rolebinding\nnamespace: cattle-logging\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: default-psp-role\nsubjects:\n- kind: Group\nname: system:serviceaccounts:cattle-logging\napiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"000020224/#resolution","title":"Resolution","text":"<p>An update to ensure the logging system-charts are compatible with the restricted PodSecurityPolicy and is available in Rancher v2.3.0 and above.</p>"},{"location":"000020224/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020225/","title":"How to migrate from CentOS packaged to upstream Docker","text":"<p>This document (000020225) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020225/#situation","title":"Situation","text":""},{"location":"000020225/#task","title":"Task","text":"<p>This article will describe the process by which you can migrate a CentOS node in a Rancher cluster from running the CentOS packaged Docker package to the upstream package from Docker.</p> <p>In order to perform this migration you will be required to first uninstall the CentOS packaged Docker, before installing the upstream version. This process is destructive, and will remove all container state from the host. As a result the process outlined below, will guide you through first removing the node from the Rancher cluster, before conducting the package migration, then finally re-adding the node to the cluster.</p>"},{"location":"000020225/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched with the Rancher Kubernetes Engine (RKE) CLI, v0.1.x or v0.2.x, or a Rancher v2.x launched Kubernetes cluster on custom nodes</li> <li>Nodes running CentOS 7.x, with Docker installed from the CentOS extras repository.</li> </ul>"},{"location":"000020225/#resolution","title":"Resolution","text":""},{"location":"000020225/#cluster-launched-by-the-rke-cli","title":"Cluster launched by the RKE CLI","text":""},{"location":"000020225/#create-a-backup","title":"Create a Backup","text":"<p>As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the RKE documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster.</p>"},{"location":"000020225/#perform-migration-on-each-cluster-node-in-turn","title":"Perform migration on each cluster node in turn","text":"<ol> <li>Check if you should first add an additional node to the cluster, to replace the node during its migration:</li> </ol> <p>Controlplane or etcd nodes     In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, or add the role(s) to an existing node, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node to the cluster configuration YAML and run <code>rke up</code> to provision this.</p> <p>Worker nodes     If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node to the cluster configuration YAML and run <code>rke up</code> to provision this.</p> <ol> <li> <p>Remove the node which you are migrating from the cluster, to do so remove the node from the cluster configuration YAML and then run <code>rke up</code> to reconcile the cluster.</p> </li> <li> <p>Once the <code>rke up</code> invocation in step 2. completes successfully, run the Extended Rancher 2 cleanup script on the node that you are migrating, to clean up Rancher state.</p> </li> <li>Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section <code>Uninstall old versions</code> here.</li> <li>Add the node back to the cluster configuration YAML and run <code>rke up</code> to provision it.</li> </ol>"},{"location":"000020225/#custom-cluster-launched-by-rancher","title":"Custom cluster launched by Rancher","text":""},{"location":"000020225/#create-a-backup_1","title":"Create a Backup","text":"<p>As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the Rancher documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster, if S3 backups are not configured for the cluster.</p>"},{"location":"000020225/#perform-migration-on-each-cluster-node-in-turn_1","title":"Perform migration on each cluster node in turn","text":"<ol> <li>Check if you should first add an additional node to the cluster, to replace the node during its migration:</li> </ol> <p>Controlplane or etcd nodes     In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the replacement node.</p> <p>Worker nodes     If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node by running the Rancher agent command from the 'Edit Cluster' view, with the worker role, on the replacement node.</p> <ol> <li> <p>Remove the node which you are migrating from the cluster, to do so delete it from the node list for the cluster within Rancher.</p> </li> <li> <p>Once the cluster reconciliation triggered by step 2. is complete, and the cluster no longer shows as updating within Rancher, run the Extended Rancher 2 cleanup script on the node that you are migrating to clean up Rancher state.</p> </li> <li>Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section <code>Uninstall old versions</code> here.</li> <li>Add the node back by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the node.</li> </ol>"},{"location":"000020225/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020226/","title":"How to rotate certificates for clusters launched by RKE v0.1.x or Rancher v2.0.x and v2.1.x","text":"<p>This document (000020226) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020226/#situation","title":"Situation","text":""},{"location":"000020226/#task","title":"Task","text":"<p>Kubernetes clusters use multiple certificates to provide both encryption of traffic to the Kubernetes components as well as authentication of these requests. These certificates are auto-generated for clusters launched by Rancher and also clusters launched by the Rancher Kubernetes Engine (RKE) CLI.</p> <p>In Rancher v2.0.x and v2.1.x, the auto-generated certificates for Rancher-launched Kubernetes clusters have a validity period of one year, meaning these certificates will expire one year after the cluster is provisioned. The same applies to Kubernetes clusters provisioned by v0.1.x of the Rancher Kubernetes Engine (RKE) CLI.</p> <p>If you created a Rancher-launched or RKE-provisioned Kubernetes cluster about 1 year ago, and have not already rotated the certificates, you need to rotate the certificates. If no action is taken, then when the certificates expire, the cluster will go into an error state and the Kubernetes API for the cluster will become unavailable. Rancher recommends that you rotate the certificates before they expire to avoid an unexpected service interruption. The rotation is a one time operation, and the newly-generated certificates will be valid for the next 10 years.</p>"},{"location":"000020226/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched by RKE CLI v0.1.x, or Rancher v2.0.x and v2.1.x</li> </ul>"},{"location":"000020226/#resolution","title":"Resolution","text":"<p>Full details on who to rotate the certificates for the both RKE and Rancher launched clusters can be found in the Rancher blog post \"Manual Rotation of Certificates in Rancher Kubernetes Clusters\".</p>"},{"location":"000020226/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020227/","title":"Cluster and Project monitoring fail to deploy with restricted PodSecurityPolicy in Rancher v2.2","text":"<p>This document (000020227) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020227/#situation","title":"Situation","text":""},{"location":"000020227/#issue","title":"Issue","text":"<p>Attempting to enable Cluster or Project monitoring, in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the Cluster or Project, fails with the Grafana and Prometheus Pods in a CrashLoopBackOff.</p> <p>The <code>grafana-proxy</code> container in the <code>grafana-cluster-monitoring</code>/ <code>grafana-project-monitoring</code> Deployment and the <code>promtheus-proxy</code> container in the <code>prometheus-cluster-monitoring</code>/ <code>prometheus-project-monitoring</code> StatefulSet fail with an error of the following format:</p> <pre><code>2019/09/20 11:54:17 [warn] 1#1: duplicate MIME type \"text/html\" in /var/run/nginx.conf:46\nnginx: [warn] duplicate MIME type \"text/html\" in /var/run/nginx.conf:46\n2019/09/20 11:54:17 [emerg] 1#1: chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted)\nnginx: [emerg] chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted)\n</code></pre> <p>The <code>grafana</code> container in the <code>grafana-cluster-monitoring</code>/ <code>grafana-project-monitoring</code> Deployment fails to start with an error of the following format shown in the events for the Pod:</p> <pre><code>Error: failed to start container \"grafana\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"/var/lib/kubelet/pods/6c9bbb63-db9a-11e9-932e-2af11a72a258/volume-subpaths/grafana-static-contents/grafana/1\\\\\\\" to rootfs \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged\\\\\\\" at \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged/usr/share/grafana/public/app/plugins/datasource/prometheus/plugin.json\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type\n</code></pre>"},{"location":"000020227/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>Cluster or Project monitoring enabled</li> <li>PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level, or on the Project for which monitoring is enabled</li> </ul>"},{"location":"000020227/#root-cause","title":"Root Cause","text":"<p>The monitoring system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, monitoring will fail to deploy successfully.</p>"},{"location":"000020227/#workaround","title":"Workaround","text":"<p>To workaround the impact to cluster monitoring, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with <code>kubectl --config &lt;cluster kubeconfig&gt; apply -f &lt;file&gt;</code>.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: default-psp-role\nnamespace: cattle-prometheus\nrules:\n- apiGroups:\n  - extensions\nresourceNames:\n  - default-psp\nresources:\n  - podsecuritypolicies\nverbs:\n  - use\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: default-psp-rolebinding\nnamespace: cattle-prometheus\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: default-psp-role\nsubjects:\n- kind: Group\nname: system:serviceaccounts:cattle-prometheus\napiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"000020227/#resolution","title":"Resolution","text":"<p>An update to ensure the monitoring system-charts are compatible with the restricted PodSecurityPolicy is available in Rancher v2.3.0 and above.</p>"},{"location":"000020227/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020228/","title":"'[object Object]' error attempting to edit Load Balancing or Service Discovery resource YAMLs via the UI in Rancher v2.1.7 - v2.2.3","text":"<p>This document (000020228) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020228/#situation","title":"Situation","text":""},{"location":"000020228/#issue","title":"Issue","text":"<p>Selecting <code>View/Edit YAML</code> on Load Balancing (Ingress) or Service Discovery (Service) resources in the Rancher UI for Rancher v2.1.7 - v2.2.3 fails to display the resource YAML and displays an error <code>[object Object]</code> per the screenshot below.</p> <p></p>"},{"location":"000020228/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This issue is only applicable to Rancher v2.1.x, starting with v2.1.7, and Rancher v2.2.x, from Rancher v2.2.0 to Rancher v2.2.3</li> </ul>"},{"location":"000020228/#root-cause","title":"Root Cause","text":"<p>Issue was caused by an incorrect content-length header returned by the Rancher API for gzipped responses ( GitHub Issue #19723) and was patched in Rancher v2.2.4.</p>"},{"location":"000020228/#workaround","title":"Workaround","text":"<p>In affected versions of Rancher it is still possible to edit the YAML of Load Balancing (Ingress) or Service Discovery (Service) resources by using the <code>kubectl</code> CLI.</p>"},{"location":"000020228/#resolution","title":"Resolution","text":"<p>Users should upgrade to Rancher v2.2.4 or above to resolve the issue.</p>"},{"location":"000020228/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020229/","title":"No monitoring for etcd in the local Rancher cluster with cluster monitoring in Rancher v2.2.x","text":"<p>This document (000020229) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020229/#situation","title":"Situation","text":""},{"location":"000020229/#issue","title":"Issue","text":"<p>When enabling cluster monitoring in Rancher v2.2.x, upon the local cluster running the Rancher server itself, no monitoring metrics are available for the etcd instances within the cluster.</p>"},{"location":"000020229/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE provisioned Rancher HA cluster</li> <li>Rancher v2.2.x</li> <li>Cluster monitoring enabled on the local Rancher cluster</li> </ul>"},{"location":"000020229/#root-cause","title":"Root cause","text":"<p>Cluster monitoring was introduced in Rancher v2.2.0 and when enabled for a Rancher launched Kubernetes clusters, i.e. a custom cluster or using node templates for an infrastructure provider, will include etcd monitoring metrics. The local cluster running Rancher, whilst provisioned via RKE, is an imported cluster from the Rancher perspective. As a result etcd monitoring metrics are not collected and displayed for this cluster.</p>"},{"location":"000020229/#resolution","title":"Resolution","text":"<p>An enhancement request to include etcd monitoring metric collection, for the local Rancher cluster, within Rancher cluster monitoring is tracked in Rancher Issue #18619.</p>"},{"location":"000020229/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020230/","title":"How to collect and share Rancher logs with Support","text":"<p>This document (000020230) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020230/#situation","title":"Situation","text":""},{"location":"000020230/#issue","title":"Issue","text":"<p>When corresponding with Rancher Support to troubleshoot an issue, a common request is to retrieve environment and log data to assist with investigation.</p> <p>To standardise and simplify this data collection, product-specific scripts exist to retrieve the information, per the details below.</p> <p>Once logs are collected from each required node, per the direction of Rancher Support, this output can be uploaded to the Support case.</p>"},{"location":"000020230/#resolution","title":"Resolution","text":""},{"location":"000020230/#rancher-v2x-linux-log-collector","title":"Rancher v2.x Linux log collector","text":"<p>Logs can be collected from a Linux node within a Rancher v2.x cluster using the Linux log collector script as detailed here.</p>"},{"location":"000020230/#rancher-v2x-windows-log-collector","title":"Rancher v2.x Windows log collector","text":"<p>Logs can be collected from a Windows node within a Rancher v2.x cluster using the Windows log collector script as detailed here.</p>"},{"location":"000020230/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020231/","title":"Network ingress traffic from 192.168.0.0/16 always SNAT'd in Kubernetes clusters with canal network provider","text":"<p>This document (000020231) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020231/#situation","title":"Situation","text":""},{"location":"000020231/#issue","title":"Issue","text":"<p>Network ingress traffic to a Kubernetes cluster with the canal network provider, from IP addresses in the range <code>192.168.0.0/16</code>, is always SNAT'd, even in instances where this is not desired.</p> <p>For example on NodePort services configured with <code>externalTrafficPolicy: Local</code> the source IP should be preserved without SNAT, per the Kubernetes documentation. With this issue the source IP is SNAT'd even in instances of NodePort services configured with <code>externalTrafficPolicy: Local</code>.</p>"},{"location":"000020231/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned via the RKE CLI or Rancher, using the canal network provider</li> </ul>"},{"location":"000020231/#root-cause","title":"Root cause","text":"<p>When a cluster is provisioned with the canal network provider selected, Flannel is used for networking and Calico for network policy enforcement, and IP address management is therefore managed by Flannel.</p> <p>The <code>calico-node</code> container in the canal pod is still configured with an (un-used) IP pool, which defaults to <code>192.168.0.0/16</code>. By default Calico programs iptables rules in the <code>cali-nat-outgoing</code> chain of the <code>nat</code> table on cluster nodes to perform SNAT on traffic from this IP pool. The purpose of these rules is to masquerade egress traffic from pods where Calico is used for networking (and not just network policy). As a result in a canal network provider cluster, where the <code>calico-node</code> container is present for network policy enforcement, these rules are programed and any ingress traffic from the range <code>192.168.0.0/16</code> will match and be SNAT'd.</p>"},{"location":"000020231/#resolution","title":"Resolution","text":"<p>The permanent solution to prevent this issue is to update the RKE deployment templates for the canal daemonset, to set the environment variable <code>CALICO_IPV4POOL_NAT_OUTGOING</code> to <code>0</code> for the <code>calico-node</code> container. This will prevent programming of the problematic <code>cali-nat-outgoing</code> iptables rules and is tracked in Rancher Issue #20500.</p> <p>In order to workaround the issue in existing clusters, the Calico ippool configuration can be edited to disable outgoing nat, which removes programming of the <code>cali-nat-outgoing</code> iptables rules. To implement this workaround run kubectl against the affected to edit the <code>default-ipv4-ippool</code> object: <code>kubectl edit ippools default-ipv4-ippool</code>. Edit the line <code>natOutgoing: true</code> to set <code>natOutgoing: false</code> and save the change. Calico will detect the configuration update and remove the <code>cali-nat-outgoing</code> iptables rules.</p>"},{"location":"000020231/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020232/","title":"Is it possible to perform etcd snapshots to an s3 endpoint with a certificate signed by a custom CA?","text":"<p>This document (000020232) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020232/#situation","title":"Situation","text":""},{"location":"000020232/#question","title":"Question","text":"<p>Is it possible to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom certificate authority (CA)?</p>"},{"location":"000020232/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes clusters provisioned via the RKE CLI v0.2.x, or Rancher launched Kubernetes clusters via Rancher v2.2.x.</li> </ul>"},{"location":"000020232/#answer","title":"Answer","text":""},{"location":"000020232/#rancher-v220-v224-and-rke-cli-v020-v024","title":"Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4","text":"<p>In Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4 it is not possible to configure etcd snapshots to use a custom CA. Attempts to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom CA will fail with an error similar to the following:</p> <pre><code>FATA[0002] Failed to take one-time snapshot, exit code [1]: time=\"2019-04-29T08:37:15Z\" level=fatal msg=\"faield to set s3 server: failed to check s3 bucket:rke, err:Get https://s3.example.com/rke/?location=: x509: certificate signed by unknown authority\"\n</code></pre>"},{"location":"000020232/#rancher-v225","title":"Rancher v2.2.5+","text":"<p>In Rancher v2.2.5 and above it is possible to specify a custom CA for the S3 endpoint within the S3 backup options. Expanding 'Show advanced options' under the 'Edit Cluster' view, a 'Custom CA Certificate' field is shown when the s3 backup target is selected, enabling you to enter the certificate or upload this from file.</p>"},{"location":"000020232/#rke-v025","title":"RKE v0.2.5+","text":"<p>With the RKE CLI v0.2.5 and above it also possible to specify a custom CA for the S3 endpoint within the S3 backup options. To do you specify the certificate via the <code>custom_ca</code> field in the <code>s3backupconfig</code> block of the cluster configuration YAML. The cert should be provided as string, with newlines replaced with \\n, per the example below:</p> <pre><code>services:\netcd:\n    backup_config:\n      interval_hours: 12\n      retention: 6\n      s3backupconfig:\n        access_key: S3_ACCESS_KEY\n        secret_key: S3_SECRET_KEY\n        bucket_name: s3-bucket-name\n        region: \"\"\n        endpoint: s3.amazonaws.com\n        custom_ca: \"-----BEGIN CERTIFICATE-----\\nMIIDazCCAlOgAwIBAgIUMoCmUpa4u2UJWqNIkizFbpeJkwowDQYJKoZIhvcNAQEL\\nBQAwRTELMAkGA1UEBhMCQVUxEzARBgNVBAgMClNvbWUtU3RhdGUxITAfBgNVBAoM\\nGEludGVybmV0IFdpZGdpdHMgUHR5IEx0ZDAeFw0xOTA5MTgwOTI4NDBaFw0yMjA3\\nMDgwOTI4NDBaMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEw\\nHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQwggEiMA0GCSqGSIb3DQEB\\nAQUAA4IBDwAwggEKAoIBAQDIW8aN2vszkiNAqykYvqivZgWPRqEukPSAZz39Qtyx\\nkv2wl3B29chBzw5+vjG6veaUnWufOpGeiwglL2PEBOMI0a62zmmm3ttyJDy1lY+A\\ncuxZ1+hveWjWrA2B2bN69/wdkQTQu6ZLoguk+8mRFBZ7ghu6YTZQfczBsHlDxUpA\\n77qQunE4RmcQzOBHoWmMkSSxSGMBsVIj2rRihtVqpgbrMr3/LtCqzqsF+UcroJPC\\nIIBd8bSFlcgkWLnJdqlSa8s1PUodcKD3q6mbMZPDudraszuRgLyC5pIylGQOk+XF\\nMjf2I8zkkAV4QtfSpgBpNXbZEZ3a6CPhveDZqoZN4rxTAgMBAAGjUzBRMB0GA1Ud\\nDgQWBBTD/EagPfxclAlfViV5kKLq0YwBYzAfBgNVHSMEGDAWgBTD/EagPfxclAlf\\nViV5kKLq0YwBYzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQB0\\nyJ6vjtmuvBEKuNgWwIJLh2CqZubUL+lUQGi1NhdFzkXj7+fLeLjqsmbi2Xj/qQ5n\\nooI/p4MeHfYrUqqS7nqTBIsRZQZDZcKUYTZWzDRBdQZtxvEsB1WUq5+nsCQqVuZO\\n+ICsXQFL45xDKaWOoRMH8z9JksYf2CSKeRWViAFElC/IDwf8d5mtufe17h5vlyPR\\nLaIMJ37vyAosN6h8icztVHRzfcIjp1KLqwaGfaOrNSCv8zja9YsD6kbYL64lKND4\\nHiOJy3oSjjjTNdnXjIO44Ngo7L4TWF1CshFlsRF3a5/Jw+NmsEV46Vq41YcuRX9E\\n5JYZWzGRsPDeG4vrzWrV\\n-----END CERTIFICATE-----\"\n</code></pre>"},{"location":"000020232/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020233/","title":"Upgrade to Rancher v2.2.4 fails for instances managing OpenStack CloudProvider enabled clusters with a Loadbalancer config: 'cannot unmarshal number into Go value of type string'","text":"<p>This document (000020233) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020233/#situation","title":"Situation","text":""},{"location":"000020233/#issue","title":"Issue","text":"<p>Upon attempting to upgrade to Rancher v2.2.4, where the Rancher instance manages an, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config, the Rancher server fails to start. Logs for the Rancher pods show error messages of the format:</p> <pre><code>E0606 07:39:20.296926       8 reflector.go:134] github.com/rancher/norman/controller/generic_controller.go:175: Failed to list *v3.Cluster: json: cannot unmarshal number into Go value of type string\n</code></pre>"},{"location":"000020233/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Upgrading Rancher to v2.2.4</li> <li>A Rancher launched, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config.</li> </ul>"},{"location":"000020233/#root-cause","title":"Root cause","text":"<p>In order to resolve Rancher/14577, the <code>monitor-delay</code> and <code>monitor-timeout</code> parameters for OpenStack cluster loadbalancer healthchecks were set from an integer type to a string, in Rancher v2.2.4.</p> <p>As the default in the Rancher API framework had configured these values to 0, upon upgrade to Rancher v2.2.4 an error occurs attempting to unmarshal these integer values of 0 to a string type. If these had been manually set to a non-zero integer value, resulting in kubelet failures in the OpenStack cluster itself previously, these will now result in failure of the Rancher pods themselves.</p>"},{"location":"000020233/#resolution","title":"Resolution","text":"<p>You can apply a one time fix, to workaround this issue, by manually editing the <code>monitor-delay</code> and <code>monitor-timeout</code> values of the <code>cluster</code> Custom Resource of affected clusters, via <code>kubectl</code> run against the Rancher management cluster.</p> <p>Using your RKE generated kube config, perform the following operations:</p> <ol> <li> <p>Identify affected clusters by running <code>kubectl get clusters</code> and checking for those with a <code>spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer</code> definition.</p> </li> <li> <p>For affected clusters run <code>kubectl edit &lt;cluster name&gt;</code>, where <code>&lt;cluster name&gt;</code> is the <code>metadata.name</code> value for the cluster and update the <code>spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-delay</code> and <code>spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-timeout</code> fields to a quoted string. Example: if it was 30, change it to \"30s\", if it was 0, change it to \"\".</p> </li> </ol>"},{"location":"000020233/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020234/","title":"Rancher 1.6.x - Rancher Server Tuning","text":"<p>This document (000020234) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020234/#situation","title":"Situation","text":"<p>Rancher Server Tuning</p> <p>Refer PDF attachment</p>"},{"location":"000020234/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020235/","title":"Rancher 1.6.x - Production Environment Sizing guide","text":"<p>This document (000020235) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020235/#situation","title":"Situation","text":"<p>Production Environment Sizing Guide</p> <p>Refer PDF attachment</p>"},{"location":"000020235/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020236/","title":"Rancher 1.6.x - MySQL Config Tuning","text":"<p>This document (000020236) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020236/#situation","title":"Situation","text":"<p>MySQL Config Tuning Guide</p> <p>Refer PDF attachment</p>"},{"location":"000020236/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020237/","title":"Rancher 2.1.x - CIS Kubernetes v1.3.0 Benchmark Self Assessment","text":"<p>This document (000020237) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020237/#situation","title":"Situation","text":""},{"location":"000020237/#rancher-v21x","title":"Rancher v2.1.x","text":"<p>Version 1.0.0 - Nov 2018</p> <p>Authors</p> <ul> <li>Jason Greathouse</li> </ul> <p>Overview</p> <p>The following document scores an RKE cluster provisioned according to the Rancher 2.1.x hardening guide against the CIS 1.3.0 Kubernetes benchmark. This document is to be used by Rancher operators, security teams, auditors and decision makers.</p> <p>Download document here:</p> <p>https://releases.rancher.com/documents/security/latest/Rancher_Benchmark_Assessment.pdf</p>"},{"location":"000020237/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020238/","title":"Rancher 2.1.x - Rancher Hardening Guide","text":"<p>This document (000020238) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020238/#situation","title":"Situation","text":""},{"location":"000020238/#rancher-v21x","title":"Rancher v2.1.x","text":"<p>Version: 0.1.0 - November 26th 2018</p> <p>Overview</p> <p>This document provides prescriptive guidance for hardening a production installation of Rancher v2.1.x. It outlines the configurations and controls required to address CIS-Kubernetes benchmark controls.</p> <ul> <li>Rancher CIS-Kubernetes self assessment using RKE</li> </ul> <p>This document has been created by the Engineering team at Rancher Labs.</p> <p>Profile Definitions</p> <p>The following profile definitions agree with the CIS Benchmarks for Kubernetes.</p> <p>Level 1</p> <p>Items in this profile intend to:</p> <ul> <li>offer practical advice appropriate for the environment;</li> <li>deliver an obvious security benefit; and</li> <li>not alter the functionality or utility of the environment beyond an acceptable margin</li> </ul> <p>Level 2</p> <p>Items in this profile extend the \u201cLevel 1\u201d profile and exhibit one or more of the following characteristics:</p> <ul> <li>are intended for use in environments or use cases where security is paramount</li> <li>act as a defense in depth measure</li> <li>may negatively impact the utility or performance of the technology</li> </ul> <p>Authors</p> <ul> <li>Jason Greathouse</li> <li>Bill Maxwell</li> </ul> <p>Download document here:</p> <p>https://releases.rancher.com/documents/security/latest/Rancher_Hardening_Guide.pdf</p>"},{"location":"000020238/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020239/","title":"How to change the log level for Rancher v2.x","text":"<p>This document (000020239) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020239/#situation","title":"Situation","text":""},{"location":"000020239/#task","title":"Task","text":"<p>By default the Rancher v2.x server log level is set to <code>info</code>; however, when investigating an issue it may be helpful to increase the log verbosity to <code>debug</code>. This article details how to control the log verbosity on Rancher v2.x containers.</p>"},{"location":"000020239/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster.</li> </ul>"},{"location":"000020239/#resolution","title":"Resolution","text":"<p>The log verbosity is set within a running Rancher server container by use of the <code>loglevel</code> command:</p> <pre><code>loglevel --set &lt;verbosity&gt;\n</code></pre> <p>Using <code>kubectl</code> with your cluster's context, you can update the log level of all your Rancher server containers by running the following:</p> <pre><code>kubectl -n cattle-system get pods -l app=rancher --no-headers -o custom-columns=name:.metadata.name | while read rancherpod; do kubectl -n cattle-system exec $rancherpod -c rancher -- loglevel --set debug; done\n</code></pre> <p>where verbosity is one of <code>error</code>, <code>info</code> or <code>debug</code>.</p> <p>Instructions on how to run this command in either a single node or High Availability installation of Rancher can be found within the Rancher documentation under the \"Logging\" troubleshooting guide.</p> <p>If the log level is increased to <code>debug</code> for troubleshooting purposes, you should be sure to reduce to <code>info</code> after the necessary logs have been captured, in order to reduce disk usage and minimise noise when reading the logs.</p>"},{"location":"000020239/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020241/","title":"Istio-init container fails to start when SElinux is enabled on RHEL 7.x","text":"<p>This document (000020241) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020241/#environment","title":"Environment","text":"<p>OS: RHEL 7.x</p> <p>ISTIO Chart Version: v1.8.300</p> <p>Rancher Version : v2.5.7</p>"},{"location":"000020241/#situation","title":"Situation","text":"<p>Starting a workload on Istio-enabled namespace fails as the istio-init container failed to start.</p> <p>The istio-init container shows below error;</p> <pre><code>The error is:\nnat\n-N ISTIO_INBOUND\n-N ISTIO_REDIRECT\n-N ISTIO_IN_REDIRECT\n-N ISTIO_OUTPUT\n-A ISTIO_INBOUND -p tcp --dport 15008 -j RETURN\n-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001\n-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006\n-A PREROUTING -p tcp -j ISTIO_INBOUND\n-A ISTIO_INBOUND -p tcp --dport 22 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15090 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15021 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15020 -j RETURN\n-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT\n-A OUTPUT -p tcp -j ISTIO_OUTPUT\n-A ISTIO_OUTPUT -o lo -s 127.0.0.6/32 -j RETURN\n-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT\n-A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT\n-A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN\n-A ISTIO_OUTPUT -j ISTIO_REDIRECT\nCOMMIT\n\niptables-restore --noflush /tmp/iptables-rules-1618985143596701894.txt019825926\niptables-restore: line 25 failed\niptables-save\n</code></pre>"},{"location":"000020241/#resolution","title":"Resolution","text":"<p>Execute the modprobe in the below order to fix this without a reboot.</p> <pre><code>modprobe br_netfilter\nmodprobe nf_nat_redirect\nmodprobe xt_REDIRECT\nmodprobe xt_owner\n</code></pre> <p>To load the modules automatically during boot, create a file inside /etc/modules-load.d/ as shown below.</p> <pre><code>cat &gt;/etc/modules-load.d/istio-iptables.conf &lt;&lt;EOF\nbr_netfilter\nnf_nat_redirect\nxt_REDIRECT\nxt_owner\nEOF\n</code></pre>"},{"location":"000020241/#cause","title":"Cause","text":"<p>The issue is caused by SELinux which prevents the istio-init to load kernel modules that are needed for the iptables rules.</p>"},{"location":"000020241/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020243/","title":"Can I receive an application backup of my SUSE Rancher Hosted environment if I decide not to renew at the end of my term?","text":"<p>This document (000020243) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020243/#situation","title":"Situation","text":""},{"location":"000020243/#resolution","title":"Resolution","text":"<p>Yes, the SUSE can provide an application backup of your SUSE Rancher Hosted environment. You can follow the Rancher documentation to restore your environment into a Rancher server running in your datacenter or cloud account.</p> <p>To request a backup of your SUSE Rancher Hosted environment, open a support ticket on our support portal.</p>"},{"location":"000020243/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020265/","title":"Node become NotReady state because of unhealthy PLEG","text":"<p>This document (000020265) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020265/#situation","title":"Situation","text":"<p>The rancher UI displays the following:</p> <p>\"PLEG is not healthy: pleg was last seen active 18h50m17.324752357s ago; threshold is 3m0s \"</p> <p>Kubelet log shows;</p> <pre><code>docker logs kubelet --tail=20 -f\n</code></pre> <p>Output:-</p> <pre><code>E0511 09:12:59.037051 10851 remote_runtime.go:312] ListContainers with filter &amp;ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)\n\nE0511 09:12:59.037105 10851 kuberuntime_container.go:382] getKubeletContainers failed: rpc error: &lt;b&gt;code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)&lt;/b&gt;\n\nE0511 09:12:59.037123 10851 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)\n</code></pre>"},{"location":"000020265/#resolution","title":"Resolution","text":"<p>The workaround here is to prune stopped or dead containers to reduce the message size.</p> <pre><code>docker system prune\n</code></pre> <p>The Paketo Buildpacks fixed this issue already.</p> <p>Please refer\u00a0GitHub issue #80\u00a0for more details.</p>"},{"location":"000020265/#cause","title":"Cause","text":"<p>The labels set on the containers build by Paketo Buildpacks\u00a0are causing a huge message size.</p> <p>This results in the default gRPC message buffer size of 16Mb overflowing in kubelet, causing PLEG to fail.</p>"},{"location":"000020265/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020274/","title":"Rancher Support Migration - Frequently Asked Questions","text":"<p>This document (000020274) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020274/#environment","title":"Environment","text":"<p>Dear SUSE Rancher Customer,</p> <p>This page is specific to customer users\u00a0who are being migrated from the Rancher Support system on Zendesk to the SUSE Customer Center (SCC). \u00a0This page is intended to answer some of the Frequently Asked Questions our team has received about the migration process. \u00a0We hope this gives you and your team some additional insight into what is changing and what is not changing.</p>"},{"location":"000020274/#will-rancher-support-be-available-during-the-maintenance-window-for-this-migration","title":"Will Rancher Support be available during the maintenance window for this migration?","text":"<p>Yes. \u00a0During this migration, our support leadership team will be monitoring an email address for any customers that are in need of immediate assistance. \u00a0If you find yourself in need of assistance please do not hesitate to email us at RancherSupportNow@suse.com.\u00a0 From here our team will assess the issue and work with you for resolution. For customers with accounts already in SCC, during the migration, you may continue to interact with our team through SCC. \u00a0For any customers that do not have SCC accounts (please don't create one until you are advised to do so), please use the\u00a0RancherSupportNow@suse.com\u00a0to interact with our team during this migration, especially to log Sev1 cases. \u00a0Once the migration is complete, SCC will be the new system moving forward and how our teams interact.</p>"},{"location":"000020274/#what-will-happen-with-my-existing-tickets-in-zendesk-after-this-migration","title":"What will happen with my existing tickets in Zendesk after this migration?","text":"<p>Ensuring the historical context of your account is important to us. \u00a0During the migration, our team will be working to ensure that all of your historical (closed) tickets are migrated. \u00a0Additionally, we will work to ensure that each of the tickets that we are currently working on is migrated. \u00a0During the migration, you may see a new ticket (in the new customer center) that informs you that your previous Zendsk ticket has been migrated to a new SalesForce ticket. \u00a0This new ticket will contain the history of that open/working ticket.</p>"},{"location":"000020274/#what-will-happen-to-our-existing-user-accounts-in-zendesk-after-the-migration","title":"What will happen to our existing user accounts in Zendesk after the migration?","text":"<p>Because we are migrating to a new CRM, each user will need to register for a new account (directions for registration will be sent in future communications). Each of the users that is currently active in Zendesk will be receiving the same messaging that contains the instructions on how they may create a new account in the new system. In the new system, the creation of new accounts is now able to be done by users on the account that have administrator permissions. \u00a0Each of these users will be able to invite users and set the level of permissions for those users.</p> <p>If a user in your team either doesn't receive information on how to create a new user account or if you simply want to add a new user that did not exist in Zendesk, your account administrators will be able to take control of the account invites. \u00a0The process of emailing us with Register in the subject will not work, so an administrator will need to be contacted to create the accounts moving forward.</p>"},{"location":"000020274/#how-will-i-access-the-knowledgebase-articles-found-at-supportranchercom","title":"How will I access the Knowledgebase articles found at\u00a0support.rancher.com?","text":"<p>During this migration period, we have set all the support documents to be viable by anyone accessing the site, this site no longer requires an agent login. \u00a0Our team is in the process of migrating these articles to\u00a0suse.com.\u00a0Once the migration of articles is complete, they will be accessible at\u00a0https://www.suse.com/support/kb/\u00a0Once our team has cloned all of the articles we will place a redirection on the support.rancher.com page informing you that all of the documents have been moved.</p>"},{"location":"000020274/#may-i-continue-to-open-and-update-tickets-via-email","title":"May I continue to open and update tickets via email?","text":"<p>No, post-migration, it won\u2019t be possible to create a new ticket (support case) by email. \u00a0After the migration, this email address\u00a0support@rancher.com\u00a0will not create new support cases. \u00a0It will be monitored by our team and there will be an automated response to emails sent to this address on how to create a new support case.\u00a0 While we will continue to monitor that mailbox and offer help on best effort, we request that you create a new support case via the SCC portal. \u00a0Creating a ticket through the portal will ensure that our engineers are aware and able to assist more quickly.</p>"},{"location":"000020274/#what-will-happen-if-i-already-have-a-support-contract-with-suse-separate-from-my-rancher-account","title":"What will happen if I already have a support contract with SUSE, separate from my Rancher account?","text":"<p>For customers that have a service account with both, we are working to ensure that the entitlements are maintained. \u00a0If you already have an account where you are accessing your SUSE entitlements there is no need to create a new account, the (SUSE) Rancher entitlements should be added to your existing account with no issue. After the migration, if you find this not to be the case please email your Customer Success Manager or your Services Delivery Manager. \u00a0If you have users that you want to</p>"},{"location":"000020274/#how-will-i-know-if-the-migration-is-successful-or-failed","title":"How will I know if the migration is successful or failed?","text":"<p>Leading up to the migration and on the day of the migration process our team will be sending out messages, when appropriate, to keep you apprised of the process. \u00a0If the migration is successful, our final message will let you know that and will include steps for how you and your team may set up your new accounts in the new customer center. If the migration is unsuccessful, our team will let you know as well. \u00a0 If the migration is unsuccessful, nothing will change with your current support and you will continue to use Zendesk. \u00a0We will then continue to work on another try to migrate</p>"},{"location":"000020274/#what-if-i-have-issues-with-my-scc-account-after-the-migration-occurs","title":"What if I have issues with my SCC account after the migration occurs?","text":"<p>If there are issues you should first check\u00a0https://suse.okta.com/help/login.\u00a0\u00a0\u00a0If you are having a password or username issue please visit\u00a0https://suse.okta.com/, select \u201cNeed Help signing in?\u201d, and complete the steps that work best for you. \u00a0If neither of these resolves your problem and you are still unable to log in our team will be available to assist with any other issues. Please email your Customer Success Manager and/or\u00a0RancherSupportNow@suse.com.</p>"},{"location":"000020274/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020277/","title":"Is SUSE Rancher Hosted data encrypted at rest?","text":"<p>This document (000020277) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020277/#environment","title":"Environment","text":"<p>Hosted Rancher</p>"},{"location":"000020277/#resolution","title":"Resolution","text":"<p>Yes, the disk volumes of the virtual machines used by SUSE Rancher Hosted are encrypted as well as the disks used by the backend databases.</p>"},{"location":"000020277/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020296/","title":"[Rancher] How do I change severity level on an open case?","text":"<p>This document (000020296) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020296/#situation","title":"Situation","text":"<p>I have reviewed the severity levels listed below and I would now like to know\u00a0how I can change the severity level of an open case myself.</p> <p>SeverityDescriptionSev 1 (Critical)\u00a0The solution is in production or is mission-critical to your business. It is inoperable, and the situation is resulting in a total disruption of work. There is no workaround available.\u00a0Sev 2 (High)Operations are severely restricted, but work can continue. Core functionalities can operate in a restricted fashion despite important features being unavailable. A workaround is available that ensures no immediate business impact.Sev3 (Medium)The solution does not work as designed, resulting in a minor loss of usage. It may also be a significant software defect that impacts the Customer when performing some actions and has no workaround.Sev 4 (Low)There is no loss of service, and this may be a request for documentation, general information, product enhancement request, Software defects with workarounds or medium or low functionality impact, etc.</p>"},{"location":"000020296/#resolution","title":"Resolution","text":"<p>During the creation of a support case, you may select the severity of the support case via the drop-down menu. The creation of a Sev1 or a Sev2 support case will trigger our escalation workflows. If during the life of your support case the initial severity is no longer accurate, you may post a new comment with the string\u00a0bump_to_sev1\u00a0or\u00a0bump_to_sev2\u00a0to escalate a ticket to higher severity. This string can be stand-alone as a comment or can be a part of a sentence. \u00a0There is no need for any special characters (such as \" \" or * *).</p> <p>During the life of the case, you can also recategorize it to sev3 or sev4 by using\u00a0bump_to_sev3\u00a0or\u00a0bump_to_sev4. Using these\u00a0bump_to_\u00a0strings for sev1 and sev2 will trigger our escalation workflows, where sev3 and sev4 will update the severity with no escalation workflows.</p>"},{"location":"000020296/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020302/","title":"How to add custom labels to Alerts in Monitoring v2","text":"<p>This document (000020302) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020302/#environment","title":"Environment","text":"<p>Rancher Version: v2.5.8</p> <p>Monitoring Chart Version: 14.5.100</p>"},{"location":"000020302/#situation","title":"Situation","text":"<p>Alerts in Monitoring v2 contain standard labels. But in some cases, users want to inject custom labels like Kubernetes cluster names to easily identify the environment or need the labels for further Alert routing.</p>"},{"location":"000020302/#resolution","title":"Resolution","text":"<p>Use\u00a0defaultRules.additionalRuleLabels\u00a0 in the Monitoring Apps's YAML spec to inject custom labels.</p> <p>To inject cluster name, open the Monitoring App in Apps&amp; Marketplace from Cluster explorer.</p> <p>Under \"Edit as YAML\", add the custom label as below.</p> <pre><code>defaultRules:\n\u00a0 additionalRuleLabels:\n\u00a0 \u00a0 cluster: \"My_Test_cluster\"\n</code></pre> <p>Then click on \"Deploy\" or \"Upgrade\" if App is already installed.</p> <p>If your receiver is webhook, then the alerts will have the custom labels as shown in the below example alert.</p> <pre><code>...\n...\nstatus\":\"firing\",\n\"labels\":{\n\u00a0 \"alertname\":\"NodeClockNotSynchronising\",\n\u00a0&lt;b&gt; &amp;#34;cluster&amp;#34;:&amp;#34;My_Test_cluster&amp;#34;&lt;/b&gt;, &lt;&lt;&lt;------\n\u00a0 \"container\":\"node-exporter\",\n\u00a0 \"endpoint\":\"metrics\",\n\u00a0 \"instance\":\"192.168.110.157:9796\",\n\u00a0 \"job\":\"node-exporter\",\n\u00a0 \"namespace\":\"cattle-monitoring-system\",\n\u00a0 \"pod\":\"rancher-monitoring-prometheus-node-exporter-lg2g6\",\n\u00a0 \"prometheus\":\"cattle-monitoring-system/rancher-monitoring-prometheus\",\n\u00a0 \"service\":\"rancher-monitoring-prometheus-node-exporter\",\n...\n...\n\n</code></pre>"},{"location":"000020302/#additional-information","title":"Additional Information","text":"<p>GitHub issue #3325 is opened to add additional labels via UI.</p>"},{"location":"000020302/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020306/","title":"What Desginated Name (DN) fields are used by Kubernetes when generating custom certificates from a provided CA Cert or PKI tool?","text":"<p>This document (000020306) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020306/#environment","title":"Environment","text":"<p>RKE Kubernetes cluster, where the user decides to generate custom certificates from an external, non-self-signed Root Cerficiate Authority (CA).</p>"},{"location":"000020306/#situation","title":"Situation","text":"<p>Upon issuing the following command,</p> <pre><code>rke up\n</code></pre> <p>a user might experience the error message below.</p> <pre><code>INFO[0092] [authz] Creating rke-job-deployer ServiceAccount FATA[0119] Failed to apply the ServiceAccount needed for job execution: clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"kube-admin\" cannot create resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\n</code></pre>"},{"location":"000020306/#resolution","title":"Resolution","text":"<p>While working with your own PKI toolsets, and providing a custom CA (Certificate Authority),</p> <p>check the Organization (O) fields for the generated certificates Common Names (CN) and their signing requests (CSR).\u00a0 The examples here are directly from Kubernetes the Hard Way, and outline a manual process for certificate generation.\u00a0Where ${Instance} is mentioned, this is a variable for the name of the node that represents each kubelet, with every node having its own certificate.</p> <p>Kubernetes Component Certificates\u00a0Common Name (CN)Organization (O)CA RootkubernetesKubernetesAdminadminsystem:mastersKubeletsystem:node:${Instance}system:nodesController Managersystem:kube-controller-managersystem:kube-controller-managerKube Proxysystem:kube-proxysystem:node-proxierSchedulersystem:kube-schedluersystem:kube-schedulerAPI ServerkubernetesKubernetesService Account Key Pairservice-accountsKubernetes</p>"},{"location":"000020306/#cause","title":"Cause","text":"<p>If the proper Organization fields are not set, Kubernetes cannot assign compatible permissions to the different underlying components.\u00a0 With self-signed certificates these fields are generated automatically.\u00a0 With custom certificates, it's important to verfiy these fields are correct, as they may have been set by external tooling or automation.</p> <p>Sometimes a Kubernetes administrator may leave all the fields of their certificates or CSRs blank or with predefined values, it can be easy to overlook the O field for a specific CN, an the proper kubernetes permissions will not get applied.</p>"},{"location":"000020306/#additional-information","title":"Additional Information","text":"<p>Kubernetes the Hard Way, Certificate Authority -\u00a0https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md</p>"},{"location":"000020306/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020328/","title":"containerd.io 1.4.4 bug advisory","text":"<p>This document (000020328) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020328/#environment","title":"Environment","text":"<p>Recently our team has been made aware of PLEG health issues caused by a recent containerd.io package version.</p> <p>This can manifest in a few ways, we usually see all Docker commands failing, but in recent cases, the Docker commands with a specific security flag fail and hang. We have isolated this to containerd.io package version 1.4.4-x, and has been logged in\u00a0this GitHub issue.</p>"},{"location":"000020328/#situation","title":"Situation","text":""},{"location":"000020328/#update-as-of-25may21","title":"******Update as of 25MAY21 ********","text":"<p>Our team is now aware that this issue is resolved in rc95 or higher. Please note that the issue was resolved in rc94, but there was a CVE tied to that rc version. Our recommendation for anyone experiencing this issue to move to rc95 or higher to resolve the containerd issues as well as avoiding the known CVE found in rc94. Even if you are not experiencing issues tied to this advisory, we would recommend you move to rc95.</p>"},{"location":"000020328/#end-update-as-of-25may21","title":"******End Update as of 25MAY21 ********","text":""},{"location":"000020328/#how-do-i-know-if-i-am-impacted","title":"How do I know if I am impacted?","text":"<p>Customers running RKE could be impacted by this. This is impacting customers running Docker 19.03 and 20.10 where\u00a0containerd.io\u00a0is using 1.4.4 -x. \u00a0Nodes running\u00a0containerd.io 1.4.4 may experience containers hanging on initialization after a certain number of containers with\u00a0<code>no-new-privileges</code>\u00a0are started.</p> <p>Often this has come as a result of upgrading Docker with Rancher 2.5.6. The symptoms include PLEG timeout errors in the Rancher UI, CoreDNS pods failing to start, and\u00a0<code>docker inspect</code>\u00a0commands to hang on certain containers.</p> <p>As the issue relates to the specific runc version (1.0.0-rc93) bundled with containerd.io, the following can be a basic test to identify if the node is running the affected runc build:</p> <pre><code>runc --version | grep -q 1.0.0-rc93 &amp;&amp; echo \"AFFECTED\" || echo \"NOT AFFECTED\"\n</code></pre>"},{"location":"000020328/#resolution","title":"Resolution","text":""},{"location":"000020328/#is-there-a-workaround","title":"Is there a workaround?","text":""},{"location":"000020328/#update-as-of-25may21_1","title":"******Update as of 25MAY21 ********","text":"<p>The below workaround should not be used any longer. \u00a0With the release of rc95 (mentioned above) any customers experiencing this issue should upgrade to rc95 as the resolution is found there.</p>"},{"location":"000020328/#end-update-as-of-25may21_1","title":"******End Update as of 25MAY21 ********","text":"<p>Yes, currently our team recommends that you take the following step:</p> <p>Downgrade or install the containerd.io package to a\u00a01.4.3-x version.\u00a0 There is no need to modify privileges on CoreDNS pods, once downgraded to 1.4.3 you should pin that version to not auto-update. Please ensure your team is aware of\u00a0CVE-2021-21334\u00a0in 1.4.3-x.</p> <p>As examples of downgrading the containerd.io package on affected nodes:</p> <p>Ubuntu:</p> <pre><code>apt install containerd.io=1.4.3-1\n</code></pre> <p>EL:</p> <pre><code>yum downgrade containerd.io-1.4.3-3.1.el7\n</code></pre> <p>As needed, drain and cordon the node, followed by restarting the Docker daemon.</p> <p>For the most accurate steps, we recommend you consult the documentation for your OS on downgrading and version pinning for the specific package manager and Linux distribution.</p> <p>For customers who have not upgraded their Rancher clusters to 2.5.6+, we recommend that you hold off on upgrading until this is resolved upstream. If you need to upgrade to Rancher 2.5.6+, you should be safe to upgrade to Rancher when using the above process to install and pin the containerd.io package to a 1.4.3-x version.</p> <p>In the meantime, if you have any questions, please reach out to your Customer Success Manager or Rancher Support via a Support Ticket.</p>"},{"location":"000020328/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020350/","title":"Streaming server stopped unexpectedly: listen tcp x.x.x.x:0: bind: cannot assign requested address","text":"<p>This document (000020350) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020350/#environment","title":"Environment","text":"<p>Rancher v2.5.7</p> <p>Kubernetes v1.20.4-rancher1</p>"},{"location":"000020350/#situation","title":"Situation","text":"<p>Adding a new node to an existing cluster stuck in \u201cregistering\u201d.</p> <p>Kubelet pod is in\u00a0<code>Restarting</code>\u00a0state on the new node.</p> <pre><code>$ docker ps -a |grep kubelet\n</code></pre> <pre><code>66bd40b36e76 rancher/hyperkube:v1.20.4-rancher1 \"/opt/rke-tools/entr\u2026\" 7 minutes ago Restarting (255) 32 seconds ago kubelet\n</code></pre> <p>The kubelet is failing with below error.</p> <pre><code>$ docker logs kubelet\n</code></pre> <pre><code>\"2021-07-26T11:13:48.270162766Z F0726 11:13:48.270086   40730 docker_service.go:415] Streaming server stopped unexpectedly: listen tcp 27.0.0.1:0: bind: cannot assign requested address\"\n</code></pre>"},{"location":"000020350/#resolution","title":"Resolution","text":"<p>Update\u00a0<code>/etc/hosts</code>\u00a0file with the correct entry.</p> <pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n</code></pre> <p>After updating the file, the kubelet will restart itself and pick up the modified /etc/host file to bind to the loopback IP.</p>"},{"location":"000020350/#cause","title":"Cause","text":"<p>File /etc/hosts in the new node had below incorrect entry.</p> <pre><code>27.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n</code></pre>"},{"location":"000020350/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020373/","title":"How to drain node from local node using docker command","text":"<p>This document (000020373) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020373/#environment","title":"Environment","text":"<p>RKE1</p> <p>Rancher v2.4.x , v2.5.x</p>"},{"location":"000020373/#situation","title":"Situation","text":"<p>Need a way to cordon or drain node when OS patching is automated, and the automation doesn't have access to Rancher API.</p>"},{"location":"000020373/#resolution","title":"Resolution","text":"<p>Integrate the below command in the automation to drain the node in the OS patching workflow.</p> <pre><code>docker exec kubelet bash -c 'kubectl --kubeconfig &lt;(kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\") drain $(hostname -s) --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s'\n</code></pre> <p>Please note that the below flags need to be changed according to your requirements.</p> <pre><code>--delete-local-data=true\n--force=true\n--grace-period=60\n--ignore-daemonsets=true\n--timeout=120s\n</code></pre>"},{"location":"000020373/#cause","title":"Cause","text":"<p>Draining operation using kubeconfig file \" /etc/kubernetes/ssl/kubecfg-kube-node.yaml\" will result in errors like below since the \" system:node\" role is not authorized to access needed API groups.</p> <pre><code>cannot delete daemonsets.apps \"nginx-ingress-controller\" is forbidden: User \"system:node\" cannot get resource \"daemonsets\" in API group \"apps\" in the namespace \"ingress-nginx\"\n</code></pre> <p>But this kubeconfig file can access the config map \"full-cluster-state\" in namespace \" kube-system\"\u00a0 contains the kubeconfig file, which has the privilege to do the drain operation.</p>"},{"location":"000020373/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020415/","title":"Updating roles in Rancher UI fails with certificate error","text":"<p>This document (000020415) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020415/#environment","title":"Environment","text":"<p>Rancher v2.5.x</p>"},{"location":"000020415/#situation","title":"Situation","text":"<p>Editing roles in Rancher UI fails with the below error.</p> <pre><code>Internal error occurred: failed calling webhook \"rancherauth.cattle.io\": Post \"https://rancher-webhook.cattle-system.svc:443/v1/webhook/validation?timeout=10s\": x509: certificate has expired or is not yet valid: current time 2021-10-25T07:43:50Z is after 2021-10-06T20:20:47Z\n\n</code></pre>"},{"location":"000020415/#resolution","title":"Resolution","text":"<ul> <li>Set kubectl context to Rancher management cluster.</li> <li>Take the backup of existing secret</li> </ul> <pre><code>kubectl get secret -n cattle-system cattle-webhook-tls -o yaml &gt; cattle-webhook-tls.yaml\n</code></pre> <ul> <li>``` Delete the secret that contains expired certificate</li> </ul> <pre><code>\n\n</code></pre> <p>kubectl delete secret -n cattle-system cattle-webhook-tls</p> <pre><code>\n- Delete the rancher webhook Pod to regenerate the expired certificate.\n\n</code></pre> <p>kubectl delete pod -n cattle-system -l app=rancher-webhook ```</p>"},{"location":"000020415/#cause","title":"Cause","text":"<p>This issue is caused by the expired certificate of the rancher webhook.</p>"},{"location":"000020415/#additional-information","title":"Additional Information","text":"<p>The issue is tracked in GitHub issue\u00a035068</p>"},{"location":"000020415/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020416/","title":"Downstream clusters flapping between available and unavailable state","text":"<p>This document (000020416) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020416/#environment","title":"Environment","text":"<p>Rancher version: v2.5.6</p> <p>Management cluster K8S version: 1.21.5</p>"},{"location":"000020416/#situation","title":"Situation","text":"<p>After upgrading the Kubernetes version of the Rancher management cluster, the downstream cluster status in the WebUI flaps between the available and unavailable states.</p> <p>Rancher Pod logs show errors like the below;</p> <pre><code>Failed to connect to peer wss://x.x.x.x/v3/connect [local ID=y.y.y.y]: websocket: bad handshake\n</code></pre>"},{"location":"000020416/#resolution","title":"Resolution","text":"<p>Upgrade Rancher to v2.6.x</p> <p>A workaround until Rancher upgarde is to reduce the Rancher deployment replicas to one.</p>"},{"location":"000020416/#cause","title":"Cause","text":"<p>Rancher is storing the service account token from the initial Pod, and then trying to reuse that on subsequent requests even though that pod has been deleted.</p> <p>As of Kubernetes version v1.21, service account tokens are pod-specific, and are invalidated when the pod is deleted, which is why Rancher is unable to use it and thus unable to reach other Rancher replica instances via web-socket.</p>"},{"location":"000020416/#additional-information","title":"Additional Information","text":"<p>The issue is tracked in the GitHub issue\u00a026082</p>"},{"location":"000020416/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020430/","title":"[Rancher] What is the difference between \"stable\" and \"latest\" release tags?","text":"<p>This document (000020430) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020430/#resolution","title":"Resolution","text":"<p>For the Rancher portfolio of products, we make available \u201clatest\u201d tagged releases for our community to test-drive a new release and provide us feedback. These \u201clatest\u201d tagged releases, whilst covered by Rancher Support Services, are not meant for production use cases. Customers are recommended to use the \u201cstable\u201d tagged releases for their own production use cases.</p>"},{"location":"000020430/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"000020430/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020431/","title":"[Rancher] What does 'Developer Support\" mean?","text":"<p>This document (000020431) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020431/#resolution","title":"Resolution","text":"<p>Any mention of \"Developer Support\" is basically a reference to this:</p> <p>Rancher Support is part of Rancher Engineering organization and has access to Engineers who developed Rancher product features. These Engineers are able to assist Rancher Support should some deep troubleshooting investigation be needed.</p>"},{"location":"000020431/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020432/","title":"Rancher Support FAQ","text":"<p>This document (000020432) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020432/#resolution","title":"Resolution","text":""},{"location":"000020432/#quick-links","title":"Quick Links","text":"<ul> <li>SUSE Rancher Product Lifecycle Information</li> <li>SUSE Rancher Product Support Matrices</li> <li>SUSE Rancher Support Advisories</li> <li>SUSE Rancher Hosted FAQ</li> <li>SUSE Customer Center\u00a0(SCC) (Support Portal)</li> <li>SCC sign-in help</li> <li>Rancher 2.x Linux log collector script</li> <li>Rancher 2.x Windows log collector script</li> <li>Rancher 2.x Systems summary script</li> </ul>"},{"location":"000020432/#severity-levels-designations-and-escalations","title":"Severity Levels, Designations, and Escalations","text":"<ul> <li>Could you help us understand\u00a0how to determine the severity level for my case, what the response times are? (SUSE Support blog post)</li> <li>How do I change the severity level on an open case?</li> <li>Could you illustrate the severity levels with subject lines of sample support cases?</li> </ul>"},{"location":"000020432/#product-releases-and-support-phases","title":"Product Releases and Support Phases","text":"<ul> <li>What is the difference between \"stable\" and \"latest\" release tags?</li> <li>What does 'Developer Support\" mean?</li> <li>What is \"Full Support\" vs \"Limited Support\"?</li> <li>Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version?</li> </ul>"},{"location":"000020432/#request-for-assistance","title":"Request for Assistance","text":"<ul> <li>Can Rancher Support validate our planned upgrade?</li> <li>We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?</li> <li>How much in advance do we need to notify Rancher Support on upgrades?</li> <li>Can Rancher Support join me as I do my upgrade?</li> <li>We need help validating our deployment design and operational readiness, can Rancher Support help?</li> </ul>"},{"location":"000020432/#rancher-and-rancheros","title":"Rancher and RancherOS","text":"<ul> <li>Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?</li> <li>Could you help us understand the development and support status of RancherOS for 2020 and beyond?</li> <li>Does Rancher support migration from single node installation to high availability installation?</li> <li>Does Rancher Support cover single node installations?</li> </ul>"},{"location":"000020432/#kubernetes","title":"Kubernetes","text":"<ul> <li>How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?</li> <li>I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?</li> <li>As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?</li> <li>Is Rancher Support only for RKE-provisioned clusters?</li> <li>Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?</li> </ul>"},{"location":"000020432/#docker-and-os","title":"Docker and OS","text":"<ul> <li>What does Rancher support for Docker on Ubuntu cover?</li> <li>Will Rancher support us should our deployment be on Red Hat Atomic?</li> <li>What does Rancher support for Docker on RHEL cover?</li> <li>What does Rancher support for Docker on CentOS cover?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL?</li> <li>What does Rancher support for Docker on Windows Server cover?</li> <li>What does Rancher support for Docker on Oracle Linux cover?</li> </ul>"},{"location":"000020432/#security","title":"Security","text":"<ul> <li>How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?</li> <li>How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?</li> </ul>"},{"location":"000020432/#certified-integrations","title":"Certified Integrations","text":"<ul> <li>What are the certified integrations with persistent volume plugins covered by Rancher Support?</li> <li>What are the certified integrations with storage class provisioners covered by Rancher Support?</li> <li>What are the certified integrations with authentication providers covered by Rancher Support?</li> <li>Could you clarify what you generally mean by a \"certified integration\" to another software system or service?</li> </ul>"},{"location":"000020432/#included-open-source-software-components","title":"Included Open Source Software Components","text":"<ul> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?</li> <li>Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?</li> <li>What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?</li> <li>Will Rancher fix problems root-caused to be in nginx?</li> <li>Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?</li> <li>We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?</li> </ul>"},{"location":"000020432/#third-party-software-components","title":"Third-party Software Components","text":"<ul> <li>Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software?</li> <li>Does Rancher include any 3rd party, commercial software components?</li> </ul>"},{"location":"000020432/#support-matrix","title":"Support Matrix","text":"<ul> <li>We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA?</li> <li>We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then?</li> <li>I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then?</li> </ul>"},{"location":"000020432/#node-drivers-and-infrastructure","title":"Node Drivers and Infrastructure","text":"<ul> <li>My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack?</li> <li>Does it matter what hardware my hosts are on? Are virtualized servers supported?</li> <li>I see node drivers tagged as \"Built-in\". What does that mean?</li> <li>I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?</li> </ul>"},{"location":"000020432/#kubernetes-cloud-providers","title":"Kubernetes Cloud Providers","text":"<ul> <li>What are the Kubernetes cloud providers supported by Rancher?</li> </ul>"},{"location":"000020432/#requests-for-enhancements-rfes","title":"Requests for Enhancements (RFEs)","text":"<ul> <li>I filed an RFE as a support case. What should I expect on how it will be followed up on?</li> </ul>"},{"location":"000020432/#support-for-all-other-rancher-software-projects","title":"Support for all other Rancher software projects","text":"<ul> <li>What about support for Harvester, Longhorn, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider?</li> <li>Does Rancher Support cover RKE (standalone CLI)?</li> <li>How about support for ancillary projects, such as an API client, from Rancher Labs?</li> </ul>"},{"location":"000020432/#customizations","title":"Customizations","text":"<ul> <li>We have a few customizations with our on- premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations?</li> <li>Is there a way we can get our custom work be included into Rancher Support?</li> </ul>"},{"location":"000020432/#localization","title":"Localization","text":"<ul> <li>What language(s) is Rancher Support service offered in?</li> <li>Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support?</li> </ul>"},{"location":"000020432/#licensing-and-usage","title":"Licensing and Usage","text":"<ul> <li>Does my support subscription to Rancher include support for Longhorn?</li> <li>Does my support subscription to Rancher include support for RancherOS?</li> <li>Can we have a mix of unsupported and supported nodes at our choice/discretion?</li> <li>Can we request support for the management/upstream cluster and certain downstream clusters and not others?</li> <li>How does Rancher track our license usage?</li> <li>Is Rancher Support only for production environments?</li> <li>I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well?</li> <li>Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue?</li> </ul>"},{"location":"000020432/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020434/","title":"[Rancher] I filed an RFE as a support case. What should I expect on how it will be followed up on?","text":"<p>This document (000020434) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020434/#resolution","title":"Resolution","text":"<p>For any RFEs received via a support case, Rancher Support will update the case with a GitHub link to the enhancement request.</p> <p>Rancher Support will try to understand your request first and then qualify the request with information on business impact, time sensitivity, and criticality. It is not uncommon at all for there to be follow-up questions from Rancher Support on your request. This information will be most helpful to advocate for the request with Rancher Product Management.</p> <p>Unless there is a pressing urgency that has been understood and acknowledged, it could take up to a few weeks to triage the RFE (from the product backlog) through our release planning and identify the earliest release vehicle the item could be considered for (or committed to).</p> <p>As there is progress and/or should there be specific questions from Engineering, Rancher Support will keep you updated via this case. As necessary, Rancher Support will also recommend and schedule/facilitate a direct web conference session for you with our product management to go over the RFE.</p> <p>For committed RFEs, Rancher Support will update you as the item gets closer to its general availability via a new Rancher release or keep you informed should there be any delays or proposed changes in the previously understood scope.</p>"},{"location":"000020434/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020435/","title":"[Rancher] What is \"Full Support\" vs \"Limited Support\"?","text":"<p>This document (000020435) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020435/#resolution","title":"Resolution","text":"<p>\"Full Support\" is the support phase when a product is between its GA (General Availability) and End of Maintenance (EOM) milestones. Support entails general troubleshooting of a specific issue to isolate potential causes. Issue resolution is pursued through one or more of the following:</p> <ul> <li>Applying configuration changes</li> <li>Upgrade recommendation to an existing newer version of the product</li> <li>Code-level maintenance in the form of product updates; typically, results in a maintenance release, which is a newer version of the product that was not existing at the time the issue was encountered</li> </ul> <p>\"Limited Support\" is the support phase when a product is between its EOM and End of Life (EOL) milestones. During this phase, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner in the form of:</p> <ul> <li>General troubleshooting of a specific issue to isolate potential causes</li> <li>Issue resolution is limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product</li> </ul>"},{"location":"000020435/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020436/","title":"[Rancher] Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version?","text":"<p>This document (000020436) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020436/#resolution","title":"Resolution","text":"<p>As our standard policy, any bug will only be fixed and delivered as a new maintenance release within the minor version of a Rancher product. Customer will be advised to upgrade to this new maintenance release that contains the fix.</p> <p>Table below illustrates this with an example:</p> <p>v2.1.3</p> <p>Version that the Customer deployment is currently on. This is the product version that the bug has been found and acknowledged in, stemming from the support issue reported by\u00a0Customer.</p> <p>v2.1.9</p> <p>Version that is the latest maintenance release of Rancher v2.1 at the time the bug was found and acknowledged.</p> <p>v2.1.c</p> <p>Version \"c\" is where the bug is likely to get fixed. Here, \"c\" = 10 or greater. This is the soonest maintenance release of Rancher 2.1 that the Customer will need to upgrade to, to realize\u00a0the bug fix.</p> <p>Should the bug fix be deemed as necessary for other minor versions of the same Rancher product, Rancher shall deliver the fix in an approach similar to the sequence in the above table. For a scenario where v2.0.14 and v2.2.2 are the latest maintenance releases of Rancher v2.0 and v2.2, and the same bug fix needs to be made available in these two minor versions, it is shall only be delivered in a release that is v2.0.15 or later and v2.2.3 or later, respectively.</p> <p>In extraordinary situations, as an exception to our standard policy and as feasible, Rancher can provide a patch fix on a specific Customer-requested version.</p>"},{"location":"000020436/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020437/","title":"[Rancher] Can Rancher Support validate our planned upgrade?","text":"<p>This document (000020437) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020437/#resolution","title":"Resolution","text":"<p>Rancher Support recommends Customers to notify them ahead of planned maintenance events for upgrade and migration. The notification can be made as a support case. At a minimum, this case should have information on:</p> <ul> <li>Current Rancher version</li> <li>Target Rancher version</li> <li>Reason for upgrade</li> <li>Upgrade date and maintenance time window</li> </ul> <p>But it would be best if Customer could provide the requested information in a document (please ask for the \" Upgrade Notification RFI Template\" in the support case). Based on the information provided, Rancher Support can provide any applicable advisories to the Customer for the planned event. Rancher Support may request Customer to run specific information gathering scripts. Data collected from these scripts will be used by Rancher Support to understand the Customer deployment and validate the upgrade path that is being considered.</p>"},{"location":"000020437/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"000020437/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020438/","title":"[Rancher] We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?","text":"<p>This document (000020438) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020438/#resolution","title":"Resolution","text":"<p>Rancher Support will not join the Customer team remotely for the duration of the event. However, they will remain on standby, on high alert, to respond per SLA should there be an issue.</p> <p>Rancher Support recommends Customer to notify in advance, via a support ticket, all necessary information about such events. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the event.</p>"},{"location":"000020438/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020439/","title":"[Rancher] How much in advance do we need to notify Rancher Support on upgrades?","text":"<p>This document (000020439) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020439/#resolution","title":"Resolution","text":"<p>On all regular, planned upgrades, Rancher Support requests Customer to notify as early as possible. Notifications that are one (1) calendar week in advance provide reasonable time for Rancher Support to follow up with any applicable advisories.</p>"},{"location":"000020439/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020440/","title":"[Rancher] Can Rancher Support join me as I do my upgrade?","text":"<p>This document (000020440) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020440/#resolution","title":"Resolution","text":"<p>Generally speaking, Rancher Support will not join a Customer for the purpose of performing upgrades of their environment. However, Customers are encouraged to notify Rancher Support of planned maintenance events for upgrade and migration. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the maintenance window. Additionally, such notifications provide for an opportunity to validate the planned upgrade proactively rather than react to issues that could have been avoided.</p>"},{"location":"000020440/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020441/","title":"[Rancher] We need help validating our deployment design and operational readiness, can Rancher Support help?","text":"<p>This document (000020441) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020441/#resolution","title":"Resolution","text":"<p>Validating deployment design and operational readiness is an activity for SUSE Rancher Premium Support and/or Consulting teams. Requests coming in as a support case for this activity shall be routed to the leadership team of Rancher Premium Support and Consulting, for follow-up with the Customer.</p> <p>Topics such as the following, but not limited to, will be handled in a similar manner:</p> <ul> <li>Best Practice Guidance</li> <li>Deployment Review</li> <li>Design Assistance</li> <li>Co-development</li> <li>Migration Assistance</li> <li>Performance Tuning</li> <li>Security Assessment</li> <li>Solution Consulting</li> <li>Topical Training</li> <li>Technology Recommendation</li> </ul> <p>Typically, these are topics that are not specific to incidents and issue troubleshooting. They are more requests with strategic drivers that usually require the help and guidance of SUSE Rancher Premium Support and/or Consulting teams.</p>"},{"location":"000020441/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"000020441/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020442/","title":"[Rancher] Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?","text":"<p>This document (000020442) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020442/#resolution","title":"Resolution","text":"<p>Whilst technically possible, running other workloads or microservices in the same Kubernetes cluster that Rancher is installed on invalidates the Rancher Support SLA. So it is something that is recommended against.</p> <p>Any technical support that is offered for tickets stemming from this scenario shall only be on a best-effort basis.</p> <p>Note:</p> <p>Run Rancher on a Separate Cluster is also one of the top items called out in our docs page on Tips for Running Rancher.</p>"},{"location":"000020442/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020443/","title":"[Rancher] Could you help us understand the development and support status of RancherOS for 2020 and beyond?","text":"<p>This document (000020443) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020443/#resolution","title":"Resolution","text":""},{"location":"000020443/#development-status","title":"Development Status","text":""},{"location":"000020443/#edit-nov-2021-rancheros-1x-is-past-its-eol-milestone-and-no-longer-maintained","title":"EDIT (NOV 2021): RancherOS 1.x is past its EOL milestone and no longer maintained.","text":"<p>RancherOS 1.x is currently in a maintain-only-as-essential mode. That is to say, it is no longer being actively maintained at a code level other than addressing critical or security fixes. There are two significant reasons behind this product decision:</p> <p>1. Docker. The current industry requirements for a container runtime is very much evolving. Container runtimes like containerd and CRIO are now being actively considered as the default choice. RancherOS 1.x, which was specifically designed around using Docker engine only, unfortunately does not lend itself, in its current design, to this new evolving requirement.</p> <p>2. ISV Support. RancherOS was specifically designed as a minimalistic OS to support purpose-built containerized applications. It was not designed to be used as a general-purpose OS (such as CentOS or Ubuntu). As such, most ISVs have not certified their software to run on RancherOS, nor does RancherOS even contain the necessary components for many of these applications to run.</p>"},{"location":"000020443/#support-status","title":"Support Status","text":""},{"location":"000020443/#rancheros-1x-is-no-longer-commercially-supported","title":"RancherOS 1.x is no longer commercially supported.","text":"<p>Please refer this FAQ: Does my support subscription to Rancher include support for RancherOS?</p> <p>Any assistance from Rancher Support on RancherOS topics, filed as support cases, is not SLA- bound.</p>"},{"location":"000020443/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020444/","title":"[Rancher] Does my support subscription to Rancher include support for RancherOS?","text":"<p>This document (000020444) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020444/#resolution","title":"Resolution","text":"<p>No. RancherOS is an independent product. A separate commercial support subscription is\u00a0needed for RancherOS support.\u00a0 But that is no longer available for purchase as RancherOS 1.x\u00a0is past its End-of-Life (EOL) and End-of-Sale (EOS)\u00a0milestone dates.</p> <p>Also, refer Could you help us understand the development and support status of RancherOS for 2020 and beyond?</p>"},{"location":"000020444/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020445/","title":"[Rancher] Does Rancher support migration from single node installation to high availability installation?","text":"<p>This document (000020445) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020445/#resolution","title":"Resolution","text":"<p>There is currently no validated path that is officially covered by Rancher Support, for any Customer starting with a single node installation, and wishing to migrate to a high availability installation at a later time. Whilst there is a Rancher blog post\u00a0that talks about one possible migration path, it is not covered by Rancher Support.</p> <p>Where scale and performance criteria are well understood to be critical, Customers are recommended to set up Rancher in a high availability configuration, right from the outset.</p>"},{"location":"000020445/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020446/","title":"[Rancher] Does Rancher Support cover single node installations?","text":"<p>This document (000020446) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020446/#resolution","title":"Resolution","text":"<p>Rancher Support can cover single node installations only for support tickets that are NOT related to (a) scale and performance and (b) recovery of data and software (embedded etcd).</p> <p>Customer environments in need of scale and performance and meeting recovery criteria should be set up as high availability installations.</p> <p>Refer this Rancher docs page for single node installation versus high availability installation. Rancher recommends high-availability installs in production environments, where the Customer's user base requires 24\u20447 access to running applications.</p>"},{"location":"000020446/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020447/","title":"[Rancher] How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?","text":"<p>This document (000020447) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020447/#resolution","title":"Resolution","text":"<p>Kubernetes versions are expressed as x.y.z, where x is the major version, y is the minor version, and z is the patch version, following Semantic Versioning terminology.</p> <p>Where an upstream version x.y.z has been posted as supported in the support matrix, under Rancher Kubernetes, z is the highest patch version for that minor version (y) of Kubernetes, that has been tested and validated for the specific Rancher product version.</p> <p>In the case of Rancher v2.6.2 , the following are listed as the supported k8s upstream versions for Rancher Kubernetes:</p> <ul> <li>v1.21.5</li> <li>v1.20.11</li> <li>v1.19.15</li> <li>v1.18.20</li> </ul> <p>That is to say, the following are the k8s versions that are supported in Rancher v2.6.2:</p> <ul> <li>v1.21.x (v1.21.0-v1.21.5)</li> <li>v1.20.x (v1.20.0-v1.20.11)</li> <li>v1.19.x (v1.19.0-v1.19.15)</li> <li>v1.18.x (v1.18.0-v1.18.20)</li> </ul> <p>Note:</p> <p>As described in this\u00a0Rancher docs page, the RKE metadata feature\u2014available as of v2.3.0\u2014allows users to provision clusters with new versions of Kubernetes as soon as they are released, without upgrading Rancher. For a specific version of Rancher, if a k8s patch version ( z+i) that is higher than what is listed in the support matrix is available via a metadata refresh, then that patch version z+i is considered supported for that version of Rancher.</p> <p>Also, see Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?</p>"},{"location":"000020447/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020448/","title":"[Rancher] Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?","text":"<p>This document (000020448) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020448/#resolution","title":"Resolution","text":"<p>Rancher supports the most recent three minor releases of kubernetes that are still active. Additionally, Rancher may also support one (or more) recent upstream version to drop off its active maintenance status.\u00a0 As an example, below are the highest k8s minor versions supported by the respective Rancher 2.x product versions (as of January 2022):</p> <ul> <li>Rancher v2.4.18 =&gt; k8s 1.15, 1.16, 1.17, 1.18</li> <li>Rancher v2.5.14 =&gt; k8s 1.17, 1.18, 1.19, 1.20</li> <li>Rancher v2.6.3 =&gt; k8s 1.18, 1.19, 1.20, 1.21</li> </ul> <p>Arithmetic progression, if any, in the sequence above is merely coincidental and should not be used to extrapolate the k8s versions that a future version of Rancher such as v2.7 would support. Also, for the most up-to-date information, please visit the All Supported Versions page.</p> <p>Generally speaking, the following should help understand the Rancher approach to supporting k8s versions:</p> <p>Rancher Labs strives to certify the latest GA release of k8s roughly in a month's timeframe from its availability. For example, k8s v1.16 became generally available in September 2019. The Rancher roadmap consideration would then be to certify and support k8s v1.16 in a release vehicle targeted for no later than October 2019.</p> <p>The ability to certify a new GA release of k8s, per above, could however be impacted by any unplanned- for CVEs that Rancher Labs needs to react to. This turned out to be the case for Rancher v2.3.1 that shipped on 16 Oct 2019.</p> <p>The focus of v2.3.1 shifted to addressing on priority a new k8s CVE (CVE-2019-11253) announced by upstream kubernetes. And, hence the support for k8s v1.16 got moved to the release vehicle after v2.3.1. And, when v1.16 is supported in that release, v1.13 shall be dropped in our support matrix from that version forward.</p> <p>This is to also keep up with the k8s version maintenance policy that you can see here: https://kubernetes.io/docs/setup/release/version-skew-policy/#supported-versions</p> <p>Specifically,</p> <p>\"The Kubernetes project maintains release branches for the most recent three minor releases.</p> <p>Minor releases occur approximately every 3 months, so each minor release branch is maintained for approximately 9 months.\"</p> <p>Also, see\u00a0How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?</p>"},{"location":"000020448/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020449/","title":"[Rancher] I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?","text":"<p>This document (000020449) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020449/#resolution","title":"Resolution","text":"<p>Rancher Support is comprehensive for RKE-provisioned clusters. In imported clusters, Rancher Support applies only to the extent of troubleshooting and root-causing. For issues in (legacy) clusters under consideration for import into Rancher, Customer is advised to take it up with the party that is the provider of support for such clusters. Post-import, Rancher Support for such clusters is per response to this question, Is Rancher Support only for RKE-provisioned clusters?</p>"},{"location":"000020449/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020450/","title":"[Rancher] Is Rancher Support only for RKE-provisioned clusters?","text":"<p>This document (000020450) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020450/#resolution","title":"Resolution","text":"<p>Update:</p> <p>In November 2019, k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here . With the general availability of k3s, Rancher Support extends to k3s clusters.</p> <p>Comprehensive Rancher Support, inclusive of Kubernetes and Docker, applies only to RKE-provisioned clusters for Rancher releases before 2.6.5. \u00a0As of the 2.6.6 release of Rancher, k3s, RKE2 are fully supported as well What this means is the following:</p> <p>In the RKE-provisioned clusters, Rancher can provide patch fixes as needed at the levels of Kubernetes and Docker.</p> <p>For clusters that are brought under management of the Rancher control plane, as imported clusters, Rancher Support applies only to those Kubernetes versions published in the support matrix and to the extent of making sure Rancher control plane functionality works as published in Rancher docs. Issues root- caused to be inside these clusters will need to be taken up by Customer with the provider of support for these clusters.</p>"},{"location":"000020450/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020451/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?","text":"<p>This document (000020451) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020451/#resolution","title":"Resolution","text":"<p>Yes, should there be a critical need, Rancher can provide patch fixes to address issues root-caused in RKE-provisioned Kubernetes clusters. As a first option, Rancher will investigate if the fix is already available in a later version of Kubernetes. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later Kubernetes version that has the fix. Or validate and certify one of its existing versions to work with the later Kubernetes version that has the fix.</p> <p>Further, Rancher can submit a PR for the fix to Kubernetes for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Kubernetes.</p>"},{"location":"000020451/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020452/","title":"[Rancher] As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?","text":"<p>This document (000020452) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020452/#resolution","title":"Resolution","text":"<p>It is recommended to move to the k8s versions listed in the matrix for a Rancher version but not doing that should not break things. You could leave the cluster (for example, k8s v1.11) as is, for a move to say Rancher v2.2.6 from v2.2.2. It should be ok. Rancher Support will continue to help should there be a need.</p> <p>That said, should there be an issue on that cluster that requires a fix in k8s v1.11 we may not be able to do that and would at that point require an upgrade to a higher supported k8s version.</p>"},{"location":"000020452/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020453/","title":"[Rancher] What are the Kubernetes cloud providers supported by Rancher?","text":"<p>This document (000020453) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020453/#resolution","title":"Resolution","text":"<p>Rancher Support is currently limited to Kubernetes cloud providers for Amazon and Azure. When adding a cluster, Amazon and Azure are the only two cloud providers that are currently surfaced up in the Rancher UX.</p> <p>For all other cloud providers, directly editing the yaml file via the custom option is the only way to pass configuration information. In this scenario, Rancher Support expects Customer to manage and troubleshoot the syntactic correctness of the yaml file, per guidance\u00a0provided by Kubernetes.</p>"},{"location":"000020453/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020454/","title":"[Rancher] What about support for Harvester, Longhorn, Rancher Desktop, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider?","text":"<p>This document (000020454) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020454/#resolution","title":"Resolution","text":"<p>Note:</p> <p>In December 2021, Harvester graduated from being just a community project led by SUSE Rancher to a product that is now commercially supported by SUSE Rancher with a subscription to an add-on plan.\u00a0 View release notes for Harvester v1.0.0 here.</p> <p>In June 2020, Longhorn graduated from being just a community project led by Rancher Labs to a product that is now commercially supported by Rancher Labs with a subscription to an add-on plan. View release notes for Longhorn v1.0.0 here.</p> <p>In June 2020, Terraform Provider for Rancher v2 graduated from a community project that was supported on a best-effort basis to being fully supported as part of an active subscription to a SUSE Rancher Support plan. Support for this project does not require any additional subscriptions. Visit the project repo here.</p> <p>In November 2019, k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here.</p> <p>Projects Rancher Desktop, Kubewarden, Hypper, Epinio, and Opni are some of the new open-source software projects led by SUSE Rancher. These projects (and some legacy ones from Rancher Labs such as rio, k3os, and Submariner) are not yet available for commercial support from SUSE Rancher. For any bugs or questions, users are encouraged to post their issues here:</p> <p>Project</p> <p>GitHub Location</p> <p>Rancher Desktop</p> <p>https://github.com/rancher-sandbox/rancher-desktop/issues</p> <p>Kubewarden</p> <p>https://github.com/kubewarden</p> <p>Hypper</p> <p>https://github.com/rancher-sandbox/hypper/issues</p> <p>Epinio</p> <p>https://github.com/epinio/epinio/issues</p> <p>Opni</p> <p>https://github.com/rancher/opni/issues</p> <p>rio</p> <p>https://github.com/rancher/rio/issues</p> <p>k3os</p> <p>https://github.com/rancher/k3os/issues</p> <p>Submariner</p> <p>https://github.com/submariner-io/submariner/issues</p>"},{"location":"000020454/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"000020454/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020456/","title":"[Rancher] How about support for ancillary projects, such as an API client, from Rancher Labs?","text":"<p>This document (000020456) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020456/#resolution","title":"Resolution","text":"<p>Projects such as the Rancher Python Client are maintained by SUSE Rancher on a best effort basis only.\u00a0 Any Rancher user is welcome to use them. However, these projects are not covered by Rancher SLA. For any bugs or questions, users are encouraged to post their issues here:</p> <p>Project</p> <p>GitHub Location</p> <p>Rancher Client</p> <p>Python</p> <p>https://github.com/rancher/client-python/issues</p>"},{"location":"000020456/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020457/","title":"[Rancher] Does Rancher Support cover RKE (standalone CLI)?","text":"<p>This document (000020457) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020457/#resolution","title":"Resolution","text":"<p>Yes, support for RKE is implicit and covered under an active Rancher subscription to one of our support plans.\u00a0There is currently no separate product SKU that offers commercial support separately and only for RKE.</p>"},{"location":"000020457/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020458/","title":"[Rancher] We have a few customizations with our on-premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations?","text":"<p>This document (000020458) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020458/#resolution","title":"Resolution","text":"<p>Customizations are defined to include any changes to the original source code, including but not limited to changes to the User Interface, the addition or modification of adapters such as for authentication, VM or server provisioning, deploying the software (e.g., the management server) on an operating system or Docker versions that are not certified by SUSE Rancher, and altering the scripts and byte code included with the product. Customizations to this software may have unintended consequences and cause issues that are not present with the original, unmodified software. As a result, it is our\u00a0policy that any bugs, defects, or other issues that are present in areas of the product that the Customer has altered must be reproduced by the Customer on an unmodified system prior to the submission of a support case or bug report. Additionally, the Customer is required to state all customizations present on the system when submitting a support case.</p>"},{"location":"000020458/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020459/","title":"[Rancher] Is there a way we can get our custom work be included into Rancher Support?","text":"<p>This document (000020459) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020459/#resolution","title":"Resolution","text":"<p>By definition, something custom is needed because it solves a unique (or specific or snowflake) requirement for one particular team. A product is meant to solve for 1000\u2019s (and more) of teams.</p> <p>To take on support for custom work is not merely about Rancher Support services looking into tickets and issues reported in them. Besides the cost of the initial vetting and validation effort on the custom work, it has ongoing Engineering and QA impact. Rancher has to keep testing (as QA) the custom work across every next release to make sure that nothing is broken. Maintain knowledge of what the custom work is about and keep maintaining (as Engineering) that piece of code for its entire life cycle including potential improvements as needed.</p> <p>This is expensive, distracting, and misaligned with the objectives of any product team. Hence, custom work will not be considered for inclusion in product/support. And, any issues in such custom work are considered out of scope of Rancher Support.</p> <p>That said, here are scenarios and options that could be considered, if what started as custom work by one team is believed to be valuable for many teams and a customer is interested in exploring its productization (and support) in Rancher:</p> <ul> <li>Rancher is 100% open source. Users are welcome to make contributions / PRs on the public Rancher repos on GitHub. Contributions will be evaluated for inclusion in product, based on merit and alignment with the Rancher roadmap. Customers are welcome to optionally advocate for their contribution via their Rancher Customer Success Manager or Account Executive.</li> <li>To accelerate and secure commitment toward productizing a specific work or feature, Customer can explore the possibility of a Non-Recurring Engineering (NRE) engagement with Rancher on a commercial basis. In this scenario, Customer is recommended to request a conversation with Rancher Product Management, via their Rancher Customer Success Manager or Account Executive. Any NRE work shall be pursued by Rancher only if it is determined by Rancher Product Management as being viable and in alignment with the product roadmap and its strategic goals.</li> </ul> <p>Lastly, most Rancher customers that have such custom software have their own dev teams that support and maintain them across the versions of all vendor and open-source software, Rancher included, that they use for their overall solution.</p>"},{"location":"000020459/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020460/","title":"[Rancher] What language(s) is Rancher Support service offered in?","text":"<p>This document (000020460) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020460/#resolution","title":"Resolution","text":"<p>English is the official language in which support is delivered to global customers, with the exception of China, where it is in Chinese. If it is deemed helpful and necessary, SUSE Rancher Support will engage colleagues, from our\u00a0Premium Support, Consulting, and Customer Success teams, who are fluent in specific local languages for assistance on a case.</p>"},{"location":"000020460/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020461/","title":"[Rancher] Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support?","text":"<p>This document (000020461) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020461/#resolution","title":"Resolution","text":"<p>No. English and Chinese (simplified+traditional) are the only languages currently covered by Rancher Support. Translations for all the other languages are maintained by Rancher Community users.</p> <p>Please visit https://translate.rancher.com/\u00a0if you would like to help translate Rancher into other languages.</p>"},{"location":"000020461/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020462/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?","text":"<p>This document (000020462) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020462/#resolution","title":"Resolution","text":"<p>Yes. Prometheus and Grafana are the components natively used and supported by Rancher v2.2+ for monitoring and dashboards. Any issues root-caused in one of these two projects, as an included component of Rancher v2.2+, will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"000020462/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020463/","title":"[Rancher] Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x?","text":"<p>This document (000020463) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020463/#resolution","title":"Resolution","text":"<p>Correct. To be more accurate, Rancher Support is only for the Prometheus/Grafana components that are natively embedded in Rancher 2.x for the functionality of monitoring and dashboards.</p> <p>In the exceptional scenario of any legacy support subscriptions that are limited to RKE-only (without Rancher 2.x), it does not include support of Prometheus/Grafana that are set up externally to monitor the RKE clusters.</p> <p>Refer this article for more details: Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?</p>"},{"location":"000020463/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020464/","title":"[Rancher] Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?","text":"<p>This document (000020464) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020464/#resolution","title":"Resolution","text":"<p>Support for Prometheus/Grafana is only for what is embedded in Rancher 2.x and natively used by it for the functionalities of monitoring and dashboards.</p> <p>Refer Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?</p> <p>Where Prometheus/Grafana has been enabled from the chart in the \"Library\" catalog, refer Are the applications underlying the charts in the \"Library\" Catalog covered by Rancher Support?</p> <p>Prometheus/Grafana installed by all other means that did not originate from Rancher fall outside the scope of Rancher Support and are not covered by the terms of service of a Rancher Support subscription.</p>"},{"location":"000020464/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020465/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?","text":"<p>This document (000020465) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020465/#resolution","title":"Resolution","text":"<p>Yes. Jenkins is the open-source component used and supported by Rancher 2.1+ as the native build engine for running pipelines. Any issues root-caused in Jenkins will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"000020465/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020466/","title":"[Rancher] What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?","text":"<p>This document (000020466) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020466/#resolution","title":"Resolution","text":"<p>Rancher supports its certified integrations with log aggregation systems such as Elasticsearch, Splunk, Kafka, Syslog, and Fluentd. What this means is the following:</p> <ul> <li>Rancher will help Customer troubleshoot the root cause for any issue related to one of these logging services.</li> <li>For issues root-caused to be in the integration to one of these services, Rancher will provide a fix to resolve such issues.</li> <li>For issues root-caused to be inside one of these logging services, Rancher will investigate if the fix is already available in a later version of the logging service. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later version that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the logging service, Rancher will advise Customer to contact the logging service vendor directly for issue resolution.</li> </ul>"},{"location":"000020466/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020467/","title":"[Rancher] Will Rancher fix problems root-caused to be in nginx?","text":"<p>This document (000020467) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020467/#resolution","title":"Resolution","text":"<p>Rancher will do one of the following, to help Customer resolve the issue root-caused to be in nginx:</p> <ul> <li>Should the issue have been resolved in a later version of nginx, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of nginx.</li> <li>If the issue is unresolved in any acceptable nginx product versions, Rancher will advise Customer to contact nginx directly for issue resolution. Should a new version or patch be provided by nginx, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>Note:</p> <p>Inclusion of nginx as a certified component in the Rancher support matrix, is based on Rancher's own testing and validation of nginx with its default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine tune the settings for scale and performance.</p>"},{"location":"000020467/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020468/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?","text":"<p>This document (000020468) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020468/#resolution","title":"Resolution","text":"<p>No. However, Rancher will do one of the following, to help Customer resolve the issue root-caused to be in an add-on such as Weave, Cisco ACI, Cilium, and Calico:</p> <ul> <li>Should the issue have been resolved in a later version of the add-on component, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of the add-on.</li> <li>If the issue is unresolved in any acceptable versions of the add-on, Rancher will advise Customer to contact the add-on vendor directly for issue resolution. Should a new version or patch be provided by the add-on vendor, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>Note:</p> <p>Any inclusion of such add-ons as a certified component in the Rancher support matrix, is based on Rancher's own testing and validation of these add-ons with their default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine-tune the settings for scale and performance.</p>"},{"location":"000020468/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020470/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?","text":"<p>This document (000020470) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020470/#resolution","title":"Resolution","text":"<p>Yes. Any issues root-caused in one of these two projects will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"000020470/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020471/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?","text":"<p>This document (000020471) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020471/#resolution","title":"Resolution","text":"<p>Yes. Istio is the open-source component used and supported by Rancher 2.3+ for service mesh functionality. Any issues root-caused in Istio will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"000020471/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020472/","title":"[Rancher] We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?","text":"<p>This document (000020472) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020472/#resolution","title":"Resolution","text":"<p>Fluentd is the open-source component used and supported by Rancher 2.x for the native log forwarder functionality. To the extent of this functionality, any issues root-caused in Fluentd will be supported fully, like how any Rancher product issue would be.</p> <p>In the context of Fluentd as a log aggregation system, Rancher Support is limited to ensuring its certified integration with Fluentd works as intended.</p>"},{"location":"000020472/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020473/","title":"[Rancher] Does Rancher include any 3rd party, commercial software components?","text":"<p>This document (000020473) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020473/#resolution","title":"Resolution","text":"<p>No. There are no commercial software components included in Rancher. Rancher products are 100% open source and only include components that are also open source and with the right kind of open source license such as, but not limited to, Apache 2.0.</p>"},{"location":"000020473/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020474/","title":"[Rancher] Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software?","text":"<p>This document (000020474) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020474/#resolution","title":"Resolution","text":"<p>Yes. Rancher products are 100% open source and free to use for anyone. There are no hidden product features that are unlocked by signing up for a Rancher Support subscription.</p> <p>Also, see Does Rancher include any 3rd party, commercial software components?</p>"},{"location":"000020474/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020475/","title":"[Rancher] How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?","text":"<p>This document (000020475) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020475/#resolution","title":"Resolution","text":"<p>\"How are you going to do the support for production systems, particularly for financial institutions that have to comply with PCI-DSS standards and that will not allow external access to the systems?\"</p> <p>Not just for customer systems that need to comply with restrictions but the following is applicable to any customer system that is covered by Rancher Support:</p> <p>A Rancher Support Engineer will never work on an issue on any customer system directly and with unmonitored access.</p> <p>Any troubleshooting will be done via a combination of the following:</p> <ul> <li>Log collection</li> <li>Over a screen share session and with the customer on a jump box</li> </ul> <p>Should there be very high-security scenarios where troubleshooting via the above is still not possible, Rancher Support can still help troubleshoot issues but help can only be provided in a second-hand manner. That is to say, any response can only be provided to the extent of the sanitized information shared by the Customer, with Rancher Support, and in a back-and-forth request-response transaction that may not be very efficient.</p> <p>Where such scenarios are well known and access-based support is still sought, Customer is requested to inquire with their Rancher Account Executive or Customer Success Manager for other commercial models of engagement such as Premium Support\u00a0or via SUSE RGS.</p>"},{"location":"000020475/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020476/","title":"[Rancher] How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?","text":"<p>This document (000020476) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020476/#environment","title":"Environment","text":"<ul> <li>Rancher</li> <li>RKE</li> <li>RKE2</li> <li>K3s</li> <li>Harvester</li> <li>Longhorn</li> <li>NeuVector</li> </ul>"},{"location":"000020476/#resolution","title":"Resolution","text":"<p>For industry-reported vulnerabilities in Rancher, RKE, RKE2, K3s, Harvester, Longhorn, NeuVector and upstream vulnerabilities in Kubernetes, Docker, and containerd, SUSE Rancher strives to adhere to industry standards and best practices. Due to the nature of upstream dependencies inherent to open-source software, the final delivery of patch releases may vary in timeline. We will prioritize our efforts and coordinate with upstream organizations and third-party entities according to the following guidelines:</p> <ul> <li>Critical: Immediate engagement to remediate the issue in code, and/or coordinate with upstream and/or third-party entities to deliver the remediation in the shortest timeline available. This includes creating an emergency release patch version when an existing one is not readily available.</li> <li>High: Prioritized engagement to align the delivery of the remediation with our next available release cycle. Emergency releases should only be needed unless the timing is such that the next available security release cycle is not in a reasonable timeline.</li> </ul>"},{"location":"000020476/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020477/","title":"[Rancher] We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA?","text":"<p>This document (000020477) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020477/#resolution","title":"Resolution","text":"<p>Rancher Support SLA is based on our QA validation and certification on default OS kernels for operating systems listed in the product support matrices.</p> <p>Antivirus software adds an unknown variable to the existing complexity of Kubernetes. Most of them have not yet kept up with newer technologies such as Kubernetes and have not reached a CNCF certified status. In environments where antivirus software had been enabled, Rancher Support has seen issues stemming from interfering actions from such software. As an example, there have been incidents where the antivirus software had pruned files in the Docker filesystem incorrectly, causing the Docker mounts to go corrupt.</p> <p>Issues resulting from third-party tools, such as antivirus and intrusion detection software, interfering with Docker or other necessary system calls are deemed resolved should disabling such tools restore functionality.</p> <p>Lastly, all certified configurations, as published in the product support matrices, are based on the default settings of individual components. Where a customer environment has deviated from certified configurations, Rancher Labs reserves the right to recommend the customer to revert to a certified configuration to resolve the reported issue.</p>"},{"location":"000020477/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020478/","title":"[Rancher] We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then?","text":"<p>This document (000020478) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020478/#resolution","title":"Resolution","text":"<p>For open source components not listed in the Rancher support matrix page, support is limited to troubleshooting for root cause up to Rancher\u2019s drivers and interfaces to those components.</p> <p>Root causes that are identified to be beyond this limit will need to be pursued by Company with the maintainers and providers of commercial support for those components.</p> <p>For ensuring best support and clarity on supportability, Company is recommended to publish to Rancher a list of components that are critical to its deployment but not explicitly called out in the support matrix.</p>"},{"location":"000020478/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020479/","title":"[Rancher] I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then?","text":"<p>This document (000020479) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020479/#resolution","title":"Resolution","text":"<p>Certified configurations in the Rancher support matrix page\u00a0are based on the default settings of individual components.</p> <p>Where Customer has deviated from certified configurations, Rancher Support reserves the right to recommend the Company to revert to a certified configuration to resolve the reported issue.</p>"},{"location":"000020479/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020480/","title":"[Rancher] What are the certified integrations with persistent volume plugins covered by Rancher Support?","text":"<p>This document (000020480) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020480/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Amazon EBS Disk Azure Disk</li> <li>Azure Filesystem</li> <li>Google Persistent Disk</li> <li>Longhorn</li> <li>Local Node Disk</li> <li>Local Node Path</li> <li>NFS Share</li> <li>VMware vSphere Volume</li> </ul> <p>Note:</p> <p>For any other persistent volume plugins from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.</p>"},{"location":"000020480/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020481/","title":"[Rancher] What are the certified integrations with storage class provisioners covered by Rancher Support?","text":"<p>This document (000020481) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020481/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Amazon EBS Disk</li> <li>Azure Disk</li> <li>Azure File</li> <li>Google Persistent Disk</li> <li>VMware vSphere Volume</li> <li>Longhorn</li> <li>Local</li> </ul> <p>Note:</p> <p>For any other storage class provisioners from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.</p>"},{"location":"000020481/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020482/","title":"[Rancher] What are the certified integrations with authentication providers covered by Rancher Support?","text":"<p>This document (000020482) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020482/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Active Directory</li> <li>Azure AD</li> <li>GitHub</li> <li>Google (supported only from Rancher v2.3 or higher)</li> <li>PingIdentity (SAML)</li> <li>Keycloak (SAML)</li> <li>AD FS (SAML)</li> <li>Okta (SAML)</li> <li>FreeIPA (LDAP)</li> <li>OpenLDAP (LDAP)</li> </ul>"},{"location":"000020482/#rancher-support-does-not-cover","title":"Rancher Support does not cover:","text":"<ul> <li>Switching between these external authentication providers.</li> <li>Migration from authentication scheme of one provider to another that would typically require maintaining existing user settings/preferences, access levels and privileges, user and group authorizations.</li> </ul>"},{"location":"000020482/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020483/","title":"[Rancher] Could you clarify what you generally mean by a \"certified integration\" to another software system or service?","text":"<p>This document (000020483) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020483/#resolution","title":"Resolution","text":"<p>Rancher supports its certified integrations with various software systems and services for functionality ranging from User Authentication to Infrastructure. What this means is the following:</p> <ul> <li>Rancher has validated the integration for one or more of its product versions to work as designed with such systems and services.</li> <li>Rancher Support will help Customer troubleshoot the root cause for any issue related to one of these integrations.</li> <li>For issues root-caused to be in the integration component to one of these systems or services, Rancher will provide a fix to resolve such issues.</li> <li>For issues root-caused to be inside one of these systems or services, Rancher will investigate if the fix is already available in a later version of the system or service. If it is, Rancher will provide a newer version of its own product that is validated and certified to work with the later version of the system that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the system or service, Rancher will advise Customer to directly contact the vendor providing support for the system or service for issue resolution.</li> </ul>"},{"location":"000020483/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020484/","title":"[Rancher] What does Rancher support for Docker on Ubuntu cover?","text":"<p>This document (000020484) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020484/#situation","title":"Situation","text":"<p>Note:</p> <p>Only upstream Docker has been validated and certified for Rancher Support. Ubuntu's own distribution of Docker is not covered by the Rancher support matrix.</p> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Ubuntu, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Ubuntu and Rancher that has been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> <li>If a code fix is not possible, Rancher will advise Customer to contact Canonical directly for issue resolution. Should a new version or patch be provided by Canonical, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul>"},{"location":"000020484/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020485/","title":"[Rancher] Will Rancher support us should our deployment be on Red Hat Atomic?","text":"<p>This document (000020485) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020485/#resolution","title":"Resolution","text":"<p>Rancher has not validated and certified Red Hat Atomic for inclusion in its Support Matrix as one of the supported operating systems. Following conditions shall apply to Customers procuring Rancher Support for running a Rancher deployment on Red Hat Atomic:</p> <ul> <li>Rancher Support is predicated on Customer running a Red Hat distribution of Docker that has been validated and certified for Rancher Support on a comparable, certified version of RHEL OS.</li> <li>Rancher Support will rely on Customer to reproduce the issue on a comparable, certified version of RHEL OS running a fully supported configuration.</li> <li>In the event of Customer encountering an issue whilst using Red Hat Atomic, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the Red Hat Atomic OS will need to be taken up by the Customer with Red Hat. Troubleshooting by Rancher Support shall be limited to running scripts and commands, log analysis, and screen share sessions with the Customer. It shall not extend to a setting up a Rancher deployment on Red Hat Atomic by Rancher Support to reproduce the issue.</li> </ul>"},{"location":"000020485/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020486/","title":"[Rancher] What does Rancher support for Docker on RHEL cover?","text":"<p>This document (000020486) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020486/#resolution","title":"Resolution","text":"<p>Note:</p> <p>Both, RHEL's own distribution of Docker and upstream Docker (Docker CE) on RHEL, have been validated and certified for Rancher Support.</p> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis).</p> <p>Where the root cause has been identified as an issue with RHEL's own distribution of Docker, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of RHEL's own distribution of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version RHEL's own distribution of Docker.</li> <li>If no fix is available yet for the issue, Rancher will advise the Customer to contact Red Hat directly for issue resolution. Should a new version or patch be provided by Red Hat, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>Where the root cause has been identified as an issue with upstream Docker (or Docker CE) on RHEL, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> </ul>"},{"location":"000020486/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020487/","title":"[Rancher] What does Rancher support for Docker on CentOS cover?","text":"<p>This document (000020487) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020487/#resolution","title":"Resolution","text":"<p>Note:</p> <p>Only upstream Docker has been validated and certified for Rancher Support.</p> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on CentOS, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of CentOS and Rancher that has been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> </ul>"},{"location":"000020487/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020488/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL?","text":"<p>This document (000020488) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020488/#resolution","title":"Resolution","text":"<p>No. In this case, Customer needs to contact Red Hat.</p>"},{"location":"000020488/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020489/","title":"[Rancher] What does Rancher support for Docker on Windows Server cover?","text":"<p>This document (000020489) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020489/#resolution","title":"Resolution","text":"<p>Note:</p> <ul> <li>Only Docker Enterprise (Docker EE) has been validated and certified for Rancher Support.</li> <li>Support for Windows Server is available only for Rancher v2.3 and higher.</li> <li>Windows clusters are supported for Kubernetes 1.15+ on Windows Server, versions 1809 and 1903. Windows clusters can only be created from new clusters and are supported only with the flannel network provider. You will not need to do any specific scheduling to ensure your Windows workloads are scheduled onto Windows nodes. When creating a Windows cluster, Rancher automatically adds taints to the required Linux nodes to prevent any Windows workloads to be scheduled. If you are trying to schedule Linux workloads into the cluster, you will need to add specific tolerations and node scheduling in order to have them deployed on the Linux nodes.</li> </ul> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker Enterprise on Windows Server, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker Enterprise, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Windows Server and Rancher that have been certified for this later version of Docker Enterprise.</li> <li>If no fix is available yet for the issue, Rancher will advise the Customer to contact Microsoft directly for issue resolution. Should a new version or patch be provided by Microsoft Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul>"},{"location":"000020489/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020490/","title":"[Rancher] What does Rancher support for Docker on Oracle Linux cover?","text":"<p>This document (000020490) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020490/#resolution","title":"Resolution","text":"<p>Note:</p> <ul> <li>Only upstream Docker has been validated and certified for Rancher Support.</li> <li>Support for Oracle Linux is available only from Rancher v2.3.2.</li> <li>Some restrictive firewall rules will need to be turned off for a supported baseline configuration of Oracle Linux. Refer this docs page for more details.</li> </ul> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Oracle Linux, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Oracle Linux and Rancher that has been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> </ul>"},{"location":"000020490/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020491/","title":"[Rancher] My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack?","text":"<p>This document (000020491) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020491/#resolution","title":"Resolution","text":"<p>OpenStack is not listed under supported node drivers in the Rancher support matrix. What this means is that currently the OpenStack node driver is not officially supported and because of this, there isn't a supported way to use Rancher as the cluster self-provisioning platform on an OpenStack-based infrastructure.</p> <p>This is not to say that clusters cannot be run in a Rancher-supported way on OpenStack-based infrastructure. To do that, the option of custom clusters will need to be used.</p> <p>In the event of a Customer encountering an issue in such custom clusters, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the OpenStack platform will need to be taken up by the Customer with the maintainer and provider of support for the OpenStack platform.</p> <p>Also, troubleshooting by Rancher Support is currently limited to log analysis and screen share sessions with the Customer and does not extend to an OpenStack setup by Rancher Support to recreate the issue.</p>"},{"location":"000020491/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020492/","title":"[Rancher] Does it matter what hardware my hosts are on? Are virtualized servers supported?","text":"<p>This document (000020492) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020492/#resolution","title":"Resolution","text":"<p>Rancher Support for Rancher 2.x covers running Rancher and Kubernetes clusters on supported OSes on a 64-bit x86 architecture host.</p> <p>Refer the support matrix\u00a0page\u00a0for supported OS by product version.</p> <p>Refer this\u00a0docs page for installation requirements and this\u00a0docs page for node requirements for user clusters.</p> <p>These host nodes could be bare-metal or a virtual server running on any Type-1 hypervisor or a cloud server on AWS, Azure, Digital Ocean, Google, and Linode.</p> <p>With the exception of KVM, hosts running on Type-2 hypervisors, such as VirtualBox, VMware Fusion or Parallels, are not in scope of Rancher Support. For use cases, where a Rancher Customer is SLA-bound to their downstream users, Rancher Support does not recommend running clusters and workloads on these hosts. Any assistance provided by Rancher Support in this scenario is not bound by the Rancher Support SLA. It shall be limited to being on a best effort basis only and not include troubleshooting issues related to the setup and configuration of the virtual infrastructure.</p> <p>Refer this\u00a0page on wikipedia for what is a Type-1 and Type-2 hypervisor. Hosts on an ARM64 architecture are not covered by Rancher Support SLA.</p>"},{"location":"000020492/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020493/","title":"[Rancher] I see node drivers tagged as \"Built-in\". What does that mean?","text":"<p>This document (000020493) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020493/#resolution","title":"Resolution","text":"<p>Built-in drivers are those that are included in a Rancher product distribution.</p>"},{"location":"000020493/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020494/","title":"[Rancher] I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?","text":"<p>This document (000020494) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020494/#resolution","title":"Resolution","text":"<p>Active drivers are those drivers that have been turned on. Only four Built-in node drivers are surfaced up in UI, as Active, in a default Rancher installation. It is only these five Built-in, Active node drivers that are validated and certified in any Rancher 2.x product version and consequently, published in the Rancher support matrix. These five Built-in, Active node drivers that are covered by Rancher Support are for:</p> <ul> <li>Digital Ocean</li> <li>AWS</li> <li>Azure</li> <li>vSphere - 6.5, 6.7, 7.0 update 2a</li> <li>Linode (supported only from Rancher v2.3 or higher)</li> </ul>"},{"location":"000020494/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020495/","title":"[Rancher] Does my support subscription to Rancher include support for Longhorn?","text":"<p>This document (000020495) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020495/#resolution","title":"Resolution","text":"<p>No, a subscription to Rancher does not include support for Longhorn. With the general availability of Longhorn in June 2020, a separate commercial subscription to an add-on plan is needed to receive SLA-based support for Longhorn.</p>"},{"location":"000020495/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020496/","title":"[Rancher] Can we have a mix of unsupported and supported nodes at our choice/discretion?","text":"<p>This document (000020496) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020496/#resolution","title":"Resolution","text":"<p>No. Within a supported Rancher environment, categorizing cluster nodes as \"supported'\" and \"unsupported\" at a team's own choice/discretion is not allowed.</p> <p>Also, see Can we request support for the management/upstream cluster and certain downstream clusters and not others?</p>"},{"location":"000020496/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020497/","title":"[Rancher] Can we request support for the management/upstream cluster and certain downstream clusters and not others?","text":"<p>This document (000020497) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020497/#resolution","title":"Resolution","text":"<p>No. Mixing the management of \"supported'\" and \"unsupported\" clusters from a single Rancher server instance in this manner is not a supported configuration, so we would suggest that you run a separate Rancher server instance to manage clusters you do not want to be covered by Rancher Support. The other option you have here would be to take advantage of our blended Platinum and Standard plan, to cover less critical clusters with the Standard plan, and you can reach out to your Account Executive if you wish to look at the options with this.</p>"},{"location":"000020497/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020498/","title":"[Rancher] How does Rancher track our license usage?","text":"<p>This document (000020498) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020498/#resolution","title":"Resolution","text":"<p>Currently, Rancher relies on customers to report their usage and procure additional licenses if their usage has increased. From time to time, as part of support calls, a Rancher Support Engineer may request customers to run a simple command-line script on the Rancher Server that will generate high-level usage information:</p> <pre><code>wget -O - https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sh - &gt; rancher_stats.txt\n</code></pre> <p>This is both to keep track of our customer's usage for license compliance as well as to offer advisories should customers be approaching any thresholds related to scale and performance.</p>"},{"location":"000020498/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020499/","title":"[Rancher] Is Rancher Support only for production environments?","text":"<p>This document (000020499) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020499/#resolution","title":"Resolution","text":"<p>No. Rancher Support can be procured for any Rancher installation/environment for which the customer wishes to get SLA-based assistance. In addition to production environments, it is very common for orgs to procure support subscriptions to cover other environments, such as their Dev/Test, Staging, Demo, Perf, PreProd, and Production Sandbox, for which they have their own SLAs to meet with their downstream customer users.</p>"},{"location":"000020499/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020500/","title":"[Rancher] I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well?","text":"<p>This document (000020500) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020500/#resolution","title":"Resolution","text":"<p>No. In examples such as this one, the customer is required to procure license(s) for any additional Rancher Management Server installations for which support has been sought. In this example, whilst Rancher Support will help customers only on a best-effort basis, for their issues in Dev/Test and Staging, they will recommend a conversation for the customer with their Rancher Account Executive for procuring additional licenses at the earliest.</p>"},{"location":"000020500/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020501/","title":"[Rancher] Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue?","text":"<p>This document (000020501) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020501/#resolution","title":"Resolution","text":"<p>We understand that this can happen, as you have your own customer adoption and growth. Yes, for sure, Rancher Support will help you on the specific issue. However, to avoid any future disruption, it will be required that the Rancher Support subscription is upgraded to the necessary quantity, at the earliest and within a reasonable period of time.</p>"},{"location":"000020501/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020502/","title":"[Rancher] Could you illustrate the severity levels with subject lines of sample support cases?","text":"<p>This document (000020502) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020502/#resolution","title":"Resolution","text":"<p>Here are\u00a0some sample subject lines by severity level:</p> <p>Severity Level</p> <p>Subject line of sample support cases</p> <p>Severity 1</p> <ol> <li>Production cluster is down</li> <li>Apps in production cluster throwing '502 Bad Gateway' Error</li> <li>Enabled enhanced cluster monitoring and the cluster sporadically goes offline</li> <li>etcd not healthy in production rancher server</li> <li>Kubernetes ingress endpoint went unresponsive</li> </ol> <p>Severity 2</p> <ol> <li>etcd restore failed for RKE-deployed cluster</li> <li>Microservices client encounters UnknownHostException in Production</li> <li>Network traffic routed over IPsec is super slow</li> <li>LDAP is intermittently failing to log in users in one of our environments</li> <li>DNS intermittent timeouts</li> </ol> <p>Severity 3</p> <ol> <li>Webhook notifier not working as expected</li> <li>Need assistance with creating custom global roles to prevent cluster creation</li> <li>Problem adding new nodes to clusters</li> <li>Ingress timeout issue in Rancher-deployed cluster</li> <li>Bug: Unable to add AD groups to Rancher ACLs</li> </ol> <p>Severity 4</p> <ol> <li>Ability to change UID and GID in docker containers on container creation.</li> <li>Feature Request: Show audit logs content in Rancher UI</li> <li>FYI: Scheduled upgrade XX/XX/XXXX XX:XXPM, request on-call assistance\u00a0as needed</li> <li>How can I disable the creation of the Prometheus-Operator CRDs on cluster creation</li> <li>Understand the impact of changing IPs of worker nodes in our k8s cluster</li> </ol>"},{"location":"000020502/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"000020502/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020526/","title":"Security vulnerability: log4j remote code execution aka log4shell CVE-2021-44228","text":"<p>This document (000020526) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020526/#environment","title":"Environment","text":"<p>All products</p>"},{"location":"000020526/#situation","title":"Situation","text":"<p>A 0-day exploit in the log4j Java logging framework was found by Chen Zhaojun of Alibaba Cloud Security Team, which allowed remote attackers able to inject strings into log4j based Java logging to execute code by</p> <p>exploiting the default enabled JNDI bindings. This is possible without any preconditions, making it critical.</p>"},{"location":"000020526/#resolution","title":"Resolution","text":"<p>SUSE considers log4j versions 2.0 and newer as affected, log4j 1.2.x does not have the same critical vulnerability and is not considered affected by this CVE.</p> <p>SUSE Linux Enterprise products do not ship log4j 2.x.</p> <p>SUSE Manager does not ship log4j 2.x.</p> <p>SUSE Enterprise Storage does not ship log4j 2.x.</p> <p>SUSE Openstack Cloud embeds log4j2 in the \"storm\" component, which will receive updates.</p> <p>SUSE NeuVector product does not ship log4j 2.x.</p> <p>SUSE Rancher is not affected by this vulnerability. The Helm chart for Istio 1.5, provided by Rancher and which is currently deprecated, includes Zipkin and is vulnerable to Log4j. Customers are advised to upgrade to the recent Istio version provided in Cluster Explorer, which does not uses Zipkin and is not affect to the vulnerability.</p> <p>Please refer to the upstream guidance from log4j on fixing and mitigation measures if you deploy your Java Application stacks.</p>"},{"location":"000020526/#status","title":"Status","text":"<p>Security Alert</p>"},{"location":"000020526/#additional-information","title":"Additional Information","text":"<p>Additional information can be found here:</p> <ul> <li>https://suse.com/security/cve/CVE-2021-44228.html</li> <li>https://www.suse.com/c/suse-statement-on-log4j-log4shell-cve-2021-44228-vulnerability/</li> <li>https://logging.apache.org/log4j/2.x/security.html</li> </ul> <p>Note in regards to SUSE Manager Server:</p> <p>The CVE-search will use meta-data within a patch to display the needed information. As there is no patch needed (as SUSE is not effected), the CVE-search for CVE-2021-44228 will return a \"not found\".</p>"},{"location":"000020526/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020535/","title":"Security vulnerability: Trojan Source, invisible source code vulnerabilities. (CVE-2021-42574)","text":"<p>This document (000020535) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020535/#environment","title":"Environment","text":"<p>All products</p>"},{"location":"000020535/#situation","title":"Situation","text":"<p>CVE-2021-42574 ('Trojan Source') refers to vulnerabilities that can come about through the use of bi-directional unicode text in contexts where it is not properly displayed. \u00a0Various source-code viewers and editors currently do not show content which is \"visually hidden by unicode\". \u00a0These may include editors and pagers such as vi, emacs and less as well as the web interfaces of tools that display source code.</p> <p>The failure to display such things as bidirectional control characters can lead to a situation in which source code when compiled or interpreted behaves in ways that someone seeing the displayed text would not expect.</p> <p>This is not a compiler issue, but future compiler versions will also have options or features to display warnings in cases where such special unicode characters are used.</p>"},{"location":"000020535/#resolution","title":"Resolution","text":"<p>Even where this does not affect SUSE products directly, SUSE is currently taking action to harden the supply chain for SUSE products in order to detect any such unicode sequences in code that could have harmful effects.</p>"},{"location":"000020535/#cause","title":"Cause","text":"<p>Unicode supports both left-to-right and right-to-left languages, and it makes use of invisible codepoints called \"bidirectional override\"\u00a0to aid writing left-to-right words inside a right-to-left sentence. It is common to find these inside a sentence of another language to embed a word with a different text direction. \u00a0Researchers discovered that these codepoints could be misused to manipulate how source code is displayed in some editors and code review tools, fooling a reviewer into approving code that behaves in unexpected ways (potentially maliciously).</p>"},{"location":"000020535/#status","title":"Status","text":"<p>Security Alert</p>"},{"location":"000020535/#additional-information","title":"Additional Information","text":"<p>Additional information can be found at:</p> <p>-\u00a0https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-42574</p>"},{"location":"000020535/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020536/","title":"[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20","text":"<p>This document (000020536) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020536/#resolution","title":"Resolution","text":"<p>Note:</p> <p>This\u00a0Rancher Labs operational advisory below was originally sent in December 2020.\u00a0 It has been published here to continue SUSE Rancher customer conversations around this topic via support cases and for sharing any relevant updates around it.</p> <p>For an update on this topic, please see\u00a0[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24.</p> <p>Dear Rancher Customer,</p> <p>This is an operational advisory from Rancher Support related to the deprecation of dockershim in Kubernetes v1.20</p> <p>As announced on the official Kubernetes blog, the dockershim, which enables the use of the Docker Daemon as the container runtime in Kubernetes, will be deprecated with the upcoming Kubernetes v1.20 release.</p>"},{"location":"000020536/#what-is-dockershim","title":"What is dockershim?","text":"<p>The dockershim is built into Kubernetes to provide a Container Runtime Interface (CRI) compliant layer between the kubelet and the Docker Daemon. The shim is necessary because Docker Daemon is not CRI-compliant.</p>"},{"location":"000020536/#what-does-deprecation-of-dockershim-in-kubernetes-v120-mean","title":"What does deprecation of dockershim in Kubernetes v1.20 mean?","text":"<p>The dockershim will only be deprecated in Kubernetes v1.20, and will not yet be removed from the kubelet. As a result, no immediate action needs to be taken and Kubernetes clusters can continue to operate with the Docker Daemon container runtime in Kubernetes v1.20. The only change at this time will be a deprecation warning printed in the kubelet logs when running on Docker.</p>"},{"location":"000020536/#what-are-ranchers-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","title":"What are Rancher's plans to ensure on-going container runtime support in future Kubernetes releases?","text":"<p>We are working on our roadmap to ensure that all Rancher provisioned clusters will continue to operate on a CRI-compliant runtime.</p> <p>For existing RKE customers, users will continue to get Kubernetes updates until the shim is officially removed. The removal is currently targeted for late 2021 and will be supported with patches during the 12-month upstream maintenance window. Before the end of maintenance, we fully expect an upgrade path from RKE to RKE2.</p> <p>Looking forward, containerd is already the default runtime in both K3s and RKE2, so any removal of dockershim will have zero impact on future releases. As with RKE, organizations currently using K3s with the Docker runtime will continue to get Kubernetes updates until the shim is officially removed. The dockershim deprecation schedule is tracked by the upstream Kubernetes community in Kubernetes Enhancement Proposal (KEP) 1985,\u00a0Rancher will continue to keep you updated with related news on the roadmap from our product management, as well as information related to migration off of Docker.</p> <p>Thanks,</p> <p>Rancher Support Team</p>"},{"location":"000020536/#additional-information","title":"Additional Information","text":"<ul> <li>[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"000020536/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020538/","title":"[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24","text":"<p>This document (000020538) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020538/#resolution","title":"Resolution","text":"<p>Note:</p> <p>This is a follow-up to the Rancher Operational Advisory that was sent on this topic in December 2020.</p> <p>In the latest announcement\u00a0from the Kubernetes blog, it has been notified that dockershim removal has been planned in Kubernetes v1.24,\u00a0slated for release around April 2022.</p>"},{"location":"000020538/#what-are-suses-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","title":"What are SUSE's plans to ensure on-going container runtime support in future Kubernetes releases?","text":"<p>Starting with Kubernetes v1.21, RKE added support for CRI plugin cri-dockerd, see here\u00a0instructions to enable. All RKE clusters will need to leverage this CRI plugin before upgrading to Kubernetes v1.24. Future updates beyond Kubernetes v1.24 RKE will rely on the cri-dockerd shim.</p> <p>For more information on the dockershim removal schedule, you can check the upstream Kubernetes Enhancement Proposal (KEP) 2221.</p> <p>K3s and RKE2 are not impacted by the removal of dockershim and use the CRI plugin containerd. We expect to deliver a migration path from RKE to RKE2 in the future.</p> <p>SUSE will continue to keep you updated with related news on the roadmap from our product management</p>"},{"location":"000020538/#additional-information","title":"Additional Information","text":"<ul> <li>[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"000020538/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020543/","title":"[Rancher] Support Advisories","text":"<p>This document (000020543) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020543/#resolution","title":"Resolution","text":"<ul> <li>[Rancher] Operational Advisory, 20220405:\u00a0Rancher Kubernetes Distributions and Etcd 3.5 Updates</li> <li>[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5</li> <li>[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24</li> <li>[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20</li> <li>[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits</li> </ul>"},{"location":"000020543/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"000020543/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020556/","title":"SUSE Rancher Hosted FAQ","text":"<p>This document (000020556) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020556/#resolution","title":"Resolution","text":""},{"location":"000020556/#general","title":"General","text":"<ul> <li>What is SUSE Rancher Hosted?</li> <li>What do I need to provide to get started on SUSE Rancher Hosted?</li> <li>Do you have a whitepaper available for SUSE Rancher Hosted?</li> <li>Can I move from self-managed Rancher to SUSE Rancher Hosted?</li> <li>What if I no longer want SUSE Rancher Hosted to manage my downstream clusters?</li> <li>Is it possible to have alpha, beta, or release candidate (RC) versions on SUSE Rancher Hosted?</li> <li>Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers?</li> <li>Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage?</li> <li>Do I have access to the SUSE Rancher Hosted \"local\" cluster in the management UI?</li> <li>Where is SUSE Rancher Hosted hosted?</li> <li>Can I have more than one SUSE Rancher Hosted environment?</li> <li>What type of cluster is SUSE Rancher Hosted running on?</li> <li>Does SUSE Rancher Hosted provide downstream clusters?</li> <li>Can I move an existing cluster to SUSE Rancher Hosted?</li> <li>How is SUSE Rancher Hosted different than the open-source Rancher I can download for free?</li> </ul>"},{"location":"000020556/#maintenance-operations","title":"Maintenance &amp; Operations","text":"<ul> <li>Who creates user accounts in SUSE Rancher Hosted?</li> <li>How is my SUSE Rancher Hosted environment monitored?</li> <li>How often is maintenance performed on SUSE Rancher Hosted?</li> <li>Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted?</li> <li>Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters?</li> <li>Does SUSE Rancher Hosted offer a support SLA?</li> <li>How long are SUSE Rancher Hosted backups retained?</li> </ul>"},{"location":"000020556/#upgrades-uptime","title":"Upgrades &amp; Uptime","text":"<ul> <li>What can be expected during a SUSE Rancher Hosted upgrade?</li> <li>How often is SUSE Rancher Hosted upgraded?</li> <li>How is uptime measured for my SUSE Rancher Hosted?</li> <li>Does SUSE Rancher Hosted offer an uptime SLA?</li> </ul>"},{"location":"000020556/#network-security-logging","title":"Network, Security, &amp; Logging","text":"<ul> <li>What are the networking requirements for using SUSE Rancher Hosted?</li> <li>Can I use my own SSL/TLS certificates with SUSE Rancher Hosted?</li> <li>How to connect your SUSE Rancher Hosted network to your AWS transit gateway?</li> <li>How to make a VPN connection to your SUSE Rancher Hosted network?</li> <li>Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP-based directory service?</li> <li>Does SUSE Rancher Hosted support multi-factor authentication (MFA)?</li> <li>Are API audit logs enabled in SUSE Rancher Hosted?</li> <li>Can I get a copy of the Rancher API audit logs?</li> <li>What information is stored in SUSE Rancher Hosted and where is it stored?</li> <li>Do SUSE employees have a login account for my SUSE Rancher Hosted environment?</li> <li>Do SUSE employees have the credentials to my \u201cadmin\u201d account?</li> <li>Is SUSE Rancher Hosted data encrypted at rest?</li> </ul>"},{"location":"000020556/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020557/","title":"Do you have a whitepaper available for SUSE Rancher Hosted?","text":"<p>This document (000020557) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020557/#resolution","title":"Resolution","text":"<p>Yes, there is a SUSE Rancher Hosted architecture whitepaper that can be downloaded on SUSE's website. It can be found here - https://more.suse.com/fy21-global-web-landing-page-hosted-rancher-technical-guide</p>"},{"location":"000020557/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020558/","title":"What can be expected during a SUSE Rancher Hosted upgrade?","text":"<p>This document (000020558) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020558/#resolution","title":"Resolution","text":"<p>There is minimal impact or disruption during a SUSE Rancher Hosted upgrade. During the one-hour maintenance window you can expect a brief (usually 1-2 minutes) when your Rancher control plane UI/API is inaccessible. This does not impact the workloads on your managed downstream clusters, only your ability to make changes to these clusters. Immediately following the Rancher control plane upgrade, the cluster and node agents running in your managed downstream clusters will be upgraded and restarted. This also only takes a few minutes (unless you have very large clusters or very poor network speeds) and during this time Rancher will be unable to manage the cluster, but again the workloads running on the clusters should operate normally.</p>"},{"location":"000020558/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020559/","title":"What if I no longer want SUSE Rancher Hosted to manage my downstream clusters?","text":"<p>This document (000020559) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020559/#resolution","title":"Resolution","text":"<p>If you decide you no longer want to continue using the SUSE Rancher Hosted service, we can provide a one-time backup of your Rancher control plane which you can use to restore into a self-managed Rancher management server. Information on the restore process can be found in our documentation. To request a backup file, you can submit a support case on our support portal. After restoring Rancher, you will need to reconfigure your downstream clusters to point to the new server URL of your self-managed Rancher management server.</p>"},{"location":"000020559/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020561/","title":"How to connect your SUSE Rancher Hosted network to your AWS transit gateway?","text":"<p>This document (000020561) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020561/#resolution","title":"Resolution","text":"<p>The following steps can be taken to connect your SUSE Rancher Hosted network to an AWS transit gateway running in your AWS account.</p> <ol> <li>Make sure you have provided the SUSE Rancher Hosted team with a CIDR that does not overlap with your existing infrastructure. If not, your SUSE Rancher Hosted environment may need to be redeployed with the new CIDR. The CIDR must be a /25 block or larger. Using a /24 is normally preferred.</li> <li>If you haven't already, create a transit gateway in your AWS account. See Create a transit gateway.</li> <li>In the AWS console, go to Resource Access Manager (RAM) service.</li> <li>In RAM, click the orange button in the top right corner labeled \"Create a resource share\".</li> <li>For the name, use something descriptive that includes both your company name and \"SUSE Rancher Hosted\". For example, \"Widget Corp transit gateway for SUSE Rancher Hosted\". For resource type, select Transit Gateways. Select the transit gateway you want to share. In Principals, check Allow external accounts and enter the AWS account number provided by the SUSE Rancher Hosted team. Click the orange \"Create resource share\" in the bottom right corner.</li> <li>Let the SUSE Rancher Hosted team know you have created the share. We will accept the share and make a request to attach the transit gateway to your SUSE Rancher Hosted VPC.</li> <li>Accept the request to attach your transit gateway to the SUSE Rancher Hosted VPC. To do this, go to the VPC service, click \"Transit Gateway Attachments\" in the navigation pane, select the transit gateway attachment, choose Actions -&gt; Accept.</li> <li>Provide the SUSE Rancher Hosted team with a list of CIDRs you want to be routed through the transit gateway.</li> </ol> <p>See also Transit gateways and Transit gateway sharing considerations for more information.</p>"},{"location":"000020561/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020562/","title":"How to make a VPN connection to your SUSE Rancher Hosted network?","text":"<p>This document (000020562) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020562/#resolution","title":"Resolution","text":"<p>It's normally preferred to connect SUSE Rancher Hosted with your network using VPC peering through an AWS transit gateway. This is the most cost-effective, secure, and manageable solution. However, if this is not an option, a VPN connection can be established between your corporate network and SUSE Rancher Hosted through an IPSec VPN tunnel. The following steps are required to set this up:</p> <ol> <li>Provide the SUSE Rancher Hosted team with the following information about your VPN device:</li> <li>Public IP address for your VPN endpoint</li> <li>Routing option: a) static (no BGP support) or b) dynamic (BGP support)</li> <li>BGP ASN (only if dynamic routing)</li> <li>VPN device make and model used on-premise that we'll be connecting to.</li> <li>The SUSE Rancher Hosted team will configure the VPN connection and provide configuration information based on the VPN device</li> <li>Customer will configure their VPN device to connect to SUSE Rancher Hosted's network.</li> </ol> <p>See also AWS Site-to-Site VPN User Guide.</p>"},{"location":"000020562/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020563/","title":"Can I get a copy of the Rancher API audit logs?","text":"<p>This document (000020563) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020563/#resolution","title":"Resolution","text":"<p>Yes, if you provide a log flow configuration to your logging solution, such as AWS CloudWatch, Elasticsearch, Splunk, etc. we can stream Rancher API audit logs to you.</p>"},{"location":"000020563/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020592/","title":"Rancher on Windows RKE 1 to RKE 2","text":"<p>This document (000020592) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020592/#environment","title":"Environment","text":"<p>Any customer running Rancher on Windows on RKE1 is impacted by this change. Rancher has been aware of a shifting trend in the cloud native ecosystem to move toward container runtimes with a smaller surface area than Docker</p>"},{"location":"000020592/#situation","title":"Situation","text":"<p>Rancher foresaw this trend and has built two Kubernetes distributions on the open source containerd container runtime, K3s and RKE 2.\u00a0 RKE 2 is built on K3s and is fully conformant Kubernetes distribution that focuses on security and compliance.</p> <p>The future of Windows containers on Rancher is only found on RKE 2</p>"},{"location":"000020592/#resolution","title":"Resolution","text":"<p>Customers currently running on RKE 1 will need to move to RKE 2</p> <p>Because of the change in approach to provisioning Windows-specific clusters in RKE 1 to free-form mixed-OS clusters RKE 2, there is no direct migration path for Windows containers on RKE 1 to RKE 2.</p> <p>Rancher Labs recommends as part of customers testing workloads on the Windows containers on RKE 2 technical preview that they begin planning to refactor their Windows workloads on RKE2 using Fleet. Fleet is a GitOps solution from Rancher, now integrated directly into Rancher, with support for Windows containers.</p> <p>Windows containers on RKE 2 remains in technical preview as of Rancher 2.6.3, the current release of Rancher. Rancher 2.6.4 will be released in March bringing the Windows container experience on RKE 2 to general availability (GA). Windows containers on RKE 2 will match and then exceed the Windows containers on RKE 1 feature set, reaching even greater feature parity between Windows and Linux containers on Rancher. The Windows on Rancher team develops in the open, with full transparency into their development processes. Rancher Labs anticipates Windows containers on RKE 2 reaching general availability (GA) with official support alongside the Rancher Cluster Provisioning v2 in early March 2022.</p>"},{"location":"000020592/#additional-information","title":"Additional Information","text":"<p>Additional guidance and a guide for transitioning the most common workloads from RKE 1 to RKE 2 will be forthcoming. Customers seeking additional assistance in migrating between RKE 1 and RKE 2 should consult with SUSE Global Services. Customers with additional questions or concerns regarding this transition should contact the Windows on Rancher team:</p> <ul> <li> <p>By filing issues in the Windows on Rancher GitHub</p> </li> <li> <p>By joining the Rancher Users Slack and posting in #windows</p> </li> <li> <p>Reaching out to their Customer Success Manager</p> </li> </ul> <p>See our blog post here https://community.suse.com/posts/the-future-of-windows-containers-on-rancher?agree=true</p>"},{"location":"000020592/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020630/","title":"[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits","text":"<p>This document (000020630) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020630/#resolution","title":"Resolution","text":"<p>Note:</p> <p>Below is the Rancher Customer email advisory sent in Nov 2020 on the topic of Docker Hub rate limits.</p> <p>Dear Rancher Customer,</p> <p>As\u00a0announced by Docker Inc, the rate-limiting by Docker Hub is expected to progressively take effect beginning Nov 2.</p> <p>We have been engaged in direct conversations with many of you on the possible impact of this\u00a0rate-limiting and steps toward managing that.\u00a0 This operational advisory is a summary of those conversations.</p>"},{"location":"000020630/#do-the-rate-limits-apply-to-rancher-images-that-we-pull-anonymously","title":"Do the rate limits apply to Rancher images that we pull anonymously?","text":"<p>No.\u00a0 To ensure customers pulling Rancher resources are not affected by this, Rancher Labs has partnered with Docker, Inc. so that\u00a0pulls from the Rancher namespace on Docker Hub are exempt from these limits.</p> <p>If you run into any rate-limiting issues with images hosted in the Rancher namespace, please let us know.</p>"},{"location":"000020630/#what-about-images-that-are-outside-the-rancher-namespace-that-we-pull-anonymously","title":"What about images that are outside the Rancher namespace that we pull anonymously?","text":"<p>Yes. Rate limits do apply to the images that are outside of the Rancher namespace.</p>"},{"location":"000020630/#what-can-we-do-about-the-limits-on-images-outside-the-rancher-namespace","title":"What can we do about the limits on images outside the Rancher namespace?","text":"<p>We can introduce you to the right contact at Docker Inc should you be interested in procuring an exemption for your org based on something like an IP range.\u00a0 This is to derisk being limited on image pulls outside of the Rancher namespace.</p>"},{"location":"000020630/#what-other-practical-options-can-we-pursue","title":"What other practical options can we pursue?","text":"<p>Other options to mitigate this issue are:</p> <ul> <li>Moving from anonymous pulls to authenticated pulls on Docker Hub</li> <li>Copying resources to a private registry</li> </ul> <p>The viability of these options depends on the specific context of your environment.</p>"},{"location":"000020630/#are-there-any-plans-to-host-rancher-images-elsewhere-to-help-alleviate-the-potential-issues-caused-by-this","title":"Are there any plans to host Rancher images elsewhere to help alleviate the potential issues caused by this?","text":"<p>We are looking into alternates to Docker Hub. There have been recent offerings announced by\u00a0AWS and\u00a0GitHub. We are exploring them as well as other options and should have an update on this from our product management in the near future.</p> <p>Thanks,</p> <p>Rancher Support Team</p>"},{"location":"000020630/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"000020630/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020631/","title":"[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5","text":"<p>This document (000020631) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020631/#resolution","title":"Resolution","text":"<p>Note:</p> <p>This Rancher Customer advisory below was originally sent by email to subscribed customer users on March 30, 2022.</p> <p>Dear SUSE Rancher user,</p> <p>We have been sharing important SUSE Rancher product lifecycle dates on our\u00a0Product Support Lifecycle page.</p> <p>To help you plan for any necessary upgrades for your deployments, we would like to bring to your attention information on some Rancher Manager product versions that are approaching (or have reached) their End of Maintenance (EOM)\u00a0and\u00a0End of Life (EOL)\u00a0milestones.</p> <p>EOM Dates</p> <p>Rancher Version</p> <p>EOM Date</p> <p>v2.4.x</p> <p>July 30, 2021</p> <p>v2.5.x</p> <p>January 5, 2022</p> <p>EOL Dates</p> <p>Rancher Version</p> <p>EOL Date</p> <p>v2.4.x</p> <p>March 31, 2022</p> <p>v2.5.x</p> <p>October 5, 2022</p>"},{"location":"000020631/#what-does-the-above-mean","title":"What does the above mean?","text":"<p>After a product release reaches its EOM date, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner until it reaches EOL, in the form of:</p> <ul> <li>General troubleshooting of a specific issue to isolate potential causes</li> <li>Upgrade recommendation to an existing newer version of product</li> <li>Issue resolution limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product</li> </ul> <p>Once a product release reaches its EOL date, a Rancher user may continue to use the product within the terms of the product licensing agreement. However, Support Plan SLAs from SUSE Rancher do not apply to product versions that are past their EOL dates.</p> <p>Please review in detail the following resources to understand changes and prepare for your upgrade.</p> <ul> <li>Rancher Support Matrix</li> <li>Rancher 2.5.0 release notes</li> <li>Rancher 2.6.0 release notes</li> <li>Rancher Support Upgrade checklist</li> <li>Rancher Support FAQs</li> </ul> <p>In addition, please note that with the upcoming release of Kubernetes 1.24, scheduled for April 19th, dockershim removal from Kubernetes will be final. Please see the\u00a0Kubernetes blog and our\u00a0Rancher Support Advisory for more information.</p> <p>If you have any questions on this\u00a0advisory or would like assistance validating your upgrade path, simply contact your Customer Success Manager or open a new support case via the\u00a0SCC portal referencing this advisory.</p> <p>Thanks,</p> <p>SUSE Rancher Support Team</p>"},{"location":"000020631/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"000020631/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020632/","title":"[Rancher] Operational Advisory, 20220405: Rancher Kubernetes Distributions and Etcd 3.5 Updates","text":"<p>This document (000020632) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020632/#environment","title":"Environment","text":"<p>The etcd maintainers have recommended against the use of etcd 3.5.0-3.5.2 for new production workloads, due to a recently discovered bug that may cause data loss when etcd is killed under high load. Their published\u00a0advisory2\u00a0provides recommendations of how to avoid triggering the issue, but as of today, there is no official fix/resolution to the existing\u00a0issue4.</p>"},{"location":"000020632/#situation","title":"Situation","text":"<p>Who does this affect?</p> <ul> <li>Users running Rancher 2.6.4+ who have deployed Rancher on a\u00a0single node as a Docker installation.\u00a0Reminder: This installation method is not recommended for any production environment and is only recommended for development/sandbox testing. If you are running Rancher on a managed Kubernetes cluster, then you will have to refer to your Kubernetes service provider to determine if you are affected by this advisory.</li> <li>Users running Kubernetes 1.22+ and 1.23+ of any of the Rancher Kubernetes Distributions (RKE, RKE2, K3s) and using etcd as your datastore. The default datastore for RKE and RKE2 is etcd. This applies to standalone Kubernetes clusters as well as any downstream clusters provisioned by Rancher.</li> </ul>"},{"location":"000020632/#resolution","title":"Resolution","text":"<p>What should you do?</p> <ul> <li>Stop deploying into production any new Kubernetes clusters using Rancher Kubernetes distributions versions 1.22/1.23 until a proper fix is provided by the etcd maintainers and included into the affected distribution.</li> <li>Update your etcd configuration to enable the\u00a0experimental-initial-corrupt-checkoption. This flag will be turned on by default in etcd v3.6, but does not by itself fix the problem; it can only detect and repair the issue if it does occur.</li> </ul> <p>Note: each distribution has its own recommendation on how to enable this option; see below for more details. - Avoid terminating etcd unexpectedly (using\u00a0kill \u20139, etc)   - For RKE1 clusters, avoid stopping/killing the etcd containers adhoc without properly cordoning/draining nodes and taking backups   - For RKE2 clusters,     - Avoid sending SIGKILL to the etcd or rke2 process.     - Avoid using the killall script (rke2-killall.sh) to stop RKE2 on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services.   - For k3s clusters,     - Avoid sending SIGKILL to the k3s process.     - Avoid using the killall script (k3s-killall.sh) to stop K3s on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services. - Ensure nodes are not under significant memory pressure that may cause the Linux kernel to terminate the etcd process.</p> <p>Ensure that nodes are not terminated unexpectedly. Avoid force-terminating VMs, unexpected power loss, etc.</p> <p>How do I enable the recommended flag in etccd?</p> <p>For Users provisioning RKE/k3s/RKE2 clusters through Rancher</p> <p>Provisioned RKE Clusters</p> <p>If you are running 1.22 or 1.23, upgrade to the following respective versions to enable the recommended\u00a0experimental-initial-corrupt-check\u00a0flag in etcd.</p> <ul> <li>RKE 1.22 -\u00a0v1.22.7-rancher1-2</li> <li>RKE 1.23 (Experimental) -\u00a0v1.23.4-rancher1-2</li> </ul> <p>Provisioned K3s/RKE2 Clusters (Tech Preview)</p> <p>Provisioned k3s/RKE2 clusters are still in tech preview, so we do not recommend running production workloads on these clusters. If you have provisioned clusters, you can enable the recommended\u00a0experimental-initial-corrupt-check\u00a0flag by editing the cluster as YAML. If you have an imported k3s/RKE2 cluster, review the standalone Kubernetes distribution section.</p> <ol> <li>From the \u201cCluster Management\u201d page, click the vertical three-dots on the right-hand side for the cluster you want to edit.</li> <li>From the menu, select \u201cEdit YAML\u201d.</li> <li>Edit the\u00a0spec.rkeConfig.machineGlobalConfig.etcd-arg\u00a0section of the YAML to add in an etcd argument. Note: Your YAML may be slightly different from the example below.</li> </ol> <p>Example:</p> <p>spec:</p> <p>cloudCredentialSecretName: cattle-global-data:cc-xxxxx</p> <p>kubernetesVersion: v1.22.7+rke2r2</p> <p>localClusterAuthEndpoint: {}</p> <p>rkeConfig:</p> <p>chartValues:</p> <p>rke2-calico: {}</p> <p>etcd:</p> <p>snapshotRetention: 5</p> <p>snapshotScheduleCron: 0 */5 * * *</p> <p>machineGlobalConfig:</p> <p>cni: calico</p> <p>etcd-arg: [\"experimental-initial-corrupt-check=true\"]</p> <ul> <li>Click \u201cSave\u201d at the bottom. Rancher will update the configuration and restart the necessary services.</li> </ul> <p>For Users running standalone Kubernetes distributions</p> <p>RKE Clusters</p> <p>As of RKE v1.3.8, the default version of Kubernetes was set to 1.22.x. In order to not use the default Kubernetes version, please set the\u00a0kubernetes_version\u00a0to other available versions in your\u00a0cluster.yml\u00a0file for any new deployments through RKE.</p> <ul> <li>If you already have an existing RKE1 cluster using an affected version, you can set\u00a0experimental-initial-corrupt-check: true\u00a0in\u00a0extra_args\u00a0for etcd.</li> </ul> <p>RKE v1.3.9 was released where the default version of Kubernetes is still set to 1.22.x, but the default version (1.22-rancher1-2) has the recommended flag enabled by default.</p> <p>RKE2/k3s Clusters</p> <p>The flag only needs to be added if you are using HA with embedded etcd. Single-server clusters with sqlite, or clusters using HA with an external SQL datastore are not affected. The flag only needs to be enabled on servers, as agents do not run etcd.</p> <p>Customization of etcd was introduced with 1.22.4 and 1.23.0, so if you are running a lower version of 1.22.x, then you will need to upgrade to at least 1.22.4 in order to customize the etcd configuration.</p> <p>RKE2 Clusters</p> <ol> <li>Create or edit the config file at\u00a0/etc/rancher/rke2/config.yaml.</li> <li>Add the following line to the end of the file:</li> </ol> <p>etcd-arg: [\"experimental-initial-corrupt-check=true\"]</p> <ol> <li>Save the config file and then run\u00a0systemctl restart rke2-server\u00a0to apply the change.</li> </ol> <p>K3S Clusters</p> <ol> <li>Create or edit the config file at\u00a0/etc/rancher/k3s/config.yaml.</li> <li>Add the following line to the end of the file:</li> </ol> <p>etcd-arg: [\"experimental-initial-corrupt-check=true\"]</p> <ol> <li>Save the config file and then run\u00a0systemctl restart k3s\u00a0to apply the change.</li> </ol>"},{"location":"000020632/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"000020632/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020657/","title":"Exclude cattle-system namespace from Dynatrace monitoring","text":"<p>This document (000020657) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020657/#environment","title":"Environment","text":"<p>The cattle-cluster-agent pod is stuck in a CrashLoopBackOff, logging the following messages:</p> <pre><code>usage: agent &lt;absolute path of static Go application&gt; [Go application arguments]\n\n</code></pre>"},{"location":"000020657/#situation","title":"Situation","text":"<p>We have observed messages from downstream clusters where the cattle-cluster-agent on any Rancher version can contain no logging outputs, with messages only relating to environment variables.</p> <p>Running kubectl exec to access a shell on the pod and running /usr/bin/run.sh also produces the error.</p>"},{"location":"000020657/#resolution","title":"Resolution","text":"<p>It is recommend to exclude the cattle-system namespace from being monitored by Dynatrace. The below documentation is specific for Dynatrace, however other solutions could have similar options.</p> <p>Option 3:\u00a0https://www.dynatrace.com/support/help/setup-and-configuration/setup-on-container-platforms/kubernetes/get-started-with-kubernetes-monitoring/dto-config-options-k8s#annotate</p>"},{"location":"000020657/#cause","title":"Cause","text":"<p>Solutions like Dynatrace can potentially inject sidecar containers alongside cattle-cluster-agent/cattle-node-agent containers, causing the agent to exit unpredictably..</p>"},{"location":"000020657/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020682/","title":"Azure AD API Removal","text":"<p>This document (000020682) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020682/#situation","title":"Situation","text":""},{"location":"000020682/#summary-of-changes","title":"Summary of Changes","text":"<p>Microsoft is ending support of the existing AzureAD Graph API before 2023. Accordingly, Rancher has updated our AzureAD auth provider to use the new Microsoft Graph API to access users and groups in Active Directory.</p>"},{"location":"000020682/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>Old</p> <ul> <li>ADAL is the authentication library we use to get access tokens to the deprecated Azure AD Graph API.</li> </ul> <p>New</p> <ul> <li>MSAL is the new authentication library we will instead use to get access tokens to the new Microsoft Graph API.</li> </ul>"},{"location":"000020682/#actions-required-of-users","title":"Actions Required of Users","text":"<ul> <li>New users of v2.6.x and v2.7.x will use the new Microsoft Graph API when they register Rancher with Azure AD. There will be no need for a transition.</li> <li>Existing users who have Azure AD as the auth provider will see an informational notification/banner that will urge them to upgrade Rancher's auth provider before the end of 2022. Beforehand, their app in Azure will need to have the necessary permissions for Rancher to be able to work with Users and Groups in AD. To upgrade, the UI will have a button to instruct the backend to use the new authentication/authorization flow without requiring Rancher admins to reconfigure the existing auth provider.</li> <li>AD admins must add the necessary Microsoft Graph permissions to their apps. Specifically, User.Read.All and Group.Read.All - both must be Application (not Delegated) permissions.</li> </ul>"},{"location":"000020682/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>When you choose to upgrade the existing Azure AD auth provider configuration in Rancher, please keep in mind that all users' access tokens to the deprecated Azure AD Graph API will be deleted, since Rancher won't need them anymore because it won't be communicating with it.</p> <p>Instead, Rancher will store in a secret only one access token to the new Microsoft Graph API - that of the service principal associated with the App registration in Azure AD. This token is refreshed once an hour (not in the background, but when its use triggers a refresh).</p> <p>Additional migration instructions can be found at these links:</p> <p>For Rancher 2.6.x</p> <p>For Rancher 2.7.x</p>"},{"location":"000020682/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020683/","title":"vSphere 6.7 EOL","text":"<p>This document (000020683) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020683/#situation","title":"Situation","text":""},{"location":"000020683/#summary-of-changes","title":"Summary of Changes","text":"<p>VMWare vSphere 6.7 will enter end of life (EoL) status on October 15, 2022. The original date was November 15, 2021, however, VMWare opted to extend their end of general support (EoGS) date based on customer requests at the time. As far as Rancher is concerned, vSphere 6.7 will reach EoL in the Rancher Support Matrix on the stated VMWare vSphere 6.7 EoGS date of October 15, 2022.</p> <p>VMWare Blog Announcing vSphere 6.7 EoGS https://blogs.vmware.com/vsphere/2020/06/announcing-extension-of-vsphere-6-7-general-support-period.html</p> <p>VMWare Product Lifecycle Matrix https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/support/product-lifecycle-matrix.pdf</p>"},{"location":"000020683/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>vSphere 7, released in early 2020, introduces native support and integrations for Kubernetes. This includes new functionality utilized in the Rancher vSphere CSI and CPI charts.</p>"},{"location":"000020683/#actions-required-of-users","title":"Actions Required of Users","text":"<p>End-users must upgrade to vSphere 7.0 by October 15, 2022, to stay in compliance with the Rancher Support Matrix.</p>"},{"location":"000020683/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>vSphere 7.0 has been supported since Rancher 2.4.9 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-4-9/, 2.5.2 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-5-2/, and v2.6.0 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-6-0/.</p> <p>Customers have had ample time to migrate from vSphere 6.7 to 7.0. There will not be any extensions of the Rancher EoL date for vSphere 6.7.</p> <p>The original VMWare vSphere EoTG (End of Technical Guidance) date of November 15, 2023 still applies for vSphere 6.7. Rancher Support should expect customers to potentially remain on vSphere 6.7 beyond the end of general support (EoGS) period. However, once the EoGS date is reached, Rancher and its products will no longer be validated on 6.7 and customers should expect a best effort from Support and Engineering for issues involving vSphere 6.7 environments</p>"},{"location":"000020683/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020684/","title":"Windows RKE1 EOL","text":"<p>This document (000020684) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020684/#situation","title":"Situation","text":""},{"location":"000020684/#summary-of-changes","title":"Summary of Changes","text":"<p>RKE1 Windows will move to EOL status on September 1, 2022, due to the deprecation of Docker EE support by Microsoft. Docker EE is the only supported container runtime for Windows nodes in RKE1.</p> <p>The impact on customers and internal development/QA efforts after September 1, 2022, is summarized below:</p> <p>- Rancher users should expect that they are unable to provision new RKE1 Windows clusters.</p> <p>- Rancher users should expect to have no available upgrade path for existing RKE1 Windows clusters.</p> <p>- Customers who are unable to move to RKE2 Windows would need to purchase a support contract for Mirantis Container Runtime, previously known as Docker Engine - Enterprise.</p> <p>- Rancher Support is unable to provide full-stack support for workloads deployed on the Mirantis Container Runtime.</p> <p>- We expect cloud providers to completely remove images that contain Docker EE for Windows Server.</p> <p>- Microsoft has indicated to us in an unofficial capacity that the current and only installation method for Docker EE on Windows Server, which is through the PSGallery, will be removed as part of the EOL of Docker EE.</p>"},{"location":"000020684/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>RKE2 Windows is the only path forward for Windows support in Rancher.</p> <p>RKE1 Windows Clusters: Each Linux node in an RKE1 Windows cluster, regardless of the role assigned to it, will have have a default taint that prevents workloads to be scheduled on it unless the workload has a toleration configured. This is a major design feature for RKE1 Windows clusters which were designed to only run Windows workloads.</p> <p>RKE2 Hybrid Clusters: Based on feedback and requests for hybrid workload support, RKE2 Windows was designed to support both Linux and Windows workloads by default. RKE2 scheduling relies on node selectors. This is a marked change from RKE1 as taints and tolerations were not incorporated into RKE2. Node selectors were a critical part of RKE1 Windows clusters, which makes for an easy migration of your workloads.</p>"},{"location":"000020684/#actions-required-of-users","title":"Actions Required of Users","text":"<p>Moving forward, Rancher customers will need to upgrade to Rancher v2.6.5+ (to have GA of RKE2 Windows provisioning available) and migrate their container workloads to run on RKE2 Hybrid clusters, which are built on the containerd runtime. These steps will be required for them to stay in compliance with the Rancher Support Matrix.</p>"},{"location":"000020684/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>It should be assumed that all existing functionality required for creating new RKE1 Windows clusters or upgrading existing RKE1 Windows clusters will cease functioning on this date. Rancher will be unable to publish any future versions of RKE1 with Windows Support for Docker EE. RKE2 is the only option for Windows customers who wish to stay in a supported configuration.</p> <p>Windows Server 2019 LTSC and Windows Server 2022 LTSC are the only supported versions of Windows for RKE2.</p>"},{"location":"000020684/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020699/","title":"How to recreate rancher-webhook-tls secret if incorrectly deleted","text":"<p>This document (000020699) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020699/#environment","title":"Environment","text":"<p>Rancher 2.5.8 or higher, incorrectly deleted\u00a0rancher-webhook-tls secret instead of\u00a0cattle-webhook-tls secret</p>"},{"location":"000020699/#situation","title":"Situation","text":"<p>The rancher-webhook-tls is expired on the local rancher cluster.</p> <p>After following the documentation to renew the certificate, the rancher-webhook pods cannot start.</p> <p>https://rancher.com/docs/rancher/v2.6/en/troubleshooting/expired-webhook-certificates</p>"},{"location":"000020699/#resolution","title":"Resolution","text":"<p>Trigger recreation of the rancher-webhook-tls secret:</p> <p>1. Remove\u00a0rancher.cattle.io\u00a0validating and mutating webhooks, as well as the webhook-service:</p> <pre><code>kubectl delete mutatingwebhookconfigurations\u00a0rancher.cattle.io\nkubectl delete validatingwebhookconfigurations\u00a0rancher.cattle.io\nkubectl -n cattle-system delete service webhook-service\n</code></pre> <p>2. Navigate to Apps &amp; Marketplace in the local cluster Explorer, Installed Apps, and perform an 'upgrade'</p> <p>of\u00a0rancher-webhook to trigger the recreation of deleted resources and a new rancher-webhook-tls</p> <p>certificate secret.</p>"},{"location":"000020699/#cause","title":"Cause","text":"<p>Unintentionally deletion of\u00a0_rancher-webhook-tls_secretinstead of_cattle-webhook-tls_secret</p>"},{"location":"000020699/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020710/","title":"After Rancher 2.6.x upgrade, HTTP 403 Errors in Rancher UI","text":"<p>This document (000020710) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020710/#environment","title":"Environment","text":"<p>Several features of Rancher UI don't work and return HTTP 403 for some users after Rancher upgrade from 2.6.x :</p> <p>- Shell execution</p> <p>- Yaml editing</p>"},{"location":"000020710/#situation","title":"Situation","text":"<p>For some users, several Rancher features are not working and returning HTTP 403 (Forbidden)</p> <p>Rancher Trace log:</p> <pre><code>User-system-serviceaccount-cattle-impersonation-system-cattle-impersonation-u-vnds56pccy-cannot-impersonate-resource-users-in-API-group-at-the-cluster-scope-due-to-missing-clusterrolebinding\n</code></pre>"},{"location":"000020710/#resolution","title":"Resolution","text":"<p>1. Check RBAC Clusterroles and Clusterrolebindings of the affected user</p> <pre><code>## Clusterroles of the user\n$ kubectl get clusterrole | grep u-b3l74guter\n\n## Clusterrolebindings of the  user\n$ kubectl get clusterrolebinding | grep u-b3l74guter\n</code></pre> <p>2. From the previous output, the expected Clusterrole cattle-impersonation-u-xxxxxxxx is present, but the Clusterrolebinding is absent.</p> <p>3. Delete the cattle-impersonation-user-xxxx Clusterrole of the user</p> <pre><code>$ kubectl delete clusterrole\u00a0cattle-impersonation-u-b3l74guter\n</code></pre> <p>4. Trigger the recreation of the Clusterrole and Clusterrolebinding by browsing to a Rancher feature.</p> <p>e.g: open a Monitoring link in the cluster</p> <p>This action triggered the recreation of the Clusterrole and Clusterrolebinding</p>"},{"location":"000020710/#additional-information","title":"Additional Information","text":"<p>https://github.com/rancher/rancher/issues/33912</p>"},{"location":"000020710/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020712/","title":"Is it possible use mTLS for rancher agent connectivity?","text":"<p>This document (000020712) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020712/#resolution","title":"Resolution","text":"<p>It is not possible to configure mTLS authentication for the rancher-agent connectivity; however, the connection to Rancher is secured via TLS and the agents use a token to authenticate themselves to Rancher.</p>"},{"location":"000020712/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020713/","title":"Firewalld block rancher cluster dns: Weave CNI does not work with Firewalld on RHEL 8 based OSs","text":"<p>This document (000020713) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020713/#environment","title":"Environment","text":"<p>Issue:</p> <p>Firewalld service block Rancher cluster DNS on Rhel8.</p> <p>Steps to reproduce:</p> <p>1. Install and setup RKE on RHEL 8</p> <p>2. CoreDNS is deployed as part of the RKE setup</p> <p>3. Start firewalld service on RHEL8 nodes.</p> <p>After starting firewalld service, k8s pod logs return connection error:</p> <p>ent-041273.voicelab.local. A: read udp 172.21.0.19:58953-&gt;1.10.64.26:53: i/o timeout --------------</p>"},{"location":"000020713/#situation","title":"Situation","text":"<p>The Internal Kubernetes DNS server (coredns) is blocked. Once firewalld is stopped, the Kubernetes DNS works well.</p> <p>Firewalld block these ports that are required:</p> <p>-\u00a02379-2380/tcp</p> <p>-\u00a04789/udp</p> <p>-\u00a05000/tcp</p> <p>-\u00a06443/tcp</p> <p>-\u00a06783/tcp</p> <p>-\u00a06783-6784/udp</p> <p>-\u00a09100/tcp</p> <p>-\u00a010250/tcp</p> <p>-\u00a010257/tcp</p> <p>-\u00a010259/tcp</p>"},{"location":"000020713/#resolution","title":"Resolution","text":"<p>Stop firewalld on RHEL8 nodes, it is a requirement as described in Rancher requirements:</p> <p>https://rancher.com/docs/rancher/v2.6/en/installation/requirements/#operating-systems-and-container-runtime-requirements</p>"},{"location":"000020713/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020721/","title":"How to test Rancher RC/Alpha versions","text":"<p>This document (000020721) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020721/#environment","title":"Environment","text":"<p>Rancher RC versions</p>"},{"location":"000020721/#situation","title":"Situation","text":"<p>By default, Helm only returns the final releases. This article provides details on how to install Rancher RC versions.</p>"},{"location":"000020721/#resolution","title":"Resolution","text":"<p>First, you need to add the Rancher 'latest' helm repository.</p> <pre><code>helm repo add rancher-latest &lt;strong&gt;https://releases.rancher.com/server-charts/latest&lt;/strong&gt;\n</code></pre> <p>Then you can list the current repositories on your configuration machine with</p> <pre><code>$ helm repo ls\nNAME            URL\ncoredns         https://coredns.github.io/helm\nrancher-charts  https://charts.rancher.io\nrancher-stable  https://releases.rancher.com/server-charts/stable\nrancher-latest  https://releases.rancher.com/server-charts/latest\n</code></pre> <p>You can list the final releases with</p> <pre><code>$ helm search repo \"rancher-latest\" --versions\nNAME                    CHART VERSION   APP VERSION DESCRIPTION\nrancher-latest/rancher  2.6.6           v2.6.6      Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.5           v2.6.5      Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.4           v2.6.4      Install Rancher Server to manage Kubernetes clu...\n[...]\nrancher-latest/rancher  2.0.4           v2.0.4      Install Rancher Server to manage Kubernetes clu...\n\n</code></pre> <p>To list the RC versions, you can use the --devel argument.</p> <pre><code>helm search repo \"rancher-latest\" --versions --devel\nNAME                    CHART VERSION       APP VERSION         DESCRIPTION\nrancher-latest/rancher  2.6.7-rc7           v2.6.7-rc7          Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.7-rc6           v2.6.7-rc6          Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.7-rc5           v2.6.7-rc5          Install Rancher Server to manage Kubernetes clu...\n[...]\nrancher-latest/rancher  2.0.4               v2.0.4              Install Rancher Server to manage Kubernetes clu...\n\n</code></pre> <p>The Rancher installation command line becomes</p> <pre><code>helm install rancher &lt;rancher-latest-repo&gt;\n  --devel\n  --version 2.6.7-rc1\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set replicas=3\n</code></pre>"},{"location":"000020721/#additional-information","title":"Additional Information","text":"<p>https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart</p>"},{"location":"000020721/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020727/","title":"Rancher upgrade FAQ","text":"<p>This document (000020727) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020727/#situation","title":"Situation","text":"<p>As we near the end of maintenance, end of life, and end of support for Rancher 2.5, we felt it pertinent to provide a one-stop FAQ page as customers begin to plan their upgrades.</p> <p>All upgrades should go from the latest to the latest. For instance, if you are on 2.5.3, you should go to 2.5.latest and then to 2.6.latest. The stop on 2.5.latest should be about a week to ensure you can catch any issues before moving forward.</p> <p>After the upgrade to 2.5.latest, you should then upgrade the underlying Kubernetes version to the latest supported version of the Rancher release. And again, the new Kubernetes version should be tested and verified for around one week. From there, progress to the next Rancher upgrade, test, and then Kubernetes.</p> <p>Many customers ask why we recommend one week. The 1-week recommendation is because this is often enough time for your clusters and app to be thoroughly tested and any issues flagged. We have seen customers who do less of a testing phase and only find issues when their cluster is being fully used and under normal \"strain.\"</p> <p>Here are some useful links for upgrades \u2014 see link (1) below for our team's general best practices around upgrade paths.</p> <p>During the course of your upgrade, see link (2) for how you and your team can bump the severity of this case if there is an impacting event during your upgrade. Doing so will notify our on-call engineer, who will engage as quickly as possible.</p> <p>Please review link (3) to verify that the Rancher and Kubernetes versions remain inline. It is best to ensure that any testing is done in a lower environment first so that you and your team can be more aware of any issues that the version jumps could have on your environment and applications.</p> <p>Lastly, for our team to better understand the updated environment, would you please run our system support script (4). This script will give our team a better understanding of the upgraded environment and the ability to call out items that may be out of our best practices.</p> <p>(1) https://www.suse.com/support/kb/doc/?id=000020061</p> <p>(2) https://www.suse.com/support/kb/doc/?id=000020296</p> <p>(3) https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/</p> <p>(4) https://www.suse.com/support/kb/doc/?id=000020192</p>"},{"location":"000020727/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020728/","title":"Collecte de journaux Linux Rancher v2.x","text":"<p>This document (000020728) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020728/#situation","title":"Situation","text":""},{"location":"000020728/#collecte-de-journaux-linux-rancher-v2x_1","title":"Collecte de journaux Linux Rancher v2.x","text":"<p>Les journaux peuvent \u00eatre collect\u00e9s \u00e0 partir d'un n\u0153ud Linux dans un cluster Rancher v2.x \u00e0 l'aide du script de collecte de journaux Rancher v2.x.</p> <p>Important:Ce script ne peut \u00eatre utilis\u00e9 que pour collecter des journaux \u00e0 partir de clusters provisionn\u00e9s par l'interface de ligne de commande Rancher Kubernetes Engine (RKE) , de clusters K3s, de clusters personnalis\u00e9s\u00a0provisionn\u00e9s par Rancher et de clusters provisionn\u00e9s par Rancher \u00e0 l'aide d'un pilote de n\u0153ud.</p> <p>Ce script n'est pas adapt\u00e9 \u00e0 la collecte de journaux \u00e0 partir de clusters de fournisseurs Kubernetes h\u00e9berg\u00e9s.</p> <p>Le script doit \u00eatre t\u00e9l\u00e9charg\u00e9 et ex\u00e9cut\u00e9 directement sur l'h\u00f4te en utilisant l'utilisateur root ou en utilisant sudo, comme suit:</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s\n\n</code></pre> <p>Par d\u00e9faut, la sortie sera \u00e9crite dans /tmpdans un fichier tar gzipp\u00e9 nomm\u00e9 -.tar.gz"},{"location":"000020728/#choix","title":"Choix","text":"<p>Les indicateurs disponibles pouvant \u00eatre transmis au script se trouvent dans le script de collecteur de journaux Rancher v2.x README</p>"},{"location":"000020728/#disclaimer","title":"Disclaimer","text":"<p>Cette base de connaissances de support technique fournit un outil pr\u00e9cieux aux clients SUSE et autres parties int\u00e9ress\u00e9es par nos produits et solutions pour obtenir des informations, des id\u00e9es et apprendre r\u00e9ciproquement. Les documents sont fournis \u00e0 des fins d'information, personnelles ou non commerciales au sein de votre organisation et sont pr\u00e9sent\u00e9s \u00abEN L'\u00c9TAT \u00bb SANS GARANTIE D'AUCUNE SORTE.</p>"},{"location":"000020731/","title":"Tuning for nodes with a high number of CPUs allocated","text":"<p>This document (000020731) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020731/#environment","title":"Environment","text":"<p>An RKE cluster built by Rancher, or the RKE CLI</p>"},{"location":"000020731/#situation","title":"Situation","text":"<p>Some components in a Kubernetes cluster apply a linear scaling mechanism, often based on the number of CPU cores allocated.</p> <p>For nodes that have a high number CPU cores allocated the defaults can create a steep scaling curve and can introduce issues.</p> <p>Two components provided with RKE that scale in this way are kube-proxy and ingress-nginx. However, additional workloads (like nginx) may be deployed to the cluster and also need consideration.</p> <p>Adjusting the scaling for these components can avoid these issues.</p>"},{"location":"000020731/#resolution","title":"Resolution","text":""},{"location":"000020731/#kube-proxy","title":"kube-proxy","text":"<p>As explained in the Kubernetes GitHub issue here, the default scaling of the conntrack-max setting allocates 32K of memory per CPU core.</p> <p>This can manifest in the below events in OS logs:</p> <pre><code>kernel: nf_conntrack: falling back to vmalloc.\n</code></pre> <p>This static default can present issues with contiguous memory being allocated for the conntrack table, or reach unnecessary levels of space allocated. When observed frequently, this has been associated with network instability.</p> <p>As a starting point, the suggestion is to halve this amount for a cluster with affected nodes, this can be done by editing the cluster as YAML, or the cluster.yml file when using the RKE CLI.</p> <pre><code>kubeproxy:\nextra_args:\n    conntrack-max-per-core: '16384'\n</code></pre>"},{"location":"000020731/#ingress-nginx","title":"ingress-nginx","text":"<p>A common configuration of nginx is to set the worker_processes to auto. When set, nginx will scale the worker_processes to the number of CPU cores on the node. This can result in high numbers of PIDs and consume open files with the threads consumed (number of cores * 32\u00a0(default thread_pool size)).</p> <p>\\* http://nginx.org/en/docs/ngx_core_module.html#worker_processes</p> <p>\\* http://nginx.org/en/docs/ngx_core_module.html#thread_pool</p> <p>This can be adjusted by editing the cluster as YAML, or the cluster.yml file when using the RKE CLI. An example of 8 worker_processes is used below. For a nodes that may process a high amount of ingress traffic, you may wish to use a higher number.</p> <pre><code>ingress:\nprovider: nginx\noptions:\n    worker-processes: \"8\"\n\n</code></pre>"},{"location":"000020731/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020733/","title":"Reduce Memory and CPU footprint of Prometheus Monitoring Operator","text":"<p>This document (000020733) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020733/#environment","title":"Environment","text":"<p>rancher-monitoring:\u00a0100.1.3+up19.0.3</p>"},{"location":"000020733/#situation","title":"Situation","text":"<p>On a fresh install of rancher-monitoring, the Prometheus monitoring operator consumes high CPU and Memory resources without any custom Prometheus CRDs configured. You would notice the following error messages on the Prometheus Operator Pod very frequently.</p> <pre><code>$ kubectl logs -n cattle-monitoring-system rancher-monitoring-operator-784c69bc54-dvndg -f\nlevel=info ts=2022-07-26T09:34:55.46606005Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\nlevel=info ts=2022-07-26T09:34:55.612269913Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:55.694485011Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:55.92009322Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\nlevel=info ts=2022-07-26T09:34:59.042606472Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:59.043983987Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\n\n</code></pre>"},{"location":"000020733/#resolution","title":"Resolution","text":"<p>Add SecretListWatchSelector to reduce memory and CPU footprint.</p> <pre><code>prometheusOperator:\nsecretFieldSelector: \"type!=helm.sh/release.v1\"\n</code></pre>"},{"location":"000020733/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"000020733/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020737/","title":"How to set up Alertmanager configs in Monitoring V2 in Rancher","text":"<p>This document (000020737) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020737/#environment","title":"Environment","text":"<p>Before we get started:</p> <ul> <li>Admin permissions are needed</li> <li>Gather the required information for the alerting you'd like to set up</li> <li>Monitoring will need to be installed</li> </ul> <p>Please note that I will set up Slack alerts in this example</p> <ul> <li>To set up Slack, add and configure the Incoming webhooks app within your Slack environment</li> <li>Copy the Webhook URL and paste it into a blank notepad</li> </ul>"},{"location":"000020737/#situation","title":"Situation","text":""},{"location":"000020737/#resolution","title":"Resolution","text":"<ul> <li> <p>Create an Opaque Secret in the cattle-monitoring-system namespace</p> </li> <li> <p>Click on Projects/Namespaces</p> </li> <li>Select cattle-monitoring-system</li> <li>Then click on Secrets</li> <li>Then select Create</li> <li>Choose Opaque Secret</li> <li>Specify a key name (Insert name here for the secret)</li> <li>Then paste the Slack Webhook URL under Value</li> <li>Verify that the secret has been created successfully</li> <li> <p>Next, select the Monitoring tab on the left side</p> </li> <li> <p>Select Alerting</p> </li> <li>Then create a new AlertManagerConfig in the same namespace</li> <li>Add a name for this configuration</li> <li>Then select Create</li> <li>Once added, we will need to Edit Config</li> <li>From there, we can add a new Receiver(This is where we can specify which type of notifications to receive)</li> <li>For Slack, select Add Slack</li> <li>Under Secret with Slack Webhook URL, select the Secret name</li> <li>Then under the Key drop-down, we'll see the new, generated webhook secret key</li> <li>Next, specify the channel that the notifications will be sent to<ul> <li>(Optional) specify a proxy if applicable</li> </ul> </li> <li>Then select Create</li> <li>After creation, a Slack receiver will be shown, pointing to the webhook URL secret</li> <li>Next, edit the AlertManagerConfig again and point the base route of the config to the slack receiver</li> <li>Under the drop-down, our receiver will show up</li> <li>Select the Receiver</li> <li> <p>Specify Groupings and Matchers and set different intervals here</p> <ul> <li>For faster testing, change the default interval times to quicker times like 5 seconds, 10 seconds, 1 minute, etc.</li> <li>Then click Save</li> <li>Under status, now we see the resulting config in the Alertmanager section in the Rancher UI</li> <li>Then, if everything has been set up correctly, a notification should be sent to the desired Slack channel</li> </ul> </li> </ul>"},{"location":"000020737/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020745/","title":"Troubleshooting downstream cluster disconnections and kubectl timeouts in Rancher 2.6.4 and 2.6.5","text":"<p>This document (000020745) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020745/#environment","title":"Environment","text":"<p>Rancher Version 2.6.4 and 2.6.5 with multiple downstream clusters</p>"},{"location":"000020745/#situation","title":"Situation","text":"<p>Clusters will be unavailable or running kubectl commands from the built-in shell from the UI will timeout.</p> <p>Examples:</p> <pre><code>kubectl get nodes\nUnable to connect to the server: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\nThere are no log errors in rancher or downstream agents\nLog nginx: upstream_status=\"-\" upstream_response_time=\"32.000\" request_uri=\"/k8s/clusters/[cluster]/apis?timeout=32s\"\n\n</code></pre>"},{"location":"000020745/#resolution","title":"Resolution","text":"<p>To restart the Rancher Pods:</p> <pre><code>kubectl rollout restart deploy rancher -n cattle-system ; kubectl rollout status deploy rancher -n cattle-system\n\n</code></pre>"},{"location":"000020745/#cause","title":"Cause","text":"<p>https://github.com/rancher/rancher/issues/37250</p>"},{"location":"000020745/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020747/","title":"Prometheus Exporter not able to scrape filesystem metrics from nodes when SELinux is enabled.","text":"<p>This document (000020747) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020747/#environment","title":"Environment","text":"<p>Any Linux distribution with SELinux enabled would have this issue.</p>"},{"location":"000020747/#situation","title":"Situation","text":"<p>Using Prometheus Graphs, when users are trying to query filesystem metrics from nodes, they would not see any metrics due to SELinux being enabled. SELinux policies reject the query due to insufficient privileges.</p> <p>PromQL Query:</p> <pre><code>node_filesystem_*\n</code></pre> <p>Error on the Prometheus exporter pod:</p> <pre><code>level=error ts=2022-07-21T16:13:08.502Z caller=collector.go:169 msg=\"collector failed\" name=filesystem duration_seconds=0.000837846 err=\"open /host/proc/1/mounts: permission denied\"\n\n</code></pre>"},{"location":"000020747/#resolution","title":"Resolution","text":"<p>This is not an issue with the Prometheus node exporter from the rancher monitoring chart but rather an issue from the SELinux side. There are three ways to verify that the SELinux team needs to be involved in fixing this issue.</p> <p>1.) Disable SELinux, and the Prometheus node exporter should be able to start querying the node-level metrics. (NOT RECOMMENDED)</p> <p>2.) If SELinux cannot be disabled, you can modify the helm chart and update the Prometheus-node-exporter section with the below seLinuxOption called spc_t, which gives container super-privileged access. (NOT RECOMMENDED)</p> <pre><code>securityContext:\n    seLinuxOptions:\n      type: spc_t\n</code></pre> <p>3.) If super-privileged access cannot be provided to the container, ask the SELinux team to create necessary policies for processes to access the files that node-exporter pods are looking for. (RECOMMENDED APPROACH)</p>"},{"location":"000020747/#cause","title":"Cause","text":"<p>We notice this issue with SELinux enabled cluster nodes.</p>"},{"location":"000020747/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"000020747/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020750/","title":"Customize Helm Chart values for RKE2 default addons","text":"<p>This document (000020750) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020750/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>RKE2 cluster</p>"},{"location":"000020750/#situation","title":"Situation","text":"<p>By default, RKE2 installs multiple addons, including CoreDNS, Local-Storage, Nginx-Ingress, etc.:</p> <pre><code>kubectl -n kube-system get addons\n</code></pre> <p>Many of these addons are deployed from a Helm chart and represented within the cluster via a HelmChart custom resource:</p> <pre><code>kubectl -n kube-system get helmchart\n</code></pre> <p>These built-in addons deployed from a Helm chart can be customized with the use of a HelmChartConfig custom resource:</p> <pre><code>kubectl -n kube-system get helmchartconfig\n</code></pre> <p>This is where you can use the Helm chart values to change an addon's default installation.</p>"},{"location":"000020750/#resolution","title":"Resolution","text":"<p>To edit the values of a Helm chart, you must find the currently installed version. To do this navigate\u00a0to the RKE2 GitHub repository releases page to find the Packaged Component Versions (https://github.com/rancher/rke2/releases/) for the specific RKE release.</p> <p>For example, RKE2 1.23.10+rke2r1 uses ingress-nginx 4.1.0 (https://github.com/rancher/rke2/releases/tag/v1.23.10+rke2r1). Checking the values for this version of the ingress-nginx chart \u00a0within the ingress-nginx GitHub repository\u00a0you can determine the possible values</p> <p>(https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.1.0/charts/ingress-nginx/values.yaml).</p> <p>Thus, to add tolerations to the ingress-nginx controller, you can use the below manifest as an example. All of the values from the chart can be customised via this schema.</p> <pre><code>---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\nname: rke2-ingress-nginx\nnamespace: kube-system\nspec:\nvaluesContent: |-\n    controller:\n      tolerations:\n        - key: \"key\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n</code></pre> <p>After creating the HelmChartConfig manifest, you need to apply it via Rancher. To do so:</p> <p>1. Navigate to Cluster Management.</p> <p>2. On the selected cluster, click\u00a0Edit Config.</p> <p>3. Click on the Add-On Config tab and enter the manifest at the bottom in\u00a0Additional Manifest.</p> <p></p>"},{"location":"000020750/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020767/","title":"Downstream Cluster not Available with Websockets failing","text":"<p>This document (000020767) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020767/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>AKS 1.22+</p>"},{"location":"000020767/#situation","title":"Situation","text":"<p>After upgrading AKS to version 1.22+ users may experience a situation where the Downstream clusters show as unavailable on Rancher.</p> <p>Testing the Websocket using these instructions will show the following error:</p> <pre><code>Bad Request\n{\"baseType\":\"error\",\"code\":\"ServerError\",\"message\":\"websocket: the client is not using the websocket protocol: 'upgrade' token not found in 'Connection' header\",\"status\":400,\"type\":\"error\"}\n</code></pre>"},{"location":"000020767/#resolution","title":"Resolution","text":"<p>Update the Kubernetes Ingress NGINX with the tag --set controller.watchIngressWithoutClass=true:</p> <pre><code>helm upgrade --install \\\ningress-nginx ingress-nginx/ingress-nginx \\\n  --namespace ingress-nginx \\\n  --set controller.service.type=LoadBalancer \\\n  --version 4.0.18 \\\n  --create-namespace \\\n  --set controller.watchIngressWithoutClass=true\n\n</code></pre> <p>Alternatively, on Rancher 2.6.7 onward, you can add the class name on the helm install/upgrade steps :</p> <pre><code>--set ingress.ingressClassName=nginx\n</code></pre>"},{"location":"000020767/#cause","title":"Cause","text":"<p>Kubernetes version 1.22 deprecated versions of the Ingress APIs in favor of the stable <code>networking.k8s.io/v1</code> API. That leads to this scenario, where we update the controller.watchIngressWithoutClass tag.</p>"},{"location":"000020767/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020771/","title":"How to create seperate ETCD and Controlplane nodes in RKE2","text":"<p>This document (000020771) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020771/#environment","title":"Environment","text":"<p>RKE2 1.21.2 and higher</p>"},{"location":"000020771/#situation","title":"Situation","text":"<p>As part of a HA set-up, it may be required to run RKE2 with the ETCD database split from the Control plane nodes.</p>"},{"location":"000020771/#resolution","title":"Resolution","text":"<p>1. On the desired ETCD node create `/etc/rancher/rke2/config.yaml` with the following contents:</p> <pre><code>disable-apiserver: true\ndisable-controller-manager: true\ndisable-kube-proxy: false\ndisable-scheduler: true\n</code></pre> <p>2. On the etcd node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"\" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` <p>3. On the controlplane node create `/etc/rancher/rke2/config.yaml` with the following contents:</p> <pre><code>server: https://&lt;ip of the etcd node&gt;:9345\ntoken: &lt;token string from /var/lib/rancher/rke2/server/node-token on the etcd node&gt;\ndisable-etcd: true\ndisable-kube-proxy: false\netcd-expose-metrics: false\n</code></pre> <p>4. On the controlplane node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"\" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` <p>5. Add agent nodes (https://docs.rke2.io/install/ha/#5-optional-join-agent-nodes).</p>"},{"location":"000020771/#additional-information","title":"Additional Information","text":"<p>This is only an example to show this configuration in a working state. In a prod environment, you should configure a fixed registration address per the documentation at https://docs.rke2.io/install/ha/#1-configure-the-fixed-registration-address</p>"},{"location":"000020771/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020788/","title":"How to clean the orphaned cluster objects from the deleted cluster namespaces.","text":"<p>This document (000020788) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020788/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p>"},{"location":"000020788/#situation","title":"Situation","text":"<p>In some cases there may be orphaned cluster objects left behind after the in-proper deletion of a downstream cluster in Rancher. These orphaned objects could introduce a condition that causes the leader Rancher pod to enter a CrashLoop state.</p> <p>Examples of errors from the Rancher pod logs.</p> <pre><code>[ERROR] failed to call leader func: namespaces \"&lt;strong&gt;c-xxxxx&lt;/strong&gt;\" not found\nfatal error: concurrent map read and map write\n</code></pre> <pre><code>[ERROR] error syncing \u2018&lt;strong&gt;c-xxxx/p-xxxx&lt;/strong&gt;\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing\n[ERROR] error syncing \u2018&lt;strong&gt;c-xxxxx/p-xxxxx&lt;/strong&gt;\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing\n[ERROR] error syncing \u2018&lt;strong&gt;c-xxxxx/p-xxxxx&lt;/strong&gt;\u2019: handler cluster-registration-token: clusters.management.cattle.io \"&lt;strong&gt;c-xxxxx&lt;/strong&gt;\" not found, requeuing\n\n</code></pre>"},{"location":"000020788/#resolution","title":"Resolution","text":"<p>Find the objects under the deleted cluster namespaces and manually delete each objects. Make sure there are no such orphaned objects or namespaces left in the local cluster.</p> <p>1. Set a kubeconfig for the Rancher (local) management cluster to be used with the following steps</p> <p>2. Verify the Active downstream clusters</p> <pre><code>kubectl get clusters.management.cattle.io -o custom-columns=\"ID:.metadata.name,NAME:.spec.displayName,K8S_VERSION:.status.version.gitVersion,CREATED:.metadata.creationTimestamp,DELETED:.metadata.deletionTimestamp,LAST_READY:.status.conditions[?(@.type == 'Ready')].lastUpdateTime,READY:.status.conditions[?(@.type == 'Ready')].status\" --sort-by=.metadata.creationTimestamp\n</code></pre> <p>3. Cross verify with the Rancher pod logs to get the deleted downstream cluster namespace and collect the details. Compare with the active list of clusters versus the cluster namespaces.</p> <pre><code>kubectl logs -n cattle-system -l app=rancher -c rancher\n\n</code></pre> <pre><code>kubectl get ns -A |grep \"c-\"\n</code></pre> <p>4. If there is a cluster that is stuck deleting, this may not complete. In this case, the finalizer object can be removed from the cluster.management.cattle.io object. Please note the c-xxxxx needs to be replaced with the cluster ID that is stuck deleting.</p> <pre><code>kubectl patch clusters.management.cattle.io &lt;strong&gt;&amp;lt;c-xxxxx&amp;gt;&lt;/strong&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre> <p>5. If there is a namespace for a cluster that no longer exists, get the orphaned object details under the deleted cluster namespace.</p> <pre><code>kubectl api-resources --verbs=list --namespaced -o name  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;strong&gt;&amp;lt;c-xxxxx&amp;gt;&lt;/strong&gt;\n</code></pre> <p>6. Do the cleanup of orphaned objects.</p> <ul> <li>Create the cluster namespace which is deleted, ignore if the cluster namespace is present</li> </ul> <pre><code>kubectl create ns &lt;strong&gt;&amp;lt;c-xxxxx&amp;gt;&lt;/strong&gt;\n</code></pre> <ul> <li>Check the objects detected (in step 5) if desired, each object should have a deletion timestamp if a finalizer is preventing the object from being deleted.</li> </ul> <pre><code>kubectl -n &lt;strong&gt;&amp;lt;c-xxxxx&amp;gt;&lt;/strong&gt; get &lt;resource type&gt; &lt;name of object&gt; -o yaml\n</code></pre> <ul> <li>Remove the finalizer to unblock the deletion of the objects. The command needs to be run for each object.</li> </ul> <pre><code>kubectl -n &lt;strong&gt;&amp;lt;c-xxxxx&amp;gt;&lt;/strong&gt; patch &lt;strong&gt;&amp;lt;resource type&amp;gt; &amp;lt;name of object&amp;gt;&lt;/strong&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre> <ul> <li>Make sure there are no objects left in the namespace.</li> </ul> <pre><code>kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;strong&gt;&amp;lt;c-xxxxx&amp;gt;&lt;/strong&gt;\n</code></pre> <ul> <li>Finally, delete the namespace.</li> </ul> <pre><code>kubectl delete ns &lt;strong&gt;&amp;lt;c-xxxxx&amp;gt;&lt;/strong&gt;\n</code></pre>"},{"location":"000020788/#cause","title":"Cause","text":"<p>It is important to delete downstream clusters in a process to allow Rancher to delete clusters and clean nodes that are in an Active state.</p> <p>Downstream cluster deletion is ideally performed from the Rancher UI / API, where nodes are available and able to be gracefully removed. For example, where possible do not terminate nodes in the infrastructure before the deletion is completed.</p>"},{"location":"000020788/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"000020788/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020792/","title":"How to make a simple terraform API request","text":"<p>This document (000020792) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020792/#environment","title":"Environment","text":"<p>Latest version of Rancher 2.6.X</p> <p>Rancher Terraform Provider 1.24.1+</p> <p>Latest Version of Terraform 1.3.1+</p> <p>User with a token created via the API and proper permissions to the local Rancher cluster.</p> <p>Local directory for terraform plan files ( name.tf) and a local terraform.tfstate file.</p>"},{"location":"000020792/#situation","title":"Situation","text":"<p>Sometimes it is necessary to create a basic skeleton for beginning a task, like using the Rancher2 Terraform Provider to speak with the Rancher API.</p> <p>This represents a starting point to choose a simple read-only task like \"query the cluster information for the local Rancher cluster\".</p>"},{"location":"000020792/#resolution","title":"Resolution","text":"<p>Terraform commands are very easy, below are the main options one may typically use.</p> <ul> <li>terraform init -- download needed files like the rancher2 terraform provider</li> <li>terraform plan -- compare the environment to the state file, plan is like a diff of any changes to be made</li> <li>terraform apply -- apply the planned changes</li> <li>terraform refresh -- update the state to match remote systems</li> <li>terraform output -- show output values from the main.tf plan</li> <li>terraform destroy -- clean up anything created by terraform</li> <li>terraform fmt -- spacing is important in HCL, terraform's language, use this command to format all spacing in the current working directory</li> </ul> <p>To get started, create a directory to hold all of the files.\u00a0 Terraform will examine the local file or files, and then populate a local terraform.tfstate data file which represents the most recent refresh of the information from the Rancher API.\u00a0 The files below can be separate or all together in a main.tf file.\u00a0 Separating plan files into individual pieces can make managing a larger project easier.\u00a0 Terraform will take actions required using variables supplied by the user or admin, or computed during the \"apply\" operation.\u00a0 As a typical rule of thumb for any provider, \"data\" sources are read operations while \"resource\" operations are write/create/change.</p> <p>Upon running \"terraform apply\" with the main.tf file below, terraform will contact the Rancher API, authenticate, request the cluster_info for the local Rancher cluster with ID \"local\" and store it into the terraform statefile, as well as output to the screen.\u00a0 The comments explain a potential name for each file, the only requirement that it ends in \"tf\".</p> <pre><code>### tfvars.tf or environment.tf\n\n#  these outline the url speaking to, and the authorization token\n\nvariable \"api_url\" {\ndescription = \"rancher api url\"\ndefault     = \"https://urlto.rancher-fqdn.com/v3\"\n}\n\nvariable \"token_key\" {\ndescription = \"api key to use for tf\"\ndefault     = \"token-nameid:jwt-long-hash-string\"\n}\n\n### providers.tf\n\n# use the variables from the earlier section to define the provider\n\nprovider \"rancher2\" {\napi_url   = var.api_url\ntoken_key = var.token_key\ninsecure  = true\n}\n\n### versions.tf\n\n# tell terraform what versions of providers and terraform itself, to expect\n\nterraform {\nrequired_providers {\n    rancher2 = {\n      source  = \"rancher/rancher2\"\n      version = \"&gt;= 1.24.1\"\n    }\n}\nrequired_version = \"&gt;= 1.3.1\"\n}\n\n### main.tf\n\n## hard-coded example, read cluster info for local\n## export with 'terraform output cluster_info'\n\ndata \"rancher2_cluster\" \"local\" {\nname = \"local\"\n}\n\noutput \"cluster_info\" {\nvalue     = data.rancher2_cluster.local\n}\n\n</code></pre>"},{"location":"000020792/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"000020792/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020803/","title":"Restore k3s from MySQL dump","text":"<p>This document (000020803) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020803/#environment","title":"Environment","text":"<p>k3s with an external database MySQL (this has been tested using Azure MySQL)</p>"},{"location":"000020803/#situation","title":"Situation","text":"<p>MySQL DB dump is available, and the cluster token from /var/lib/rancher/k3s/server/token</p>"},{"location":"000020803/#resolution","title":"Resolution","text":"<ol> <li>In the new MySQL instance create a database</li> <li>Here we set the Character set to latin1 and collation to latin1_swedish_ci, as the original DB</li> <li>we also chose the same name, as it is a new instance</li> <li>Restore the dump, you may use mysql db_name &lt; backup-file.sql</li> <li>on the first node start k3s with:</li> <li>curl -sfL https://get.k3s.io | sh -s - server --token  --datastore-endpoint=\"mysql://:@tcp(.mysql.database.azure.com:3306)/?tls=true\" <li>the token is retrieve from the failed cluster in /var/lib/rancher/k3s/server/token</li> <li>Remove the failed nodes running:</li> <li>k3s kubectl get nodes</li> <li>k3s kubectl delete nodes  <li>Join any additional node using the instructions from k3s documentation</li>"},{"location":"000020803/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020805/","title":"Rancher upgrade has failed with an error no matches for kind \"Issuer\" in version \"cert-manager.io/v1alpha2\"","text":"<p>This document (000020805) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020805/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p>"},{"location":"000020805/#situation","title":"Situation","text":"<p>Rancher upgrade is failing due to the deprecated apiVersion for the cert-manager CRD. This affects cert-manager upgrades from an earlier release, for example upgrading cert-manager from 0.12 to 1.7.1, which in turn has the potential to create a deprecated apiVersion within the existing Rancher release manifest.</p> <p>The relevant error message may appear as below and occurs when running the helm upgrade command to upgrade Rancher.</p> <pre><code>Error: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: resource mapping not found for name: \"rancher\" namespace: \"\" from \"\": no matches for kind \"Issuer\" in version \"cert-manager.io/v1alpha2\" ensure CRDs are installed first\n</code></pre>"},{"location":"000020805/#resolution","title":"Resolution","text":"<p>Follow the below steps to edit the latest Helm v3 config for Rancher, and replace cert-manager.io/v1alpha2 with cert-manager.io/v1.</p> <p>1. Execute the below command and locate the latest version of\u00a0sh.helm.release.v1.rancher.v*</p> <pre><code>kubectl get secrets -n cattle-system\n</code></pre> <p>2. Back up the object, this example assumes sh.helm.release.v1.rancher.v1 is the latest</p> <pre><code>kubectl get secret sh.helm.release.v1.rancher.v1 -n cattle-system -o yaml &gt; helm-rancher-config.yaml\n\n</code></pre> <p>3. Decode the data.release field and save the output to yaml (jq must be installed before executing the below steps)</p> <pre><code>kubectl get secrets sh.helm.release.v1.rancher.v1 -n cattle-system -o json | jq .data.release | tr -d '\"' | base64 -d | base64 -d | gzip -d &gt; helm-rancher-config-data-decoded.yaml\n</code></pre> <p>4. Change the apiVersion from v1/alpha2 to v1.</p> <pre><code>sed -e 's/cert-manager.io\\/v1alpha2/cert-manager.io\\/v1/' helm-rancher-config-data-decoded.yaml &gt; helm-rancher-config-data-decoded-replaced.yaml\n\n</code></pre> <p>5. Store the encoded data in a variable to reuse in the next step</p> <pre><code>releaseData=$(cat helm-rancher-config-data-decoded-replaced.yaml | gzip | base64 | base64 | tr -d \"\\n\")\n</code></pre> <p>6. Replace the release data</p> <pre><code>sed 's/^\\(\\s*release\\s*:\\s*\\).*/\\1'$releaseData'/' helm-rancher-config.yaml &gt; helm-rancher-config-final.yaml\n</code></pre> <p>7. Apply the yaml</p> <pre><code>kubectl apply -f helm-rancher-config-final.yaml -n cattle-system\n</code></pre>"},{"location":"000020805/#cause","title":"Cause","text":"<p>Old CRD's are not deleted properly after the upgrade of cert-manager, this may cause a deprecated apiVersion to be used in the Rancher release manifest.</p>"},{"location":"000020805/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"000020805/#additional-information","title":"Additional Information","text":"<p>The correct way of upgrading cert-manager is in the below link</p> <p>https://docs.ranchermanager.rancher.io/getting-started/installation-and-upgrade/resources/upgrade-cert-manager#option-a-upgrade-cert-manager-with-internet-access</p> <p>Below is a snippet of helm get manifest -n cattle-system rancher which uses old CRDs, and thus has deprecated apiVersions.</p> <pre><code>---\n# Source: rancher/templates/issuer-rancher.yaml\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\nname: rancher\nlabels:\n    app: rancher\n    chart: rancher-2.6.6\n    heritage: Helm\n    release: rancher\nspec:\nca:\n    secretName: tls-rancher\n\n</code></pre> <p>As in the above, /v1apha2 is referenced, this version has been deprecated.</p> <p>Command to get the available apiVersion for cert-manager</p> <pre><code>kubectl get --raw /apis/cert-manager.io | jq .\n</code></pre>"},{"location":"000020805/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020831/","title":"How to troubleshoot Overlay Network Connectivity issues","text":"<p>This document (000020831) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020831/#situation","title":"Situation","text":"<p>pod-to-pod communication not happening</p>"},{"location":"000020831/#resolution","title":"Resolution","text":"<p>Pod-to-Pod communication should depend on multiple factors. Mainly network communication should be allowed in between the nodes. The following checkpoints help us trace the problem's root cause.</p> <ul> <li> <p>Check ports for their overlay are open between nodes (if they have multiple subnets/VLANs/DCs); testing from just one node to nodes in the other network should be good enough,\u00a0for e.g.,\u00a0`nc -uvz  8472`\u00a0(if they\u2019re using canal, change the port as needed).[https://rancher.com/docs/rancher/v2.6/en/installation/requirements/ports/#commonly-used-ports] <li> <p>Check the DNS from a test pod with suitable\u00a0tools (not busybox, it has nslookup issues),\u00a0`rancherlabs/swiss-army-knife`\u00a0is good for this.\u00a0`dig  @`, do this for all coredns pod IPs. <p>-Use the same test pod to test their upstream nameservers (all 3, over a few retries),\u00a0`dig  \u00a0@` <p>[ https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/dns]</p> <p>Note: \u00a0In an air-gap environment, Swiss-army-knife is not available. You can try a specific busy box image with network tools like busybox image v1.28.</p> <ul> <li>Run the overlay test mentioned in the Rancher documentation to test pod-to-pod communication. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Overlay network test steps test the pod to pod connectivity between the nodes \u00a0:https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/networking#check-if-overlay-network-is-functioning-correctly.</li> </ul> <p>[Note: This overlay test performs the pod-to-pod communication using ICMP protocol, which means you will still see networking issues because TCP communication might be blocked even though the test passes. So you have to test with good network tools like NC and iperf.]</p> <ul> <li>Check the Infra VMS \u00a0knowns issues and overlay network ports are allowed at the switch level.</li> </ul> <p>e.g., In case of Vmware\u00a0vSphere version 6.7u2.</p> <ol> <li>Change the VXLAN port to 8472 (when NSX is not used) or 4789 (when NSX is used)</li> <li>Disable the VXLAN hardware offload feature on the VMXNET3 NIC (which recent Linux driver version enable by default.\u00a0 [https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer\u00a0PR 2766401 , https://github.com/projectcalico/calico/issues/4727\u00a0]</li> </ol>"},{"location":"000020831/#additional-information","title":"Additional Information","text":"<p>Reference Artiles&amp; Links:</p> <p>https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer\u00a0PR 2766401</p> <p>https://github.com/projectcalico/calico/issues/4727</p>"},{"location":"000020831/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020834/","title":"How to define additional static pods on an RKE2 cluster node","text":"<p>This document (000020834) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020834/#situation","title":"Situation","text":"<p>Is it possible to define additional static Pods to start on an RKE2 during RKE2 node initialization?</p>"},{"location":"000020834/#resolution","title":"Resolution","text":"<p>Yes, you can define additional static Pods on an RKE2 host by placing the manifests into the directory /var/lib/rancher/rke2/agent/pod-manifests/ Any Pod manifests within this directory will be created by the kubelet, as static pods, during the RKE2 node agent startup.</p>"},{"location":"000020834/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020859/","title":"Resolving a fleet-agent that is stuck in the Pending-Upgrade state","text":"<p>This document (000020859) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020859/#situation","title":"Situation","text":"<p>The fleet-agent is stuck in a \"Pending-Upgrade\" state and showing the following error:</p> <pre><code>time=\"2022-07-19T16:12:08Z\" level=info msg=\"preparing upgrade for fleet-agent-c-97hcq\" time=\"2022-07-19T16:12:08Z\" level=info msg=\"getting history for release fleet-agent-c-97hcq\" time=\"2022-07-19T16:12:08Z\" level=error msg=\"error syncing 'cluster-fleet-default-c-97hcq-86145229ab95/fleet-agent-c-97hcq': handler bundle-deploy: another operation (install/upgrade/rollback) is in progress, requeuing\"\n</code></pre>"},{"location":"000020859/#resolution","title":"Resolution","text":"<p>Run the following command against the cluster where the fleet-agent is running:</p> <pre><code>kubectl get secret -A -l status=pending-upgrade\n</code></pre> <p>It will show the output of a secret that is causing the pending-upgrade state as follows:</p> <pre><code>NAMESPACE             NAME                                        TYPE                 DATA   AGE\ncattle-fleet-system   sh.helm.release.v1.fleet-agent-c-97hcq.v2   helm.sh/release.v1   1      132d\n</code></pre> <p>Based on the above output above, run through the following steps:</p> <p>1. Backup the yaml (and save it to a persistent location) for the fleet secret that is causing the pending-upgrade state:</p> <pre><code>kubectl get secret -n cattle-fleet-system sh.helm.release.v1.fleet-agent-c-97hcq.v2 -oyaml &gt; fleet-agent-c-97hcq.yaml\n</code></pre> <p>2. Delete the secret:</p> <pre><code>kubectl delete secret -n cattle-fleet-system sh.helm.release.v1.fleet-agent-c-97hcq.v2\n</code></pre> <p>3. In the Rancher UI, go to Continuous Delivery and do a \"Force Update\" on the downstream cluster in question</p>"},{"location":"000020859/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020895/","title":"Adding tolerations to components in the Rancher Logging chart","text":"<p>This document (000020895) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020895/#environment","title":"Environment","text":"<p>- Rancher v2.5.x, v2.6.x or v2.7.x</p> <p>- Rancher Logging Chart</p> <p>- Taints defined on cluster nodes</p>"},{"location":"000020895/#situation","title":"Situation","text":"<p>When deploying the Rancher logging chart, Pods are not scheduled, where user-added taints\u00a0are present on cluster nodes, and tolerations are not set on the rancher-logging chart. If only some nodes within the cluster are tainted, logs will be missing from those nodes. If all nodes are tainted, no logs will be forwarded and Pods for Deployments within the Rancher Logging will fail to schedule with an error of the following format:</p> <pre><code>Events:\nType Reason Age From Message\n---- ------ ---- ---- -------\nWarning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate\n\n</code></pre>"},{"location":"000020895/#resolution","title":"Resolution","text":"<p>If there are user-added\u00a0taints on nodes within the cluster, tolerations for these taints must be added in the rancher-logging chart via the tolerations value, alongside the default\u00a0cattle.io/os=linux\u00a0NoSchedule toleration.</p> <p>For example, if the taint with key=foo, value=bar and effect=NoSchedule is present on nodes within the cluster, the following tolerations should be defined in the values of the rancher-logging Chart:</p> <pre><code>tolerations:\n  - key: cattle.io/os\n    operator: \"Equal\"\n    value: \"linux\"\n    effect: NoSchedule\n\u00a0 - key: foo\n\u00a0   operator: \"Equal\"\n\u00a0   value: \"bar\"\n\u00a0   effect: NoSchedule\n</code></pre> <pre><code>\n\n</code></pre>"},{"location":"000020895/#cause","title":"Cause","text":"<p>Nodes with specific taints will deny the scheduling of any pod if no matching toleration is present on the workload.</p>"},{"location":"000020895/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020907/","title":"Cannot provision new RKE Cluster from Template: Unable to validate S3 backup target configuration","text":"<p>This document (000020907) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020907/#environment","title":"Environment","text":"<ul> <li>Suse Rancher 2.5.9</li> <li>Air-gapped environment</li> <li>1 or more RKE downstream clusters</li> <li>RKE template configured for the downstream clusters</li> </ul>"},{"location":"000020907/#situation","title":"Situation","text":""},{"location":"000020907/#the-provisioning-of-an-rke-downstream-cluster-fails-after-adding-extra_args-for-the-kubelet-service-to-a-new-rke-template-and-using-this-newly-created-template-to-provision-the-rke-cluster","title":"The provisioning of an RKE downstream cluster fails after adding extra_args for the kubelet service to a new RKE template and using this newly created template to provision the RKE cluster.","text":"<p>Error:</p> <pre><code>Error message while creating a new RKE cluster from the new RKE Template:\nUnable to validate S3 backup target configuration: Get\u00a0http://169.254.169.254/latest/meta-data/iam/security-credentials/\": dial tcp 169.254.169.254:80: i/o timeout\n</code></pre> <p>Configuration:</p> <pre><code>E.g. extra_args for the kubelet added to the RKE Template:\n\n    kubelet:\n      ...\n      extra_args:\n        ...\n        image-gc-high-threshold: '80'\n        image-gc-low-threshold: '75'\n</code></pre> <pre><code>\n\n</code></pre>"},{"location":"000020907/#resolution","title":"Resolution","text":"<p>Verify if the RKE Template has configurable questions:</p> <ul> <li>Check in\u00a0the RKE Template if there is any setting set as a configurable question for the end-user</li> <li>Go to Cluster Management =&gt; RKE1 Configuration =&gt; RKE Templates</li> <li>Select your desired RKE Template, and click on the 3-dot menu far on the right menu, select \"Clone revision\"</li> <li>Go to the \"Cluster Option Overrides\" section lower down on the cloned template, and uncheck any available options.\u00a0After removing these configurable questions,\u00a0\u00a0the value set in the template is used without user entry.</li> <li>Set a Name for the new Template and click on Save. Select the newly created template and set it as Default Template if this is going to be your default template.</li> </ul>"},{"location":"000020907/#cause","title":"Cause","text":"<p>Configurable questions on the RKE template would require the end-user to answer those questions to set up a new RKE cluster and won't allow a new RKE cluster to be deployed automatically.</p>"},{"location":"000020907/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020909/","title":"How to enable fluent-bit debug logging","text":"<p>This document (000020909) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020909/#environment","title":"Environment","text":"<p>Rancher with Rancher-Logging chart v2</p>"},{"location":"000020909/#situation","title":"Situation","text":"<p>When troubleshooting the fluent-bit log level might need to be increased to debug level.</p>"},{"location":"000020909/#resolution","title":"Resolution","text":"<p>Fluent-bit debug level logging can be changed by modifying the logLevel spec of the logging CRD</p> <p>The following steps will help enable or disable debug-level logging for fluent-bit:</p> <p>1.\u00a0Edit rancher logging CRD</p> <pre><code>&gt; kubectl edit logging rancher-logging-root\n</code></pre> <p>2. Add logLevel debug to the fluentbit spec</p> <pre><code>spec:\ncontrolNamespace: cattle-logging-system\nfluentbit:\n    image:\n      repository: rancher/mirrored-fluent-fluent-bit\n      tag: 1.9.3\n    logLevel: debug\n</code></pre> <p>3. Confirm log level is set to debug</p> <pre><code>&gt; kubectl get pods -n cattle-logging-system -l app.kubernetes.io/name=fluentbit NAME READY STATUS RESTARTS AGE rancher-logging-root-fluentbit-gqq8l 1/1 Running 0 19s\n\n&gt; kubectl logs --tail=10 -n cattle-logging-system rancher-logging-root-fluentbit-gqq8l\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1841\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=927\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1840\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1840\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=927\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=913\n</code></pre> <p>Notes:</p> <ul> <li>To reset the logging level to the default info, re-run the same steps and change the logLevel to info.</li> <li>The logLevel value is not exposed via the rancher-logging chart and cannot be configured persistently. As a result, the\u00a0change\u00a0will be overwritten and logLevel reset to the default of info after a rancher-logging chart\u00a0upgrade.</li> </ul> <p>Reference:</p> <ul> <li>https://banzaicloud.com/docs/one-eye/logging-operator/configuration/crds/v1beta1/fluentbit_types/#fluentbitspec-loglevel</li> </ul>"},{"location":"000020909/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"000020910/","title":"Downstream clusters in unavailable state after upgrade from Rancher v2.5 at v2.5.16 or above to Rancher v2.6 below v2.6.7","text":"<p>This document (000020910) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"000020910/#environment","title":"Environment","text":"<p>Rancher v2.5 at or above patch release v2.5.16 upgraded to Rancher v2.6 below patch release v2.6.7</p>"},{"location":"000020910/#situation","title":"Situation","text":"<p>After the upgrade of a Rancher v2.5 environment running patch release v2.5.16 or above, to Rancher v2.6 below patch release v2.6.7 (e.g. an upgrade from Rancher v2.5.16 to v2.6.6) downstream clusters are in an unavailable state within Rancher.</p> <p>Rancher Pod logs contain error messages of the following format:</p> <pre><code>2022/12/17 08:47:18 [ERROR] error syncing 'c-ayhjd': handler cluster-deploy: cluster context c-ayhjd is unavaiblable, requeuing\n2022/12/17 08:47:25 [ERROR] error syncing '_all_': handler user-controllers-controller: failed to start user controllers for cluster c-ayhjd: ClusterUnavailable 503: cluster not found\n</code></pre>"},{"location":"000020910/#resolution","title":"Resolution","text":"<p>To resolve the issue it is necessary to set the status.serviceAccountToken field on the cluster.management.cattle.io object for each downstream cluster from the service account token secret for the cluster. This can be done with the following BASH one-liner, with a kubeconfig sourced for the Rancher local cluster:</p> <pre><code>for cluster in $(kubectl get clusters.management.cattle.io --field-selector metadata.name!=local -o custom-columns=NAME:.metadata.name --no-headers); do echo $cluster; kubectl patch -v=9 cluster.management.cattle.io $cluster --type=merge -p \"{\\\"status\\\":{\\\"serviceAccountToken\\\":\\\"`kubectl -n cattle-global-data get secret -o jsonpath=\\\"{.items[?(@.metadata.ownerReferences[0].name==\\\\\"$cluster\\\\\")].data.credential}\\\"|base64 -d`\\\"}}\"; done\n</code></pre> <p>Next, edit the cluster.management.cattle.io resource in the Rancher local cluster, for each downstream cluster, to set the status of the ServiceAccountMigrated condition from True to Unknown. This action is taken to ensure that on upgrade to Rancher v2.6.7+ the secretAccountToken field is again removed and migrated to a secret. With a kubeconfig sourced for the Rancher local cluster, get the cluster IDs for all downstream clusters:</p> <pre><code>kubectl get clusters.management.cattle.io --field-selector metadata.name!=local -o custom-columns=NAME:.metadata.name --no-headers\n</code></pre> <p>One at a time for each cluster ID listed execute `kubectl edit cluster.management.cattle.io ` locate the condition with the type ServiceAccountMigrated in the status.conditions array, and update the status from \"True\" to \"Unknown\" per the following example: <pre><code>[...]\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:57Z\"\n\u00a0 \u00a0 status: \"True\"\n\u00a0 \u00a0 type: Updated\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:51Z\"\n\u00a0 \u00a0 status: \"Unknown\"\n\u00a0 \u00a0 type: ServiceAccountMigrated\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:57Z\"\n\u00a0 \u00a0 status: \"True\"\n\u00a0 \u00a0 type: GlobalAdminsSynced\n\u00a0 - lastUpdateTime: \"2023-01-04T12:17:40Z\"\n\u00a0 [...]\n</code></pre> <p>Finally, take a copy of the service account token secrets and then remove these, as they are no longer used and fresh secrets will be created upon upgrade to Rancher v2.6.7+.</p> <p>With a kubeconfig for the Rancher local cluster sourced, first take a copy of the service account token secret manifests, with tthe following bash one-liner:</p> <pre><code>for secret in `kubectl -n cattle-global-data get secrets -o name | grep \"cluster-serviceaccounttoken-\"`; do kubectl -n cattle-global-data get $secret -o yaml &gt;&gt; cluster-serviceaccounttoken-secrets.yaml; echo \"---\" &gt;&gt; cluster-serviceaccounttoken-secrets.yaml; done\n</code></pre> <p>Then with the Rancher local cluster kubeconfig still sourced, delete the secrets:</p> <pre><code>for secret in `kubectl -n cattle-global-data get secrets -o name | grep \"cluster-serviceaccounttoken-\"`; do kubectl -n cattle-global-data delete $secret; done\n</code></pre>"},{"location":"000020910/#cause","title":"Cause","text":"<p>In order to address CVE-2021-36782 the service account token used by Rancher to connect to the Kubernetes API Server of a downstream cluster was moved from the status.serviceAccountToken field of the cluster.management.cattle.io resource to a secret referenced by the status.serviceAccountTokenSecret field. This fix was introduced to Rancher v2.6 in patch release v2.6.7 and above; and to Rancher v2.5 in patch release v2.5.16 and above. Rancher versions v2.5.0 - v2.5.15 and v2.6.0 - v2.6.6 inclusive use the status.serviceAccountToken field to store and retrieve the service account token for downstream clusters.</p> <p>As a result, where a Rancher environment is upgraded from Rancher v2.5 at v2.5.16 or above (containing the fix), to Rancher v2.6 below patch release v2.6.7 (which does not contain the fix), the status.serviceAccountToken field will be missing from the cluster.management.cattle.io resource and Rancher will be unable to connect to existing downstream clusters.</p>"},{"location":"000020910/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"}]}