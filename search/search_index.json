{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Rancher KB Alt website This website provide another look and feel and a better search to the SUSE Rancher KB articles.","title":"Welcome to Rancher KB Alt website"},{"location":"#welcome-to-rancher-kb-alt-website","text":"This website provide another look and feel and a better search to the SUSE Rancher KB articles.","title":"Welcome to Rancher KB Alt website"},{"location":"000020059/","text":"Who creates user accounts on SUSE Rancher Hosted? This document (000020059) is provided subject to the disclaimer at the end of this document. Resolution When a new SUSE Rancher Hosted environment is created, it will have a default admin account and a password will be provided to you for that account. You'll be asked to change the admin password when logging in for the first time. It is the customer's responsibility to create new user accounts. Most customers leverage Rancher's external authentication provider . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Who creates user accounts on SUSE Rancher Hosted?"},{"location":"000020059/#who-creates-user-accounts-on-suse-rancher-hosted","text":"This document (000020059) is provided subject to the disclaimer at the end of this document.","title":"Who creates user accounts on SUSE Rancher Hosted?"},{"location":"000020059/#resolution","text":"When a new SUSE Rancher Hosted environment is created, it will have a default admin account and a password will be provided to you for that account. You'll be asked to change the admin password when logging in for the first time. It is the customer's responsibility to create new user accounts. Most customers leverage Rancher's external authentication provider .","title":"Resolution"},{"location":"000020059/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020061/","text":"Rancher Upgrade Checklist This document (000020061) is provided subject to the disclaimer at the end of this document. Situation Task This article details the high-level steps required when planning and performing a Rancher and/or Kubernetes upgrade. Planning The following are the high-level rules for planning a Rancher/Kubernetes/Docker upgrade. The support matrix gives a great idea of the versions that are certified by Rancher and work best together. The recommended order of upgrades is: Rancher, Kubernetes, and then Docker/OS All upgrades should be performed in a lab or non-prod environment first Please see the following recommendations when planning version upgrades: Rancher : Do no skip any minor version when upgrading. For example: when upgrading from v2.1.x -> v2.3.x we encourage upgrading v2.1.x -> v2.2.x -> v2.3.x Kubernetes : Avoid skipping minor versions, as this can increase the chances of an issue due to accumulated changes. For example: when upgrading from v1.13.x -> v1.16.x we encourage upgrading v1.13.x -> v1.14.x -> v1.15.x -> v1.16.x RKE : perform one major RKE version jump at a time For example: when upgrading from v0.1.x -> v1.1.0 instead do v0.1.x -> v0.2.x -> v.0.3.x -> v1.0.x -> v1.1.x Adding a grace period between upgrades is recommended For example: add 24 hours between upgrading Rancher, the local cluster, and downstream clusters It is not required, but it is recommended to pause any application deployments using the Rancher API during an upgrade Data collection Before you start your upgrade, please collect the following pieces of information to best prepare yourself in case you need to open a support ticket. Scheduled change window: Current Rancher version ( rancher/rancher image tag, or shown bottom left in the UI): Target Rancher version: Installation option (single install/HA): Current Kubernetes version of Rancher local cluster (use kubectl version ): Current Docker version (use docker version ): Rancher Pre-Upgrade Check if the Rancher UI is accessible Check if all clusters in UI are in an Active state Check if all pods in kube-system and cattle-system namespaces are running (in both Rancher and downstream clusters) kubectl get pods -n kube-system kubectl get pods -n cattle-system Verify the datastore has scheduled snapshots configured, and these are working. RKE : if Rancher is deployed on a Kubernetes cluster built with RKE, verify etcd snapshots are enabled and working, on etcd nodes you can confirm with the following: bash ls -l /opt/rke/etcd-snapshots docker logs etcd-rolling-snapshots k3s : if Rancher is deployed on a k3s Kubernetes cluster, ensure scheduled backups are configured and working. Please see the k3s documentation pages for further information on this. Create a one-time datastore snapshot, please see the following documentation for RKE and k3s , and the single node Docker install options for more information RKE : check for expired/expiring Kubernetes certs for i in $(ls /etc/kubernetes/ssl/*.pem|grep -v key); do echo -n $i\" \"; openssl x509 -startdate -enddate -noout -in $i | grep 'notAfter='; done Rancher Upgrade steps The Rancher upgrade process is detailed in the upgrade documentation for both HA , and single node using Docker . Rancher Review/Verify After the upgrade is completed, go through the following checklist to verify your environment is in working order. Check if the Rancher UI is accessible You should be able to login into Rancher, view clusters, and browse to workloads Verify the Rancher version has changed in UI After logging into Rancher, review the version in the bottom left corner of the page Check if all clusters in UI are in an Active state Check if all pods in kube-system and cattle-system are running (in both Rancher and downstream clusters) Check the cattle-cluster-agent and cattle-node-agent pods are running in all downstream clusters and running the latest version The rollout of the updated agent versions can take some time if there are a lot of downstream clusters or nodes Create a one-time datastore snapshot, please see the following documentation for RKE and k3s , and the single node Docker install options for more information Rancher Rollback steps The Rancher rollback process is detailed in the rollback documentation , please follow the relevant link for Rancher installed on a Kubernetes cluster, or Docker Follow-up tasks (optional) Upgrade the Rancher management cluster, this is often a follow-up to the Rancher upgrade. Please see the RKE and k3s upgrade documentation for the upgrade process, as mentioned it is best to avoid skipping minor versions Upgrade the downstream clusters, please see the documentation for more information. A snapshot of both the local and downstream clusters before the upgrade is recommended to provides the maximum amount of recoverability options in the event of a rollback Docker/OS upgrades, please our article on performing rolling changes to nodes Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher Upgrade Checklist"},{"location":"000020061/#rancher-upgrade-checklist","text":"This document (000020061) is provided subject to the disclaimer at the end of this document.","title":"Rancher Upgrade Checklist"},{"location":"000020061/#situation","text":"","title":"Situation"},{"location":"000020061/#task","text":"This article details the high-level steps required when planning and performing a Rancher and/or Kubernetes upgrade.","title":"Task"},{"location":"000020061/#planning","text":"The following are the high-level rules for planning a Rancher/Kubernetes/Docker upgrade. The support matrix gives a great idea of the versions that are certified by Rancher and work best together. The recommended order of upgrades is: Rancher, Kubernetes, and then Docker/OS All upgrades should be performed in a lab or non-prod environment first Please see the following recommendations when planning version upgrades: Rancher : Do no skip any minor version when upgrading. For example: when upgrading from v2.1.x -> v2.3.x we encourage upgrading v2.1.x -> v2.2.x -> v2.3.x Kubernetes : Avoid skipping minor versions, as this can increase the chances of an issue due to accumulated changes. For example: when upgrading from v1.13.x -> v1.16.x we encourage upgrading v1.13.x -> v1.14.x -> v1.15.x -> v1.16.x RKE : perform one major RKE version jump at a time For example: when upgrading from v0.1.x -> v1.1.0 instead do v0.1.x -> v0.2.x -> v.0.3.x -> v1.0.x -> v1.1.x Adding a grace period between upgrades is recommended For example: add 24 hours between upgrading Rancher, the local cluster, and downstream clusters It is not required, but it is recommended to pause any application deployments using the Rancher API during an upgrade","title":"Planning"},{"location":"000020061/#data-collection","text":"Before you start your upgrade, please collect the following pieces of information to best prepare yourself in case you need to open a support ticket. Scheduled change window: Current Rancher version ( rancher/rancher image tag, or shown bottom left in the UI): Target Rancher version: Installation option (single install/HA): Current Kubernetes version of Rancher local cluster (use kubectl version ): Current Docker version (use docker version ):","title":"Data collection"},{"location":"000020061/#rancher-pre-upgrade","text":"Check if the Rancher UI is accessible Check if all clusters in UI are in an Active state Check if all pods in kube-system and cattle-system namespaces are running (in both Rancher and downstream clusters) kubectl get pods -n kube-system kubectl get pods -n cattle-system Verify the datastore has scheduled snapshots configured, and these are working. RKE : if Rancher is deployed on a Kubernetes cluster built with RKE, verify etcd snapshots are enabled and working, on etcd nodes you can confirm with the following: bash ls -l /opt/rke/etcd-snapshots docker logs etcd-rolling-snapshots k3s : if Rancher is deployed on a k3s Kubernetes cluster, ensure scheduled backups are configured and working. Please see the k3s documentation pages for further information on this. Create a one-time datastore snapshot, please see the following documentation for RKE and k3s , and the single node Docker install options for more information RKE : check for expired/expiring Kubernetes certs for i in $(ls /etc/kubernetes/ssl/*.pem|grep -v key); do echo -n $i\" \"; openssl x509 -startdate -enddate -noout -in $i | grep 'notAfter='; done","title":"Rancher Pre-Upgrade"},{"location":"000020061/#rancher-upgrade-steps","text":"The Rancher upgrade process is detailed in the upgrade documentation for both HA , and single node using Docker .","title":"Rancher Upgrade steps"},{"location":"000020061/#rancher-reviewverify","text":"After the upgrade is completed, go through the following checklist to verify your environment is in working order. Check if the Rancher UI is accessible You should be able to login into Rancher, view clusters, and browse to workloads Verify the Rancher version has changed in UI After logging into Rancher, review the version in the bottom left corner of the page Check if all clusters in UI are in an Active state Check if all pods in kube-system and cattle-system are running (in both Rancher and downstream clusters) Check the cattle-cluster-agent and cattle-node-agent pods are running in all downstream clusters and running the latest version The rollout of the updated agent versions can take some time if there are a lot of downstream clusters or nodes Create a one-time datastore snapshot, please see the following documentation for RKE and k3s , and the single node Docker install options for more information","title":"Rancher Review/Verify"},{"location":"000020061/#rancher-rollback-steps","text":"The Rancher rollback process is detailed in the rollback documentation , please follow the relevant link for Rancher installed on a Kubernetes cluster, or Docker","title":"Rancher Rollback steps"},{"location":"000020061/#follow-up-tasks-optional","text":"Upgrade the Rancher management cluster, this is often a follow-up to the Rancher upgrade. Please see the RKE and k3s upgrade documentation for the upgrade process, as mentioned it is best to avoid skipping minor versions Upgrade the downstream clusters, please see the documentation for more information. A snapshot of both the local and downstream clusters before the upgrade is recommended to provides the maximum amount of recoverability options in the event of a rollback Docker/OS upgrades, please our article on performing rolling changes to nodes","title":"Follow-up tasks (optional)"},{"location":"000020061/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020062/","text":"How to rotate the Rancher SSL certificate with a single node Docker installation This document (000020062) is provided subject to the disclaimer at the end of this document. Situation Task One installation method for Rancher 2.x is to run Rancher in a Docker container on a single node . This approach is designed for a short-lived development/test environment and bundles a minimal footprint of all the components needed by Rancher into the container image. When the default self-signed SSL certificate option is used, the lifetime of the SSL certificate is 1 year. If the container is run for a long period the certificate will need to be rotated. The below sections provide steps needed to rotate the certificate for different versions of Rancher. Pre-requisites A Rancher v2.x single node Docker installation Access to the node where Rancher is running to run Docker commands A backup of the Rancher container Resolution To perform the certificate rotation, please ensure a backup of the Rancher container has been completed, this can be used as a rollback in the event any previous data needs to be restored. The process is different between different versions of Rancher, please select your version below as needed and set the container ID of the Rancher container. Rancher v2.4.x and above If the certificate is expiring in less than 90 days, certificate rotation occurs automatically. When expiry falls within this period, certificates will be rotated on the next start of the Rancher container. rancher_container_id=xxx docker restart ${rancher_container_id} Rancher v2.3.x rancher_container_id=xxx docker exec -ti ${rancher_container_id} bash cp -rp /var/lib/rancher/k3s/server/tls /var/lib/rancher/k3s/server/tls.backup cd /var/lib/rancher/k3s/server/tls rm -rf *.crt *.key temporary-certs/ cp -p /var/lib/rancher/k3s/server/tls.backup/*-ca.* . exit docker restart ${rancher_container_id} Rancher v2.2.x rancher_container_id=xxx docker exec ${rancher_container_id} mv /var/lib/rancher/management-state/tls/localhost.crt /var/lib/rancher/management-state/tls/localhost.crt.backup docker exec ${rancher_container_id} mv /var/lib/rancher/management-state/tls/localhost.key /var/lib/rancher/management-state/tls/localhost.key.backup docker restart ${rancher_container_id} Rancher v2.0.14+, v2.1.9+ rancher_container_id=xxx docker exec ${rancher_container_id} mv /var/lib/rancher/management-state/certs/bundle.json /var/lib/rancher/management-state/certs/bundle.json.backup docker restart ${rancher_container_id} Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to rotate the Rancher SSL certificate with a single node Docker installation"},{"location":"000020062/#how-to-rotate-the-rancher-ssl-certificate-with-a-single-node-docker-installation","text":"This document (000020062) is provided subject to the disclaimer at the end of this document.","title":"How to rotate the Rancher SSL certificate with a single node Docker installation"},{"location":"000020062/#situation","text":"","title":"Situation"},{"location":"000020062/#task","text":"One installation method for Rancher 2.x is to run Rancher in a Docker container on a single node . This approach is designed for a short-lived development/test environment and bundles a minimal footprint of all the components needed by Rancher into the container image. When the default self-signed SSL certificate option is used, the lifetime of the SSL certificate is 1 year. If the container is run for a long period the certificate will need to be rotated. The below sections provide steps needed to rotate the certificate for different versions of Rancher.","title":"Task"},{"location":"000020062/#pre-requisites","text":"A Rancher v2.x single node Docker installation Access to the node where Rancher is running to run Docker commands A backup of the Rancher container","title":"Pre-requisites"},{"location":"000020062/#resolution","text":"To perform the certificate rotation, please ensure a backup of the Rancher container has been completed, this can be used as a rollback in the event any previous data needs to be restored. The process is different between different versions of Rancher, please select your version below as needed and set the container ID of the Rancher container.","title":"Resolution"},{"location":"000020062/#rancher-v24x-and-above","text":"If the certificate is expiring in less than 90 days, certificate rotation occurs automatically. When expiry falls within this period, certificates will be rotated on the next start of the Rancher container. rancher_container_id=xxx docker restart ${rancher_container_id}","title":"Rancher v2.4.x and above"},{"location":"000020062/#rancher-v23x","text":"rancher_container_id=xxx docker exec -ti ${rancher_container_id} bash cp -rp /var/lib/rancher/k3s/server/tls /var/lib/rancher/k3s/server/tls.backup cd /var/lib/rancher/k3s/server/tls rm -rf *.crt *.key temporary-certs/ cp -p /var/lib/rancher/k3s/server/tls.backup/*-ca.* . exit docker restart ${rancher_container_id}","title":"Rancher v2.3.x"},{"location":"000020062/#rancher-v22x","text":"rancher_container_id=xxx docker exec ${rancher_container_id} mv /var/lib/rancher/management-state/tls/localhost.crt /var/lib/rancher/management-state/tls/localhost.crt.backup docker exec ${rancher_container_id} mv /var/lib/rancher/management-state/tls/localhost.key /var/lib/rancher/management-state/tls/localhost.key.backup docker restart ${rancher_container_id}","title":"Rancher v2.2.x"},{"location":"000020062/#rancher-v2014-v219","text":"rancher_container_id=xxx docker exec ${rancher_container_id} mv /var/lib/rancher/management-state/certs/bundle.json /var/lib/rancher/management-state/certs/bundle.json.backup docker restart ${rancher_container_id}","title":"Rancher v2.0.14+, v2.1.9+"},{"location":"000020062/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020063/","text":"How to increase the log level for Calico components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster This document (000020063) is provided subject to the disclaimer at the end of this document. Situation Task During network troubleshooting it may be useful to increase the log level of the Calico components. This article details how to set verbose debug-level Calico component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters. Pre-requisites A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Calico Network Provider Resolution N.B. As these instructions involve editing the Calico DaemonSet directly, the change will not persist cluster update events, i.e. invocations of rke up for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting. Via the Rancher UI For a Rancher v2.x managed cluster, the Calico component log level can be adjusted via the Rancher UI, per the following process: Navigate to the System project of the relevant cluster within the Rancher UI. Locate the calico DaemonSet workload within the kube-system namespace, click the vertical elipses ( \u22ee ) and select Edit. Click to Edit the calico-node container. Add CALICO_STARTUP_LOGLEVEL = DEBUG , F\u200bELIX_LOGSEVERITYSCREEN = Debug , BGP_LOGSEVERITYSCREEN = Debug in Environment Variables section and click Save . Via kubectl With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Calico component log level can be adjusted via kubectl, per the following process: Run kubectl -n kube-system edit daemonset calico-node . In the env definition for the calico-node container add an environment variable with the name CALICO_STARTUP_LOGLEVEL and value DEBUG , F\u200bELIX_LOGSEVERITYSCREEN and value Debug and BGP_LOGSEVERITYSCREEN and value Debug , e.g.: [...] containers: - env: [...] - name: CALICO_STARTUP_LOGLEVEL value: DEBUG - name: BGP_LOGSEVERITYSCREEN value: Debug - name: FELIX_LOGSEVERITYSCREEN value: Debug [...] Save the file. Further reading Calico configuration documentation Calico component logs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to increase the log level for Calico components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020063/#how-to-increase-the-log-level-for-calico-components-in-a-rancher-kubernetes-engine-rke-or-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020063) is provided subject to the disclaimer at the end of this document.","title":"How to increase the log level for Calico components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020063/#situation","text":"","title":"Situation"},{"location":"000020063/#task","text":"During network troubleshooting it may be useful to increase the log level of the Calico components. This article details how to set verbose debug-level Calico component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","title":"Task"},{"location":"000020063/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Calico Network Provider","title":"Pre-requisites"},{"location":"000020063/#resolution","text":"N.B. As these instructions involve editing the Calico DaemonSet directly, the change will not persist cluster update events, i.e. invocations of rke up for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting.","title":"Resolution"},{"location":"000020063/#via-the-rancher-ui","text":"For a Rancher v2.x managed cluster, the Calico component log level can be adjusted via the Rancher UI, per the following process: Navigate to the System project of the relevant cluster within the Rancher UI. Locate the calico DaemonSet workload within the kube-system namespace, click the vertical elipses ( \u22ee ) and select Edit. Click to Edit the calico-node container. Add CALICO_STARTUP_LOGLEVEL = DEBUG , F\u200bELIX_LOGSEVERITYSCREEN = Debug , BGP_LOGSEVERITYSCREEN = Debug in Environment Variables section and click Save .","title":"Via the Rancher UI"},{"location":"000020063/#via-kubectl","text":"With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Calico component log level can be adjusted via kubectl, per the following process: Run kubectl -n kube-system edit daemonset calico-node . In the env definition for the calico-node container add an environment variable with the name CALICO_STARTUP_LOGLEVEL and value DEBUG , F\u200bELIX_LOGSEVERITYSCREEN and value Debug and BGP_LOGSEVERITYSCREEN and value Debug , e.g.: [...] containers: - env: [...] - name: CALICO_STARTUP_LOGLEVEL value: DEBUG - name: BGP_LOGSEVERITYSCREEN value: Debug - name: FELIX_LOGSEVERITYSCREEN value: Debug [...] Save the file.","title":"Via kubectl"},{"location":"000020063/#further-reading","text":"Calico configuration documentation Calico component logs","title":"Further reading"},{"location":"000020063/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020064/","text":"The Rancher v2.x Windows log collector script This document (000020064) is provided subject to the disclaimer at the end of this document. Situation Rancher v2.x Windows worker node log collection Logs can be collected from a Windows worker node within a Rancher v2.x cluster using the Rancher v2.x Windows worker node log collection script. N.B. The script needs to be downloaded and run directly on a Windows worker node using a Powershell session with Administrator Privileges. To run the script, open a new Powershell window with Administrator Privileges and run the following command: Set-ExecutionPolicy Bypass | . {iwr -useb https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/windows-log-collector/win-log-collect.ps1} | iex Upon successful completion, the log bundle will be written to the root of the C:\\ drive in a file named rancher_<hostname>_<datetime>.tar.gz . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"The Rancher v2.x Windows log collector script"},{"location":"000020064/#the-rancher-v2x-windows-log-collector-script","text":"This document (000020064) is provided subject to the disclaimer at the end of this document.","title":"The Rancher v2.x Windows log collector script"},{"location":"000020064/#situation","text":"","title":"Situation"},{"location":"000020064/#rancher-v2x-windows-worker-node-log-collection","text":"Logs can be collected from a Windows worker node within a Rancher v2.x cluster using the Rancher v2.x Windows worker node log collection script. N.B. The script needs to be downloaded and run directly on a Windows worker node using a Powershell session with Administrator Privileges. To run the script, open a new Powershell window with Administrator Privileges and run the following command: Set-ExecutionPolicy Bypass | . {iwr -useb https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/windows-log-collector/win-log-collect.ps1} | iex Upon successful completion, the log bundle will be written to the root of the C:\\ drive in a file named rancher_<hostname>_<datetime>.tar.gz .","title":"Rancher v2.x Windows worker node log collection"},{"location":"000020064/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020065/","text":"401 Unauthorized error using the Rancher2 Terraform Provider v1.4.0 with `access_key` and `secret_key` authentication This document (000020065) is provided subject to the disclaimer at the end of this document. Situation Issue Executing Terraform actions against Rancher v2.x, with the Rancher2 Terraform Provider v1.4.0, results in an error of the following format, when using access_key and secret_key authentication: Error: Bad response statusCode [401]. Status [401 Unauthorized]. Body: [message=must authenticate] from [https://xyz.rancher.com/v3] Pre-requisites A Rancher v2.x instance The Rancher2 Terraform Provider v1.4.0 Rancher2 Provider authentication with separate access_key and secret_key Workaround To workaround the issue use token_key authentication. Resolution The issue was fixed in Rancher2 Terraform Provider v1.4.1, so you should upgrade to a later version of the provider. Further reading Rancher2 Terraform Provider Documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"401 Unauthorized error using the Rancher2 Terraform Provider v1.4.0 with \\`access\\_key\\` and \\`secret\\_key\\` authentication"},{"location":"000020065/#401-unauthorized-error-using-the-rancher2-terraform-provider-v140-with-access_key-and-secret_key-authentication","text":"This document (000020065) is provided subject to the disclaimer at the end of this document.","title":"401 Unauthorized error using the Rancher2 Terraform Provider v1.4.0 with `access_key` and `secret_key` authentication"},{"location":"000020065/#situation","text":"","title":"Situation"},{"location":"000020065/#issue","text":"Executing Terraform actions against Rancher v2.x, with the Rancher2 Terraform Provider v1.4.0, results in an error of the following format, when using access_key and secret_key authentication: Error: Bad response statusCode [401]. Status [401 Unauthorized]. Body: [message=must authenticate] from [https://xyz.rancher.com/v3]","title":"Issue"},{"location":"000020065/#pre-requisites","text":"A Rancher v2.x instance The Rancher2 Terraform Provider v1.4.0 Rancher2 Provider authentication with separate access_key and secret_key","title":"Pre-requisites"},{"location":"000020065/#workaround","text":"To workaround the issue use token_key authentication.","title":"Workaround"},{"location":"000020065/#resolution","text":"The issue was fixed in Rancher2 Terraform Provider v1.4.1, so you should upgrade to a later version of the provider.","title":"Resolution"},{"location":"000020065/#further-reading","text":"Rancher2 Terraform Provider Documentation","title":"Further reading"},{"location":"000020065/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020066/","text":"nginx-ingress-controller pods failing to load configuration with \"client intended to send too large body\" error in nginx-ingress-controller < nginx-0.32.0-rancher1 This document (000020066) is provided subject to the disclaimer at the end of this document. Situation Issue nginx-ingress-controller pods fail to load configuration successfully, resulting in failure for some Ingress resources. Logs of the nginx-ingress-controller pods reveal error messages of the following format: 2020-09-22T19:06:19.696272452Z 2020/09/22 19:06:19 [error] 5832#5832: *28190476 client intended to send too large body: 10696855 bytes, client: unix:, server: , request: \"POST /configuration/servers HTTP/1.1\", host: \"nginx-status\" 2020-09-22T19:06:19.718950185Z W0922 19:06:19.718851 8 controller.go:176] Dynamic reconfiguration failed: unexpected error code: 413 The nginx-ingress-controller dynamically updates its configuration by POST'ing the data to the /configuration endpoint. In nginx-ingress-controller versions lower than nginx-0.26.0 the client_max_body_size for this endpoint is hardcoded to 10m . As a result, if the configuration data is greater than 10m the request will fail (in the log entry above the configuration body is 10696855 bytes, which is equal to ~10.2m ). The configuration request will be repeatedly retried, resulting in increased CPU usage by the nginx-ingress-controller pods. Pre-requisites A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster nginx-ingress-controller version lower than nginx-0.32.0-rancher1 Workaround Remove some Ingress resources in the cluster to reduce the configuration size below the 10m limit. N.B. To remove recently added Ingress resources that pushed the configuration size over the limit of 10m, check the age of the Ingresses. Resolution In ingress-nginx versions 0.26.0 and above, the client_max_body_size for the /configuration endpoint is dynamic . To take advantage of this fix, upgrade the Kubernetes version of the cluster to one of the below or later, which use nginx-ingress-controller:nginx-0.32.0-rancher1 or above: v1.15.12-rancher1-1 v1.16.10-rancher2-1 v1.17.11-rancher1-1 v1.18.3-rancher2-1 RKE CLI and Rancher v2.x provisioned Kubernetes clusters, with Kubernetes v1.19+ run a higher version of the ingress-nginx , which also includes the fix. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"nginx-ingress-controller pods failing to load configuration with \"client intended to send too large body\" error in nginx-ingress-controller < nginx-0.32.0-rancher1"},{"location":"000020066/#nginx-ingress-controller-pods-failing-to-load-configuration-with-client-intended-to-send-too-large-body-error-in-nginx-ingress-controller-nginx-0320-rancher1","text":"This document (000020066) is provided subject to the disclaimer at the end of this document.","title":"nginx-ingress-controller pods failing to load configuration with \"client intended to send too large body\" error in nginx-ingress-controller &lt; nginx-0.32.0-rancher1"},{"location":"000020066/#situation","text":"","title":"Situation"},{"location":"000020066/#issue","text":"nginx-ingress-controller pods fail to load configuration successfully, resulting in failure for some Ingress resources. Logs of the nginx-ingress-controller pods reveal error messages of the following format: 2020-09-22T19:06:19.696272452Z 2020/09/22 19:06:19 [error] 5832#5832: *28190476 client intended to send too large body: 10696855 bytes, client: unix:, server: , request: \"POST /configuration/servers HTTP/1.1\", host: \"nginx-status\" 2020-09-22T19:06:19.718950185Z W0922 19:06:19.718851 8 controller.go:176] Dynamic reconfiguration failed: unexpected error code: 413 The nginx-ingress-controller dynamically updates its configuration by POST'ing the data to the /configuration endpoint. In nginx-ingress-controller versions lower than nginx-0.26.0 the client_max_body_size for this endpoint is hardcoded to 10m . As a result, if the configuration data is greater than 10m the request will fail (in the log entry above the configuration body is 10696855 bytes, which is equal to ~10.2m ). The configuration request will be repeatedly retried, resulting in increased CPU usage by the nginx-ingress-controller pods.","title":"Issue"},{"location":"000020066/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster nginx-ingress-controller version lower than nginx-0.32.0-rancher1","title":"Pre-requisites"},{"location":"000020066/#workaround","text":"Remove some Ingress resources in the cluster to reduce the configuration size below the 10m limit. N.B. To remove recently added Ingress resources that pushed the configuration size over the limit of 10m, check the age of the Ingresses.","title":"Workaround"},{"location":"000020066/#resolution","text":"In ingress-nginx versions 0.26.0 and above, the client_max_body_size for the /configuration endpoint is dynamic . To take advantage of this fix, upgrade the Kubernetes version of the cluster to one of the below or later, which use nginx-ingress-controller:nginx-0.32.0-rancher1 or above: v1.15.12-rancher1-1 v1.16.10-rancher2-1 v1.17.11-rancher1-1 v1.18.3-rancher2-1 RKE CLI and Rancher v2.x provisioned Kubernetes clusters, with Kubernetes v1.19+ run a higher version of the ingress-nginx , which also includes the fix.","title":"Resolution"},{"location":"000020066/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020067/","text":"Logs not forwarded by Rancher Logging in Rancher v2.x when Docker daemon logging driver is not set to json-file This document (000020067) is provided subject to the disclaimer at the end of this document. Situation Issue The Rancher v2.x Logging feature enables you to configure log forwarding for Pods, as well as system component containers, in a cluster to a logging endpoint such as Elasticsearch or Splunk. This feature works by deploying a workload to each node in the cluster that mounts the container log directory from the host to parse the Docker container json log files. This is dependent upon use of the json-file Docker logging driver . In the event that the Docker daemon is configured with an alternative logging driver, the logging feature will be unable to parse the logs and will not forward these. In CentOS and RHEL packaged Docker 1.13.1, the default log driver configured is journald, which will prevent log forwarding functioning. Meanwhile, whilst json-file is the default log driver in the upstream Docker packages, if an alternative has been configured on nodes this will also prevent the correct functioning of the log forwarding. You can verify the currently configured Docker logging driver on a node by running docker info | grep Logging , which will show output of the following format: Logging Driver: journald . In the event that json-file is not the configured logging driver, the output of ls -la /var/log/containers/ on the node should also be empty. With json-file configured this would display symoblic links to paths under /var/log/pods , containing symbolic links which in turn point to the Docker container json log files. Pre-requisites Rancher v2.x managed cluster with Rancher logging enabled Resolution CentOS or RHEL packaged Docker Update /etc/sysconfig/docker , as shown in the screenshot below, to set --log-driver=json-file instead of journald . Restart the Docker daemon: systemctl restart docker You should now see symlinked logs created under /var/log/containers Upstream Docker Configure the json-file Docker logging driver in /etc/docker/daemon.json per the Docker documentation Restart the Docker daemon: systemctl restart docker You should now see symlinked logs created under /var/log/containers Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Logs not forwarded by Rancher Logging in Rancher v2.x when Docker daemon logging driver is not set to json-file"},{"location":"000020067/#logs-not-forwarded-by-rancher-logging-in-rancher-v2x-when-docker-daemon-logging-driver-is-not-set-to-json-file","text":"This document (000020067) is provided subject to the disclaimer at the end of this document.","title":"Logs not forwarded by Rancher Logging in Rancher v2.x when Docker daemon logging driver is not set to json-file"},{"location":"000020067/#situation","text":"","title":"Situation"},{"location":"000020067/#issue","text":"The Rancher v2.x Logging feature enables you to configure log forwarding for Pods, as well as system component containers, in a cluster to a logging endpoint such as Elasticsearch or Splunk. This feature works by deploying a workload to each node in the cluster that mounts the container log directory from the host to parse the Docker container json log files. This is dependent upon use of the json-file Docker logging driver . In the event that the Docker daemon is configured with an alternative logging driver, the logging feature will be unable to parse the logs and will not forward these. In CentOS and RHEL packaged Docker 1.13.1, the default log driver configured is journald, which will prevent log forwarding functioning. Meanwhile, whilst json-file is the default log driver in the upstream Docker packages, if an alternative has been configured on nodes this will also prevent the correct functioning of the log forwarding. You can verify the currently configured Docker logging driver on a node by running docker info | grep Logging , which will show output of the following format: Logging Driver: journald . In the event that json-file is not the configured logging driver, the output of ls -la /var/log/containers/ on the node should also be empty. With json-file configured this would display symoblic links to paths under /var/log/pods , containing symbolic links which in turn point to the Docker container json log files.","title":"Issue"},{"location":"000020067/#pre-requisites","text":"Rancher v2.x managed cluster with Rancher logging enabled","title":"Pre-requisites"},{"location":"000020067/#resolution","text":"","title":"Resolution"},{"location":"000020067/#centos-or-rhel-packaged-docker","text":"Update /etc/sysconfig/docker , as shown in the screenshot below, to set --log-driver=json-file instead of journald . Restart the Docker daemon: systemctl restart docker You should now see symlinked logs created under /var/log/containers","title":"CentOS or RHEL packaged Docker"},{"location":"000020067/#upstream-docker","text":"Configure the json-file Docker logging driver in /etc/docker/daemon.json per the Docker documentation Restart the Docker daemon: systemctl restart docker You should now see symlinked logs created under /var/log/containers","title":"Upstream Docker"},{"location":"000020067/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020068/","text":"\"ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend\" error messages logged by the Docker daemon upon daemon startup This document (000020068) is provided subject to the disclaimer at the end of this document. Situation Issue During startup of the Docker daemon, an error message of the following format is present in the system logs: Jun 13 13:55:47 hostname container-storage-setup: ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend; consider different driver or separate volume or OS reprovision Pre-requisites Docker daemon with the overlay or overlay2 storage driver Resolution An xfs formatted filesystem is only supported as backing for the overlay or overlay2 Docker storage drivers if formatted with d_type set to true . The d_type value of an xfs filesystem can be verified with the xfs_info utility. Example output for this command can be found in the xfs_info man pages . If ftype=1 the filesystem was formatted with d_type true and the filesystem is suitable for use as backing for the overlay or overlay2 storage drivers. If the value is set to 0 the filesystem is not suitable for use with the overlay or overlay2 storage drivers, and would need to be reformated with the flag -n ftype=1 . Per the Docker documentation : \"Running on XFS without d_type support now causes Docker to skip the attempt to use the overlay or overlay2 driver. Existing installs will continue to run, but produce an error. This is to allow users to migrate their data. In a future version, this will be a fatal error, which will prevent Docker from starting.\" Further reading Docker documentation on the overlay and overlay2 storage drivers Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\"ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend\" error messages logged by the Docker daemon upon daemon startup"},{"location":"000020068/#error-xfs-filesystem-at-var-has-ftype0-cannot-use-overlay-backend-error-messages-logged-by-the-docker-daemon-upon-daemon-startup","text":"This document (000020068) is provided subject to the disclaimer at the end of this document.","title":"\"ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend\" error messages logged by the Docker daemon upon daemon startup"},{"location":"000020068/#situation","text":"","title":"Situation"},{"location":"000020068/#issue","text":"During startup of the Docker daemon, an error message of the following format is present in the system logs: Jun 13 13:55:47 hostname container-storage-setup: ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend; consider different driver or separate volume or OS reprovision","title":"Issue"},{"location":"000020068/#pre-requisites","text":"Docker daemon with the overlay or overlay2 storage driver","title":"Pre-requisites"},{"location":"000020068/#resolution","text":"An xfs formatted filesystem is only supported as backing for the overlay or overlay2 Docker storage drivers if formatted with d_type set to true . The d_type value of an xfs filesystem can be verified with the xfs_info utility. Example output for this command can be found in the xfs_info man pages . If ftype=1 the filesystem was formatted with d_type true and the filesystem is suitable for use as backing for the overlay or overlay2 storage drivers. If the value is set to 0 the filesystem is not suitable for use with the overlay or overlay2 storage drivers, and would need to be reformated with the flag -n ftype=1 . Per the Docker documentation : \"Running on XFS without d_type support now causes Docker to skip the attempt to use the overlay or overlay2 driver. Existing installs will continue to run, but produce an error. This is to allow users to migrate their data. In a future version, this will be a fatal error, which will prevent Docker from starting.\"","title":"Resolution"},{"location":"000020068/#further-reading","text":"Docker documentation on the overlay and overlay2 storage drivers","title":"Further reading"},{"location":"000020068/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020069/","text":"[JP] How to troubleshoot using the namespace of a container This document (000020069) is provided subject to the disclaimer at the end of this document. Situation \u80cc\u666f \u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u5834\u5408\u3001\u767a\u751f\u3057\u305f\u554f\u984c\u3068\u4e00\u81f4\u3059\u308b\u518d\u73fe\u74b0\u5883\u304c\u5fc5\u8981\u3067\u3059\u3002\u305f\u3060\u3057\u30b3\u30f3\u30c6\u30ca\u30fc\u74b0\u5883\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3067\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u30fc\u5185\u3067\u30c4\u30fc\u30eb\u3068\u30b7\u30a7\u30eb\u74b0\u5883\u304c\u7c21\u5358\u306b\u5229\u7528\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u518d\u73fe\u74b0\u5883\u306e\u69cb\u7bc9\u304c\u56f0\u96e3\u306b\u306a\u308a\u307e\u3059\u3002 \u624b\u9806 \u4e0a\u8a18\u306e\u8ab2\u984c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u4ee5\u4e0b\u4e8c\u3064\u306e\u624b\u9806\u304c\u3042\u308a\u307e\u3059\u3002 Sidecar container \u3092\u5229\u7528 \u554f\u984c\u3092\u6301\u3064\u30b3\u30f3\u30c6\u30ca\u30fc\u304c\u5c5e\u3059\u308bNamespace\u3067\u65b0\u3057\u3044sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u3053\u306esidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u7528\u3044\u3066\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u306f\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3057\u306a\u304c\u3089\u3001\u540c\u3058Network\u3068PID\u306eNamespace\u3092\u4f7f\u7528\u3057\u3066\u8d77\u52d5\u3067\u304d\u307e\u3059\u3002 \u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ID=<container ID or name> \u540c\u3058Network\u3001PID\u306eNamespace\u3068Volume\u3092\u4f7f\u7528\u3057sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh \u3053\u308c\u304b\u3089\u554f\u984c\u304c\u3042\u308bcontainer\u3084Pod\u3068\u540c\u3058\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5185\u3067\u3001alpine\u30b3\u30f3\u30c6\u30ca\u30fc\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001Pod\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001Sidecar Container\u306b\u5165\u308a\u3001\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u306a\u304c\u3089\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u74b0\u5883\u306e\u8a2d\u5b9a\u3092\u78ba\u8a8d\u3057\u305f\u308a\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u5fc5\u8981\u306b\u5fdc\u3058\u3066\u3001alpine container\u3092\u5225\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u7f6e\u304d\u63db\u3048\u3066\u304f\u3060\u3055\u3044\u3002 \u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3067\u304d\u307e\u3059\u304c\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u306eRead\uff0fWrite\u30ec\u30a4\u30e4\u306e\u30a2\u30af\u30bb\u30b9\u306f\u3067\u304d\u307e\u305b\u3093\u3002\u540c\u3058\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044\u5834\u5408\u306f\u4e0b\u8a18\u306ensenter\u306e\u4f8b\u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002 nsenter \u3092\u5229\u7528 nsenter \u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30ce\u30fc\u30c9\u4e0a\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 nsenter \u30b3\u30de\u30f3\u30c9\u306f\u3001\u307b\u3068\u3093\u3069\u306eLinux\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u7684\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001Ubuntu\u3067\u306f\u3001util-linux\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u3063\u3066\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002 \u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ID=<container ID or name> Container\u5185\u306e\u521d\u3081\u3066\u306e\u30d7\u30ed\u30bb\u30b9(PID 1)\u306e\u756a\u53f7\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002 PID=$(docker inspect --format '{{ .State.Pid }}' $ID) nsenter\u3092\u4f7f\u3063\u3066Container/Pod\u306e\u5168\u7a2e\u985e\u306eNamespace\u3067\u3001\u30ce\u30fc\u30c9\u4e0a\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 nsenter -a -t $PID <command> \u4f8b\u3048\u3070\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u6642\u3001\u30ce\u30fc\u30c9\u4e0a\u306b\u3042\u308b\u30b3\u30de\u30f3\u30c9\u3001tcpdump\u3001curl\u3001dig\u3084mtr\u306a\u3069\u304c\u4f7f\u7528\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002 -a \u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u6700\u8fd1\u306e nsenter \u3067\u5229\u7528\u53ef\u80fd\u3060\u304c\u3001\u3082\u3057\u3053\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u4f7f\u3048\u306a\u304b\u3063\u305f\u3089\u3001\u5358\u72ec\u306eNamespace\u306b\u5165\u308b\u3088\u3046\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3057\u3066\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f nsenter --help \u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[JP\\] How to troubleshoot using the namespace of a container"},{"location":"000020069/#jp-how-to-troubleshoot-using-the-namespace-of-a-container","text":"This document (000020069) is provided subject to the disclaimer at the end of this document.","title":"[JP] How to troubleshoot using the namespace of a container"},{"location":"000020069/#situation","text":"","title":"Situation"},{"location":"000020069/#_1","text":"\u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u5834\u5408\u3001\u767a\u751f\u3057\u305f\u554f\u984c\u3068\u4e00\u81f4\u3059\u308b\u518d\u73fe\u74b0\u5883\u304c\u5fc5\u8981\u3067\u3059\u3002\u305f\u3060\u3057\u30b3\u30f3\u30c6\u30ca\u30fc\u74b0\u5883\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3067\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u30fc\u5185\u3067\u30c4\u30fc\u30eb\u3068\u30b7\u30a7\u30eb\u74b0\u5883\u304c\u7c21\u5358\u306b\u5229\u7528\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u518d\u73fe\u74b0\u5883\u306e\u69cb\u7bc9\u304c\u56f0\u96e3\u306b\u306a\u308a\u307e\u3059\u3002","title":"\u80cc\u666f"},{"location":"000020069/#_2","text":"\u4e0a\u8a18\u306e\u8ab2\u984c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u4ee5\u4e0b\u4e8c\u3064\u306e\u624b\u9806\u304c\u3042\u308a\u307e\u3059\u3002","title":"\u624b\u9806"},{"location":"000020069/#sidecar-container","text":"\u554f\u984c\u3092\u6301\u3064\u30b3\u30f3\u30c6\u30ca\u30fc\u304c\u5c5e\u3059\u308bNamespace\u3067\u65b0\u3057\u3044sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u3053\u306esidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u7528\u3044\u3066\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u306f\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3057\u306a\u304c\u3089\u3001\u540c\u3058Network\u3068PID\u306eNamespace\u3092\u4f7f\u7528\u3057\u3066\u8d77\u52d5\u3067\u304d\u307e\u3059\u3002 \u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ID=<container ID or name> \u540c\u3058Network\u3001PID\u306eNamespace\u3068Volume\u3092\u4f7f\u7528\u3057sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002 docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh \u3053\u308c\u304b\u3089\u554f\u984c\u304c\u3042\u308bcontainer\u3084Pod\u3068\u540c\u3058\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5185\u3067\u3001alpine\u30b3\u30f3\u30c6\u30ca\u30fc\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001Pod\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001Sidecar Container\u306b\u5165\u308a\u3001\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u306a\u304c\u3089\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u74b0\u5883\u306e\u8a2d\u5b9a\u3092\u78ba\u8a8d\u3057\u305f\u308a\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u5fc5\u8981\u306b\u5fdc\u3058\u3066\u3001alpine container\u3092\u5225\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u7f6e\u304d\u63db\u3048\u3066\u304f\u3060\u3055\u3044\u3002 \u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3067\u304d\u307e\u3059\u304c\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u306eRead\uff0fWrite\u30ec\u30a4\u30e4\u306e\u30a2\u30af\u30bb\u30b9\u306f\u3067\u304d\u307e\u305b\u3093\u3002\u540c\u3058\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044\u5834\u5408\u306f\u4e0b\u8a18\u306ensenter\u306e\u4f8b\u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002","title":"Sidecar container\u00a0\u3092\u5229\u7528"},{"location":"000020069/#nsenter","text":"nsenter \u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30ce\u30fc\u30c9\u4e0a\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 nsenter \u30b3\u30de\u30f3\u30c9\u306f\u3001\u307b\u3068\u3093\u3069\u306eLinux\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u7684\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001Ubuntu\u3067\u306f\u3001util-linux\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u3063\u3066\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002 \u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002 ID=<container ID or name> Container\u5185\u306e\u521d\u3081\u3066\u306e\u30d7\u30ed\u30bb\u30b9(PID 1)\u306e\u756a\u53f7\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002 PID=$(docker inspect --format '{{ .State.Pid }}' $ID) nsenter\u3092\u4f7f\u3063\u3066Container/Pod\u306e\u5168\u7a2e\u985e\u306eNamespace\u3067\u3001\u30ce\u30fc\u30c9\u4e0a\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 nsenter -a -t $PID <command> \u4f8b\u3048\u3070\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u6642\u3001\u30ce\u30fc\u30c9\u4e0a\u306b\u3042\u308b\u30b3\u30de\u30f3\u30c9\u3001tcpdump\u3001curl\u3001dig\u3084mtr\u306a\u3069\u304c\u4f7f\u7528\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002 -a \u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u6700\u8fd1\u306e nsenter \u3067\u5229\u7528\u53ef\u80fd\u3060\u304c\u3001\u3082\u3057\u3053\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u4f7f\u3048\u306a\u304b\u3063\u305f\u3089\u3001\u5358\u72ec\u306eNamespace\u306b\u5165\u308b\u3088\u3046\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3057\u3066\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f nsenter --help \u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002","title":"nsenter \u3092\u5229\u7528"},{"location":"000020069/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020070/","text":"How to enable support for use-forwarded-headers in ingress-nginx This document (000020070) is provided subject to the disclaimer at the end of this document. Situation Task Per the [ingress-nginx documentation], the use-forwarded-headers configuration option enables passing \"the incoming X-Forwarded-* headers to upstreams. Use this option when NGINX is behind another L7 proxy / load balancer that is setting these headers.\" This article details how to enable the use-forwarded-headers option in the ingress-nginx instance of Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters. Pre-requisites A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher Resolution Configuration for RKE CLI provisioned clusters Edit the cluster configuration YAML file to include the use-forwarded-headers: true option for the ingress, as follows: ingress: provider: nginx options: use-forwarded-headers: true Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Verify the new configuration: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done Configuration for Rancher v2.x provisioned clusters Log in to the Rancher UI. Go to Global -> Clusters -> Cluster Name. From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Include the use-forwarded-headers option for the ingress, as follows: ingress: provider: nginx options: use-forwarded-headers: true Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done Further reading ingress-nginx ConfigMap configuration documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable support for use-forwarded-headers in ingress-nginx"},{"location":"000020070/#how-to-enable-support-for-use-forwarded-headers-in-ingress-nginx","text":"This document (000020070) is provided subject to the disclaimer at the end of this document.","title":"How to enable support for use-forwarded-headers in ingress-nginx"},{"location":"000020070/#situation","text":"","title":"Situation"},{"location":"000020070/#task","text":"Per the [ingress-nginx documentation], the use-forwarded-headers configuration option enables passing \"the incoming X-Forwarded-* headers to upstreams. Use this option when NGINX is behind another L7 proxy / load balancer that is setting these headers.\" This article details how to enable the use-forwarded-headers option in the ingress-nginx instance of Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","title":"Task"},{"location":"000020070/#pre-requisites","text":"A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher","title":"Pre-requisites"},{"location":"000020070/#resolution","text":"","title":"Resolution"},{"location":"000020070/#configuration-for-rke-cli-provisioned-clusters","text":"Edit the cluster configuration YAML file to include the use-forwarded-headers: true option for the ingress, as follows: ingress: provider: nginx options: use-forwarded-headers: true Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Verify the new configuration: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done","title":"Configuration for RKE CLI provisioned clusters"},{"location":"000020070/#configuration-for-rancher-v2x-provisioned-clusters","text":"Log in to the Rancher UI. Go to Global -> Clusters -> Cluster Name. From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Include the use-forwarded-headers option for the ingress, as follows: ingress: provider: nginx options: use-forwarded-headers: true Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done","title":"Configuration for Rancher v2.x provisioned clusters"},{"location":"000020070/#further-reading","text":"ingress-nginx ConfigMap configuration documentation","title":"Further reading"},{"location":"000020070/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020071/","text":"How to enable container log rotation with k3s or containerd This document (000020071) is provided subject to the disclaimer at the end of this document. Situation Task In a Kubernetes cluster running an alternative container runtime, such as containerd, instead of Docker, the kubelet manages container logs. The kubelet default values in relation to log rotation can be found in the upstream kubelet | Kubernetes documentation ,-%2D%2Dcontainer%2Druntime%20string). These values can be adjusted by adding flags to the kubelet process. Pre-requisites These steps have been validated for a k3s cluster using the default containerd runtime, in theory these same flags should work for any Kubernetes cluster which does not use Docker as the container runtime. Resolution Two kubelet flags need to be added to configure log rotation, the flags will take effect only at start time. In the case of k3s, passing the needed flags can be done a number of ways, the most common perhaps is with the INSTALL_K3S_EXEC environment variable when installing k3s as a service. These same flags can be added to a previous install command to update the service configuration of an existing install of k3s. Note When updating an existing k3s install, the following command will restart k3s. curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--kubelet-arg \"container-log-max-files=4\" --kubelet-arg \"container-log-max-size=50Mi\"\" sh - However, flags can also be supplied to the k3s binary directly if a service is not being used. A restart of k3s is required, using the updated flags. k3s server --kubelet-arg container-log-max-files=4 --kubelet-arg container-log-max-size=50Mi Note please adjust the values to suit your needs, for demonstration purposes the above commands used 4 log files of 50MB, allowing for 200MB of total space to be retained per container. Further reading Please reference the k3s and kubelet documentation pages to find more information on these flags. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable container log rotation with k3s or containerd"},{"location":"000020071/#how-to-enable-container-log-rotation-with-k3s-or-containerd","text":"This document (000020071) is provided subject to the disclaimer at the end of this document.","title":"How to enable container log rotation with k3s or containerd"},{"location":"000020071/#situation","text":"","title":"Situation"},{"location":"000020071/#task","text":"In a Kubernetes cluster running an alternative container runtime, such as containerd, instead of Docker, the kubelet manages container logs. The kubelet default values in relation to log rotation can be found in the upstream kubelet | Kubernetes documentation ,-%2D%2Dcontainer%2Druntime%20string). These values can be adjusted by adding flags to the kubelet process.","title":"Task"},{"location":"000020071/#pre-requisites","text":"These steps have been validated for a k3s cluster using the default containerd runtime, in theory these same flags should work for any Kubernetes cluster which does not use Docker as the container runtime.","title":"Pre-requisites"},{"location":"000020071/#resolution","text":"Two kubelet flags need to be added to configure log rotation, the flags will take effect only at start time. In the case of k3s, passing the needed flags can be done a number of ways, the most common perhaps is with the INSTALL_K3S_EXEC environment variable when installing k3s as a service. These same flags can be added to a previous install command to update the service configuration of an existing install of k3s. Note When updating an existing k3s install, the following command will restart k3s. curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--kubelet-arg \"container-log-max-files=4\" --kubelet-arg \"container-log-max-size=50Mi\"\" sh - However, flags can also be supplied to the k3s binary directly if a service is not being used. A restart of k3s is required, using the updated flags. k3s server --kubelet-arg container-log-max-files=4 --kubelet-arg container-log-max-size=50Mi Note please adjust the values to suit your needs, for demonstration purposes the above commands used 4 log files of 50MB, allowing for 200MB of total space to be retained per container.","title":"Resolution"},{"location":"000020071/#further-reading","text":"Please reference the k3s and kubelet documentation pages to find more information on these flags.","title":"Further reading"},{"location":"000020071/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020072/","text":"How to create an RKE template and template revision using the Rancher2 Terraform Provider This document (000020072) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to create an RKE cluster template revision using the Rancher2 Terraform provider . Pre-requisites A Rancher v2.x instance, from v2.3.0 and above Terraform and the Rancher2 Terraform Provider , authenticated with a Rancher user who has permission to create RKE Templates and RKE Template Revisions Resolution RKE cluster templates can be created using the Rancher2 Terraform Provider per the documentation on the rancher2_cluster_template resource . An example of this resource can be found below: resource \"rancher2_cluster_template\" \"foo\" { name = \"foo\" members { access_type = \"owner\" user_principal_id = \"local://user-XXXXX\" } template_revisions { name = \"V1\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"6h\" retention = \"24h\" } } } } default = true } description = \"Terraform cluster template foo\" } Having configured the Rancher2 Terraform Provider and added the above example resource, adjusting as desired and replacing local://user-XXXXX with a valid user prinical ID, run terraform apply to create the RKE template. N.B. the default = true flag, which will specify this V1 revision as the the default revision. To add additional revisions, each one will be nested as a new template_revisions block for that resource. Here is an example V2 revision: template_revisions { name = \"V2\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"3h\" retention = \"12h\" } } } } } So, the full resource block would now look like this: resource \"rancher2_cluster_template\" \"foo\" { name = \"foo\" members { access_type = \"owner\" user_principal_id = \"local://user-XXXXX\" } template_revisions { name = \"V1\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"6h\" retention = \"24h\" } } } } default = true } template_revisions { name = \"V2\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"3h\" retention = \"12h\" } } } } } description = \"Terraform cluster template foo\" } Run terraform apply and observe this second V2 revision created for the RKE template. N.B. the default revision is still set to V1 ; this can be changed as needed. Further reading Rancher RKE Template documentation . Rancher2 Terraform Provider rancher2_cluster_template resource documentation . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to create an RKE template and template revision using the Rancher2 Terraform Provider"},{"location":"000020072/#how-to-create-an-rke-template-and-template-revision-using-the-rancher2-terraform-provider","text":"This document (000020072) is provided subject to the disclaimer at the end of this document.","title":"How to create an RKE template and template revision using the Rancher2 Terraform Provider"},{"location":"000020072/#situation","text":"","title":"Situation"},{"location":"000020072/#task","text":"This article details how to create an RKE cluster template revision using the Rancher2 Terraform provider .","title":"Task"},{"location":"000020072/#pre-requisites","text":"A Rancher v2.x instance, from v2.3.0 and above Terraform and the Rancher2 Terraform Provider , authenticated with a Rancher user who has permission to create RKE Templates and RKE Template Revisions","title":"Pre-requisites"},{"location":"000020072/#resolution","text":"RKE cluster templates can be created using the Rancher2 Terraform Provider per the documentation on the rancher2_cluster_template resource . An example of this resource can be found below: resource \"rancher2_cluster_template\" \"foo\" { name = \"foo\" members { access_type = \"owner\" user_principal_id = \"local://user-XXXXX\" } template_revisions { name = \"V1\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"6h\" retention = \"24h\" } } } } default = true } description = \"Terraform cluster template foo\" } Having configured the Rancher2 Terraform Provider and added the above example resource, adjusting as desired and replacing local://user-XXXXX with a valid user prinical ID, run terraform apply to create the RKE template. N.B. the default = true flag, which will specify this V1 revision as the the default revision. To add additional revisions, each one will be nested as a new template_revisions block for that resource. Here is an example V2 revision: template_revisions { name = \"V2\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"3h\" retention = \"12h\" } } } } } So, the full resource block would now look like this: resource \"rancher2_cluster_template\" \"foo\" { name = \"foo\" members { access_type = \"owner\" user_principal_id = \"local://user-XXXXX\" } template_revisions { name = \"V1\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"6h\" retention = \"24h\" } } } } default = true } template_revisions { name = \"V2\" cluster_config { rke_config { network { plugin = \"canal\" } services { etcd { creation = \"3h\" retention = \"12h\" } } } } } description = \"Terraform cluster template foo\" } Run terraform apply and observe this second V2 revision created for the RKE template. N.B. the default revision is still set to V1 ; this can be changed as needed.","title":"Resolution"},{"location":"000020072/#further-reading","text":"Rancher RKE Template documentation . Rancher2 Terraform Provider rancher2_cluster_template resource documentation .","title":"Further reading"},{"location":"000020072/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020073/","text":"How to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed cluster This document (000020073) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed Kubernetes cluster. Pre-requisites A Rancher v2.x managed Kubernetes cluster Resolution In Rancher v2.x you can create a custom Project Role that provides the permissions to enable a user to view Pods, Pod logs and to exec into Pods. You can then grant this role to users on Projects to provide them this access where necessary. Navigate to Security -> Roles from the Global namespace. From the Projects tab, select Add Project Role. Provide a name for the role. Under Grant Resources, select Add Resource and fill in the information for each of the following: Permission(s)ResourceCreatepods/execGet, ListpodsGet, Listpods/log Select Create at the bottom. Further reading Rancher Docs: Project Administration Rancher Docs: Cluster and Project Roles - Defining Custom Roles Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed cluster"},{"location":"000020073/#how-to-create-a-custom-project-rbac-role-to-grant-log-access-and-exec-permission-on-pods-in-a-rancher-v2x-managed-cluster","text":"This document (000020073) is provided subject to the disclaimer at the end of this document.","title":"How to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed cluster"},{"location":"000020073/#situation","text":"","title":"Situation"},{"location":"000020073/#task","text":"This article details how to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed Kubernetes cluster.","title":"Task"},{"location":"000020073/#pre-requisites","text":"A Rancher v2.x managed Kubernetes cluster","title":"Pre-requisites"},{"location":"000020073/#resolution","text":"In Rancher v2.x you can create a custom Project Role that provides the permissions to enable a user to view Pods, Pod logs and to exec into Pods. You can then grant this role to users on Projects to provide them this access where necessary. Navigate to Security -> Roles from the Global namespace. From the Projects tab, select Add Project Role. Provide a name for the role. Under Grant Resources, select Add Resource and fill in the information for each of the following: Permission(s)ResourceCreatepods/execGet, ListpodsGet, Listpods/log Select Create at the bottom.","title":"Resolution"},{"location":"000020073/#further-reading","text":"Rancher Docs: Project Administration Rancher Docs: Cluster and Project Roles - Defining Custom Roles","title":"Further reading"},{"location":"000020073/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020074/","text":"How to troubleshoot HTTP 400 response codes from ingress-nginx ingresses This document (000020074) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to troubleshoot failing requests, with a HTTP 400 response code, when using an ingress to access a service in a Kubernetes cluster. This error message is typically returned due to a bad request, or as a result of an issue with the request headers or cookies. This article is not intended to be exhaustive as there are a wide variety of causes, however some possible issues are covered. Pre-requisites A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster Resolution Large request headers exceeding header buffer Nginx has a client header buffer configuration of 4x 8KB by default. Per the Nginx documentation : \"A request line cannot exceed the size of one buffer, or the 414 (Request-URI Too Large) error is returned to the client.\" \"A request header field cannot exceed the size of one buffer as well, or the 400 (Bad Request) error is returned to the client.\" The buffer sizes can be extended by defining the large-client-header-buffers option in the nginx-configuration ConfigMap (see below). Large URI is duplicated into a new header passed to the backend This issue is commonly seen where the app itself responds when queried directly with a large path, but returns a 400 error when queried through an ingress. By default, when ingress-nginx receives a request, it adds the original request's URI to the X-Original-Uri header that it passes on to the backend. This can result in the app being unable to handle the large sized headers, in addition to the long path. This behaviour can be disabled by setting the proxy-add-original-uri-header option to false in your nginx-configuration ConfigMap (see below). Adding options to the ingress-nginx nginx-configuration ConfigMap This configuration map is populated by RKE from configuration defined in the cluster config: If the cluster is provisioned by Rancher v2.x: Edit the cluster (navigate to the cluster within Rancher, select the triple-dot button and then \"Edit\") Select \"Edit as YAML\" to open the cluster configuration as YAML, instead of a form. Add the desired configuration within the ingress block in the following format: ingress: provider: nginx options: name: value Save the cluster configuration changes. RKE will go through and apply the config defined during its update process. If the cluster is provisioned by the RKE CLI: The process is largely the same as the Rancher process above, but the configuration is defined in the cluster.yml for this cluster: Open the cluster configuration yaml with your editor and add the ingress.options block: yaml ingress: provider: nginx options: name: value Apply this config with rke up --config <cluster configuration yaml> Ensure you have an up-to-date cluster.rkestate within the same directory before running rke up Further reading ingress-nginx ConfigMap documentation RKE documentation on ingress-nginx configuration Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to troubleshoot HTTP 400 response codes from ingress-nginx ingresses"},{"location":"000020074/#how-to-troubleshoot-http-400-response-codes-from-ingress-nginx-ingresses","text":"This document (000020074) is provided subject to the disclaimer at the end of this document.","title":"How to troubleshoot HTTP 400 response codes from ingress-nginx ingresses"},{"location":"000020074/#situation","text":"","title":"Situation"},{"location":"000020074/#task","text":"This article details how to troubleshoot failing requests, with a HTTP 400 response code, when using an ingress to access a service in a Kubernetes cluster. This error message is typically returned due to a bad request, or as a result of an issue with the request headers or cookies. This article is not intended to be exhaustive as there are a wide variety of causes, however some possible issues are covered.","title":"Task"},{"location":"000020074/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","title":"Pre-requisites"},{"location":"000020074/#resolution","text":"","title":"Resolution"},{"location":"000020074/#large-request-headers-exceeding-header-buffer","text":"Nginx has a client header buffer configuration of 4x 8KB by default. Per the Nginx documentation : \"A request line cannot exceed the size of one buffer, or the 414 (Request-URI Too Large) error is returned to the client.\" \"A request header field cannot exceed the size of one buffer as well, or the 400 (Bad Request) error is returned to the client.\" The buffer sizes can be extended by defining the large-client-header-buffers option in the nginx-configuration ConfigMap (see below).","title":"Large request headers exceeding header buffer"},{"location":"000020074/#large-uri-is-duplicated-into-a-new-header-passed-to-the-backend","text":"This issue is commonly seen where the app itself responds when queried directly with a large path, but returns a 400 error when queried through an ingress. By default, when ingress-nginx receives a request, it adds the original request's URI to the X-Original-Uri header that it passes on to the backend. This can result in the app being unable to handle the large sized headers, in addition to the long path. This behaviour can be disabled by setting the proxy-add-original-uri-header option to false in your nginx-configuration ConfigMap (see below).","title":"Large URI is duplicated into a new header passed to the backend"},{"location":"000020074/#adding-options-to-the-ingress-nginx-nginx-configuration-configmap","text":"This configuration map is populated by RKE from configuration defined in the cluster config:","title":"Adding options to the ingress-nginx nginx-configuration ConfigMap"},{"location":"000020074/#if-the-cluster-is-provisioned-by-rancher-v2x","text":"Edit the cluster (navigate to the cluster within Rancher, select the triple-dot button and then \"Edit\") Select \"Edit as YAML\" to open the cluster configuration as YAML, instead of a form. Add the desired configuration within the ingress block in the following format: ingress: provider: nginx options: name: value Save the cluster configuration changes. RKE will go through and apply the config defined during its update process.","title":"If the cluster is provisioned by Rancher v2.x:"},{"location":"000020074/#if-the-cluster-is-provisioned-by-the-rke-cli","text":"The process is largely the same as the Rancher process above, but the configuration is defined in the cluster.yml for this cluster: Open the cluster configuration yaml with your editor and add the ingress.options block: yaml ingress: provider: nginx options: name: value Apply this config with rke up --config <cluster configuration yaml> Ensure you have an up-to-date cluster.rkestate within the same directory before running rke up","title":"If the cluster is provisioned by the RKE CLI:"},{"location":"000020074/#further-reading","text":"ingress-nginx ConfigMap documentation RKE documentation on ingress-nginx configuration","title":"Further reading"},{"location":"000020074/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020075/","text":"How to increase the log level for Canal components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster This document (000020075) is provided subject to the disclaimer at the end of this document. Situation Task During network troubleshooting it may be useful to increase the log level of the Canal components. This article details how to set verbose debug-level Canal component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters. Pre-requisites A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Canal Network Provider Resolution N.B. As these instructions involve editing the Canal DaemonSet directly, the change will not persist cluster update events, i.e. invocations of rke up for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting. Via the Rancher UI For a Rancher v2.x managed cluster, the Canal component log level can be adjusted via the Rancher UI, per the following process: Navigate to the System project of the relevant cluster within the Rancher UI. Locate the canal DaemonSet workload within the kube-system namespace, click the vertical elipses ( \u22ee ) and select Edit. Click to Edit the calico-node container. Add CALICO_STARTUP_LOGLEVEL = DEBUG in the Environment Variables section and click Save . Click Edit for the canal DaemonSet again. This time click to Edit the kube-flannel container. Click Show advanced options . In the Command section add --v=10 to the Entrypoint e.g.: /opt/bin/flanneld --ip-masq --kube-subnet-mgr --v=10 , and click Save . Via kubectl With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Canal component log level can be adjusted via kubectl, per the following process: Run kubectl -n kube-system edit daemonset canal . In the env definition for the calico-node container add an environment variable with the name CALICO_STARTUP_LOGLEVEL and value DEBUG , e.g.: [...] containers: - env: [...] - name: CALICO_STARTUP_LOGLEVEL value: DEBUG [...] In the command definition for the kube-flannel container add --v=10 to the command, e.g.: [...] - commmand: - /opt/bin/flanneld - --ip-masq - --kube-subnet-mgr - --v=10 [...] Save the file. Further reading Calico configuration documentation Flannel troubleshooting documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to increase the log level for Canal components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020075/#how-to-increase-the-log-level-for-canal-components-in-a-rancher-kubernetes-engine-rke-or-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020075) is provided subject to the disclaimer at the end of this document.","title":"How to increase the log level for Canal components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020075/#situation","text":"","title":"Situation"},{"location":"000020075/#task","text":"During network troubleshooting it may be useful to increase the log level of the Canal components. This article details how to set verbose debug-level Canal component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","title":"Task"},{"location":"000020075/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Canal Network Provider","title":"Pre-requisites"},{"location":"000020075/#resolution","text":"N.B. As these instructions involve editing the Canal DaemonSet directly, the change will not persist cluster update events, i.e. invocations of rke up for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting.","title":"Resolution"},{"location":"000020075/#via-the-rancher-ui","text":"For a Rancher v2.x managed cluster, the Canal component log level can be adjusted via the Rancher UI, per the following process: Navigate to the System project of the relevant cluster within the Rancher UI. Locate the canal DaemonSet workload within the kube-system namespace, click the vertical elipses ( \u22ee ) and select Edit. Click to Edit the calico-node container. Add CALICO_STARTUP_LOGLEVEL = DEBUG in the Environment Variables section and click Save . Click Edit for the canal DaemonSet again. This time click to Edit the kube-flannel container. Click Show advanced options . In the Command section add --v=10 to the Entrypoint e.g.: /opt/bin/flanneld --ip-masq --kube-subnet-mgr --v=10 , and click Save .","title":"Via the Rancher UI"},{"location":"000020075/#via-kubectl","text":"With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Canal component log level can be adjusted via kubectl, per the following process: Run kubectl -n kube-system edit daemonset canal . In the env definition for the calico-node container add an environment variable with the name CALICO_STARTUP_LOGLEVEL and value DEBUG , e.g.: [...] containers: - env: [...] - name: CALICO_STARTUP_LOGLEVEL value: DEBUG [...] In the command definition for the kube-flannel container add --v=10 to the command, e.g.: [...] - commmand: - /opt/bin/flanneld - --ip-masq - --kube-subnet-mgr - --v=10 [...] Save the file.","title":"Via kubectl"},{"location":"000020075/#further-reading","text":"Calico configuration documentation Flannel troubleshooting documentation","title":"Further reading"},{"location":"000020075/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020076/","text":"How to enable legacy TLS versions for ingress-nginx in Rancher Kubernetes Engine (RKE) CLI and Rancher v2.x provisioned Kubernetes clusters This document (000020076) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to enable TLS 1.1 on the ingress-nginx controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters. Pre-requisites A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher Resolution Configuration for RKE provisioned clusters Edit the cluster configuration YAML file to include the ssl-protocols option for the ingress, as follows: ingress: provider: nginx options: ssl-protocols: \"TLSv1.1 TLSv1.2\" Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Verify the new configuration: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done Configuration for Rancher provisioned clusters Login into the Rancher UI. Go to Global -> Clusters -> Cluster Name From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Include the ssl-protocols option for the ingress, as follows: ingress: provider: nginx options: ssl-protocols: \"TLSv1.1 TLSv1.2\" Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable legacy TLS versions for ingress-nginx in Rancher Kubernetes Engine (RKE) CLI and Rancher v2.x provisioned Kubernetes clusters"},{"location":"000020076/#how-to-enable-legacy-tls-versions-for-ingress-nginx-in-rancher-kubernetes-engine-rke-cli-and-rancher-v2x-provisioned-kubernetes-clusters","text":"This document (000020076) is provided subject to the disclaimer at the end of this document.","title":"How to enable legacy TLS versions for ingress-nginx in Rancher Kubernetes Engine (RKE) CLI and Rancher v2.x provisioned Kubernetes clusters"},{"location":"000020076/#situation","text":"","title":"Situation"},{"location":"000020076/#task","text":"This article details how to enable TLS 1.1 on the ingress-nginx controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","title":"Task"},{"location":"000020076/#pre-requisites","text":"A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher","title":"Pre-requisites"},{"location":"000020076/#resolution","text":"","title":"Resolution"},{"location":"000020076/#configuration-for-rke-provisioned-clusters","text":"Edit the cluster configuration YAML file to include the ssl-protocols option for the ingress, as follows: ingress: provider: nginx options: ssl-protocols: \"TLSv1.1 TLSv1.2\" Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Verify the new configuration: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done","title":"Configuration for RKE provisioned clusters"},{"location":"000020076/#configuration-for-rancher-provisioned-clusters","text":"Login into the Rancher UI. Go to Global -> Clusters -> Cluster Name From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Include the ssl-protocols option for the ingress, as follows: ingress: provider: nginx options: ssl-protocols: \"TLSv1.1 TLSv1.2\" Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done","title":"Configuration for Rancher provisioned clusters"},{"location":"000020076/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020077/","text":"How to disable Grafana usage analytics reporting for cluster and project monitoring in Rancher v2.2.x - v2.4.x This document (000020077) is provided subject to the disclaimer at the end of this document. Situation Task By default, Grafana will report usage analytics to the endpoint at stats.grafana.org. In an air-gapped environment this can result in many connection timeout or proxy certificate errors in the Grafana pod logs of cluster and project monitoring, as in the following example: lvl=eror msg=\"Failed to send usage stats\" logger=metrics err=\"Post https://stats.grafana.org/grafana-usage-report: x509: certificate signed by unknown authority\" This article outlines how to disable this usage analytics reporting in the Grafana instance of cluster and project monitoring for Rancher v2.2.x - v2.4.x. Pre-requisites Rancher v2.2.x - v2.4.x Cluster or project monitoring enabled Resolution Navigate to either the main cluster page or a project for which you have configured monitoring. Click \"Tools\" in the top menu bar and then \"Monitoring\". Click \"Show advanced options\" at the bottom of the page to reveal the \"Answers\" fields and add the following two answers: grafana.extraVars[0].name=GF_ANALYTICS_REPORTING_ENABLED grafana.extraVars[0].value='false' Click \"Save\". You can verify this by checking the logs for the Grafana Pod, which should show the following near the top of the logs at container startup: lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_ANALYTICS_REPORTING_ENABLED='false'\" Further reading Rancher Cluster Monitoring documentation Grafana analytics configuration documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to disable Grafana usage analytics reporting for cluster and project monitoring in Rancher v2.2.x - v2.4.x"},{"location":"000020077/#how-to-disable-grafana-usage-analytics-reporting-for-cluster-and-project-monitoring-in-rancher-v22x-v24x","text":"This document (000020077) is provided subject to the disclaimer at the end of this document.","title":"How to disable Grafana usage analytics reporting for cluster and project monitoring in Rancher v2.2.x - v2.4.x"},{"location":"000020077/#situation","text":"","title":"Situation"},{"location":"000020077/#task","text":"By default, Grafana will report usage analytics to the endpoint at stats.grafana.org. In an air-gapped environment this can result in many connection timeout or proxy certificate errors in the Grafana pod logs of cluster and project monitoring, as in the following example: lvl=eror msg=\"Failed to send usage stats\" logger=metrics err=\"Post https://stats.grafana.org/grafana-usage-report: x509: certificate signed by unknown authority\" This article outlines how to disable this usage analytics reporting in the Grafana instance of cluster and project monitoring for Rancher v2.2.x - v2.4.x.","title":"Task"},{"location":"000020077/#pre-requisites","text":"Rancher v2.2.x - v2.4.x Cluster or project monitoring enabled","title":"Pre-requisites"},{"location":"000020077/#resolution","text":"Navigate to either the main cluster page or a project for which you have configured monitoring. Click \"Tools\" in the top menu bar and then \"Monitoring\". Click \"Show advanced options\" at the bottom of the page to reveal the \"Answers\" fields and add the following two answers: grafana.extraVars[0].name=GF_ANALYTICS_REPORTING_ENABLED grafana.extraVars[0].value='false' Click \"Save\". You can verify this by checking the logs for the Grafana Pod, which should show the following near the top of the logs at container startup: lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_ANALYTICS_REPORTING_ENABLED='false'\"","title":"Resolution"},{"location":"000020077/#further-reading","text":"Rancher Cluster Monitoring documentation Grafana analytics configuration documentation","title":"Further reading"},{"location":"000020077/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020078/","text":"How to confirm a version upgrade of Rancher v2.x is completed successfully This document (000020078) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to confirm that a Rancher version upgrade has successfully completed. Pre-requisites A Rancher v2.x instance, either a single Docker container or a Highly Available (HA) installation in Kubernetes A Rancher version upgrade performed per the Rancher upgrade documentation Resolution The following can be verified to confirm that the Rancher component containers have all been successfully upgrade to the newer version: Within the Rancher UI, confirm the version in the bottom-left corner displays the newer version. For a HA installation, confirm the rancher Deployment Pods within the cattle-system namespace of the Rancher cluster have all been updated to the newer version. Confirm that the Rancher agent workloads (the cattle-node-agent DaemonSet and cattle-cluster-agent Deployment in the cattle-system namespace) in all of the Rancher managed clusters have been updated to the newer version. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to confirm a version upgrade of Rancher v2.x is completed successfully"},{"location":"000020078/#how-to-confirm-a-version-upgrade-of-rancher-v2x-is-completed-successfully","text":"This document (000020078) is provided subject to the disclaimer at the end of this document.","title":"How to confirm a version upgrade of Rancher v2.x is completed successfully"},{"location":"000020078/#situation","text":"","title":"Situation"},{"location":"000020078/#task","text":"This article details how to confirm that a Rancher version upgrade has successfully completed.","title":"Task"},{"location":"000020078/#pre-requisites","text":"A Rancher v2.x instance, either a single Docker container or a Highly Available (HA) installation in Kubernetes A Rancher version upgrade performed per the Rancher upgrade documentation","title":"Pre-requisites"},{"location":"000020078/#resolution","text":"The following can be verified to confirm that the Rancher component containers have all been successfully upgrade to the newer version: Within the Rancher UI, confirm the version in the bottom-left corner displays the newer version. For a HA installation, confirm the rancher Deployment Pods within the cattle-system namespace of the Rancher cluster have all been updated to the newer version. Confirm that the Rancher agent workloads (the cattle-node-agent DaemonSet and cattle-cluster-agent Deployment in the cattle-system namespace) in all of the Rancher managed clusters have been updated to the newer version.","title":"Resolution"},{"location":"000020078/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020079/","text":"How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile This document (000020079) is provided subject to the disclaimer at the end of this document. Situation Question How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile Answer About the parameters; worker_processes This parameter determines the number of Nginx worker processes to spawn during startup. worker_rlimit_nofile This parameter controls the open file limit per worker process. More details can be found on Nginx documentation Both worker_processes and worker_rlimit_nofile are calculated dynamically by Nginx Ingress during startup. Based on the source code of Ingress Nginx ; worker_processes = Number of CPUs ($ grep -c processor /proc/cpuinfo) worker_rlimit_nofile = ( RLIMIT_NOFILE / worker_processes ) - 1024 where RLIMIT_NOFILE is the maximum allowed open files by the process ( ulimit -n ) From Nginx Ingress shell, you can verify the same. # kubectl exec -it -n ingress-nginx nginx-ingress-controller-8ln2b -- bash bash-5.0$ ulimit -n 1048576 bash-5.0$ bash-5.0$ grep -c processor /proc/cpuinfo 2 <<---- worker_processes bash-5.0$ bash-5.0$ echo $(((1048576/2)-1024)) 523264 <<--- worker_rlimit_nofile bash-5.0$ bash-5.0$ egrep \"worker_processes|worker_rlimit_nofile\" /etc/nginx/nginx.conf worker_processes 2; worker_rlimit_nofile 523264; bash-5.0$ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How Nginx Ingress calculates the worker\\_processes and worker\\_rlimit\\_nofile"},{"location":"000020079/#how-nginx-ingress-calculates-the-worker_processes-and-worker_rlimit_nofile","text":"This document (000020079) is provided subject to the disclaimer at the end of this document.","title":"How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile"},{"location":"000020079/#situation","text":"","title":"Situation"},{"location":"000020079/#question","text":"How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile","title":"Question"},{"location":"000020079/#answer","text":"About the parameters; worker_processes This parameter determines the number of Nginx worker processes to spawn during startup. worker_rlimit_nofile This parameter controls the open file limit per worker process. More details can be found on Nginx documentation Both worker_processes and worker_rlimit_nofile are calculated dynamically by Nginx Ingress during startup. Based on the source code of Ingress Nginx ; worker_processes = Number of CPUs ($ grep -c processor /proc/cpuinfo) worker_rlimit_nofile = ( RLIMIT_NOFILE / worker_processes ) - 1024 where RLIMIT_NOFILE is the maximum allowed open files by the process ( ulimit -n ) From Nginx Ingress shell, you can verify the same. # kubectl exec -it -n ingress-nginx nginx-ingress-controller-8ln2b -- bash bash-5.0$ ulimit -n 1048576 bash-5.0$ bash-5.0$ grep -c processor /proc/cpuinfo 2 <<---- worker_processes bash-5.0$ bash-5.0$ echo $(((1048576/2)-1024)) 523264 <<--- worker_rlimit_nofile bash-5.0$ bash-5.0$ egrep \"worker_processes|worker_rlimit_nofile\" /etc/nginx/nginx.conf worker_processes 2; worker_rlimit_nofile 523264; bash-5.0$","title":"Answer"},{"location":"000020079/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020080/","text":"How can I tell whether my app is installed with Helm v2 or Helm v3? This document (000020080) is provided subject to the disclaimer at the end of this document. Situation Question How can I tell whether my app was installed with Helm v2 or Helm v3? Pre-requisites kubectl access to the cluster the app is deployed in Answer The easiest way is to check what version of Helm was used to deploy resources is to look at the heritage label. For example, to check whether Rancher was installed via Helm v2 or v3, run: kubectl get deployment -n cattle-system rancher -o yaml | grep heritage The heritage version defines what version of helm was used to install this chart. heritage: Tiller - This is a Helm v2 resource heritage: Helm - This is a Helm v3 resource Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How can I tell whether my app is installed with Helm v2 or Helm v3?"},{"location":"000020080/#how-can-i-tell-whether-my-app-is-installed-with-helm-v2-or-helm-v3","text":"This document (000020080) is provided subject to the disclaimer at the end of this document.","title":"How can I tell whether my app is installed with Helm v2 or Helm v3?"},{"location":"000020080/#situation","text":"","title":"Situation"},{"location":"000020080/#question","text":"How can I tell whether my app was installed with Helm v2 or Helm v3?","title":"Question"},{"location":"000020080/#pre-requisites","text":"kubectl access to the cluster the app is deployed in","title":"Pre-requisites"},{"location":"000020080/#answer","text":"The easiest way is to check what version of Helm was used to deploy resources is to look at the heritage label. For example, to check whether Rancher was installed via Helm v2 or v3, run: kubectl get deployment -n cattle-system rancher -o yaml | grep heritage The heritage version defines what version of helm was used to install this chart. heritage: Tiller - This is a Helm v2 resource heritage: Helm - This is a Helm v3 resource","title":"Answer"},{"location":"000020080/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020081/","text":"Troubleshooting - Nodes wont join cluster or show unavailable This document (000020081) is provided subject to the disclaimer at the end of this document. Situation Issue - Nodes are not added to Rancher or are not provisioned correctly The following article should help empower Rancher administrators diagnose and troubleshoot when a node is not added to Rancher or when a node is not provisioned correctly. We'll outline the process nodes undergo when they are added to a cluster. Scope We'll kick off by scoping what cluster types this document might pertain to. We're speaking specifically about custom clusters and clusters launched with a node driver. Mention of node driver will be synonymous with 'With RKE and new nodes in an infrastructure provider' in the Rancher UI. Tracing the steps during the bootstrapping of a node. Whether you're selecting custom clusters or clusters launched with a node driver, the way to add nodes to the cluster is by executing a docker run command generated for the created cluster. In case of a custom cluster, the command will be generated and displayed on the final step of cluster creation. In case of a cluster launched with a node driver, the command is generated and executed as final command after creating the node and installing Docker. Note: not all roles may be present in the generated command, depending on what role(s) is/are selected. sudo docker run -d \\ --privileged \\ --restart=unless-stopped \\ --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:<version> --server https://<server_url> \\ --token <token> \\ --ca-checksum <checksum_value> \\ --etcd \\ --controlplane \\ --worker What happens next: 1. The docker run command launches a bootstrap agent container. It will be identified with a randomly generated name - The entrypoint is a shell script which parses the flags and runs some validation tests on said flags and their provided values - A token is then used to authenticate against your Rancher server in order to interact with it. - The agent retrieves the CA certificate from the Rancher server and places it in /etc/kubernetes/ssl/certs/serverca, then the checksum is used to validate if the CA certificate retrieved from Rancher matches. This only applies when a self signed certificate is in use. - Runs an agent binary and connects to Rancher using a WebSocket connection - Agent then checks in with the Rancher server to see if the node is unique, and gets a node plan - Agent executes the node plan provided by the Rancher server - Docker run command will create the path /etc/kubernetes if it doesn't exist - Rancher will run cluster provisioning/reconcile based on the desired role for the node being added (etcd and control plane nodes only). This process will copy certificates down from the server via the built in rke cluster provisioning. - On worker nodes, the process is slightly different. The agent requests a node plan from the Rancher server. The Rancher server generates the node config then sends it back down to the agent. The agent then executes the plan contained in the node config. This involves; certificate generation for the Kubernetes components, and the container create commands to create the following services; kubelet, kube-proxy, and nginx-proxy. - The Rancher agent uses the node plan to write out a cloud-config to configure cloud provider settings. If provisioning of the node succeeds, the node will be registering to the Kubernetes cluster and cattle-node-agent DaemonSet pods will be scheduled to the node, and the pod will remove and replace the agent container that was created via the Docker run command The share-mnt binary (aka bootstrap phase 2) - The share-mnt container runs the share-root.sh which creates filesystem resources that other container end up using. Certificate folders, configuration files, etc... - The container spings up another container that runs a share mount binary. This container makes sure /var/lib/kubelet or /var/lib/rancher have the right share permissions for systems like boot2docker. Note: All Kubernetes control plane components talk directly with the Kubernetes API server that's housed on the same node. This proxy is configured to front all k8s API servers within the cluster. It's nginx.conf should reflect that. If all goes well, the share-mnt bootstrap and share-root container exit and the share-root container gets removed. The kubelet starts, registers with Kubernetes, and cattle-node-agent DaemonSet schedules a pod. The pod should then take over the websocket connection to the rancher server. This should end our provisioning journey and hopefully lead to a functional, happy cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Troubleshooting - Nodes wont join cluster or show unavailable"},{"location":"000020081/#troubleshooting-nodes-wont-join-cluster-or-show-unavailable","text":"This document (000020081) is provided subject to the disclaimer at the end of this document.","title":"Troubleshooting - Nodes wont join cluster or show unavailable"},{"location":"000020081/#situation","text":"","title":"Situation"},{"location":"000020081/#issue-nodes-are-not-added-to-rancher-or-are-not-provisioned-correctly","text":"The following article should help empower Rancher administrators diagnose and troubleshoot when a node is not added to Rancher or when a node is not provisioned correctly. We'll outline the process nodes undergo when they are added to a cluster.","title":"Issue - Nodes are not added to Rancher or are not provisioned correctly"},{"location":"000020081/#scope","text":"We'll kick off by scoping what cluster types this document might pertain to. We're speaking specifically about custom clusters and clusters launched with a node driver. Mention of node driver will be synonymous with 'With RKE and new nodes in an infrastructure provider' in the Rancher UI.","title":"Scope"},{"location":"000020081/#tracing-the-steps-during-the-bootstrapping-of-a-node","text":"Whether you're selecting custom clusters or clusters launched with a node driver, the way to add nodes to the cluster is by executing a docker run command generated for the created cluster. In case of a custom cluster, the command will be generated and displayed on the final step of cluster creation. In case of a cluster launched with a node driver, the command is generated and executed as final command after creating the node and installing Docker. Note: not all roles may be present in the generated command, depending on what role(s) is/are selected. sudo docker run -d \\ --privileged \\ --restart=unless-stopped \\ --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:<version> --server https://<server_url> \\ --token <token> \\ --ca-checksum <checksum_value> \\ --etcd \\ --controlplane \\ --worker What happens next: 1. The docker run command launches a bootstrap agent container. It will be identified with a randomly generated name - The entrypoint is a shell script which parses the flags and runs some validation tests on said flags and their provided values - A token is then used to authenticate against your Rancher server in order to interact with it. - The agent retrieves the CA certificate from the Rancher server and places it in /etc/kubernetes/ssl/certs/serverca, then the checksum is used to validate if the CA certificate retrieved from Rancher matches. This only applies when a self signed certificate is in use. - Runs an agent binary and connects to Rancher using a WebSocket connection - Agent then checks in with the Rancher server to see if the node is unique, and gets a node plan - Agent executes the node plan provided by the Rancher server - Docker run command will create the path /etc/kubernetes if it doesn't exist - Rancher will run cluster provisioning/reconcile based on the desired role for the node being added (etcd and control plane nodes only). This process will copy certificates down from the server via the built in rke cluster provisioning. - On worker nodes, the process is slightly different. The agent requests a node plan from the Rancher server. The Rancher server generates the node config then sends it back down to the agent. The agent then executes the plan contained in the node config. This involves; certificate generation for the Kubernetes components, and the container create commands to create the following services; kubelet, kube-proxy, and nginx-proxy. - The Rancher agent uses the node plan to write out a cloud-config to configure cloud provider settings. If provisioning of the node succeeds, the node will be registering to the Kubernetes cluster and cattle-node-agent DaemonSet pods will be scheduled to the node, and the pod will remove and replace the agent container that was created via the Docker run command The share-mnt binary (aka bootstrap phase 2) - The share-mnt container runs the share-root.sh which creates filesystem resources that other container end up using. Certificate folders, configuration files, etc... - The container spings up another container that runs a share mount binary. This container makes sure /var/lib/kubelet or /var/lib/rancher have the right share permissions for systems like boot2docker. Note: All Kubernetes control plane components talk directly with the Kubernetes API server that's housed on the same node. This proxy is configured to front all k8s API servers within the cluster. It's nginx.conf should reflect that. If all goes well, the share-mnt bootstrap and share-root container exit and the share-root container gets removed. The kubelet starts, registers with Kubernetes, and cattle-node-agent DaemonSet schedules a pod. The pod should then take over the websocket connection to the rancher server. This should end our provisioning journey and hopefully lead to a functional, happy cluster.","title":"Tracing the steps during the bootstrapping of a node."},{"location":"000020081/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020082/","text":"How to deploy Nginx instead of Traefik as your ingress controller on K3s This document (000020082) is provided subject to the disclaimer at the end of this document. Situation Task This knowledge base article will provide the directions for deploying NGINX instead of Traefik as your Kubernetes ingress controller on K3s. Please note that Traefik is the support ingress controller for K3s and NGINX is not officially supported by SUSE Rancher support. Requirements K3s 1.17+ (may apply to other versions) Background By default, K3s uses Traefik as the ingress controller for your cluster. The decision to use Traefik over NGINX was based on multi-architecture support across x86 and ARM based platforms. Normally Traefik meets the needs of most Kubernetes clusters. However, there are unique use cases where NGINX may be required or preferred. If you don't think you need NGINX, it's recommended to stick with Traefik. Solution The first step to using NGINX or any alternative ingress controller is to tell K3s that you do not want to deploy Traefik. When installing K3s add the following --no-deploy traefik flag to the INSTALL_K3S_EXEC environment variable: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--no-deploy traefik\" sh -s - If you have already downloaded the k3s.sh install script, you can run the following: INSTALL_K3S_EXEC=\"--no-deploy traefik\" k3s.sh This will install the K3s server and form a single node cluster. You can confirm the cluster is operational (\"Ready\") by running: $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-0-100 Ready master 1m v1.18.4+k3s1 Note, if you already had the kubectl binary installed on your host and it is not configured correctly, you may need to run k3s kubectl instead of kubectl . Next, confirm your out-of-box pods are running and Traefik is not running: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-58fb86bdfd-vt57d 1/1 Running 0 1m kube-system metrics-server-6d684c7b5-qmlcn 1/1 Running 0 1m kube-system coredns-d798c9dd-72qrq 1/1 Running 0 1m K3s has a nice feature that allows you to deploy Helm Charts by placing a HelmChart YAML in /var/lib/rancher/k3s/server/manifests . Create this file by running: cat >/var/lib/rancher/k3s/server/manifests/ingress-nginx.yaml <<EOF apiVersion: v1 kind: Namespace metadata: name: ingress-nginx --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: ingress-nginx namespace: kube-system spec: chart: ingress-nginx repo: https://kubernetes.github.io/ingress-nginx targetNamespace: ingress-nginx version: v3.29.0 set: valuesContent: |- fullnameOverride: ingress-nginx controller: kind: DaemonSet hostNetwork: true hostPort: enabled: true service: enabled: false publishService: enabled: false metrics: enabled: true serviceMonitor: enabled: true config: use-forwarded-headers: \"true\" EOF K3s periodically polls the manifests folder and applies the YAML in these files. After about a minute, you should see new pods running, including the NGINX Ingress Controller and default backend: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-58fb86bdfd-vt57d 1/1 Running 0 2m kube-system metrics-server-6d684c7b5-qmlcn 1/1 Running 0 2m kube-system coredns-d798c9dd-72qrq 1/1 Running 0 2m kube-system helm-install-ingress-nginx-s99ct 0/1 Completed 0 1m ingress-nginx ingress-nginx-default-backend-7fb8995f4d-h6rkb 1/1 Running 0 1m ingress-nginx ingress-nginx-controller-c8mkg 1/1 Running 0 1m You'll also see a helm-install-ingress-nginx pod in your environment. K3s uses this pod to deploy the Helm Chart and it's normal for it to be in a READY=0/1 and STATUS=Completed state once the Helm Chart has been successfully deployed. In the event your Helm Chart failed to deploy, you can view the logs of this pod to troubleshoot further. Reference K3s documentation NGINX Ingress Controller Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to deploy Nginx instead of Traefik as your ingress controller on K3s"},{"location":"000020082/#how-to-deploy-nginx-instead-of-traefik-as-your-ingress-controller-on-k3s","text":"This document (000020082) is provided subject to the disclaimer at the end of this document.","title":"How to deploy Nginx instead of Traefik as your ingress controller on K3s"},{"location":"000020082/#situation","text":"","title":"Situation"},{"location":"000020082/#task","text":"This knowledge base article will provide the directions for deploying NGINX instead of Traefik as your Kubernetes ingress controller on K3s. Please note that Traefik is the support ingress controller for K3s and NGINX is not officially supported by SUSE Rancher support.","title":"Task"},{"location":"000020082/#requirements","text":"K3s 1.17+ (may apply to other versions)","title":"Requirements"},{"location":"000020082/#background","text":"By default, K3s uses Traefik as the ingress controller for your cluster. The decision to use Traefik over NGINX was based on multi-architecture support across x86 and ARM based platforms. Normally Traefik meets the needs of most Kubernetes clusters. However, there are unique use cases where NGINX may be required or preferred. If you don't think you need NGINX, it's recommended to stick with Traefik.","title":"Background"},{"location":"000020082/#solution","text":"The first step to using NGINX or any alternative ingress controller is to tell K3s that you do not want to deploy Traefik. When installing K3s add the following --no-deploy traefik flag to the INSTALL_K3S_EXEC environment variable: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--no-deploy traefik\" sh -s - If you have already downloaded the k3s.sh install script, you can run the following: INSTALL_K3S_EXEC=\"--no-deploy traefik\" k3s.sh This will install the K3s server and form a single node cluster. You can confirm the cluster is operational (\"Ready\") by running: $ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-0-100 Ready master 1m v1.18.4+k3s1 Note, if you already had the kubectl binary installed on your host and it is not configured correctly, you may need to run k3s kubectl instead of kubectl . Next, confirm your out-of-box pods are running and Traefik is not running: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-58fb86bdfd-vt57d 1/1 Running 0 1m kube-system metrics-server-6d684c7b5-qmlcn 1/1 Running 0 1m kube-system coredns-d798c9dd-72qrq 1/1 Running 0 1m K3s has a nice feature that allows you to deploy Helm Charts by placing a HelmChart YAML in /var/lib/rancher/k3s/server/manifests . Create this file by running: cat >/var/lib/rancher/k3s/server/manifests/ingress-nginx.yaml <<EOF apiVersion: v1 kind: Namespace metadata: name: ingress-nginx --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: ingress-nginx namespace: kube-system spec: chart: ingress-nginx repo: https://kubernetes.github.io/ingress-nginx targetNamespace: ingress-nginx version: v3.29.0 set: valuesContent: |- fullnameOverride: ingress-nginx controller: kind: DaemonSet hostNetwork: true hostPort: enabled: true service: enabled: false publishService: enabled: false metrics: enabled: true serviceMonitor: enabled: true config: use-forwarded-headers: \"true\" EOF K3s periodically polls the manifests folder and applies the YAML in these files. After about a minute, you should see new pods running, including the NGINX Ingress Controller and default backend: $ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-58fb86bdfd-vt57d 1/1 Running 0 2m kube-system metrics-server-6d684c7b5-qmlcn 1/1 Running 0 2m kube-system coredns-d798c9dd-72qrq 1/1 Running 0 2m kube-system helm-install-ingress-nginx-s99ct 0/1 Completed 0 1m ingress-nginx ingress-nginx-default-backend-7fb8995f4d-h6rkb 1/1 Running 0 1m ingress-nginx ingress-nginx-controller-c8mkg 1/1 Running 0 1m You'll also see a helm-install-ingress-nginx pod in your environment. K3s uses this pod to deploy the Helm Chart and it's normal for it to be in a READY=0/1 and STATUS=Completed state once the Helm Chart has been successfully deployed. In the event your Helm Chart failed to deploy, you can view the logs of this pod to troubleshoot further.","title":"Solution"},{"location":"000020082/#reference","text":"K3s documentation NGINX Ingress Controller","title":"Reference"},{"location":"000020082/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020083/","text":"How to deploy the AWS EBS CSI driver on K3s This document (000020083) is provided subject to the disclaimer at the end of this document. Situation Task This knowledge base article will provide the directions for deploying and testing the AWS EBS CSI driver and storage class on K3s. Requirements K3s 1.18+ (may apply to other versions) Amazon Web Services (AWS) account with privileges to launch EC2 instances and create IAM policies. Background K3s has all in-tree storage providers removed since Kubernetes is shifting to out of tree providers for Container Storage Interface (CSI) and Cloud Provider Interface (CPI). While in-tree providers are convenient, they add a lot of bloat to Kubernetes and will eventually be removed from upstream Kubernetes, possibly in 2021. This how-to guide will instruct you on installing and configuring the AWS EBS CSI driver and storage class. This will allow you to dynamically provision and attach an EBS volume to your pod without having to manually create a persistent volume (PV) and EBS volume in advance. In the event that your node crashes and your pod is re-launched on another node, your pod will be reattached to the volume assuming that node is running in the same availability zone used by the defunct node. Solution Assuming you want the CSI and storage class automatically deployed by K3s, copy the following YAML to a file in your manifests folder on one or all of your K3s servers. For example, /var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml : apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: aws-ebs-csi-driver namespace: kube-system spec: chart: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/v0.5.0/helm-chart.tgz version: v0.5.0 targetNamespace: kube-system valuesContent: |- enableVolumeScheduling: true enableVolumeResizing: true enableVolumeSnapshot: true extraVolumeTags: Name: k3s-ebs anothertag: anothervalue --- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ebs-storageclass provisioner: ebs.csi.aws.com volumeBindingMode: WaitForFirstConsumer First, note at the time of this writing, v0.5.0 is the latest version of the driver. If there is a newer version available, you can replace this in the chart and version tags. See the AWS EBS CSI readme for documentation on the versions currently available. Second, you can customize the enableVolumeScheduling , enableVolumeResizing , enableVolumeSnaphost , and extraVolumeTags based on your needs. These parameters and others are documented in the Helm chart . Next, you need to give the driver IAM permissions to manage EBS volumes. This can be done one of two ways. You can either feed your AWS access key and secret key as a Kubernetes secret, or use an AWS instance profile. Since the first option involves passing sensitive keys in clear text and storing them directly in Kubernetes, the second option is usually preferred. I will go over both options. For either option, make sure your access keys or instance profile has the following permissions set in IAM: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:AttachVolume\", \"ec2:CreateSnapshot\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:DeleteSnapshot\", \"ec2:DeleteTags\", \"ec2:DeleteVolume\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeInstances\", \"ec2:DescribeSnapshots\", \"ec2:DescribeTags\", \"ec2:DescribeVolumes\", \"ec2:DescribeVolumesModifications\", \"ec2:DetachVolume\", \"ec2:ModifyVolume\" ], \"Resource\": \"*\" } ] } Reference: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/example-iam-policy.json Option 1: Kubernetes Secret You can place your AWS access key and secret key into a Kubernetes secret. Create a YAML file with the following contents and run a kubectl apply. You can also place this inside your /var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml file. Keep in mind this is not a terribly secure option and anyone with access to these files or secrets in the kube-system namespace will be able to obtain your AWS access keys. apiVersion: v1 kind: Secret metadata: name: aws-secret namespace: kube-system stringData: key_id: \"AKI**********\" access_key: \"**********\" Option 2: Instance Profile This option to more secure and should not expose your keys in clear text or in a Kubernetes secret object. You'll need to make sure when your EC2 instances are launched, you've attached an instance profile that has the permissions defined above in the JSON block. Verifying and Testing You can now check your pods to see if the CSI pods are running. You should see something like this: # kubectl get pods -n kube-system | grep ebs ebs-snapshot-controller-0 1/1 Running 0 15m ebs-csi-node-k2gh5 3/3 Running 0 15m ebs-csi-node-xdcvn 3/3 Running 0 15m ebs-csi-controller-6f799b5548-46jqr 6/6 Running 0 15m ebs-csi-controller-6f799b5548-h4nbb 6/6 Running 0 15m Time to test things out. The following command can be run that should provision a 1GB EBS and attach it to your pod: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce storageClassName: ebs-storageclass resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: storage-test spec: containers: - name: \"storage-test\" image: \"ubuntu:latest\" command: [\"/bin/sleep\"] args: [\"infinity\"] volumeMounts: - name: myebs mountPath: /mnt/test volumes: - name: myebs persistentVolumeClaim: claimName: myclaim EOF In your AWS console, you should see a new EBS volume has been created. After about a minute, you should be able to exec into your pod and see the volume mounted in your pod: # kubectl exec storage-test -- df -h Filesystem Size Used Avail Use% Mounted on overlay 31G 6.2G 25G 20% / tmpfs 64M 0 64M 0% /dev tmpfs 3.8G 0 3.8G 0% /sys/fs/cgroup /dev/nvme2n1 976M 2.6M 958M 1% /mnt/test /dev/root 31G 6.2G 25G 20% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 3.8G 12K 3.8G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.8G 0 3.8G 0% /proc/acpi tmpfs 3.8G 0 3.8G 0% /proc/scsi tmpfs 3.8G 0 3.8G 0% /sys/firmware Cleaning Up Remove the test pod by running the following: kubectl delete pod storage-test Remove the PVC by running: kubectl delete pvc myclaim Check the AWS console and you should see your EBS volume has been removed automatically by the AWS EBS CSI driver. Reference K3s documentation AWS EBS CSI documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to deploy the AWS EBS CSI driver on K3s"},{"location":"000020083/#how-to-deploy-the-aws-ebs-csi-driver-on-k3s","text":"This document (000020083) is provided subject to the disclaimer at the end of this document.","title":"How to deploy the AWS EBS CSI driver on K3s"},{"location":"000020083/#situation","text":"","title":"Situation"},{"location":"000020083/#task","text":"This knowledge base article will provide the directions for deploying and testing the AWS EBS CSI driver and storage class on K3s.","title":"Task"},{"location":"000020083/#requirements","text":"K3s 1.18+ (may apply to other versions) Amazon Web Services (AWS) account with privileges to launch EC2 instances and create IAM policies.","title":"Requirements"},{"location":"000020083/#background","text":"K3s has all in-tree storage providers removed since Kubernetes is shifting to out of tree providers for Container Storage Interface (CSI) and Cloud Provider Interface (CPI). While in-tree providers are convenient, they add a lot of bloat to Kubernetes and will eventually be removed from upstream Kubernetes, possibly in 2021. This how-to guide will instruct you on installing and configuring the AWS EBS CSI driver and storage class. This will allow you to dynamically provision and attach an EBS volume to your pod without having to manually create a persistent volume (PV) and EBS volume in advance. In the event that your node crashes and your pod is re-launched on another node, your pod will be reattached to the volume assuming that node is running in the same availability zone used by the defunct node.","title":"Background"},{"location":"000020083/#solution","text":"Assuming you want the CSI and storage class automatically deployed by K3s, copy the following YAML to a file in your manifests folder on one or all of your K3s servers. For example, /var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml : apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: aws-ebs-csi-driver namespace: kube-system spec: chart: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/v0.5.0/helm-chart.tgz version: v0.5.0 targetNamespace: kube-system valuesContent: |- enableVolumeScheduling: true enableVolumeResizing: true enableVolumeSnapshot: true extraVolumeTags: Name: k3s-ebs anothertag: anothervalue --- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ebs-storageclass provisioner: ebs.csi.aws.com volumeBindingMode: WaitForFirstConsumer First, note at the time of this writing, v0.5.0 is the latest version of the driver. If there is a newer version available, you can replace this in the chart and version tags. See the AWS EBS CSI readme for documentation on the versions currently available. Second, you can customize the enableVolumeScheduling , enableVolumeResizing , enableVolumeSnaphost , and extraVolumeTags based on your needs. These parameters and others are documented in the Helm chart . Next, you need to give the driver IAM permissions to manage EBS volumes. This can be done one of two ways. You can either feed your AWS access key and secret key as a Kubernetes secret, or use an AWS instance profile. Since the first option involves passing sensitive keys in clear text and storing them directly in Kubernetes, the second option is usually preferred. I will go over both options. For either option, make sure your access keys or instance profile has the following permissions set in IAM: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:AttachVolume\", \"ec2:CreateSnapshot\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:DeleteSnapshot\", \"ec2:DeleteTags\", \"ec2:DeleteVolume\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeInstances\", \"ec2:DescribeSnapshots\", \"ec2:DescribeTags\", \"ec2:DescribeVolumes\", \"ec2:DescribeVolumesModifications\", \"ec2:DetachVolume\", \"ec2:ModifyVolume\" ], \"Resource\": \"*\" } ] } Reference: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/example-iam-policy.json","title":"Solution"},{"location":"000020083/#option-1-kubernetes-secret","text":"You can place your AWS access key and secret key into a Kubernetes secret. Create a YAML file with the following contents and run a kubectl apply. You can also place this inside your /var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml file. Keep in mind this is not a terribly secure option and anyone with access to these files or secrets in the kube-system namespace will be able to obtain your AWS access keys. apiVersion: v1 kind: Secret metadata: name: aws-secret namespace: kube-system stringData: key_id: \"AKI**********\" access_key: \"**********\"","title":"Option 1: Kubernetes Secret"},{"location":"000020083/#option-2-instance-profile","text":"This option to more secure and should not expose your keys in clear text or in a Kubernetes secret object. You'll need to make sure when your EC2 instances are launched, you've attached an instance profile that has the permissions defined above in the JSON block.","title":"Option 2: Instance Profile"},{"location":"000020083/#verifying-and-testing","text":"You can now check your pods to see if the CSI pods are running. You should see something like this: # kubectl get pods -n kube-system | grep ebs ebs-snapshot-controller-0 1/1 Running 0 15m ebs-csi-node-k2gh5 3/3 Running 0 15m ebs-csi-node-xdcvn 3/3 Running 0 15m ebs-csi-controller-6f799b5548-46jqr 6/6 Running 0 15m ebs-csi-controller-6f799b5548-h4nbb 6/6 Running 0 15m Time to test things out. The following command can be run that should provision a 1GB EBS and attach it to your pod: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce storageClassName: ebs-storageclass resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: storage-test spec: containers: - name: \"storage-test\" image: \"ubuntu:latest\" command: [\"/bin/sleep\"] args: [\"infinity\"] volumeMounts: - name: myebs mountPath: /mnt/test volumes: - name: myebs persistentVolumeClaim: claimName: myclaim EOF In your AWS console, you should see a new EBS volume has been created. After about a minute, you should be able to exec into your pod and see the volume mounted in your pod: # kubectl exec storage-test -- df -h Filesystem Size Used Avail Use% Mounted on overlay 31G 6.2G 25G 20% / tmpfs 64M 0 64M 0% /dev tmpfs 3.8G 0 3.8G 0% /sys/fs/cgroup /dev/nvme2n1 976M 2.6M 958M 1% /mnt/test /dev/root 31G 6.2G 25G 20% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 3.8G 12K 3.8G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 3.8G 0 3.8G 0% /proc/acpi tmpfs 3.8G 0 3.8G 0% /proc/scsi tmpfs 3.8G 0 3.8G 0% /sys/firmware","title":"Verifying and Testing"},{"location":"000020083/#cleaning-up","text":"Remove the test pod by running the following: kubectl delete pod storage-test Remove the PVC by running: kubectl delete pvc myclaim Check the AWS console and you should see your EBS volume has been removed automatically by the AWS EBS CSI driver.","title":"Cleaning Up"},{"location":"000020083/#reference","text":"K3s documentation AWS EBS CSI documentation","title":"Reference"},{"location":"000020083/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020084/","text":"How to perform a rolling change to nodes This document (000020084) is provided subject to the disclaimer at the end of this document. Situation Task In a Kubernetes cluster nodes can be treated as ephemeral building blocks providing the resources necessary for all workloads. Managing nodes in an immutable way is particularly common in a cloud environment. In an on premise environment however, nodes can be recycled and updated, in general it's typical that nodes have a longer lifecycle. There may be significant changes to nodes over time, for example: IP addresses, storage/filesystems migration to other hypervisors, data centers, large OS updates, or even migration between clusters. To perform large changes like this, this article aims to provide example steps to apply large changes like this safely in a rolling fashion. Pre-requisites A custom or imported cluster managed by Rancher, or an RKE/k3s cluster Access to the nodes in the cluster with sudo/root Permission to perform drain and delete actions on the nodes If there are any single replica workloads, whenever possible it is ideal to ensure at least 2 replicas are configured for availablity during rolling changes. These are best scheduled on separate nodes, a preferred anti-affinity can help with this. Steps While performing a rolling change to nodes you will need to determine a batch size, effectively how many nodes you wish to take out of service at a time. Initially, it is recommended to perform the change on one node as a canary first, and testing the change has the desired outcome before doing more at once. If you wish to maintain the number of nodes in the cluster while performing the rolling change, at this point you may wish to add new nodes, this ensures that when nodes are out of service the cluster maintains at least the original number of available nodes. Drain the node, this can be done with kubectl drain <node> , or in the Rancher UI. This is particularly important to avoid disruptions to services, by draining first, service endpoints are updated to remove the pods from services, stopped, started on a new node in the cluster, and added back to the service safely. If there are pods using local storage (commonly emptyDir volumes), and these should be drained, the --delete-local-data=true will be needed, beware: the data will be lost. Optional Delete the node(s) from the cluster, this can be done with kubectl delete <node> . This is needed for changes that cannot be performed on existing nodes, such as IP address, hostnames, moving nodes to another cluster, and large configuration updates. Any pods and Kubernetes components running on the nodes will be removed. Note: if this is an etcd node, ensure that the cluster has quorum and at least two remaining etcd nodes to maintain HA before performing this step. For an imported cluster, there is no automated cleanup so at this point you would remove the node from the cluster configuration RKE, remove the node from the cluster.yaml file followed by an rke up k3s, stop the k3s service and uninstall k3s using the script Optional If the node has been deleted in step 3, cleaning the node is important to ensure all previous history of the cluster, CNI devices, volumes, and containers are removed. This is especially important if the node is to be re-used in another cluster. Perform the changes to the node, this could be automated with configuration management, scripted or manual steps. Once step 5 is complete, add the node back to the desired cluster. In a custom cluster this can be done with the docker run command supplied in the Rancher UI For an imported cluster the steps are different RKE, you would add this node to the cluster by configuring it in the cluster.yaml file, followed with an rke up k3s, re-install k3s using the correct flags/variables Test the nodes with running workloads, and monitor before proceeding with the next node, or a larger batch size of nodes. If additional nodes were added in step 1, these can be removed from the cluster at this point by following steps 2, 3, and 4. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to perform a rolling change to nodes"},{"location":"000020084/#how-to-perform-a-rolling-change-to-nodes","text":"This document (000020084) is provided subject to the disclaimer at the end of this document.","title":"How to perform a rolling change to nodes"},{"location":"000020084/#situation","text":"","title":"Situation"},{"location":"000020084/#task","text":"In a Kubernetes cluster nodes can be treated as ephemeral building blocks providing the resources necessary for all workloads. Managing nodes in an immutable way is particularly common in a cloud environment. In an on premise environment however, nodes can be recycled and updated, in general it's typical that nodes have a longer lifecycle. There may be significant changes to nodes over time, for example: IP addresses, storage/filesystems migration to other hypervisors, data centers, large OS updates, or even migration between clusters. To perform large changes like this, this article aims to provide example steps to apply large changes like this safely in a rolling fashion.","title":"Task"},{"location":"000020084/#pre-requisites","text":"A custom or imported cluster managed by Rancher, or an RKE/k3s cluster Access to the nodes in the cluster with sudo/root Permission to perform drain and delete actions on the nodes If there are any single replica workloads, whenever possible it is ideal to ensure at least 2 replicas are configured for availablity during rolling changes. These are best scheduled on separate nodes, a preferred anti-affinity can help with this.","title":"Pre-requisites"},{"location":"000020084/#steps","text":"While performing a rolling change to nodes you will need to determine a batch size, effectively how many nodes you wish to take out of service at a time. Initially, it is recommended to perform the change on one node as a canary first, and testing the change has the desired outcome before doing more at once. If you wish to maintain the number of nodes in the cluster while performing the rolling change, at this point you may wish to add new nodes, this ensures that when nodes are out of service the cluster maintains at least the original number of available nodes. Drain the node, this can be done with kubectl drain <node> , or in the Rancher UI. This is particularly important to avoid disruptions to services, by draining first, service endpoints are updated to remove the pods from services, stopped, started on a new node in the cluster, and added back to the service safely. If there are pods using local storage (commonly emptyDir volumes), and these should be drained, the --delete-local-data=true will be needed, beware: the data will be lost. Optional Delete the node(s) from the cluster, this can be done with kubectl delete <node> . This is needed for changes that cannot be performed on existing nodes, such as IP address, hostnames, moving nodes to another cluster, and large configuration updates. Any pods and Kubernetes components running on the nodes will be removed. Note: if this is an etcd node, ensure that the cluster has quorum and at least two remaining etcd nodes to maintain HA before performing this step. For an imported cluster, there is no automated cleanup so at this point you would remove the node from the cluster configuration RKE, remove the node from the cluster.yaml file followed by an rke up k3s, stop the k3s service and uninstall k3s using the script Optional If the node has been deleted in step 3, cleaning the node is important to ensure all previous history of the cluster, CNI devices, volumes, and containers are removed. This is especially important if the node is to be re-used in another cluster. Perform the changes to the node, this could be automated with configuration management, scripted or manual steps. Once step 5 is complete, add the node back to the desired cluster. In a custom cluster this can be done with the docker run command supplied in the Rancher UI For an imported cluster the steps are different RKE, you would add this node to the cluster by configuring it in the cluster.yaml file, followed with an rke up k3s, re-install k3s using the correct flags/variables Test the nodes with running workloads, and monitor before proceeding with the next node, or a larger batch size of nodes. If additional nodes were added in step 1, these can be removed from the cluster at this point by following steps 2, 3, and 4.","title":"Steps"},{"location":"000020084/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020085/","text":"How is my SUSE Rancher Hosted environment monitored? This document (000020085) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted is monitored by multiple systems which will trigger an email/Slack/SMS notification to a SUSE Rancher Hosted DevOps on-call engineer in the event there's a problem with your environment. Prometheus and Grafana are used to monitor the health of the VMs running SUSE Rancher Hosted and look at CPU, load, memory, and disk metrics. CloudWatch is used to monitor response times and database health inside the cloud infrastructure. Pingdom and Datadog are used to monitor uptime and availability from multiple geographies. If at any time you notice a performance or availability problem with SUSE Rancher Hosted, please open a support case on our support portal . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How is my SUSE Rancher Hosted environment monitored?"},{"location":"000020085/#how-is-my-suse-rancher-hosted-environment-monitored","text":"This document (000020085) is provided subject to the disclaimer at the end of this document.","title":"How is my SUSE Rancher Hosted environment monitored?"},{"location":"000020085/#resolution","text":"SUSE Rancher Hosted is monitored by multiple systems which will trigger an email/Slack/SMS notification to a SUSE Rancher Hosted DevOps on-call engineer in the event there's a problem with your environment. Prometheus and Grafana are used to monitor the health of the VMs running SUSE Rancher Hosted and look at CPU, load, memory, and disk metrics. CloudWatch is used to monitor response times and database health inside the cloud infrastructure. Pingdom and Datadog are used to monitor uptime and availability from multiple geographies. If at any time you notice a performance or availability problem with SUSE Rancher Hosted, please open a support case on our support portal .","title":"Resolution"},{"location":"000020085/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020086/","text":"How often is maintenance performed on SUSE Rancher Hosted? This document (000020086) is provided subject to the disclaimer at the end of this document. Resolution Paid customers with an SLA are given the choice of a one hour weekly maintenance window, so maintenance is done at most on a weekly basis with the exception of emergency maintenance to address an outage or high severity issue. Maintenance typically involves upgrading the underlying Kubernetes cluster or operating system patches and updates. Most maintenance can be done with little or no interruption to service. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How often is maintenance performed on SUSE Rancher Hosted?"},{"location":"000020086/#how-often-is-maintenance-performed-on-suse-rancher-hosted","text":"This document (000020086) is provided subject to the disclaimer at the end of this document.","title":"How often is maintenance performed on SUSE Rancher Hosted?"},{"location":"000020086/#resolution","text":"Paid customers with an SLA are given the choice of a one hour weekly maintenance window, so maintenance is done at most on a weekly basis with the exception of emergency maintenance to address an outage or high severity issue. Maintenance typically involves upgrading the underlying Kubernetes cluster or operating system patches and updates. Most maintenance can be done with little or no interruption to service.","title":"Resolution"},{"location":"000020086/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020087/","text":"Is it possible to have alpha, beta, or release candidate (RC) available on SUSE Rancher Hosted? This document (000020087) is provided subject to the disclaimer at the end of this document. Situation Resolution While it may be technically possible to run an alpha, beta, or release candidate version of Rancher on SUSE Rancher Hosted, we don't typically offer it so that we can deliver our 99.9% uptime SLA. If you want to test a version of Rancher that is not GA, it's recommended that you use your own on-premise or cloud infrastructure. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is it possible to have alpha, beta, or release candidate (RC) available on SUSE Rancher Hosted?"},{"location":"000020087/#is-it-possible-to-have-alpha-beta-or-release-candidate-rc-available-on-suse-rancher-hosted","text":"This document (000020087) is provided subject to the disclaimer at the end of this document.","title":"Is it possible to have alpha, beta, or release candidate (RC) available on SUSE Rancher Hosted?"},{"location":"000020087/#situation","text":"","title":"Situation"},{"location":"000020087/#resolution","text":"While it may be technically possible to run an alpha, beta, or release candidate version of Rancher on SUSE Rancher Hosted, we don't typically offer it so that we can deliver our 99.9% uptime SLA. If you want to test a version of Rancher that is not GA, it's recommended that you use your own on-premise or cloud infrastructure.","title":"Resolution"},{"location":"000020087/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020088/","text":"Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted environment? This document (000020088) is provided subject to the disclaimer at the end of this document. Resolution Yes, if you find yourself locked out of the admin account on Hosted Rancher, the Rancher operations team can reset it for you using the method defined in our documentation . To initiate this request, please file a support case through the Rancher Support Portal . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted environment?"},{"location":"000020088/#can-the-admin-password-be-reset-if-im-locked-out-of-my-suse-rancher-hosted-environment","text":"This document (000020088) is provided subject to the disclaimer at the end of this document.","title":"Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted environment?"},{"location":"000020088/#resolution","text":"Yes, if you find yourself locked out of the admin account on Hosted Rancher, the Rancher operations team can reset it for you using the method defined in our documentation . To initiate this request, please file a support case through the Rancher Support Portal .","title":"Resolution"},{"location":"000020088/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020089/","text":"Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters? This document (000020089) is provided subject to the disclaimer at the end of this document. Resolution Customers are responsible for upgrading all downstream clusters that SUSE Rancher Hosted manages. RKE clusters can be easily upgraded and rolled back by using the UI or API. See Rancher docs for more details. K3s, RKE2, EKS, AKS, and GKE clusters can also be easily upgraded in the UI. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters?"},{"location":"000020089/#who-upgrades-kubernetes-on-my-suse-rancher-hosted-downstream-clusters","text":"This document (000020089) is provided subject to the disclaimer at the end of this document.","title":"Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters?"},{"location":"000020089/#resolution","text":"Customers are responsible for upgrading all downstream clusters that SUSE Rancher Hosted manages. RKE clusters can be easily upgraded and rolled back by using the UI or API. See Rancher docs for more details. K3s, RKE2, EKS, AKS, and GKE clusters can also be easily upgraded in the UI.","title":"Resolution"},{"location":"000020089/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020090/","text":"Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers? This document (000020090) is provided subject to the disclaimer at the end of this document. Resolution Yes, there are a variety of ways you can accomplish this. You can provision your VMs or bare metal servers ahead of time and use the Custom Cluster option when creating your Kubernetes cluster. A shell command will be provided which you can run on each server to join the cluster. Your on-premise servers will only require outbound (egress) access to SUSE Rancher Hosted. If you want to use the vSphere node driver to have SUSE Rancher Hosted provision your infrastructure, SUSE Rancher Hosted will need inbound (ingress) access to your on-premise infrastructure. This can be accomplished one of three ways: Open firewall rules on your corporate network. Establish a VPC peering connection between SUSE Rancher Hosted and your AWS cloud account. This requires that your AWS cloud account is connected to your on-premise infrastructure through Direct Connect or VPN. Establish a VPN connection between SUSE Rancher Hosted and your on-premise network. More details can be provided on each of these three options. See also SUSE Rancher Hosted Whitepaper . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers?"},{"location":"000020090/#can-suse-rancher-hosted-manage-my-on-premise-clusters-running-on-vmware-or-bare-metal-servers","text":"This document (000020090) is provided subject to the disclaimer at the end of this document.","title":"Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers?"},{"location":"000020090/#resolution","text":"Yes, there are a variety of ways you can accomplish this. You can provision your VMs or bare metal servers ahead of time and use the Custom Cluster option when creating your Kubernetes cluster. A shell command will be provided which you can run on each server to join the cluster. Your on-premise servers will only require outbound (egress) access to SUSE Rancher Hosted. If you want to use the vSphere node driver to have SUSE Rancher Hosted provision your infrastructure, SUSE Rancher Hosted will need inbound (ingress) access to your on-premise infrastructure. This can be accomplished one of three ways: Open firewall rules on your corporate network. Establish a VPC peering connection between SUSE Rancher Hosted and your AWS cloud account. This requires that your AWS cloud account is connected to your on-premise infrastructure through Direct Connect or VPN. Establish a VPN connection between SUSE Rancher Hosted and your on-premise network. More details can be provided on each of these three options. See also SUSE Rancher Hosted Whitepaper .","title":"Resolution"},{"location":"000020090/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020091/","text":"Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage? This document (000020091) is provided subject to the disclaimer at the end of this document. Situation Resolution SUSE Rancher Hosted can manage up to 2,000 downstream clusters and a total of 20,000 nodes across all clusters. Check with your account executive on the node and cluster limits for your support contract. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage?"},{"location":"000020091/#is-there-any-limit-on-the-number-of-downstream-clusters-or-nodes-suse-rancher-hosted-can-manage","text":"This document (000020091) is provided subject to the disclaimer at the end of this document.","title":"Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage?"},{"location":"000020091/#situation","text":"","title":"Situation"},{"location":"000020091/#resolution","text":"SUSE Rancher Hosted can manage up to 2,000 downstream clusters and a total of 20,000 nodes across all clusters. Check with your account executive on the node and cluster limits for your support contract.","title":"Resolution"},{"location":"000020091/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020092/","text":"How often is SUSE Rancher Hosted upgraded? This document (000020092) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted is upgraded normally within two weeks after a stable release. There are typically one or two Rancher releases a quarter. SUSE will contact you to schedule the upgrade. In the future, we plan to have upgrades self-service by letting customers trigger an upgrade through the UI. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How often is SUSE Rancher Hosted upgraded?"},{"location":"000020092/#how-often-is-suse-rancher-hosted-upgraded","text":"This document (000020092) is provided subject to the disclaimer at the end of this document.","title":"How often is SUSE Rancher Hosted upgraded?"},{"location":"000020092/#resolution","text":"SUSE Rancher Hosted is upgraded normally within two weeks after a stable release. There are typically one or two Rancher releases a quarter. SUSE will contact you to schedule the upgrade. In the future, we plan to have upgrades self-service by letting customers trigger an upgrade through the UI.","title":"Resolution"},{"location":"000020092/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020093/","text":"How is SUSE Rancher Hosted different than the open-source Rancher I can download for free? This document (000020093) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted is built on the same Rancher open-source software that can be downloaded for free. SUSE Rancher Hosted's value proposition is that SUSE installs, upgrades, backs up, monitors, and completely manages the software for you. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How is SUSE Rancher Hosted different than the open-source Rancher I can download for free?"},{"location":"000020093/#how-is-suse-rancher-hosted-different-than-the-open-source-rancher-i-can-download-for-free","text":"This document (000020093) is provided subject to the disclaimer at the end of this document.","title":"How is SUSE Rancher Hosted different than the open-source Rancher I can download for free?"},{"location":"000020093/#resolution","text":"SUSE Rancher Hosted is built on the same Rancher open-source software that can be downloaded for free. SUSE Rancher Hosted's value proposition is that SUSE installs, upgrades, backs up, monitors, and completely manages the software for you.","title":"Resolution"},{"location":"000020093/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020094/","text":"Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP based directory service? This document (000020094) is provided subject to the disclaimer at the end of this document. Resolution Yes, the option to do authentication integration is available in SUSE Rancher Hosted and you can find all the options and directions in our documentation . Integration with external authentication services such as Okta or Azure Active Directory are fairly trivial. For integration with a private or on-premise directory service, you may need to open ports in your firewall or use SUSE Rancher Hosted's network peering or VPN capabilities. SUSE can guide you through this setup if needed. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP based directory service?"},{"location":"000020094/#can-i-integrate-suse-rancher-hosted-with-my-active-directory-saml-or-ldap-based-directory-service","text":"This document (000020094) is provided subject to the disclaimer at the end of this document.","title":"Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP based directory service?"},{"location":"000020094/#resolution","text":"Yes, the option to do authentication integration is available in SUSE Rancher Hosted and you can find all the options and directions in our documentation . Integration with external authentication services such as Okta or Azure Active Directory are fairly trivial. For integration with a private or on-premise directory service, you may need to open ports in your firewall or use SUSE Rancher Hosted's network peering or VPN capabilities. SUSE can guide you through this setup if needed.","title":"Resolution"},{"location":"000020094/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020095/","text":"Does SUSE Rancher Hosted support multi-factor authentication (MFA)? This document (000020095) is provided subject to the disclaimer at the end of this document. Resolution Local user accounts in SUSE Rancher Hosted authenticate only using a login and password, so multi-factor authentication (MFA) is not supported. However, SUSE Rancher Hosted can be integrated with many authentication providers that do support MFA, such as Microsoft Azure Active Directory. For a full list of authentication providers, see the Rancher 2.x Authentication Documentation . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Does SUSE Rancher Hosted support multi-factor authentication (MFA)?"},{"location":"000020095/#does-suse-rancher-hosted-support-multi-factor-authentication-mfa","text":"This document (000020095) is provided subject to the disclaimer at the end of this document.","title":"Does SUSE Rancher Hosted support multi-factor authentication (MFA)?"},{"location":"000020095/#resolution","text":"Local user accounts in SUSE Rancher Hosted authenticate only using a login and password, so multi-factor authentication (MFA) is not supported. However, SUSE Rancher Hosted can be integrated with many authentication providers that do support MFA, such as Microsoft Azure Active Directory. For a full list of authentication providers, see the Rancher 2.x Authentication Documentation .","title":"Resolution"},{"location":"000020095/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020096/","text":"Do I have access to the \"local\" cluster in the management UI for my SUSE Rancher Hosted environment? This document (000020096) is provided subject to the disclaimer at the end of this document. Resolution No, your SUSE Rancher Hosted environment will not display the \"local\" cluster in the UI and it is not accessible through the API. The local cluster is the Kubernetes cluster that is running the Rancher server workloads and is fully managed by the SUSE Rancher Hosted DevOps team. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Do I have access to the \"local\" cluster in the management UI for my SUSE Rancher Hosted environment?"},{"location":"000020096/#do-i-have-access-to-the-local-cluster-in-the-management-ui-for-my-suse-rancher-hosted-environment","text":"This document (000020096) is provided subject to the disclaimer at the end of this document.","title":"Do I have access to the \"local\" cluster in the management UI for my SUSE Rancher Hosted environment?"},{"location":"000020096/#resolution","text":"No, your SUSE Rancher Hosted environment will not display the \"local\" cluster in the UI and it is not accessible through the API. The local cluster is the Kubernetes cluster that is running the Rancher server workloads and is fully managed by the SUSE Rancher Hosted DevOps team.","title":"Resolution"},{"location":"000020096/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020097/","text":"What are the \"-promoted\" Cluster Roles in Rancher? This document (000020097) is provided subject to the disclaimer at the end of this document. Situation Question When I query for Cluster Roles via kubectl, I see some entries with \"-promoted\" appended to them. What are these and why is Rancher creating them? Pre-requisites Rancher server with RKE clusters added Users added to a Project Answer The ClusterRole with \"-promoted\" at the end, is created if the Project role given to a Project member contains any of these resources: storageClass, persistentVolumes, and apiServices. These resources are not scoped to a namespace. They do not belong to any Project but the entire Cluster. That is why Rancher creates an additional ClusterRole. Further Reading https://rancher.com/docs/rancher/v2.x/en/admin-settings/rbac/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What are the \"-promoted\" Cluster Roles in Rancher?"},{"location":"000020097/#what-are-the-promoted-cluster-roles-in-rancher","text":"This document (000020097) is provided subject to the disclaimer at the end of this document.","title":"What are the \"-promoted\" Cluster Roles in Rancher?"},{"location":"000020097/#situation","text":"","title":"Situation"},{"location":"000020097/#question","text":"When I query for Cluster Roles via kubectl, I see some entries with \"-promoted\" appended to them. What are these and why is Rancher creating them?","title":"Question"},{"location":"000020097/#pre-requisites","text":"Rancher server with RKE clusters added Users added to a Project","title":"Pre-requisites"},{"location":"000020097/#answer","text":"The ClusterRole with \"-promoted\" at the end, is created if the Project role given to a Project member contains any of these resources: storageClass, persistentVolumes, and apiServices. These resources are not scoped to a namespace. They do not belong to any Project but the entire Cluster. That is why Rancher creates an additional ClusterRole.","title":"Answer"},{"location":"000020097/#further-reading","text":"https://rancher.com/docs/rancher/v2.x/en/admin-settings/rbac/","title":"Further Reading"},{"location":"000020097/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020098/","text":"Can Rancher migrate my helm2 app to helm3? This document (000020098) is provided subject to the disclaimer at the end of this document. Situation Question Can I use Rancher to migrate a Rancher app I deployed from a Helm v2 catalog to Helm v3? Answer No, Rancher currently does not support migrating an app from Helm v2 to Helm v3. To migrate an app from Helm v2 to Helm v3, you would need to delete the app, re-add the catalog as a helm_v3 catalog and re-install the app. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can Rancher migrate my helm2 app to helm3?"},{"location":"000020098/#can-rancher-migrate-my-helm2-app-to-helm3","text":"This document (000020098) is provided subject to the disclaimer at the end of this document.","title":"Can Rancher migrate my helm2 app to helm3?"},{"location":"000020098/#situation","text":"","title":"Situation"},{"location":"000020098/#question","text":"Can I use Rancher to migrate a Rancher app I deployed from a Helm v2 catalog to Helm v3?","title":"Question"},{"location":"000020098/#answer","text":"No, Rancher currently does not support migrating an app from Helm v2 to Helm v3. To migrate an app from Helm v2 to Helm v3, you would need to delete the app, re-add the catalog as a helm_v3 catalog and re-install the app.","title":"Answer"},{"location":"000020098/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020099/","text":"Loading the new Rancher Dashboard in an airgapped environment redirects to /fail-whale This document (000020099) is provided subject to the disclaimer at the end of this document. Situation Issue When attempting to view the new Rancher dashboard in an airgapped environment, or one that requires a proxy to access the internet, the dashboard eventually times out and the user is redirected to https://rancher_server/fail-whale Root cause As the dashboard is currently in beta testing, the code for it resides in our CDN instead of being included in our images. The service responsible for pulling this code currently does not support proxy configuration. Resolution Until the dashboard goes into a General Release status, there is a requirement for internet connectivity to https://releases.rancher.com and https://github.com from both the Rancher cluster and also downstream controlplane nodes for this functionality to work. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Loading the new Rancher Dashboard in an airgapped environment redirects to /fail-whale"},{"location":"000020099/#loading-the-new-rancher-dashboard-in-an-airgapped-environment-redirects-to-fail-whale","text":"This document (000020099) is provided subject to the disclaimer at the end of this document.","title":"Loading the new Rancher Dashboard in an airgapped environment redirects to /fail-whale"},{"location":"000020099/#situation","text":"","title":"Situation"},{"location":"000020099/#issue","text":"When attempting to view the new Rancher dashboard in an airgapped environment, or one that requires a proxy to access the internet, the dashboard eventually times out and the user is redirected to https://rancher_server/fail-whale","title":"Issue"},{"location":"000020099/#root-cause","text":"As the dashboard is currently in beta testing, the code for it resides in our CDN instead of being included in our images. The service responsible for pulling this code currently does not support proxy configuration.","title":"Root cause"},{"location":"000020099/#resolution","text":"Until the dashboard goes into a General Release status, there is a requirement for internet connectivity to https://releases.rancher.com and https://github.com from both the Rancher cluster and also downstream controlplane nodes for this functionality to work.","title":"Resolution"},{"location":"000020099/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020100/","text":"Slow etcd performance (performance testing and optimization) This document (000020100) is provided subject to the disclaimer at the end of this document. Situation Issue If your etcd logs start showing messages like the following, your storage might be too slow for etcd or the server might be doing too much for etcd to operate properly: 2019-08-11 23:27:04.344948 W | etcdserver: read-only range request \"key:\\\"/registry/services/specs/default/kubernetes\\\" \" with result \"range_response_count:1 size:293\" took too long (1.530802357s) to execute If your storage is really slow you will even see it throwing alerts in your monitoring system. What can you do to the verify the performance of your storage? If the storage is is not performing correctly, how can you fix it? After researching this I found an IBM article that went over this extensively. Their findings on how to test were very helpful. The biggest factor is your storage latency. If it is not well below 10ms in the 99th percentile, you will see warnings in the etcd logs. We can test this with a tool called fio which I will outline below. Testing etcd performance Download and install the latest version of fio. This is important because older versions do not provide storage latency. I have a very simple script below to download and install this. curl -LO https://github.com/rancherlabs/support-tools/raw/master/instant-fio-master/instant-fio-master.sh bash instant-fio-master.sh Test the storage, create a directory on the device you want to test then run the fio command as shown below. export PATH=/usr/local/bin:$PATH mkdir test-data fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest Below is an example output from an etcd,controlplane,worker node of a Rancher installation cluster running on an AWS ec2 instance type of t2.large. [root@ip-172-31-14-184 ~]# fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest mytest: (g=0): rw=write, bs=(R) 2300B-2300B, (W) 2300B-2300B, (T) 2300B-2300B, ioengine=sync, iodepth=1 fio-3.15-23-g937e Starting 1 process mytest: Laying out IO file (1 file / 100MiB) Jobs: 1 (f=1): [W(1)][100.0%][w=2684KiB/s][w=1195 IOPS][eta 00m:00s] mytest: (groupid=0, jobs=1): err= 0: pid=21203: Sun Aug 11 23:47:30 2019 write: IOPS=1196, BW=2687KiB/s (2752kB/s)(99.0MiB/38105msec) clat (nsec): min=2840, max=99026, avg=8551.56, stdev=3187.53 lat (nsec): min=3337, max=99664, avg=9191.92, stdev=3285.92 clat percentiles (nsec): | 1.00th=[ 4640], 5.00th=[ 5536], 10.00th=[ 5728], 20.00th=[ 6176], | 30.00th=[ 6624], 40.00th=[ 7264], 50.00th=[ 7968], 60.00th=[ 8768], | 70.00th=[ 9408], 80.00th=[10304], 90.00th=[11840], 95.00th=[13760], | 99.00th=[19328], 99.50th=[23168], 99.90th=[35584], 99.95th=[44288], | 99.99th=[63744] bw ( KiB/s): min= 2398, max= 2852, per=99.95%, avg=2685.79, stdev=104.84, samples=76 iops : min= 1068, max= 1270, avg=1195.96, stdev=46.66, samples=76 lat (usec) : 4=0.52%, 10=76.28%, 20=22.34%, 50=0.82%, 100=0.04% fsync/fdatasync/sync_file_range: sync (usec): min=352, max=21253, avg=822.36, stdev=652.94 sync percentiles (usec): | 1.00th=[ 400], 5.00th=[ 420], 10.00th=[ 437], 20.00th=[ 457], | 30.00th=[ 478], 40.00th=[ 529], 50.00th=[ 906], 60.00th=[ 947], | 70.00th=[ 988], 80.00th=[ 1020], 90.00th=[ 1090], 95.00th=[ 1156], | 99.00th=[ 2245], 99.50th=[ 5932], 99.90th=[ 8717], 99.95th=[11600], | 99.99th=[16581] cpu : usr=0.79%, sys=7.38%, ctx=119920, majf=0, minf=35 IO depths : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,45590,0,0 short=45590,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=2687KiB/s (2752kB/s), 2687KiB/s-2687KiB/s (2752kB/s-2752kB/s), io=99.0MiB (105MB), run=38105-38105msec Disk stats (read/write): xvda: ios=0/96829, merge=0/3, ticks=0/47440, in_queue=47432, util=92.25% In the fsync data section you can see that the 99th percentile is 2245 or about 2.2ms of latency. This storage is well suited for an etcd node. The etcd documentation suggests that for storage to be fast enough, the 99th percentile of fdatasync invocations when writing to the WAL file must be less than 10ms. Resolution What if your node's storage isn't fast enough? The simple solution is to upgrade the storage but that isn't always an option. If you are on the cusp of acceptable, there are things you can do to optimize your storage so that etcd is happy. Don't run etcd on a node with other roles. A general rule of thumb is to never have the worker role on the same node as etcd. However many environments have etcd and controlplane roles on the same node and run just fine. If this is the case for your environment then you should consider separating etcd and controlplane nodes. If you've separated etcd and the controlplane node and are still having issues, you can mount a separate volume for etcd so that read write operations for everything else on the node do not impact etcd's performance. This is mostly applicable to Cloud hosted nodes since each volume mounted has its own allocated set of resources. If you are on a dedicated server and would like to separate etcd read write operations from the rest of the server, you should install a new storage device for etcd mounts. Always use SSD's for your etcd nodes, whether it is dedicated or in the cloud. Set the priority of the etcd container so that it is higher than other processes but not too high that it overwhelms the server. ionice -c2 -n0 -p `pgrep -x etcd` Further reading Below is a list of links that I used for my research. I highly recommend reading these as they contain more information than I've posted in this article. IBM blog post on use of fio to test etcd storage performance etcd performance documentation etcd documentation on node sizing examples etcd metrics documentation etcd tuning documentation AWS blog post on the difference between burst and baseline performance in EC2 storage Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Slow etcd performance (performance testing and optimization)"},{"location":"000020100/#slow-etcd-performance-performance-testing-and-optimization","text":"This document (000020100) is provided subject to the disclaimer at the end of this document.","title":"Slow etcd performance (performance testing and optimization)"},{"location":"000020100/#situation","text":"","title":"Situation"},{"location":"000020100/#issue","text":"If your etcd logs start showing messages like the following, your storage might be too slow for etcd or the server might be doing too much for etcd to operate properly: 2019-08-11 23:27:04.344948 W | etcdserver: read-only range request \"key:\\\"/registry/services/specs/default/kubernetes\\\" \" with result \"range_response_count:1 size:293\" took too long (1.530802357s) to execute If your storage is really slow you will even see it throwing alerts in your monitoring system. What can you do to the verify the performance of your storage? If the storage is is not performing correctly, how can you fix it? After researching this I found an IBM article that went over this extensively. Their findings on how to test were very helpful. The biggest factor is your storage latency. If it is not well below 10ms in the 99th percentile, you will see warnings in the etcd logs. We can test this with a tool called fio which I will outline below.","title":"Issue"},{"location":"000020100/#testing-etcd-performance","text":"Download and install the latest version of fio. This is important because older versions do not provide storage latency. I have a very simple script below to download and install this. curl -LO https://github.com/rancherlabs/support-tools/raw/master/instant-fio-master/instant-fio-master.sh bash instant-fio-master.sh Test the storage, create a directory on the device you want to test then run the fio command as shown below. export PATH=/usr/local/bin:$PATH mkdir test-data fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest Below is an example output from an etcd,controlplane,worker node of a Rancher installation cluster running on an AWS ec2 instance type of t2.large. [root@ip-172-31-14-184 ~]# fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest mytest: (g=0): rw=write, bs=(R) 2300B-2300B, (W) 2300B-2300B, (T) 2300B-2300B, ioengine=sync, iodepth=1 fio-3.15-23-g937e Starting 1 process mytest: Laying out IO file (1 file / 100MiB) Jobs: 1 (f=1): [W(1)][100.0%][w=2684KiB/s][w=1195 IOPS][eta 00m:00s] mytest: (groupid=0, jobs=1): err= 0: pid=21203: Sun Aug 11 23:47:30 2019 write: IOPS=1196, BW=2687KiB/s (2752kB/s)(99.0MiB/38105msec) clat (nsec): min=2840, max=99026, avg=8551.56, stdev=3187.53 lat (nsec): min=3337, max=99664, avg=9191.92, stdev=3285.92 clat percentiles (nsec): | 1.00th=[ 4640], 5.00th=[ 5536], 10.00th=[ 5728], 20.00th=[ 6176], | 30.00th=[ 6624], 40.00th=[ 7264], 50.00th=[ 7968], 60.00th=[ 8768], | 70.00th=[ 9408], 80.00th=[10304], 90.00th=[11840], 95.00th=[13760], | 99.00th=[19328], 99.50th=[23168], 99.90th=[35584], 99.95th=[44288], | 99.99th=[63744] bw ( KiB/s): min= 2398, max= 2852, per=99.95%, avg=2685.79, stdev=104.84, samples=76 iops : min= 1068, max= 1270, avg=1195.96, stdev=46.66, samples=76 lat (usec) : 4=0.52%, 10=76.28%, 20=22.34%, 50=0.82%, 100=0.04% fsync/fdatasync/sync_file_range: sync (usec): min=352, max=21253, avg=822.36, stdev=652.94 sync percentiles (usec): | 1.00th=[ 400], 5.00th=[ 420], 10.00th=[ 437], 20.00th=[ 457], | 30.00th=[ 478], 40.00th=[ 529], 50.00th=[ 906], 60.00th=[ 947], | 70.00th=[ 988], 80.00th=[ 1020], 90.00th=[ 1090], 95.00th=[ 1156], | 99.00th=[ 2245], 99.50th=[ 5932], 99.90th=[ 8717], 99.95th=[11600], | 99.99th=[16581] cpu : usr=0.79%, sys=7.38%, ctx=119920, majf=0, minf=35 IO depths : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=0,45590,0,0 short=45590,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=2687KiB/s (2752kB/s), 2687KiB/s-2687KiB/s (2752kB/s-2752kB/s), io=99.0MiB (105MB), run=38105-38105msec Disk stats (read/write): xvda: ios=0/96829, merge=0/3, ticks=0/47440, in_queue=47432, util=92.25% In the fsync data section you can see that the 99th percentile is 2245 or about 2.2ms of latency. This storage is well suited for an etcd node. The etcd documentation suggests that for storage to be fast enough, the 99th percentile of fdatasync invocations when writing to the WAL file must be less than 10ms.","title":"Testing etcd performance"},{"location":"000020100/#resolution","text":"What if your node's storage isn't fast enough? The simple solution is to upgrade the storage but that isn't always an option. If you are on the cusp of acceptable, there are things you can do to optimize your storage so that etcd is happy. Don't run etcd on a node with other roles. A general rule of thumb is to never have the worker role on the same node as etcd. However many environments have etcd and controlplane roles on the same node and run just fine. If this is the case for your environment then you should consider separating etcd and controlplane nodes. If you've separated etcd and the controlplane node and are still having issues, you can mount a separate volume for etcd so that read write operations for everything else on the node do not impact etcd's performance. This is mostly applicable to Cloud hosted nodes since each volume mounted has its own allocated set of resources. If you are on a dedicated server and would like to separate etcd read write operations from the rest of the server, you should install a new storage device for etcd mounts. Always use SSD's for your etcd nodes, whether it is dedicated or in the cloud. Set the priority of the etcd container so that it is higher than other processes but not too high that it overwhelms the server. ionice -c2 -n0 -p `pgrep -x etcd`","title":"Resolution"},{"location":"000020100/#further-reading","text":"Below is a list of links that I used for my research. I highly recommend reading these as they contain more information than I've posted in this article. IBM blog post on use of fio to test etcd storage performance etcd performance documentation etcd documentation on node sizing examples etcd metrics documentation etcd tuning documentation AWS blog post on the difference between burst and baseline performance in EC2 storage","title":"Further reading"},{"location":"000020100/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020101/","text":"How to create docker goroutine, and memory heap, dumps This document (000020101) is provided subject to the disclaimer at the end of this document. Situation Task It's important to observe Docker as it operates to help drive troubleshooting an issue. Here are some commands to generate memory heap and goroutine dumps without killing the Docker process. Pre-requisites Docker with an exposed socket (typically found at /var/run/docker.sock ) Collecting dumps Heap dump Heap dumps report a sampling of memory allocations of live objects. curl --unix-socket /var/run/docker.sock http://./debug/pprof/heap?debug=2 Goroutine dump The goroutine dump reports stack traces of all current goroutines for the docker process. curl --unix-socket /var/run/docker.sock http://./debug/pprof/goroutine?debug=2 The output normally is output to stdout , where it can be redirected to a file. Depending on how Docker is configured, and where its configured to log to, the traces could end up in the docker.log file or with the system logs (syslog, journalctl, kern.log, messages, etc...). Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to create docker goroutine, and memory heap, dumps"},{"location":"000020101/#how-to-create-docker-goroutine-and-memory-heap-dumps","text":"This document (000020101) is provided subject to the disclaimer at the end of this document.","title":"How to create docker goroutine, and memory heap, dumps"},{"location":"000020101/#situation","text":"","title":"Situation"},{"location":"000020101/#task","text":"It's important to observe Docker as it operates to help drive troubleshooting an issue. Here are some commands to generate memory heap and goroutine dumps without killing the Docker process.","title":"Task"},{"location":"000020101/#pre-requisites","text":"Docker with an exposed socket (typically found at /var/run/docker.sock )","title":"Pre-requisites"},{"location":"000020101/#collecting-dumps","text":"","title":"Collecting dumps"},{"location":"000020101/#heap-dump","text":"Heap dumps report a sampling of memory allocations of live objects. curl --unix-socket /var/run/docker.sock http://./debug/pprof/heap?debug=2","title":"Heap dump"},{"location":"000020101/#goroutine-dump","text":"The goroutine dump reports stack traces of all current goroutines for the docker process. curl --unix-socket /var/run/docker.sock http://./debug/pprof/goroutine?debug=2 The output normally is output to stdout , where it can be redirected to a file. Depending on how Docker is configured, and where its configured to log to, the traces could end up in the docker.log file or with the system logs (syslog, journalctl, kern.log, messages, etc...).","title":"Goroutine dump"},{"location":"000020101/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020102/","text":"Are API audit logs enabled in SUSE Rancher Hosted? This document (000020102) is provided subject to the disclaimer at the end of this document. Resolution Yes, API audit logs are enabled in SUSE Rancher Hosted at level 2. Level 2 includes log event metadata and request body, but does not include response metadata and response body. Rancher APIs typically do not contain personally identifiable information (PII). One exception is the user API which can contain a user's full name. More details on Rancher's API audit logging can be found in the Rancher Documentation . Audit logs are stored in the same region as your SUSE Rancher Hosted environment and retained for 1 month. Only the SUSE Rancher team has access to these logs for troubleshooting purposes. Customers may request logs by filing a support case on SCC and providing a date and time range in UTC. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Are API audit logs enabled in SUSE Rancher Hosted?"},{"location":"000020102/#are-api-audit-logs-enabled-in-suse-rancher-hosted","text":"This document (000020102) is provided subject to the disclaimer at the end of this document.","title":"Are API audit logs enabled in SUSE Rancher Hosted?"},{"location":"000020102/#resolution","text":"Yes, API audit logs are enabled in SUSE Rancher Hosted at level 2. Level 2 includes log event metadata and request body, but does not include response metadata and response body. Rancher APIs typically do not contain personally identifiable information (PII). One exception is the user API which can contain a user's full name. More details on Rancher's API audit logging can be found in the Rancher Documentation . Audit logs are stored in the same region as your SUSE Rancher Hosted environment and retained for 1 month. Only the SUSE Rancher team has access to these logs for troubleshooting purposes. Customers may request logs by filing a support case on SCC and providing a date and time range in UTC.","title":"Resolution"},{"location":"000020102/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020103/","text":"What information is stored in SUSE Rancher Hosted and where is it stored? This document (000020103) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted stores the following information: User Data First and last name of users (aka Display Name) Login id and password. Password is stored using one-way encryption and transported using TLS. Other user information from GitHub, Okta, Microsoft Active Directory, etc. if authentication integration is enabled. Cloud Provider Credentials (if provided) Amazon Web Services Access Key and Secret Key Microsoft Azure Subscription ID, Client ID, Client Secret DigitalOcean Access Token Linode Access Token VMWare vSphere endpoint, Username, and Password Similar types of keys, tokens, or credentials for other cloud providers that are enabled by the customer. Other Application Data Catalogs and Helm Charts CIS Scan Results Cluster Monitoring Metrics (if turned on) Cluster infrastructure, including node roles, node hardware specs, node software versions, workload metadata, workload logs. Anything else entered by the end-user in the Rancher user interface, API, or CLI which could change from version to version. Data is stored in our third-party cloud service provider on virtual machines managed by the SUSE Rancher Hosted operations team in the region/country selected by the customer. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What information is stored in SUSE Rancher Hosted and where is it stored?"},{"location":"000020103/#what-information-is-stored-in-suse-rancher-hosted-and-where-is-it-stored","text":"This document (000020103) is provided subject to the disclaimer at the end of this document.","title":"What information is stored in SUSE Rancher Hosted and where is it stored?"},{"location":"000020103/#resolution","text":"SUSE Rancher Hosted stores the following information:","title":"Resolution"},{"location":"000020103/#user-data","text":"First and last name of users (aka Display Name) Login id and password. Password is stored using one-way encryption and transported using TLS. Other user information from GitHub, Okta, Microsoft Active Directory, etc. if authentication integration is enabled.","title":"User Data"},{"location":"000020103/#cloud-provider-credentials-if-provided","text":"Amazon Web Services Access Key and Secret Key Microsoft Azure Subscription ID, Client ID, Client Secret DigitalOcean Access Token Linode Access Token VMWare vSphere endpoint, Username, and Password Similar types of keys, tokens, or credentials for other cloud providers that are enabled by the customer.","title":"Cloud Provider Credentials (if provided)"},{"location":"000020103/#other-application-data","text":"Catalogs and Helm Charts CIS Scan Results Cluster Monitoring Metrics (if turned on) Cluster infrastructure, including node roles, node hardware specs, node software versions, workload metadata, workload logs. Anything else entered by the end-user in the Rancher user interface, API, or CLI which could change from version to version. Data is stored in our third-party cloud service provider on virtual machines managed by the SUSE Rancher Hosted operations team in the region/country selected by the customer.","title":"Other Application Data"},{"location":"000020103/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020104/","text":"Filesystem actions in containers fail with `Too many levels of symbolic links` This document (000020104) is provided subject to the disclaimer at the end of this document. Situation Issue When attempting to perform a filesystem action inside a container with a volume located on an autofs directory, the error Too many levels of symbolic links is thrown and the action fails. bash: cd: /data: Too many levels of symbolic links Pre-requisites Docker container or Kubernetes Pod with a volume defined that is mounted on the host with autofs, typically backed by NFS Root cause As the share that backs the autofs volume isn't mounted until the directory specified is accessed, it is typically not mounted when a container is run. With the default Docker bind-mount propagation of rprivate , containers do not receive mount changes for volumes from the host. Resolution Docker - Mount the volume in question with the flag slave , rslave , shared , or rshared to ensure that mount changes are propagated to the container. Example: docker run -d -v /path/to/autofs:/data:shared ubuntu See the links at the bottom of this article for info on what each of these flags does Kubernetes - Define mountPropagation for the volume in question as either HostToContainer (same as Docker's rslave ) or Bidirectional (same as Docker's rshared ): kind: Pod apiVersion: v1 metadata: name: test-app spec: containers: - name: test image: busybox volumeMounts: - mountPath: \"/data\" name: test-app-vol mountPropagation: HostToContainer volumes: - name: test-app-vol hostPath: path: /data Further reading Docker bind propagation - https://docs.docker.com/storage/bind-mounts/#configure-bind-propagation Kubernetes mountPropagation - https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation Linux Kernel Shared Subtree - https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Filesystem actions in containers fail with \\`Too many levels of symbolic links\\`"},{"location":"000020104/#filesystem-actions-in-containers-fail-with-too-many-levels-of-symbolic-links","text":"This document (000020104) is provided subject to the disclaimer at the end of this document.","title":"Filesystem actions in containers fail with `Too many levels of symbolic links`"},{"location":"000020104/#situation","text":"","title":"Situation"},{"location":"000020104/#issue","text":"When attempting to perform a filesystem action inside a container with a volume located on an autofs directory, the error Too many levels of symbolic links is thrown and the action fails. bash: cd: /data: Too many levels of symbolic links","title":"Issue"},{"location":"000020104/#pre-requisites","text":"Docker container or Kubernetes Pod with a volume defined that is mounted on the host with autofs, typically backed by NFS","title":"Pre-requisites"},{"location":"000020104/#root-cause","text":"As the share that backs the autofs volume isn't mounted until the directory specified is accessed, it is typically not mounted when a container is run. With the default Docker bind-mount propagation of rprivate , containers do not receive mount changes for volumes from the host.","title":"Root cause"},{"location":"000020104/#resolution","text":"Docker - Mount the volume in question with the flag slave , rslave , shared , or rshared to ensure that mount changes are propagated to the container. Example: docker run -d -v /path/to/autofs:/data:shared ubuntu See the links at the bottom of this article for info on what each of these flags does Kubernetes - Define mountPropagation for the volume in question as either HostToContainer (same as Docker's rslave ) or Bidirectional (same as Docker's rshared ): kind: Pod apiVersion: v1 metadata: name: test-app spec: containers: - name: test image: busybox volumeMounts: - mountPath: \"/data\" name: test-app-vol mountPropagation: HostToContainer volumes: - name: test-app-vol hostPath: path: /data","title":"Resolution"},{"location":"000020104/#further-reading","text":"Docker bind propagation - https://docs.docker.com/storage/bind-mounts/#configure-bind-propagation Kubernetes mountPropagation - https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation Linux Kernel Shared Subtree - https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt","title":"Further reading"},{"location":"000020104/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020105/","text":"Best Practices Rancher This document (000020105) is provided subject to the disclaimer at the end of this document. Situation This article aims to provide a number of checks that can be evaluated to ensure best practices are in place when planning, building or preparing a Rancher 2.x and Kubernetes environment. 1. Architecture 1.1 Nodes Understanding workload resource needs in downstream clusters upfront can help choose an appropriate node configuration; some nodes may need different configurations; however, all nodes of the same role are generally configured the same. Checks Standardize on supported versions and ensure minimum requirements are met: Confirm the OS is covered in the supported versions Resource needs can vary based on cluster size and workload, however, in general, no less than 8GB of memory and 2 vCPUs is recommended SSD storage is recommended, especially for nodes with the etcd role Firewall rules allow connectivity for nodes ( k3s , RKE ) A static IP for all nodes is required, if using DHCP, all nodes should have a reserved address Swap is disabled on the nodes NTP is enabled on the nodes 1.2 Separation of concerns The Rancher management cluster should be dedicated to running the Rancher deployment, additional workloads added to the cluster can contend for resources and impact the performance and predictability of Rancher. This is also important to consider in downstream clusters, the etcd and control plane nodes (RKE), and server nodes (k3s) should be dedicated to the purpose. When possible, it is recommended that each node have a single role , for example, separate nodes for the etcd and control plane roles. Checks Using the following commands on each cluster, check and confirm for any unexpected workloads running on the Rancher management cluster, or running on the server or etcd/control plane nodes of a downstream cluster. Rancher management cluster Check for any unexpected pods running in the cluster: kubectl get pods --all-namespaces Check for any single points of failure or discrepancies in OS, kernel and CRI version: kubectl get nodes -o wide Downstream cluster k3sRKE Check for any unexpected pods running on server nodes: for n in $(kubectl get nodes -l node-role.kubernetes.io/master=true --no-headers | cut -d \" \" -f1) do kubectl get nodes --field-selector metadata.name=${n} --no-headers kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo done Check for any unexpected pods running on etcd nodes: for n in $(kubectl get nodes -l node-role.kubernetes.io/etcd=true --no-headers | cut -d \" \" -f1) do kubectl get nodes --field-selector metadata.name=${n} --no-headers kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo done Check for any unexpected pods running on control plane nodes: for n in $(kubectl get nodes -l node-role.kubernetes.io/controlplane=true --no-headers | cut -d \" \" -f1) do kubectl get nodes --field-selector metadata.name=${n} --no-headers kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo done 1.3 High Availability Ensure nodes within a cluster are spread across separate failure boundaries as much as possible. This could mean VMs running on separate physical hosts, data centres, switches, storage pools, etc. If running in a cloud environment, instances in separate availability zones. For High Availability in Rancher, a Kubernetes install is required. Checks When deploying the Rancher management cluster it is recommended to use the following configuration: DistributionRecommendationk3s2 server nodesRKE3 nodes with all roles Confirm the components of all clusters and external datastores (k3s) are satisfying minimum HA requirements: k3sRKE ComponentMinimumRecommendedNotesexternal datastore22 or greaterThe external datastore should provide failover to a standby using the datastore-endpoint server nodes22 or greaterAllow tolerance for at least 1 server node failureagent nodes2N/AAllow tolerance for at least 1 agent node failure, scale up to meet the workload needs ComponentMinimumRecommendedNotesetcd nodes33To maintain quorum it is important to have an uneven # of nodes to provide tolerance for at least 1 node failurecontrol plane nodes22Allow tolerance for at least 1 node failureworker nodes2N/AAllow tolerance for at least 1 worker node failure, scale up to meet the workload needs Cloud provider The following commands can also be used with clusters configured with a cloud provider to review the instance type and availability zones of each node. Kubernetes v1.17 or earlier: kubectl get nodes -L beta.kubernetes.io/instance-type -L failure-domain.beta.kubernetes.io/zone Kubernetes v1.17 or greater: kubectl get nodes -L node.kubernetes.io/instance-type -L topology.kubernetes.io/zone These labels may not be available on all cloud providers. 1.4 Load balancer To provide a consistent endpoint for the Rancher management cluster, a load balancer is highly recommended to ensure the Rancher agents, UI, and API connectivity can effectively reach the Rancher deployment. Checks The load balancer is configured: Within close proximity of the Rancher management cluster to reduce latency For high availability, with all Rancher management nodes configured as upstream targets With a health check to the following path: DistributionHealth check pathk3s /ping RKE /healthz A health check interval is generally recommended at 30 seconds or less 1.5 Proximity and latency For performance reasons, it is recommended to avoid spreading cluster nodes over long distances and unreliable networks. For example, nodes could be in separate AZs in the same region, the same datacenter, or separate nearby data centres. This is particularly important for etcd nodes which are sensitive to network latency, the RTT between etcd nodes in the cluster will determine the minimum time to complete a commit . Checks Network latency and bandwidth is adequate between locations that the cluster nodes will be provisioned A tool like mtr to gather connectivity statistics between locations over a long sample period can be useful to report on the packet loss and latency. Generally latency between etcd nodes is recommended at 5s or less 1.6 Datastore It is important to ensure that the chosen datastore is capable of handling requests inline with the workload of the cluster. Allocation of resources, storage performance, and tuning of the datastore may be needed over time, this could be due to an increase in churn in a cluster, downstream clusters growing in size, or the number of downstream clusters Rancher is managing increases. Checks Confirm the recommended options are met for the distribution in use: k3sRKE With an external datastore the general performance requirements include: SSD or similar storage providing 1,000 IOPs or greater performance Datastore servers are assigned 2 vCPUs and 4GB memory or greater A low latency connection to the datastore endpoint from all k3s server nodes MySQL 5.7 is recommended . If running in a cloud provider, you may wish to utilise a managed database service . To confirm the storage performance of etcd nodes is capable of handling the workload, a benchmark tool like fio can be used. Nodes with the etcd role have SSD or similar storage providing high IOPs and low latency On large downstream or Rancher environments, tuning etcd may be needed, including adding dedicated disk for etcd. 1.7 CIDR selection The cluster and service CIDRs cannot be changed once a cluster is provisioned. For this reason, it is important to future proof by changing the ranges to avoid routing overlaps with other areas of the network and potential cluster IP exhaustion if the defaults are not suitable. Checks The default CIDR ranges do not overlap with any area of the network The default CIDRs are below which often don't need to be changed, to ensure the are no issues with routing from or two pods you may wish to adjust these when creating clusters ( RKE , k3s ). NetworkDefault CIDRCluster10.42.0.0/16Service10.43.0.0/16 Reducing the CIDR sizes can lower the number of IPs available and therefore total number of pods and services in the cluster. In a large cluster, the CIDR ranges may need to be increased . 1.8 Authorized cluster endpoint At times connecting directly to a downstream cluster may be desired, this could be to reduce latency, avoid interruption if Rancher is unavailable, or that a high frequency of external API calls occur, for example, external monitoring, or a CI/CD pipeline. Checks Check for any use cases where an authorized cluster endpoint is needed Access directly to the downstream cluster kube-apiserver can be configured using the secondary context in the kubeconfig file. 2. Best Practices 2.1 Installing Rancher It is highly encouraged to install Rancher on a Kubernetes cluster in an HA configuration . If starting with small resource requirements, at the very minimum always install on a Kubernetes cluster with a single node, this provides a future path to adding nodes at a later date. The design of the single node Docker install is for short-lived testing environments, migration from a Docker to a Kubernetes install is not possible. Checks Rancher is installed on a Kubernetes cluster, even if that is a single node cluster 2.2 Rancher Resources The minimum resource requirements for nodes in the Rancher management cluster need to scale to match the number of downstream clusters and nodes; this may change over time and need reviewing as changes occur in the environment. Checks Verify that nodes in the Rancher management cluster meet at least the minimum requirements: ResourceRequirementsCPU/Memory Rancher v2.4.0 and greater CPU/Memory Rancher v2.4.0 and earlier Network Port requirements 2.3 Chart options When installing the Rancher helm chart, the default options may not always be the best fit for specific environments. Checks The Rancher helm chart is installed with the desired options replicas - the default number of Rancher replicas ( 3 ) may not suit your cluster, for example, a k3s cluster with 2 x server nodes using a replicas value of 2 will ensure only one Rancher pod is running per node. antiAffinity - the default preferred scheduling can mean Rancher pods become imbalanced during the lifetime of a cluster, using required can ensure Rancher is always scheduled on unique nodes To confirm the options provided on an existing Rancher install with helm v3, the following command can be used helm get values rancher -n cattle-system 2.4 Supported versions When choosing or maintaining the components for Rancher and Kubernetes clusters the product lifecycle and support matrix can be used to ensure the versions and OS configurations are certified and maintained. Checks All Rancher and Kubernetes cluster versions are under maintenance and certified As versions are a moving target, checking the current stable releases and planning for future upgrades on a schedule is recommended. 2.5 Recurring snapshots and backups It is important to configure snapshots on a recurring schedule and store these externally to the cluster for disaster recovery. Checks Recurring snapshots are configured for the distribution in use DistributionConfigurationk3sConfigure snapshots and backups on the external datastore, this can differ depending on the chosen databaseRKEConfigure recurring snapshots of etcd, with an S3 compatible endpoint for off-node copies In addition to a recurring schedule, it's important to take one-time snapshots of etcd (RKE) , or datastore (k3s) before and after significant changes. The Rancher backup operator can also be used on any distribution to backup the related objects that Rancher needs to function, this can be used to migrate Rancher between clusters. 2.6 Provisioning Provisioning nodes and resources for Rancher and downstream clusters in a repeatable and automated way will greatly improve the supportability of Rancher and Kubernetes. This allows nodes to be replaced in a cluster easily, and new clusters created in a consistent way. Checks The below points can help prepare the Rancher and Kubernetes environment with integrations and modern approaches to managing resources, such as infrastructure as code, CI/CD, immutable infrastructure, and configuration management: Manifests and configuration data are stored in source control, treated as the source of truth for containerized applications Automated build, deployment and/or configuration management The rancher2 terraform provider and pulumi package can be used to manage clusters and resources as code. 2.7 Managing node lifecycle When making significant planned changes it is important to drain nodes that are being affected to avoid disrupting in-flight connections, such as restarting Docker, patching, shutting down or removing nodes. For example, the kube-proxy component manages iptables rules on nodes to manage service endpoints, if a node is suddenly shutdown, stale endpoints and orphaned pods can be left in place for a period of time causing connectivity issues. In some cases during an unplanned issue, draining can be automated, such as when a node may be terminated, restarted, or shutdown. Checks A process is in place to drain before planned disruptive changes are performed on a node Where possible, node draining during the shutdown sequence is automated, for example, with a systemd or similar service 3. Operating Kubernetes 3.1 Capacity planning and Monitoring It is recommended to measure resource usage of all clusters by enabling monitoring in Rancher, or your chosen solution. It is recommended to alert on resource thresholds and events in the cluster. On supported platforms, using Cluster Autoscaler can be used to ensure the number of nodes is right-sized for the pod workload. Combining this with Horizontal Pod Autoscaler provides both application and infrastructure scaling capabilities. Checks Monitoring is enabled for the Rancher and downstream clusters Alert notifiers are configured to stay informed if an alarm or event occurs A process for adding/removing nodes is established, automated if possible 3.2 Probes In the defence against service and pod related failures, liveness and readiness probes are very useful; these can be in the form of HTTP requests, commands, or TCP connections. Checks Liveness and Readiness probes are configured where necessary Probes do not rely on the success of upstream dependencies, only the running application in the pod 3.3 Resources Assigning resource requests to pods allows the kube-scheduler to make more informed placement decisions, avoiding the \"bin packing\" of pods onto nodes and resource contention. Limits also offer value in the form of a safety net against pods consuming an undesired amount of resources. In addition to defining requests and limits for pods, it can also be useful to reserve capacity on nodes to prevent allocating resources that may be consumed by the kubelet and other system daemons, like Docker. Checks All pods define resource requests and have limits configured where necessary Nodes have system and daemon reservations where necessary When Rancher Monitoring is enabled, the graphs in Grafana can be used to find a baseline of CPU and Memory for resource requests 3.4 OS Limits Containerized applications can consume high amounts of OS resources, such as open files, connections, processes, filesystem space and inodes. Often the defaults are adequate; however, establishing a standardized image for all nodes can help establish a baseline for all configuration and tuning. Checks In general, the below can be used to confirm the OS limits allow for adequate headroom for the workloads File descriptor usage : cat /proc/sys/fs/file-nr User ulimits: ulimit -a Or, a particular process can be checked: cat /proc/PID/limits Conntrack limits: cat /proc/sys/net/netfilter/nf_conntrack_max cat /proc/sys/net/netfilter/nf_conntrack_count Filesystem space and inode usage: df -h and df -ih Requirements for Linux can differ slightly depending on the distribution, refer to the Linux Requirements for more information. 3.5 Log rotation To prevent large log files from accumulating, and apply a desired retention period it is recommended to rotate OS, pod log files, and configure an external log service to stream logs off the nodes for a longer-term lifecycle and easier searching. Checks Containers k3sRKE Log rotation is configured for the container logs An external logging service is configured as needed The below arguments for the INSTALL_K3S_EXEC environment variable can be used as an example to rotate container logs: INSTALL_K3S_EXEC=\"--kubelet-arg container-log-max-files=5 --kubelet-arg container-log-max-size=100Mi\" Log rotation is configured for the container logs An external logging service is configured as needed Rotating container logs can be accomplished by configuring logrotate or the /etc/daemon.json file with a size and retention configuration. OS Rotation of log files on nodes is also important, especially if a long node lifecycle is expected. 3.6 DNS scalability DNS is a critical service running within the cluster. DNS queries are distributed throughout the cluster, where the availability depends on the accessibility of the CoreDNS pods in the service. The Nodelocal DNS cache is a redesign on the architecture and is recommended for clusters that may experience high DNS workload or issues. Checks If a cluster has experienced a DNS issue, or high DNS workload is expected: Check the output of conntrack -S on related nodes. High amounts of the insert_failed counter can be indicative of a conntrack race condition, Nodelocal DNS cache is recommended to mitigate this. Status Top Issue Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Best Practices Rancher"},{"location":"000020105/#best-practices-rancher","text":"This document (000020105) is provided subject to the disclaimer at the end of this document.","title":"Best Practices Rancher"},{"location":"000020105/#situation","text":"This article aims to provide a number of checks that can be evaluated to ensure best practices are in place when planning, building or preparing a Rancher 2.x and Kubernetes environment.","title":"Situation"},{"location":"000020105/#1-architecture","text":"","title":"1. Architecture"},{"location":"000020105/#11-nodes","text":"Understanding workload resource needs in downstream clusters upfront can help choose an appropriate node configuration; some nodes may need different configurations; however, all nodes of the same role are generally configured the same. Checks Standardize on supported versions and ensure minimum requirements are met: Confirm the OS is covered in the supported versions Resource needs can vary based on cluster size and workload, however, in general, no less than 8GB of memory and 2 vCPUs is recommended SSD storage is recommended, especially for nodes with the etcd role Firewall rules allow connectivity for nodes ( k3s , RKE ) A static IP for all nodes is required, if using DHCP, all nodes should have a reserved address Swap is disabled on the nodes NTP is enabled on the nodes","title":"1.1 Nodes"},{"location":"000020105/#12-separation-of-concerns","text":"The Rancher management cluster should be dedicated to running the Rancher deployment, additional workloads added to the cluster can contend for resources and impact the performance and predictability of Rancher. This is also important to consider in downstream clusters, the etcd and control plane nodes (RKE), and server nodes (k3s) should be dedicated to the purpose. When possible, it is recommended that each node have a single role , for example, separate nodes for the etcd and control plane roles. Checks Using the following commands on each cluster, check and confirm for any unexpected workloads running on the Rancher management cluster, or running on the server or etcd/control plane nodes of a downstream cluster.","title":"1.2 Separation of concerns"},{"location":"000020105/#rancher-management-cluster","text":"Check for any unexpected pods running in the cluster: kubectl get pods --all-namespaces Check for any single points of failure or discrepancies in OS, kernel and CRI version: kubectl get nodes -o wide","title":"Rancher management cluster"},{"location":"000020105/#downstream-cluster","text":"k3sRKE Check for any unexpected pods running on server nodes: for n in $(kubectl get nodes -l node-role.kubernetes.io/master=true --no-headers | cut -d \" \" -f1) do kubectl get nodes --field-selector metadata.name=${n} --no-headers kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo done Check for any unexpected pods running on etcd nodes: for n in $(kubectl get nodes -l node-role.kubernetes.io/etcd=true --no-headers | cut -d \" \" -f1) do kubectl get nodes --field-selector metadata.name=${n} --no-headers kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo done Check for any unexpected pods running on control plane nodes: for n in $(kubectl get nodes -l node-role.kubernetes.io/controlplane=true --no-headers | cut -d \" \" -f1) do kubectl get nodes --field-selector metadata.name=${n} --no-headers kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo done","title":"Downstream cluster"},{"location":"000020105/#13-high-availability","text":"Ensure nodes within a cluster are spread across separate failure boundaries as much as possible. This could mean VMs running on separate physical hosts, data centres, switches, storage pools, etc. If running in a cloud environment, instances in separate availability zones. For High Availability in Rancher, a Kubernetes install is required. Checks When deploying the Rancher management cluster it is recommended to use the following configuration: DistributionRecommendationk3s2 server nodesRKE3 nodes with all roles Confirm the components of all clusters and external datastores (k3s) are satisfying minimum HA requirements: k3sRKE ComponentMinimumRecommendedNotesexternal datastore22 or greaterThe external datastore should provide failover to a standby using the datastore-endpoint server nodes22 or greaterAllow tolerance for at least 1 server node failureagent nodes2N/AAllow tolerance for at least 1 agent node failure, scale up to meet the workload needs ComponentMinimumRecommendedNotesetcd nodes33To maintain quorum it is important to have an uneven # of nodes to provide tolerance for at least 1 node failurecontrol plane nodes22Allow tolerance for at least 1 node failureworker nodes2N/AAllow tolerance for at least 1 worker node failure, scale up to meet the workload needs","title":"1.3 High Availability"},{"location":"000020105/#cloud-provider","text":"The following commands can also be used with clusters configured with a cloud provider to review the instance type and availability zones of each node. Kubernetes v1.17 or earlier: kubectl get nodes -L beta.kubernetes.io/instance-type -L failure-domain.beta.kubernetes.io/zone Kubernetes v1.17 or greater: kubectl get nodes -L node.kubernetes.io/instance-type -L topology.kubernetes.io/zone These labels may not be available on all cloud providers.","title":"Cloud provider"},{"location":"000020105/#14-load-balancer","text":"To provide a consistent endpoint for the Rancher management cluster, a load balancer is highly recommended to ensure the Rancher agents, UI, and API connectivity can effectively reach the Rancher deployment. Checks The load balancer is configured: Within close proximity of the Rancher management cluster to reduce latency For high availability, with all Rancher management nodes configured as upstream targets With a health check to the following path: DistributionHealth check pathk3s /ping RKE /healthz A health check interval is generally recommended at 30 seconds or less","title":"1.4 Load balancer"},{"location":"000020105/#15-proximity-and-latency","text":"For performance reasons, it is recommended to avoid spreading cluster nodes over long distances and unreliable networks. For example, nodes could be in separate AZs in the same region, the same datacenter, or separate nearby data centres. This is particularly important for etcd nodes which are sensitive to network latency, the RTT between etcd nodes in the cluster will determine the minimum time to complete a commit . Checks Network latency and bandwidth is adequate between locations that the cluster nodes will be provisioned A tool like mtr to gather connectivity statistics between locations over a long sample period can be useful to report on the packet loss and latency. Generally latency between etcd nodes is recommended at 5s or less","title":"1.5 Proximity and latency"},{"location":"000020105/#16-datastore","text":"It is important to ensure that the chosen datastore is capable of handling requests inline with the workload of the cluster. Allocation of resources, storage performance, and tuning of the datastore may be needed over time, this could be due to an increase in churn in a cluster, downstream clusters growing in size, or the number of downstream clusters Rancher is managing increases. Checks Confirm the recommended options are met for the distribution in use: k3sRKE With an external datastore the general performance requirements include: SSD or similar storage providing 1,000 IOPs or greater performance Datastore servers are assigned 2 vCPUs and 4GB memory or greater A low latency connection to the datastore endpoint from all k3s server nodes MySQL 5.7 is recommended . If running in a cloud provider, you may wish to utilise a managed database service . To confirm the storage performance of etcd nodes is capable of handling the workload, a benchmark tool like fio can be used. Nodes with the etcd role have SSD or similar storage providing high IOPs and low latency On large downstream or Rancher environments, tuning etcd may be needed, including adding dedicated disk for etcd.","title":"1.6 Datastore"},{"location":"000020105/#17-cidr-selection","text":"The cluster and service CIDRs cannot be changed once a cluster is provisioned. For this reason, it is important to future proof by changing the ranges to avoid routing overlaps with other areas of the network and potential cluster IP exhaustion if the defaults are not suitable. Checks The default CIDR ranges do not overlap with any area of the network The default CIDRs are below which often don't need to be changed, to ensure the are no issues with routing from or two pods you may wish to adjust these when creating clusters ( RKE , k3s ). NetworkDefault CIDRCluster10.42.0.0/16Service10.43.0.0/16 Reducing the CIDR sizes can lower the number of IPs available and therefore total number of pods and services in the cluster. In a large cluster, the CIDR ranges may need to be increased .","title":"1.7 CIDR selection"},{"location":"000020105/#18-authorized-cluster-endpoint","text":"At times connecting directly to a downstream cluster may be desired, this could be to reduce latency, avoid interruption if Rancher is unavailable, or that a high frequency of external API calls occur, for example, external monitoring, or a CI/CD pipeline. Checks Check for any use cases where an authorized cluster endpoint is needed Access directly to the downstream cluster kube-apiserver can be configured using the secondary context in the kubeconfig file.","title":"1.8 Authorized cluster endpoint"},{"location":"000020105/#2-best-practices","text":"","title":"2. Best Practices"},{"location":"000020105/#21-installing-rancher","text":"It is highly encouraged to install Rancher on a Kubernetes cluster in an HA configuration . If starting with small resource requirements, at the very minimum always install on a Kubernetes cluster with a single node, this provides a future path to adding nodes at a later date. The design of the single node Docker install is for short-lived testing environments, migration from a Docker to a Kubernetes install is not possible. Checks Rancher is installed on a Kubernetes cluster, even if that is a single node cluster","title":"2.1 Installing Rancher"},{"location":"000020105/#22-rancher-resources","text":"The minimum resource requirements for nodes in the Rancher management cluster need to scale to match the number of downstream clusters and nodes; this may change over time and need reviewing as changes occur in the environment. Checks Verify that nodes in the Rancher management cluster meet at least the minimum requirements: ResourceRequirementsCPU/Memory Rancher v2.4.0 and greater CPU/Memory Rancher v2.4.0 and earlier Network Port requirements","title":"2.2 Rancher Resources"},{"location":"000020105/#23-chart-options","text":"When installing the Rancher helm chart, the default options may not always be the best fit for specific environments. Checks The Rancher helm chart is installed with the desired options replicas - the default number of Rancher replicas ( 3 ) may not suit your cluster, for example, a k3s cluster with 2 x server nodes using a replicas value of 2 will ensure only one Rancher pod is running per node. antiAffinity - the default preferred scheduling can mean Rancher pods become imbalanced during the lifetime of a cluster, using required can ensure Rancher is always scheduled on unique nodes To confirm the options provided on an existing Rancher install with helm v3, the following command can be used helm get values rancher -n cattle-system","title":"2.3 Chart options"},{"location":"000020105/#24-supported-versions","text":"When choosing or maintaining the components for Rancher and Kubernetes clusters the product lifecycle and support matrix can be used to ensure the versions and OS configurations are certified and maintained. Checks All Rancher and Kubernetes cluster versions are under maintenance and certified As versions are a moving target, checking the current stable releases and planning for future upgrades on a schedule is recommended.","title":"2.4 Supported versions"},{"location":"000020105/#25-recurring-snapshots-and-backups","text":"It is important to configure snapshots on a recurring schedule and store these externally to the cluster for disaster recovery. Checks Recurring snapshots are configured for the distribution in use DistributionConfigurationk3sConfigure snapshots and backups on the external datastore, this can differ depending on the chosen databaseRKEConfigure recurring snapshots of etcd, with an S3 compatible endpoint for off-node copies In addition to a recurring schedule, it's important to take one-time snapshots of etcd (RKE) , or datastore (k3s) before and after significant changes. The Rancher backup operator can also be used on any distribution to backup the related objects that Rancher needs to function, this can be used to migrate Rancher between clusters.","title":"2.5 Recurring snapshots and backups"},{"location":"000020105/#26-provisioning","text":"Provisioning nodes and resources for Rancher and downstream clusters in a repeatable and automated way will greatly improve the supportability of Rancher and Kubernetes. This allows nodes to be replaced in a cluster easily, and new clusters created in a consistent way. Checks The below points can help prepare the Rancher and Kubernetes environment with integrations and modern approaches to managing resources, such as infrastructure as code, CI/CD, immutable infrastructure, and configuration management: Manifests and configuration data are stored in source control, treated as the source of truth for containerized applications Automated build, deployment and/or configuration management The rancher2 terraform provider and pulumi package can be used to manage clusters and resources as code.","title":"2.6 Provisioning"},{"location":"000020105/#27-managing-node-lifecycle","text":"When making significant planned changes it is important to drain nodes that are being affected to avoid disrupting in-flight connections, such as restarting Docker, patching, shutting down or removing nodes. For example, the kube-proxy component manages iptables rules on nodes to manage service endpoints, if a node is suddenly shutdown, stale endpoints and orphaned pods can be left in place for a period of time causing connectivity issues. In some cases during an unplanned issue, draining can be automated, such as when a node may be terminated, restarted, or shutdown. Checks A process is in place to drain before planned disruptive changes are performed on a node Where possible, node draining during the shutdown sequence is automated, for example, with a systemd or similar service","title":"2.7 Managing node lifecycle"},{"location":"000020105/#3-operating-kubernetes","text":"","title":"3. Operating Kubernetes"},{"location":"000020105/#31-capacity-planning-and-monitoring","text":"It is recommended to measure resource usage of all clusters by enabling monitoring in Rancher, or your chosen solution. It is recommended to alert on resource thresholds and events in the cluster. On supported platforms, using Cluster Autoscaler can be used to ensure the number of nodes is right-sized for the pod workload. Combining this with Horizontal Pod Autoscaler provides both application and infrastructure scaling capabilities. Checks Monitoring is enabled for the Rancher and downstream clusters Alert notifiers are configured to stay informed if an alarm or event occurs A process for adding/removing nodes is established, automated if possible","title":"3.1 Capacity planning and Monitoring"},{"location":"000020105/#32-probes","text":"In the defence against service and pod related failures, liveness and readiness probes are very useful; these can be in the form of HTTP requests, commands, or TCP connections. Checks Liveness and Readiness probes are configured where necessary Probes do not rely on the success of upstream dependencies, only the running application in the pod","title":"3.2 Probes"},{"location":"000020105/#33-resources","text":"Assigning resource requests to pods allows the kube-scheduler to make more informed placement decisions, avoiding the \"bin packing\" of pods onto nodes and resource contention. Limits also offer value in the form of a safety net against pods consuming an undesired amount of resources. In addition to defining requests and limits for pods, it can also be useful to reserve capacity on nodes to prevent allocating resources that may be consumed by the kubelet and other system daemons, like Docker. Checks All pods define resource requests and have limits configured where necessary Nodes have system and daemon reservations where necessary When Rancher Monitoring is enabled, the graphs in Grafana can be used to find a baseline of CPU and Memory for resource requests","title":"3.3 Resources"},{"location":"000020105/#34-os-limits","text":"Containerized applications can consume high amounts of OS resources, such as open files, connections, processes, filesystem space and inodes. Often the defaults are adequate; however, establishing a standardized image for all nodes can help establish a baseline for all configuration and tuning. Checks In general, the below can be used to confirm the OS limits allow for adequate headroom for the workloads File descriptor usage : cat /proc/sys/fs/file-nr User ulimits: ulimit -a Or, a particular process can be checked: cat /proc/PID/limits Conntrack limits: cat /proc/sys/net/netfilter/nf_conntrack_max cat /proc/sys/net/netfilter/nf_conntrack_count Filesystem space and inode usage: df -h and df -ih Requirements for Linux can differ slightly depending on the distribution, refer to the Linux Requirements for more information.","title":"3.4 OS Limits"},{"location":"000020105/#35-log-rotation","text":"To prevent large log files from accumulating, and apply a desired retention period it is recommended to rotate OS, pod log files, and configure an external log service to stream logs off the nodes for a longer-term lifecycle and easier searching. Checks","title":"3.5 Log rotation"},{"location":"000020105/#containers","text":"k3sRKE Log rotation is configured for the container logs An external logging service is configured as needed The below arguments for the INSTALL_K3S_EXEC environment variable can be used as an example to rotate container logs: INSTALL_K3S_EXEC=\"--kubelet-arg container-log-max-files=5 --kubelet-arg container-log-max-size=100Mi\" Log rotation is configured for the container logs An external logging service is configured as needed Rotating container logs can be accomplished by configuring logrotate or the /etc/daemon.json file with a size and retention configuration.","title":"Containers"},{"location":"000020105/#os","text":"Rotation of log files on nodes is also important, especially if a long node lifecycle is expected.","title":"OS"},{"location":"000020105/#36-dns-scalability","text":"DNS is a critical service running within the cluster. DNS queries are distributed throughout the cluster, where the availability depends on the accessibility of the CoreDNS pods in the service. The Nodelocal DNS cache is a redesign on the architecture and is recommended for clusters that may experience high DNS workload or issues. Checks If a cluster has experienced a DNS issue, or high DNS workload is expected: Check the output of conntrack -S on related nodes. High amounts of the insert_failed counter can be indicative of a conntrack race condition, Nodelocal DNS cache is recommended to mitigate this.","title":"3.6 DNS scalability"},{"location":"000020105/#status","text":"Top Issue","title":"Status"},{"location":"000020105/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020106/","text":"What permissions are required to grant access to manage Cluster Logging in Rancher v2.x This document (000020106) is provided subject to the disclaimer at the end of this document. Situation Question By default, only Global Admins or Cluster Owners have access to configure and manage Cluster Logging in a Rancher v2.x managed cluster. This article details the permissions required to grant this access to other users. Pre-requisites A Kubernetes cluster managed by Rancher v2.x Answer Cluster Logging configuration is managed by the ClusterLoggings Custom Resource in the management.cattle.io API Group. In order to create a role that grants permission to manage the logging configuration for a cluster, you should therefore grant all verbs on the CluserLoggings Resource in the management.cattle.io API group. You can define a custom Cluster Role via the Rancher UI, by navigating to the Global view, and selecting Security -> Roles -> Cluster, creating a custom role with these permissions. Granting this custom role on a cluster to a user or group will then provide access to manage the Cluster Logging configuration for that cluster. Further Reading Rancher v2.x Cluster Logging Documentation Rancher v2.x Role-Based Access Control (RBAC) Documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What permissions are required to grant access to manage Cluster Logging in Rancher v2.x"},{"location":"000020106/#what-permissions-are-required-to-grant-access-to-manage-cluster-logging-in-rancher-v2x","text":"This document (000020106) is provided subject to the disclaimer at the end of this document.","title":"What permissions are required to grant access to manage Cluster Logging in Rancher v2.x"},{"location":"000020106/#situation","text":"","title":"Situation"},{"location":"000020106/#question","text":"By default, only Global Admins or Cluster Owners have access to configure and manage Cluster Logging in a Rancher v2.x managed cluster. This article details the permissions required to grant this access to other users.","title":"Question"},{"location":"000020106/#pre-requisites","text":"A Kubernetes cluster managed by Rancher v2.x","title":"Pre-requisites"},{"location":"000020106/#answer","text":"Cluster Logging configuration is managed by the ClusterLoggings Custom Resource in the management.cattle.io API Group. In order to create a role that grants permission to manage the logging configuration for a cluster, you should therefore grant all verbs on the CluserLoggings Resource in the management.cattle.io API group. You can define a custom Cluster Role via the Rancher UI, by navigating to the Global view, and selecting Security -> Roles -> Cluster, creating a custom role with these permissions. Granting this custom role on a cluster to a user or group will then provide access to manage the Cluster Logging configuration for that cluster.","title":"Answer"},{"location":"000020106/#further-reading","text":"Rancher v2.x Cluster Logging Documentation Rancher v2.x Role-Based Access Control (RBAC) Documentation","title":"Further Reading"},{"location":"000020106/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020107/","text":"How to pull the logs from the rancher-wins service on a Windows node in a Rancher v2.x provisioned Kubernetes cluster This document (000020107) is provided subject to the disclaimer at the end of this document. Situation Task In Windows Kubernetes clusters , available in Rancher v2.3.0 and above, the rancher-wins service provides a method for Rancher to operate the Windows host. Whilst troubleshooting a Windows cluster issue it may be necessary to pull the logs from this service, as documented in this article. Pre-requisites A Windows Kubernetes cluster provisioned by Rancher v2.3.0 and above Steps To pull the logs from the rancher-wins service, execute the following command in a Powershell session on the node: Get-EventLog -LogName Application -Source rancher-wins > wins.log This will write the logs to the file wins.log in the working directory, which you can then provide in your Rancher Support Ticket, for analysis. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to pull the logs from the rancher-wins service on a Windows node in a Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020107/#how-to-pull-the-logs-from-the-rancher-wins-service-on-a-windows-node-in-a-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020107) is provided subject to the disclaimer at the end of this document.","title":"How to pull the logs from the rancher-wins service on a Windows node in a Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020107/#situation","text":"","title":"Situation"},{"location":"000020107/#task","text":"In Windows Kubernetes clusters , available in Rancher v2.3.0 and above, the rancher-wins service provides a method for Rancher to operate the Windows host. Whilst troubleshooting a Windows cluster issue it may be necessary to pull the logs from this service, as documented in this article.","title":"Task"},{"location":"000020107/#pre-requisites","text":"A Windows Kubernetes cluster provisioned by Rancher v2.3.0 and above","title":"Pre-requisites"},{"location":"000020107/#steps","text":"To pull the logs from the rancher-wins service, execute the following command in a Powershell session on the node: Get-EventLog -LogName Application -Source rancher-wins > wins.log This will write the logs to the file wins.log in the working directory, which you can then provide in your Rancher Support Ticket, for analysis.","title":"Steps"},{"location":"000020107/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020108/","text":"How to enable CoreDNS query logging in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster This document (000020108) is provided subject to the disclaimer at the end of this document. Situation Task By default, DNS query logging is disabled in CoreDNS, this article details the steps to enable query logging for CoreDNS in a Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x. Pre-requisites A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS dns add-on. Steps To enable DNS query logging the log plugin needs to be configured, by addition of log to the Corefile in the coredns ConfigMap of the kube-system Namespace. For example, to use the default log plugin configuration and log all queries, the Corefile definition would be updated as follows: .:53 { log errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy random } cache 30 loop reload loadbalance } Steps to update the CoreDNS ConfigMap and persist these changes can be found in the article \"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster\". For the full list of available options when configuring the log plugin refer to the plugin documentation . Further reading CoreDNS log plugin documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable CoreDNS query logging in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020108/#how-to-enable-coredns-query-logging-in-a-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020108) is provided subject to the disclaimer at the end of this document.","title":"How to enable CoreDNS query logging in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020108/#situation","text":"","title":"Situation"},{"location":"000020108/#task","text":"By default, DNS query logging is disabled in CoreDNS, this article details the steps to enable query logging for CoreDNS in a Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x.","title":"Task"},{"location":"000020108/#pre-requisites","text":"A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS dns add-on.","title":"Pre-requisites"},{"location":"000020108/#steps","text":"To enable DNS query logging the log plugin needs to be configured, by addition of log to the Corefile in the coredns ConfigMap of the kube-system Namespace. For example, to use the default log plugin configuration and log all queries, the Corefile definition would be updated as follows: .:53 { log errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy random } cache 30 loop reload loadbalance } Steps to update the CoreDNS ConfigMap and persist these changes can be found in the article \"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster\". For the full list of available options when configuring the log plugin refer to the plugin documentation .","title":"Steps"},{"location":"000020108/#further-reading","text":"CoreDNS log plugin documentation","title":"Further reading"},{"location":"000020108/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020109/","text":"How to use External TLS Termination with AWS This document (000020109) is provided subject to the disclaimer at the end of this document. Situation Task This document covers setting up Rancher using an AWS SSL certificate and an ALB (Application Load Balancer). Requirements Running Rancher management servers on AWS Resolution Configure the SSL certificate If you are using your own certificate follow the AWS documentation to import the certificate . If you are using an AWS certificate following the AWS documentation to request a public ACM certificate . Create the Target Group Log into the AWS Console to get started. Use Create a Target Group to create a Target group using the data in the tables below to complete the procedure: - Target Group Name: rancher-http-80 - Protocol: http - Port: 80 - Target type: instance - VPC: Choose your VPC - Protocol (Health Check): http - Path (Health Check): /healthz Use Register Targets to Rancher management servers making sure to use the port 80. Create the ALB From your web browser, navigate to the Amazon EC2 Console . From the navigation pane, choose LOAD BALANCING > Load Balancers. Click Create Load Balancer. Choose Application Load Balancer. Complete the Step 1: Configure Load Balancer form: - Basic Configuration - Name: rancher-http - Scheme: internet-facing - IP address type: ipv4 - Listeners - Add the Load Balancer Protocols and Load Balancer Ports below. - HTTP: 80 - HTTPS: 443 - Availability Zones - Select Your VPC and Availability Zones. Complete the Step 2: Configure Security Settings form. - Configure the certificate you want to use for SSL termination. Complete the Step 3: Configure Security Groups form. Complete the Step 4: Configure Routing form. - From the Target Group drop-down, choose Existing target group. - Add target group rancher-http-80. Complete Step 5: Register Targets. Since you registered your targets earlier, all you have to do it click Next: Review. Complete Step 6: Review. Look over the load balancer details and click Create when you\u2019re satisfied. After AWS creates the ALB, click Close. Configure External TLS Termination for Rancher You need to add the option --set tls=external to your Rancher install, per the following example: helm install rancher rancher-latest/rancher --namespace cattle-system --set hostname=mmattox-example.support.rancher.space --version 2.3.6 --set tls=external Verification Run the following command to verify new certificate: curl --insecure -v https://<<Rancher Hostname>> 2>&1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }' Example output: * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server did not agree to a protocol * Server certificate: * subject: OU=Domain Control Validated; CN=*.rancher.tools * start date: Jul 2 00:42:01 2019 GMT * expire date: May 2 00:19:41 2020 GMT * issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Mark bundle as not supporting multiuse * Connection #0 to host mmattox-example.support.rancher.space left intact NOTE: Some browsers will cache the certificate. Details on how to clear the SSL state in a browser can be found here. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to use External TLS Termination with AWS"},{"location":"000020109/#how-to-use-external-tls-termination-with-aws","text":"This document (000020109) is provided subject to the disclaimer at the end of this document.","title":"How to use External TLS Termination with AWS"},{"location":"000020109/#situation","text":"","title":"Situation"},{"location":"000020109/#task","text":"This document covers setting up Rancher using an AWS SSL certificate and an ALB (Application Load Balancer).","title":"Task"},{"location":"000020109/#requirements","text":"Running Rancher management servers on AWS","title":"Requirements"},{"location":"000020109/#resolution","text":"","title":"Resolution"},{"location":"000020109/#configure-the-ssl-certificate","text":"If you are using your own certificate follow the AWS documentation to import the certificate . If you are using an AWS certificate following the AWS documentation to request a public ACM certificate .","title":"Configure the SSL certificate"},{"location":"000020109/#create-the-target-group","text":"Log into the AWS Console to get started. Use Create a Target Group to create a Target group using the data in the tables below to complete the procedure: - Target Group Name: rancher-http-80 - Protocol: http - Port: 80 - Target type: instance - VPC: Choose your VPC - Protocol (Health Check): http - Path (Health Check): /healthz Use Register Targets to Rancher management servers making sure to use the port 80.","title":"Create the Target Group"},{"location":"000020109/#create-the-alb","text":"From your web browser, navigate to the Amazon EC2 Console . From the navigation pane, choose LOAD BALANCING > Load Balancers. Click Create Load Balancer. Choose Application Load Balancer. Complete the Step 1: Configure Load Balancer form: - Basic Configuration - Name: rancher-http - Scheme: internet-facing - IP address type: ipv4 - Listeners - Add the Load Balancer Protocols and Load Balancer Ports below. - HTTP: 80 - HTTPS: 443 - Availability Zones - Select Your VPC and Availability Zones. Complete the Step 2: Configure Security Settings form. - Configure the certificate you want to use for SSL termination. Complete the Step 3: Configure Security Groups form. Complete the Step 4: Configure Routing form. - From the Target Group drop-down, choose Existing target group. - Add target group rancher-http-80. Complete Step 5: Register Targets. Since you registered your targets earlier, all you have to do it click Next: Review. Complete Step 6: Review. Look over the load balancer details and click Create when you\u2019re satisfied. After AWS creates the ALB, click Close.","title":"Create the ALB"},{"location":"000020109/#configure-external-tls-termination-for-rancher","text":"You need to add the option --set tls=external to your Rancher install, per the following example: helm install rancher rancher-latest/rancher --namespace cattle-system --set hostname=mmattox-example.support.rancher.space --version 2.3.6 --set tls=external","title":"Configure External TLS Termination for Rancher"},{"location":"000020109/#verification","text":"Run the following command to verify new certificate: curl --insecure -v https://<<Rancher Hostname>> 2>&1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }' Example output: * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server did not agree to a protocol * Server certificate: * subject: OU=Domain Control Validated; CN=*.rancher.tools * start date: Jul 2 00:42:01 2019 GMT * expire date: May 2 00:19:41 2020 GMT * issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Mark bundle as not supporting multiuse * Connection #0 to host mmattox-example.support.rancher.space left intact NOTE: Some browsers will cache the certificate. Details on how to clear the SSL state in a browser can be found here.","title":"Verification"},{"location":"000020109/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020110/","text":"Can I Use Rancher 2.4 Dashboard Feature in Air-Gapped environment ? This document (000020110) is provided subject to the disclaimer at the end of this document. Situation Issue In an Air-Gapped environment, when I try to access the Dashboard Feature, it is not working. I keep having an error 500 from Rancher. Pre-requisites Rancher 2.4.x Air-Gapped environment Experimental Dashboard Feature enabled. Resolution The Dashboard feature is still experimental and is currently not bundled in the Rancher releases. It is currently hosted on https://releases.rancher.com/dashboard/latest/ This allows our Engineers to do some changes outside of the regular release cycle. The same rules apply if the Kubernetes cluster is using a proxy. The proxy should allow external access to this URL. Further Reading https://rancher.com/docs/rancher/v2.x/en/installation/options/feature-flags/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can I Use Rancher 2.4 Dashboard Feature in Air-Gapped environment ?"},{"location":"000020110/#can-i-use-rancher-24-dashboard-feature-in-air-gapped-environment","text":"This document (000020110) is provided subject to the disclaimer at the end of this document.","title":"Can I Use Rancher 2.4 Dashboard Feature in Air-Gapped environment ?"},{"location":"000020110/#situation","text":"","title":"Situation"},{"location":"000020110/#issue","text":"In an Air-Gapped environment, when I try to access the Dashboard Feature, it is not working. I keep having an error 500 from Rancher.","title":"Issue"},{"location":"000020110/#pre-requisites","text":"Rancher 2.4.x Air-Gapped environment Experimental Dashboard Feature enabled.","title":"Pre-requisites"},{"location":"000020110/#resolution","text":"The Dashboard feature is still experimental and is currently not bundled in the Rancher releases. It is currently hosted on https://releases.rancher.com/dashboard/latest/ This allows our Engineers to do some changes outside of the regular release cycle. The same rules apply if the Kubernetes cluster is using a proxy. The proxy should allow external access to this URL.","title":"Resolution"},{"location":"000020110/#further-reading","text":"https://rancher.com/docs/rancher/v2.x/en/installation/options/feature-flags/","title":"Further Reading"},{"location":"000020110/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020111/","text":"Kernel crash after \"unregister_netdevice: waiting for lo to become free. Usage count\" This document (000020111) is provided subject to the disclaimer at the end of this document. Situation Issue In a Linux Kubernetes cluster that has frequent pod creations and deletions along with large amounts of pod network traffic, the following error may be logged to the host system logs: \"unregister_netdevice: waiting for lo to become free. Usage count = 1\" The kernel will typically be in a semi-hung state after this, causing major system instability. Resolution This issue is fixed upstream in the Linux Kernel by this commit which was released in version 4.4.0. We recommend upgrading to the latest linux kernel available in your distribution. We have seen cases where certain kernel modules can cause this issue while loaded, even on a kernel that includes the fix above. If you are running a kernel higher than 4.4.0 and still seeing this issue, try disabling any third-party kernel modules to test. Project/OS Specific bugs: Docker - https://github.com/moby/moby/issues/5618 Ubuntu - https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1403152 - https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1711407 RedHat - https://access.redhat.com/solutions/3105941 - https://access.redhat.com/solutions/3659011 Centos - https://bugs.centos.org/view.php?id=12711 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Kernel crash after \"unregister\\_netdevice: waiting for lo to become free. Usage count\""},{"location":"000020111/#kernel-crash-after-unregister_netdevice-waiting-for-lo-to-become-free-usage-count","text":"This document (000020111) is provided subject to the disclaimer at the end of this document.","title":"Kernel crash after \"unregister_netdevice: waiting for lo to become free. Usage count\""},{"location":"000020111/#situation","text":"","title":"Situation"},{"location":"000020111/#issue","text":"In a Linux Kubernetes cluster that has frequent pod creations and deletions along with large amounts of pod network traffic, the following error may be logged to the host system logs: \"unregister_netdevice: waiting for lo to become free. Usage count = 1\" The kernel will typically be in a semi-hung state after this, causing major system instability.","title":"Issue"},{"location":"000020111/#resolution","text":"This issue is fixed upstream in the Linux Kernel by this commit which was released in version 4.4.0. We recommend upgrading to the latest linux kernel available in your distribution. We have seen cases where certain kernel modules can cause this issue while loaded, even on a kernel that includes the fix above. If you are running a kernel higher than 4.4.0 and still seeing this issue, try disabling any third-party kernel modules to test.","title":"Resolution"},{"location":"000020111/#projectos-specific-bugs","text":"Docker - https://github.com/moby/moby/issues/5618 Ubuntu - https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1403152 - https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1711407 RedHat - https://access.redhat.com/solutions/3105941 - https://access.redhat.com/solutions/3659011 Centos - https://bugs.centos.org/view.php?id=12711","title":"Project/OS Specific bugs:"},{"location":"000020111/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020112/","text":"Experimental Dashboard feature causes memory leak in rancher server and cattle-cluster-agent processes in Rancher v2.4.0 - v2.4.2 This document (000020112) is provided subject to the disclaimer at the end of this document. Situation Issue With the experimental Dashboard feature enabled in Rancher v2.4.0 through v2.4.2, the rancher server and cattle-cluster-agent processes will leak memory. As a result the memory usage of these Pods will grow over time, until they restart due to an Out of Memory (OOM) condition. Pre-requisites A Rancher install with either v2.4.0 or v2.4.2 Workaround Disable the experimental Dashboard feature within the Rancher UI: Navigate to the Global View -> Settings -> Feature Flags. Click the elipses for the dashboard entry and click Deactivate . Resolution Upgrade to a newer version of Rancher v2.4.3+. Further Reading GitHub issue #26577 GitHub issue #26633 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Experimental Dashboard feature causes memory leak in rancher server and cattle-cluster-agent processes in Rancher v2.4.0 - v2.4.2"},{"location":"000020112/#experimental-dashboard-feature-causes-memory-leak-in-rancher-server-and-cattle-cluster-agent-processes-in-rancher-v240-v242","text":"This document (000020112) is provided subject to the disclaimer at the end of this document.","title":"Experimental Dashboard feature causes memory leak in rancher server and cattle-cluster-agent processes in Rancher v2.4.0 - v2.4.2"},{"location":"000020112/#situation","text":"","title":"Situation"},{"location":"000020112/#issue","text":"With the experimental Dashboard feature enabled in Rancher v2.4.0 through v2.4.2, the rancher server and cattle-cluster-agent processes will leak memory. As a result the memory usage of these Pods will grow over time, until they restart due to an Out of Memory (OOM) condition.","title":"Issue"},{"location":"000020112/#pre-requisites","text":"A Rancher install with either v2.4.0 or v2.4.2","title":"Pre-requisites"},{"location":"000020112/#workaround","text":"Disable the experimental Dashboard feature within the Rancher UI: Navigate to the Global View -> Settings -> Feature Flags. Click the elipses for the dashboard entry and click Deactivate .","title":"Workaround"},{"location":"000020112/#resolution","text":"Upgrade to a newer version of Rancher v2.4.3+.","title":"Resolution"},{"location":"000020112/#further-reading","text":"GitHub issue #26577 GitHub issue #26633","title":"Further Reading"},{"location":"000020112/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020113/","text":"Logging integration doesn't work if Docker Root is not default /var/lib/docker This document (000020113) is provided subject to the disclaimer at the end of this document. Situation Issue As of the time of this writing, Rancher Logging is broken when the Docker root is configured to something other than /var/lib/docker . This issue is tracked in GitHub issue #21112 . Pre-requisites Rancher 2.x managed/imported cluster with logging enabled. Docker root configured to something other than /var/lib/docker on the nodes (confirmed with docker info | grep Root ). Workaround These steps will assume you have the Docker data root set to /other-docker-root . Change /other-docker-root to whatever your custom path is: Rancher UI -> Cluster -> System Project -> Workloads -> cattle-logging Namespace Find workload rancher-logging-fluentd-linux Edit YAML Edit volume dockerroot Change \"Path on the Node\" from /var/lib/docker to /other-docker-root Add volume (with the following details): Volume Name: dockerrootcustom Type: bind-mount Path on the Node: /other-docker-root Mount Point: /other-docker-root Click Save At this point logging should be working with your non-default Docker root directory. You should be able to verify this on your logging target. Keep in mind it may take a few minutes for logs to show up there as fluentd is configured to clear its buffer every 60 seconds by default. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Logging integration doesn't work if Docker Root is not default /var/lib/docker"},{"location":"000020113/#logging-integration-doesnt-work-if-docker-root-is-not-default-varlibdocker","text":"This document (000020113) is provided subject to the disclaimer at the end of this document.","title":"Logging integration doesn't work if Docker Root is not default /var/lib/docker"},{"location":"000020113/#situation","text":"","title":"Situation"},{"location":"000020113/#issue","text":"As of the time of this writing, Rancher Logging is broken when the Docker root is configured to something other than /var/lib/docker . This issue is tracked in GitHub issue #21112 .","title":"Issue"},{"location":"000020113/#pre-requisites","text":"Rancher 2.x managed/imported cluster with logging enabled. Docker root configured to something other than /var/lib/docker on the nodes (confirmed with docker info | grep Root ).","title":"Pre-requisites"},{"location":"000020113/#workaround","text":"These steps will assume you have the Docker data root set to /other-docker-root . Change /other-docker-root to whatever your custom path is: Rancher UI -> Cluster -> System Project -> Workloads -> cattle-logging Namespace Find workload rancher-logging-fluentd-linux Edit YAML Edit volume dockerroot Change \"Path on the Node\" from /var/lib/docker to /other-docker-root Add volume (with the following details): Volume Name: dockerrootcustom Type: bind-mount Path on the Node: /other-docker-root Mount Point: /other-docker-root Click Save At this point logging should be working with your non-default Docker root directory. You should be able to verify this on your logging target. Keep in mind it may take a few minutes for logs to show up there as fluentd is configured to clear its buffer every 60 seconds by default.","title":"Workaround"},{"location":"000020113/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020114/","text":"Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge ' for InstanceType. This document (000020114) is provided subject to the disclaimer at the end of this document. Situation Issue When trying to deploy a node in AWS EC2 on Rancher v1.6.x, you recieve an error like Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge\\t' for InstanceType. Pre-requisites Running Rancher UI v1.6.50 or earlier (included by default in Rancher v1.6.28 or earlier) Trying to deploy an AWS EC2 node of size r5.12xlarge , r5.24xlarge , r5a.12xlarge , or r5a.24xlarge Resolution The fix is in rancher ui v1.6.51 which is introduced in Rancher v1.6.29. Upgrade to v1.6.29 or later to deploy EC2 nodes of the affected sizes. It is advised to upgrade to the latest stable Rancher v1.6.x and migrate to latest stable Rancher v2.x Further reading The issue is a trailing tab in the name of the instance type. The commit that solves the issue can be seen here: https://github.com/rancher/ui/commit/5709e997aea949f41e299db8f519dc046d731cb9 rancher/ui v1.6.51: https://github.com/rancher/ui/tree/v1.6.51/app/components/machine rancher/rancher v1.6.29: https://github.com/rancher/rancher/releases/tag/v1.6.29 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge ' for InstanceType."},{"location":"000020114/#error-in-driver-during-machine-creation-error-launching-instance-invalidparametervalue-invalid-value-r512xlarge-for-instancetype","text":"This document (000020114) is provided subject to the disclaimer at the end of this document.","title":"Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge ' for InstanceType."},{"location":"000020114/#situation","text":"","title":"Situation"},{"location":"000020114/#issue","text":"When trying to deploy a node in AWS EC2 on Rancher v1.6.x, you recieve an error like Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge\\t' for InstanceType.","title":"Issue"},{"location":"000020114/#pre-requisites","text":"Running Rancher UI v1.6.50 or earlier (included by default in Rancher v1.6.28 or earlier) Trying to deploy an AWS EC2 node of size r5.12xlarge , r5.24xlarge , r5a.12xlarge , or r5a.24xlarge","title":"Pre-requisites"},{"location":"000020114/#resolution","text":"The fix is in rancher ui v1.6.51 which is introduced in Rancher v1.6.29. Upgrade to v1.6.29 or later to deploy EC2 nodes of the affected sizes. It is advised to upgrade to the latest stable Rancher v1.6.x and migrate to latest stable Rancher v2.x","title":"Resolution"},{"location":"000020114/#further-reading","text":"The issue is a trailing tab in the name of the instance type. The commit that solves the issue can be seen here: https://github.com/rancher/ui/commit/5709e997aea949f41e299db8f519dc046d731cb9 rancher/ui v1.6.51: https://github.com/rancher/ui/tree/v1.6.51/app/components/machine rancher/rancher v1.6.29: https://github.com/rancher/rancher/releases/tag/v1.6.29","title":"Further reading"},{"location":"000020114/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020115/","text":"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster This document (000020115) is provided subject to the disclaimer at the end of this document. Situation Task You might wish to update the Corefile configuration of CoreDNS, defined via the coredns ConfigMap in the kube-system Namespace, for example, in order to enable query logging or update the resolver policy. This article details how to update this ConfigMap and persist changes in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster. Pre-requisites A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS add-on. kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user. Steps Capture the current CoreDNS ConfigMap definition, with the following kubectl command: kubectl -n kube-system get configmap coredns -o go-template={{.data.Corefile}} The output should look like the following: .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy random } cache 30 loop reload loadbalance } Edit the cluster configuration YAML, to define a custom add-on containing the CoreDNS ConfigMap, with your desired changes. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click Edit as YAML . Create the add-on with the content below, replacing the Corefile definition with the existing configuration retrieved in step 1. Then make the desired changes, in this example the resolver policy is updated from random, in the existing configuration, to sequential. apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy sequential } cache 30 loop reload loadbalance } Update the cluster with the new configuration. For RKE provisioned clusters, invoke rke up --cluster.yml ( ensure the cluster.rkestate file is present in the working directory when invoking rke up ). For Rancher provisioned clusters, click Save in the Rancher UI Edit as YAML view. Further reading How to update CoreDNS's resolver policy Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020115/#how-to-update-the-coredns-configmap-in-a-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020115) is provided subject to the disclaimer at the end of this document.","title":"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020115/#situation","text":"","title":"Situation"},{"location":"000020115/#task","text":"You might wish to update the Corefile configuration of CoreDNS, defined via the coredns ConfigMap in the kube-system Namespace, for example, in order to enable query logging or update the resolver policy. This article details how to update this ConfigMap and persist changes in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster.","title":"Task"},{"location":"000020115/#pre-requisites","text":"A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS add-on. kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user.","title":"Pre-requisites"},{"location":"000020115/#steps","text":"Capture the current CoreDNS ConfigMap definition, with the following kubectl command: kubectl -n kube-system get configmap coredns -o go-template={{.data.Corefile}} The output should look like the following: .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy random } cache 30 loop reload loadbalance } Edit the cluster configuration YAML, to define a custom add-on containing the CoreDNS ConfigMap, with your desired changes. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click Edit as YAML . Create the add-on with the content below, replacing the Corefile definition with the existing configuration retrieved in step 1. Then make the desired changes, in this example the resolver policy is updated from random, in the existing configuration, to sequential. apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy sequential } cache 30 loop reload loadbalance } Update the cluster with the new configuration. For RKE provisioned clusters, invoke rke up --cluster.yml ( ensure the cluster.rkestate file is present in the working directory when invoking rke up ). For Rancher provisioned clusters, click Save in the Rancher UI Edit as YAML view.","title":"Steps"},{"location":"000020115/#further-reading","text":"How to update CoreDNS's resolver policy","title":"Further reading"},{"location":"000020115/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020116/","text":"How to run workloads on etcd or controlplane nodes, without the worker role, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster This document (000020116) is provided subject to the disclaimer at the end of this document. Situation Task Although it is normally not advised to run workloads on your controlplane and etcd nodes, there are occasionally scenarios when this is necessary. A few common examples are virus scanning, monitoring, and log collection workloads. Pre-requisites A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster Steps Both the controlplane and etcd nodes, which are not additionaly designated the worker role, have taints. When RKE or Rancher provisions these nodes, it adds these taints automatically. Workloads that need to run on these nodes require tolerations for these taints. For Rancher managed clusters you can see these taints within the Rancher UI on the cluster node view. The following kubectl command will also list the taints for each node. $ kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints NAME TAINTS ip-10-0-2-10 [map[effect:NoExecute key:node-role.kubernetes.io/etcd value:true]] ip-10-0-2-11 [map[effect:NoSchedule key:node-role.kubernetes.io/controlplane value:true]] ip-10-0-2-12 <none> Per this output, each etcd node has the NoExecute taint node-role.kubernetes.io/etcd=true and each controlplane node has the NoSchedule taint node-role.kubernetes.io/controlplane=true . The Rancher UI does not have fields for adding tolerations, so you will need to specify the tolerations directly in the workload's YAML manifest. You can use the Import YAML button to deploy your workload and make sure to add the following tolerations block in your manifest: spec: ... template: ... spec: ... tolerations: - operator: Exists ... If you have an existing workload, you can also select the View/Edit YAML option for the workload and apply the above change. This toleration will allow you to run the workload on any nodes with taints, so use with caution. If you are using Helm charts, you can also specify the same YAML in your Helm chart. Further Reading For more information on how taints and tolerations work in Kubernetes, see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to run workloads on etcd or controlplane nodes, without the worker role, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020116/#how-to-run-workloads-on-etcd-or-controlplane-nodes-without-the-worker-role-in-a-rancher-kubernetes-engine-rke-or-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020116) is provided subject to the disclaimer at the end of this document.","title":"How to run workloads on etcd or controlplane nodes, without the worker role, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020116/#situation","text":"","title":"Situation"},{"location":"000020116/#task","text":"Although it is normally not advised to run workloads on your controlplane and etcd nodes, there are occasionally scenarios when this is necessary. A few common examples are virus scanning, monitoring, and log collection workloads.","title":"Task"},{"location":"000020116/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","title":"Pre-requisites"},{"location":"000020116/#steps","text":"Both the controlplane and etcd nodes, which are not additionaly designated the worker role, have taints. When RKE or Rancher provisions these nodes, it adds these taints automatically. Workloads that need to run on these nodes require tolerations for these taints. For Rancher managed clusters you can see these taints within the Rancher UI on the cluster node view. The following kubectl command will also list the taints for each node. $ kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints NAME TAINTS ip-10-0-2-10 [map[effect:NoExecute key:node-role.kubernetes.io/etcd value:true]] ip-10-0-2-11 [map[effect:NoSchedule key:node-role.kubernetes.io/controlplane value:true]] ip-10-0-2-12 <none> Per this output, each etcd node has the NoExecute taint node-role.kubernetes.io/etcd=true and each controlplane node has the NoSchedule taint node-role.kubernetes.io/controlplane=true . The Rancher UI does not have fields for adding tolerations, so you will need to specify the tolerations directly in the workload's YAML manifest. You can use the Import YAML button to deploy your workload and make sure to add the following tolerations block in your manifest: spec: ... template: ... spec: ... tolerations: - operator: Exists ... If you have an existing workload, you can also select the View/Edit YAML option for the workload and apply the above change. This toleration will allow you to run the workload on any nodes with taints, so use with caution. If you are using Helm charts, you can also specify the same YAML in your Helm chart.","title":"Steps"},{"location":"000020116/#further-reading","text":"For more information on how taints and tolerations work in Kubernetes, see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/","title":"Further Reading"},{"location":"000020116/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020117/","text":"How to override DNS results served by CoreDNS This document (000020117) is provided subject to the disclaimer at the end of this document. Situation Task By default, DNS requests for pods using CoreDNS will be made directly to the upstream nameservers configured in /etc/resolv.conf on the node. At times, it may not be possible to easily update records on the upstream nameservers, or specific records for the cluster may be needed. In these cases it's useful to override the results that CoreDNS will serve pods. Pre-requisites These steps should work for any cluster running CoreDNS where the coredns ConfigMap is used. Steps There are two approaches to achieve this, please read through both to understand which is best for your environment. Both approaches require editting the coredns ConfigMap, specifically the Corefile key. This can be done in the UI by clicking View/Edit YAML, Edit, or on the command line with kubectl. Along with these options, both plugins covered provide other features, like adjusting the TTL for records, see the documentation links for more information. Rewrite The rewrite plugin will perform a rewritten query to the upstream nameserver, and respond to the query with the results. The outcome would be similar to configuring a CNAME for the domain. data: Corefile: | .:53 { [...] rewrite name archive.ubuntu.com internal-mirror.ubuntu.local } In this example, pods configured with the default Ubuntu mirror are now resolving to the internal mirror without any custom configuration. The benefit of this approach is that the upstream nameserver remains the source of truth for the results. Hosts The hosts plugin provides the ability to define a list of IPs and domains in the form of /etc/hosts to respond as query results. data: Corefile: | .:53 { [...] hosts { 10.0.0.1 archive.ubuntu.com 10.0.0.2 testing.com fallthrough } } A similar example, the internal IPs listed are provided as results. A downside to this approach is that the ConfigMap becomes a source of truth for these results, if changes in the environment are not reflected these entries could become stale. However, it does provide the most flexibility without needing to depend on any upstream nameserver to serve results. Persist the changes In an RKE or Rancher environment, during cluster or addon upgrades, it's possible that changes to the coredns ConfigMap are updated to use the provided version. To persist the changes made to the ConfigMap, add the changes as a user-defined addon . The steps to do this are documented under How To Update CoreDNS's Resolver Policy article. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to override DNS results served by CoreDNS"},{"location":"000020117/#how-to-override-dns-results-served-by-coredns","text":"This document (000020117) is provided subject to the disclaimer at the end of this document.","title":"How to override DNS results served by CoreDNS"},{"location":"000020117/#situation","text":"","title":"Situation"},{"location":"000020117/#task","text":"By default, DNS requests for pods using CoreDNS will be made directly to the upstream nameservers configured in /etc/resolv.conf on the node. At times, it may not be possible to easily update records on the upstream nameservers, or specific records for the cluster may be needed. In these cases it's useful to override the results that CoreDNS will serve pods.","title":"Task"},{"location":"000020117/#pre-requisites","text":"These steps should work for any cluster running CoreDNS where the coredns ConfigMap is used.","title":"Pre-requisites"},{"location":"000020117/#steps","text":"There are two approaches to achieve this, please read through both to understand which is best for your environment. Both approaches require editting the coredns ConfigMap, specifically the Corefile key. This can be done in the UI by clicking View/Edit YAML, Edit, or on the command line with kubectl. Along with these options, both plugins covered provide other features, like adjusting the TTL for records, see the documentation links for more information.","title":"Steps"},{"location":"000020117/#rewrite","text":"The rewrite plugin will perform a rewritten query to the upstream nameserver, and respond to the query with the results. The outcome would be similar to configuring a CNAME for the domain. data: Corefile: | .:53 { [...] rewrite name archive.ubuntu.com internal-mirror.ubuntu.local } In this example, pods configured with the default Ubuntu mirror are now resolving to the internal mirror without any custom configuration. The benefit of this approach is that the upstream nameserver remains the source of truth for the results.","title":"Rewrite"},{"location":"000020117/#hosts","text":"The hosts plugin provides the ability to define a list of IPs and domains in the form of /etc/hosts to respond as query results. data: Corefile: | .:53 { [...] hosts { 10.0.0.1 archive.ubuntu.com 10.0.0.2 testing.com fallthrough } } A similar example, the internal IPs listed are provided as results. A downside to this approach is that the ConfigMap becomes a source of truth for these results, if changes in the environment are not reflected these entries could become stale. However, it does provide the most flexibility without needing to depend on any upstream nameserver to serve results.","title":"Hosts"},{"location":"000020117/#persist-the-changes","text":"In an RKE or Rancher environment, during cluster or addon upgrades, it's possible that changes to the coredns ConfigMap are updated to use the provided version. To persist the changes made to the ConfigMap, add the changes as a user-defined addon . The steps to do this are documented under How To Update CoreDNS's Resolver Policy article.","title":"Persist the changes"},{"location":"000020117/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020118/","text":"How to monitor NTP on Linux nodes with Cluster Monitoring in Rancher v2.2.x+ This document (000020118) is provided subject to the disclaimer at the end of this document. Situation Task Time drift between nodes in a Kubernetes cluster can create a range of issues, from a difficulty to correlate application log message timestamps across nodes, to a loss of etcd quorum (given the time sensitive nature of the consensus algorithm used in etcd). Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution. This article details how to monitor time drift, via the Network Time Protocol (NTP), on Linux nodes within Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters. Pre-requisites A Rancher v2.x instance, starting at v2.2.0 and above A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with Cluster Monitoring enabled, with Monitoring Version 0.2.0+ ntp configured on Linux nodes in the cluster (refer to the documentation for your Linux distribution on enabling and configuring ntp) Steps Enable the NTP collector on the Node Exporter DaemonSet Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -> Monitoring In the bottom-right corner of the form, click Show advanced options Click Add Answer Configure the variable exporter-node.collectors.ntp.enabled with value true Click Save Configure an alert for NTP time drift Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -> Alerts On the A set of alerts for node Alert Group click Add Alert Rule Set Name to Node NTP time drift equal to or greater than 1 second Select Expression and enter node_ntp_offset_seconds Click Create Configure a Notifier for the A set of alerts for node Alert Group, by clicking the elipses for this Alert Group, and configuring the desired notifier in the Alert section at the bottom of the form. Further Reading Rancher Cluster Monitoring Documentation Prometheus Node Exporter README Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to monitor NTP on Linux nodes with Cluster Monitoring in Rancher v2.2.x+"},{"location":"000020118/#how-to-monitor-ntp-on-linux-nodes-with-cluster-monitoring-in-rancher-v22x","text":"This document (000020118) is provided subject to the disclaimer at the end of this document.","title":"How to monitor NTP on Linux nodes with Cluster Monitoring in Rancher v2.2.x+"},{"location":"000020118/#situation","text":"","title":"Situation"},{"location":"000020118/#task","text":"Time drift between nodes in a Kubernetes cluster can create a range of issues, from a difficulty to correlate application log message timestamps across nodes, to a loss of etcd quorum (given the time sensitive nature of the consensus algorithm used in etcd). Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution. This article details how to monitor time drift, via the Network Time Protocol (NTP), on Linux nodes within Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters.","title":"Task"},{"location":"000020118/#pre-requisites","text":"A Rancher v2.x instance, starting at v2.2.0 and above A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with Cluster Monitoring enabled, with Monitoring Version 0.2.0+ ntp configured on Linux nodes in the cluster (refer to the documentation for your Linux distribution on enabling and configuring ntp)","title":"Pre-requisites"},{"location":"000020118/#steps","text":"","title":"Steps"},{"location":"000020118/#enable-the-ntp-collector-on-the-node-exporter-daemonset","text":"Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -> Monitoring In the bottom-right corner of the form, click Show advanced options Click Add Answer Configure the variable exporter-node.collectors.ntp.enabled with value true Click Save","title":"Enable the NTP collector on the Node Exporter DaemonSet"},{"location":"000020118/#configure-an-alert-for-ntp-time-drift","text":"Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -> Alerts On the A set of alerts for node Alert Group click Add Alert Rule Set Name to Node NTP time drift equal to or greater than 1 second Select Expression and enter node_ntp_offset_seconds Click Create Configure a Notifier for the A set of alerts for node Alert Group, by clicking the elipses for this Alert Group, and configuring the desired notifier in the Alert section at the bottom of the form.","title":"Configure an alert for NTP time drift"},{"location":"000020118/#further-reading","text":"Rancher Cluster Monitoring Documentation Prometheus Node Exporter README","title":"Further Reading"},{"location":"000020118/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020119/","text":"How to enable NGINX support for HTTP headers with underscores This document (000020119) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to enable HTTP headers with underscores on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters. Pre-requisites A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced. For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher Resolution Configuration for RKE provisioned clusters Edit the cluster configuration YAML file to include the enable-underscores-in-headers: true option for the ingress, as follows: ingress: provider: nginx options: enable-underscores-in-headers: true Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Recycle the nginx pods in-order to pick up new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done Verify the new configuration: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done Configuration for Rancher provisioned clusters Login into the Rancher UI. Go to Global -> Clusters -> Cluster Name From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Include the enable-underscores-in-headers option for the ingress, as follows: ingress: provider: nginx options: enable-underscores-in-headers: true Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done - Run the following inside the kubectl CLI to verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable NGINX support for HTTP headers with underscores"},{"location":"000020119/#how-to-enable-nginx-support-for-http-headers-with-underscores","text":"This document (000020119) is provided subject to the disclaimer at the end of this document.","title":"How to enable NGINX support for HTTP headers with underscores"},{"location":"000020119/#situation","text":"","title":"Situation"},{"location":"000020119/#task","text":"This article details how to enable HTTP headers with underscores on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","title":"Task"},{"location":"000020119/#pre-requisites","text":"A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced. For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher","title":"Pre-requisites"},{"location":"000020119/#resolution","text":"","title":"Resolution"},{"location":"000020119/#configuration-for-rke-provisioned-clusters","text":"Edit the cluster configuration YAML file to include the enable-underscores-in-headers: true option for the ingress, as follows: ingress: provider: nginx options: enable-underscores-in-headers: true Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Recycle the nginx pods in-order to pick up new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done Verify the new configuration: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done","title":"Configuration for RKE provisioned clusters"},{"location":"000020119/#configuration-for-rancher-provisioned-clusters","text":"Login into the Rancher UI. Go to Global -> Clusters -> Cluster Name From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Include the enable-underscores-in-headers option for the ingress, as follows: ingress: provider: nginx options: enable-underscores-in-headers: true Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done - Run the following inside the kubectl CLI to verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on > /dev/null 2>&1 && echo 'Good' || echo 'Bad'\"; done","title":"Configuration for Rancher provisioned clusters"},{"location":"000020119/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020120/","text":"How to enable antiAffinity for Rancher v2.x server pods This document (000020120) is provided subject to the disclaimer at the end of this document. Situation Task By default the Rancher server pods are deployed without podAntiAffinity rules. As a result of this multiple Rancher pods may be scheduled onto a single node, potentially leading to temporary service disruption if the node is unavailable or gets rebooted. Pre-requisites Running Rancher v2.x kubectl access to the cluster Rancher Kubernetes Engine (RKE) to be installed, with access to the cluster.yml and correspoding cluster.rkestate file see the RKE documentation for more information helm v3 Steps You need to add the option --set-string antiAffinity=required to your Rancher install. Details on how to add this option to both new installations of Rancher, as well as existing deployment are provided below. New Rancher installation For new installations of Rancher, add the antiAffinity option to the helm install command, per the following example: helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set hostname=mmattox-example.support.rancher.space \\ --version 2.3.6 \\ --set-string antiAffinity=required NOTE: The Rancher version is pinned with the --version flag to prevent a version upgrade. Update existing Rancher deployments To add the antiAffinity option to an existing deployment of Rancher, follow the Rancher upgrade documentation , using the --version flag to pin to the running Rancher version, preventing a version upgrade. Run helm get values rancher to get the current Rancher helm chart values, which will be used to generate the helm upgrade command with matching values. Generate and run the helm upgrade command with the chart values, including the pinned version and antiAffinity option, per the following example: helm upgrade rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=mmattox-example.support.rancher.space \\ --version 2.3.6 \\ --set-string antiAffinity=required NOTE: The Rancher version is pinned with the --version flag to prevent a version upgrade. NOTE: We recommend saving this command for future Rancher upgrades to save time. Verification Run the command kubectl get deployment -n cattle-system rancher -o yaml and verify the following podAntiAffinity spec has been added: [...] spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - rancher topologyKey: kubernetes.io/hostname [...] Rollback To remove the antiAffnitiy configuration you should remove the --set-string antiAffinity=required option from the helm upgrade command and re-run this, per the following example: helm upgrade rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=mmattox-example.support.rancher.space \\ --version 2.3.6 NOTE: The Rancher version is pinned with the --version flag to prevent a version upgrade. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable antiAffinity for Rancher v2.x server pods"},{"location":"000020120/#how-to-enable-antiaffinity-for-rancher-v2x-server-pods","text":"This document (000020120) is provided subject to the disclaimer at the end of this document.","title":"How to enable antiAffinity for Rancher v2.x server pods"},{"location":"000020120/#situation","text":"","title":"Situation"},{"location":"000020120/#task","text":"By default the Rancher server pods are deployed without podAntiAffinity rules. As a result of this multiple Rancher pods may be scheduled onto a single node, potentially leading to temporary service disruption if the node is unavailable or gets rebooted.","title":"Task"},{"location":"000020120/#pre-requisites","text":"Running Rancher v2.x kubectl access to the cluster Rancher Kubernetes Engine (RKE) to be installed, with access to the cluster.yml and correspoding cluster.rkestate file see the RKE documentation for more information helm v3","title":"Pre-requisites"},{"location":"000020120/#steps","text":"You need to add the option --set-string antiAffinity=required to your Rancher install. Details on how to add this option to both new installations of Rancher, as well as existing deployment are provided below.","title":"Steps"},{"location":"000020120/#new-rancher-installation","text":"For new installations of Rancher, add the antiAffinity option to the helm install command, per the following example: helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set hostname=mmattox-example.support.rancher.space \\ --version 2.3.6 \\ --set-string antiAffinity=required NOTE: The Rancher version is pinned with the --version flag to prevent a version upgrade.","title":"New Rancher installation"},{"location":"000020120/#update-existing-rancher-deployments","text":"To add the antiAffinity option to an existing deployment of Rancher, follow the Rancher upgrade documentation , using the --version flag to pin to the running Rancher version, preventing a version upgrade. Run helm get values rancher to get the current Rancher helm chart values, which will be used to generate the helm upgrade command with matching values. Generate and run the helm upgrade command with the chart values, including the pinned version and antiAffinity option, per the following example: helm upgrade rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=mmattox-example.support.rancher.space \\ --version 2.3.6 \\ --set-string antiAffinity=required NOTE: The Rancher version is pinned with the --version flag to prevent a version upgrade. NOTE: We recommend saving this command for future Rancher upgrades to save time.","title":"Update existing Rancher deployments"},{"location":"000020120/#verification","text":"Run the command kubectl get deployment -n cattle-system rancher -o yaml and verify the following podAntiAffinity spec has been added: [...] spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - rancher topologyKey: kubernetes.io/hostname [...]","title":"Verification"},{"location":"000020120/#rollback","text":"To remove the antiAffnitiy configuration you should remove the --set-string antiAffinity=required option from the helm upgrade command and re-run this, per the following example: helm upgrade rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=mmattox-example.support.rancher.space \\ --version 2.3.6 NOTE: The Rancher version is pinned with the --version flag to prevent a version upgrade.","title":"Rollback"},{"location":"000020120/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020121/","text":"How to create a cluster in Rancher v2.x using the Rancher CLI or v3 API This document (000020121) is provided subject to the disclaimer at the end of this document. Situation Task The process for creating Kubernetes clusters via the Rancher v2.x UI is documented in \"Setting up Kubernetes Clusters in Rancher\" . This article details the process for creating Kubernetes clusters in Rancher v2.x via the Rancher CLI or v3 API interfaces. Pre-requisites A Rancher v2.x instance The Rancher CLI installed for CLI cluster creation (this can be downloaded from the Rancher UI, via the Download CLI link in the lower-right corner) curl installed to make Rancher v3 API requests for API cluster creation A Rancher API Key for a user with cluster creation permissions A Rancher Kubernetes Engine (RKE) cluster config file in YAML or JSON format (optional) Steps The cluster creation process is detailed below for both the Rancher CLI and v3 API. Cluster creation via the Rancher CLI Log in to your Rancher Server: rancher login <server_url> --token <token> Create the cluster: To create a cluster with the default cluster configuration: rancher cluster create <new_cluster_name> If you are passing in an RKE cluster config file, do so as follows: rancher cluster create --rke-config <rke_config_file> <new_cluster_name> Cluster creation via the Rancher v3 API Create a Rancher API Key , and save the access key and secret key as environment variables ( export CATTLE_ACCESS_KEY=<access_key> && export CATTLE_SECRET_KEY=<secret_key> ). Alternatively you can pass these directly into the curl request in place of the ${CATTLE_ACCESS_KEY} and ${CATTLE_SECRET_KEY} variables in the examples below. Send a POST request to the /v3/clusters API endpoint of your Rancher server: To create a cluster with the default cluster configuration: curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\ -X POST \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{\"name\":\"test-cluster\"}' \\ 'https://<rancher_server>/v3/clusters' If you are passing in an RKE cluster config file, do so as follows: curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\ -X POST \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d @<rke_config_file> \\ 'https://<rancher_server>/v3/clusters' Additional Reading The Rancher v2.x API Documentation The Rancher v2.x API Specification Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to create a cluster in Rancher v2.x using the Rancher CLI or v3 API"},{"location":"000020121/#how-to-create-a-cluster-in-rancher-v2x-using-the-rancher-cli-or-v3-api","text":"This document (000020121) is provided subject to the disclaimer at the end of this document.","title":"How to create a cluster in Rancher v2.x using the Rancher CLI or v3 API"},{"location":"000020121/#situation","text":"","title":"Situation"},{"location":"000020121/#task","text":"The process for creating Kubernetes clusters via the Rancher v2.x UI is documented in \"Setting up Kubernetes Clusters in Rancher\" . This article details the process for creating Kubernetes clusters in Rancher v2.x via the Rancher CLI or v3 API interfaces.","title":"Task"},{"location":"000020121/#pre-requisites","text":"A Rancher v2.x instance The Rancher CLI installed for CLI cluster creation (this can be downloaded from the Rancher UI, via the Download CLI link in the lower-right corner) curl installed to make Rancher v3 API requests for API cluster creation A Rancher API Key for a user with cluster creation permissions A Rancher Kubernetes Engine (RKE) cluster config file in YAML or JSON format (optional)","title":"Pre-requisites"},{"location":"000020121/#steps","text":"The cluster creation process is detailed below for both the Rancher CLI and v3 API.","title":"Steps"},{"location":"000020121/#cluster-creation-via-the-rancher-cli","text":"Log in to your Rancher Server: rancher login <server_url> --token <token> Create the cluster: To create a cluster with the default cluster configuration: rancher cluster create <new_cluster_name> If you are passing in an RKE cluster config file, do so as follows: rancher cluster create --rke-config <rke_config_file> <new_cluster_name>","title":"Cluster creation via the Rancher CLI"},{"location":"000020121/#cluster-creation-via-the-rancher-v3-api","text":"Create a Rancher API Key , and save the access key and secret key as environment variables ( export CATTLE_ACCESS_KEY=<access_key> && export CATTLE_SECRET_KEY=<secret_key> ). Alternatively you can pass these directly into the curl request in place of the ${CATTLE_ACCESS_KEY} and ${CATTLE_SECRET_KEY} variables in the examples below. Send a POST request to the /v3/clusters API endpoint of your Rancher server: To create a cluster with the default cluster configuration: curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\ -X POST \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{\"name\":\"test-cluster\"}' \\ 'https://<rancher_server>/v3/clusters' If you are passing in an RKE cluster config file, do so as follows: curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\ -X POST \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d @<rke_config_file> \\ 'https://<rancher_server>/v3/clusters'","title":"Cluster creation via the Rancher v3 API"},{"location":"000020121/#additional-reading","text":"The Rancher v2.x API Documentation The Rancher v2.x API Specification","title":"Additional Reading"},{"location":"000020121/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020122/","text":"How to edit the upstream nameservers used by CoreDNS or kube-dns, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster This document (000020122) is provided subject to the disclaimer at the end of this document. Situation Task By default, CoreDNS and kube-dns Pods will inherit the nameserver configuration from the node. In certain circumstances it might be desired to override this, and use a specific set of nameservers for external queries. Note: These steps update the nameservers only for Pods that use either the ClusterFirst (default) or ClusterFirstWithHostNet DNS policy . Nameserver configuration for nodes and other Pods will not be affected. Pre-requisites A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, with the CoreDNS or kube-dns dns addon enabled. Note: New clusters can also be created using the same steps. Steps Option A: Update the cluster.yaml The cluster configuration YAML provides the upstreamnameservers option, to configure a list of upstream nameservers, per the example below: Add the upstreamnameservers option, with the list of nameservers, to the cluster configuration YAML. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click Edit as YAML . dns: provider: coredns upstreamnameservers: - 1.1.1.1 - 8.8.8.8 Update the cluster with the new configuration. For RKE provisioned clusters, invoke rke up --cluster.yml ( ensure the cluster.rkestate file is present in the working directory when invoking rke up ). For Rancher provisioned clusters, click Save in the Rancher UI Edit as YAML view. Note: This option is recommended as it requires minimal change, see the RKE add-ons documentation for more information. Option B: Update the kubelet resolv.conf By default, the kubelet will refer to the /etc/resolv.conf file as the source for nameserver configuration. It is possible to override this by adding an extra_args option to the kubelet service, and this is also accomplished in the cluster configuration YAML. A custom resolv.conf file can then be used by the kubelet instead, per the example below: On each of the nodes in the cluster create the custom nameserver configuration file: echo \"nameserver 8.8.8.8\" > /etc/k8s-resolv.conf Add resolv-conf , referencing the custom nameserver configuration file, to the extra_args option for the kubelet service, in the cluster configuration YAML. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click Edit as YAML . services: kubelet: extra_args: resolv-conf: /host/etc/k8s-resolv.conf Update the cluster with the new configuration. For RKE provisioned clusters, invoke rke up --cluster.yml ( ensure the cluster.rkestate file is present in the working directory when invoking rke up ). For Rancher provisioned clusters, click Save in the Rancher UI Edit as YAML view. See the RKE services documentation for more information. Note: kubelet flags are being updated, as such a restart of the kubelet component will occur on each node. Option C: Update the node resolv.conf If the nameserver configuration should be consistent between the OS and Kubernetes Pods, updating the node /etc/resolv.conf file is recommended. This could be because nameservers are changing or that the caching configuration (for example systemd-resolved) is not desired. Changes to a systemd managed resolv.conf can be dependent on the Linux distribution and you should refer to the documentation for the distribution used in the cluster. Note: The kubelet component caches the /etc/resolv.conf file at start time, as such a restart of the kubelet component needs to occur on each node manually, after updating the configuration. This can be accomplished a number of ways: docker restart kubelet on each node A drain and restart of each node Replacing nodes in the cluster with the updated configuration Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to edit the upstream nameservers used by CoreDNS or kube-dns, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020122/#how-to-edit-the-upstream-nameservers-used-by-coredns-or-kube-dns-in-a-rancher-kubernetes-engine-rke-or-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020122) is provided subject to the disclaimer at the end of this document.","title":"How to edit the upstream nameservers used by CoreDNS or kube-dns, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020122/#situation","text":"","title":"Situation"},{"location":"000020122/#task","text":"By default, CoreDNS and kube-dns Pods will inherit the nameserver configuration from the node. In certain circumstances it might be desired to override this, and use a specific set of nameservers for external queries. Note: These steps update the nameservers only for Pods that use either the ClusterFirst (default) or ClusterFirstWithHostNet DNS policy . Nameserver configuration for nodes and other Pods will not be affected.","title":"Task"},{"location":"000020122/#pre-requisites","text":"A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, with the CoreDNS or kube-dns dns addon enabled. Note: New clusters can also be created using the same steps.","title":"Pre-requisites"},{"location":"000020122/#steps","text":"","title":"Steps"},{"location":"000020122/#option-a-update-the-clusteryaml","text":"The cluster configuration YAML provides the upstreamnameservers option, to configure a list of upstream nameservers, per the example below: Add the upstreamnameservers option, with the list of nameservers, to the cluster configuration YAML. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click Edit as YAML . dns: provider: coredns upstreamnameservers: - 1.1.1.1 - 8.8.8.8 Update the cluster with the new configuration. For RKE provisioned clusters, invoke rke up --cluster.yml ( ensure the cluster.rkestate file is present in the working directory when invoking rke up ). For Rancher provisioned clusters, click Save in the Rancher UI Edit as YAML view. Note: This option is recommended as it requires minimal change, see the RKE add-ons documentation for more information.","title":"Option A: Update the cluster.yaml"},{"location":"000020122/#option-b-update-the-kubelet-resolvconf","text":"By default, the kubelet will refer to the /etc/resolv.conf file as the source for nameserver configuration. It is possible to override this by adding an extra_args option to the kubelet service, and this is also accomplished in the cluster configuration YAML. A custom resolv.conf file can then be used by the kubelet instead, per the example below: On each of the nodes in the cluster create the custom nameserver configuration file: echo \"nameserver 8.8.8.8\" > /etc/k8s-resolv.conf Add resolv-conf , referencing the custom nameserver configuration file, to the extra_args option for the kubelet service, in the cluster configuration YAML. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click Edit as YAML . services: kubelet: extra_args: resolv-conf: /host/etc/k8s-resolv.conf Update the cluster with the new configuration. For RKE provisioned clusters, invoke rke up --cluster.yml ( ensure the cluster.rkestate file is present in the working directory when invoking rke up ). For Rancher provisioned clusters, click Save in the Rancher UI Edit as YAML view. See the RKE services documentation for more information. Note: kubelet flags are being updated, as such a restart of the kubelet component will occur on each node.","title":"Option B: Update the kubelet resolv.conf"},{"location":"000020122/#option-c-update-the-node-resolvconf","text":"If the nameserver configuration should be consistent between the OS and Kubernetes Pods, updating the node /etc/resolv.conf file is recommended. This could be because nameservers are changing or that the caching configuration (for example systemd-resolved) is not desired. Changes to a systemd managed resolv.conf can be dependent on the Linux distribution and you should refer to the documentation for the distribution used in the cluster. Note: The kubelet component caches the /etc/resolv.conf file at start time, as such a restart of the kubelet component needs to occur on each node manually, after updating the configuration. This can be accomplished a number of ways: docker restart kubelet on each node A drain and restart of each node Replacing nodes in the cluster with the updated configuration","title":"Option C: Update the node resolv.conf"},{"location":"000020122/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020123/","text":"How to change etcd cipher suite This document (000020123) is provided subject to the disclaimer at the end of this document. Situation Hardening ETCD cluster communication Synopsis: This article will walk Rancher administrators through hardening the cluster communication between etcd nodes. We'll go over configuring etcd to use specific ciphers which enable stronger encryption for securing intra-cluster etcd traffic. Configuring etcd (rke and Rancher UI): To make the modifications we'll be configuring our rke cluster YAML spec. This setting would be defined, then applied at the command line with the rke CLI, or alternately via the Rancher UI. From within the Rancher UI, navigate to the cluster you're looking to modify, and click edit under the 3 dot menu. From there, you should see a button labeled 'Edit as Yaml'. At the cluster YAML spec view we define the cipher-suites parameter under the etcd service definition. We recommend testing this out in a non-vital cluster before rolling out on important clusters to become familiar with the process. services: etcd: extra_args: cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" election-timeout: \"5000\" heartbeat-interval: \"500\" Note: The cipher suites defined in the example could trade off speed for stronger encryption. Consider the level of ciphers in use and how they could impact the performance of an etcd cluster. Testing should be done to factor the spec of your hosts (cpu, memory, disk, network, etc...) and the typical types of interacting with kubernetes as well as the amount of resources under management within the k8s cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to change etcd cipher suite"},{"location":"000020123/#how-to-change-etcd-cipher-suite","text":"This document (000020123) is provided subject to the disclaimer at the end of this document.","title":"How to change etcd cipher suite"},{"location":"000020123/#situation","text":"","title":"Situation"},{"location":"000020123/#hardening-etcd-cluster-communication","text":"","title":"Hardening ETCD cluster communication"},{"location":"000020123/#synopsis","text":"This article will walk Rancher administrators through hardening the cluster communication between etcd nodes. We'll go over configuring etcd to use specific ciphers which enable stronger encryption for securing intra-cluster etcd traffic.","title":"Synopsis:"},{"location":"000020123/#configuring-etcd-rke-and-rancher-ui","text":"To make the modifications we'll be configuring our rke cluster YAML spec. This setting would be defined, then applied at the command line with the rke CLI, or alternately via the Rancher UI. From within the Rancher UI, navigate to the cluster you're looking to modify, and click edit under the 3 dot menu. From there, you should see a button labeled 'Edit as Yaml'. At the cluster YAML spec view we define the cipher-suites parameter under the etcd service definition. We recommend testing this out in a non-vital cluster before rolling out on important clusters to become familiar with the process. services: etcd: extra_args: cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" election-timeout: \"5000\" heartbeat-interval: \"500\"","title":"Configuring etcd (rke and Rancher UI):"},{"location":"000020123/#note","text":"The cipher suites defined in the example could trade off speed for stronger encryption. Consider the level of ciphers in use and how they could impact the performance of an etcd cluster. Testing should be done to factor the spec of your hosts (cpu, memory, disk, network, etc...) and the typical types of interacting with kubernetes as well as the amount of resources under management within the k8s cluster.","title":"Note:"},{"location":"000020123/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020124/","text":"How to block external connectivity with Calico This document (000020124) is provided subject to the disclaimer at the end of this document. Situation Task In cases where it is desired to control external connectivity from the cluster, such as to deny or allow specific IP addresses or ports from Pods using the CNI network, a GlobalNetworkPolicy object can be used to control the rules applied to all nodes in the cluster. The GlobalNetworkPolicy is provided by the Calico CRD deployed on RKE clusters. Pre-requisites An RKE cluster configured with the Canal or Calico CNI Steps Configure a YAML manifest the desired rules, using the nets and/or ports keys, the Calico documentation provides some more information on each field. In the below example the EC2 metadata is being denied to prevent Pods from accessing the IAM profile credentials of the instance. apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: deny-ec2-metadata spec: types: - Egress egress: - action: Deny destination: nets: - 169.254.169.254/32 - action: Allow destination: nets: - 0.0.0.0/0 Deny 80/TCP connectivity external to the cluster apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: deny-http spec: types: - Egress egress: - action: Deny protocol: TCP destination: ports: - 80 - action: Allow destination: nets: - 0.0.0.0/0 Apply the YAML file created and test connectivity from a Pod running within the cluster on the CNI network. Note: Pods running with hostnetwork: true will not be effected by the GlobalNetworkPolicy as these Pods do not use the CNI network. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to block external connectivity with Calico"},{"location":"000020124/#how-to-block-external-connectivity-with-calico","text":"This document (000020124) is provided subject to the disclaimer at the end of this document.","title":"How to block external connectivity with Calico"},{"location":"000020124/#situation","text":"","title":"Situation"},{"location":"000020124/#task","text":"In cases where it is desired to control external connectivity from the cluster, such as to deny or allow specific IP addresses or ports from Pods using the CNI network, a GlobalNetworkPolicy object can be used to control the rules applied to all nodes in the cluster. The GlobalNetworkPolicy is provided by the Calico CRD deployed on RKE clusters.","title":"Task"},{"location":"000020124/#pre-requisites","text":"An RKE cluster configured with the Canal or Calico CNI","title":"Pre-requisites"},{"location":"000020124/#steps","text":"Configure a YAML manifest the desired rules, using the nets and/or ports keys, the Calico documentation provides some more information on each field. In the below example the EC2 metadata is being denied to prevent Pods from accessing the IAM profile credentials of the instance. apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: deny-ec2-metadata spec: types: - Egress egress: - action: Deny destination: nets: - 169.254.169.254/32 - action: Allow destination: nets: - 0.0.0.0/0 Deny 80/TCP connectivity external to the cluster apiVersion: crd.projectcalico.org/v1 kind: GlobalNetworkPolicy metadata: name: deny-http spec: types: - Egress egress: - action: Deny protocol: TCP destination: ports: - 80 - action: Allow destination: nets: - 0.0.0.0/0 Apply the YAML file created and test connectivity from a Pod running within the cluster on the CNI network. Note: Pods running with hostnetwork: true will not be effected by the GlobalNetworkPolicy as these Pods do not use the CNI network.","title":"Steps"},{"location":"000020124/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020125/","text":"How to Enable Pod Presets This document (000020125) is provided subject to the disclaimer at the end of this document. Situation Task This how-to article outlines how to enable pod presets on your cluster. This is done by enabling the PodPreset admission plugin and the settings.k8s.io/v1alpha1 API for the kube-apiserver. Pre-requisites Kubernetes version 1.10 and above Access to edit the cluster in yaml or the cluster.yaml file you used with RKE. Resolution Get to the cluster yaml in Rancher by editing the cluster and selecting \"edit as yaml\" or by opening the RKE cluster.yml file. Modify the kube-api section to resemble the following and hit save or running rke up : services: kube-api: extra_args: runtime-config: authorization.k8s.io/v1beta1=true,settings.k8s.io/v1alpha1=true enable-admission-plugins: PodPreset,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize Notice that settings.k8s.io/v1alpha1/podpreset and PodPreset is added to the runtime-config and admission plugins. Further reading You can test the ability to use pod presets with this guide . More details can be found in the kubernetes docs on pod presets . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to Enable Pod Presets"},{"location":"000020125/#how-to-enable-pod-presets","text":"This document (000020125) is provided subject to the disclaimer at the end of this document.","title":"How to Enable Pod Presets"},{"location":"000020125/#situation","text":"","title":"Situation"},{"location":"000020125/#task","text":"This how-to article outlines how to enable pod presets on your cluster. This is done by enabling the PodPreset admission plugin and the settings.k8s.io/v1alpha1 API for the kube-apiserver.","title":"Task"},{"location":"000020125/#pre-requisites","text":"Kubernetes version 1.10 and above Access to edit the cluster in yaml or the cluster.yaml file you used with RKE.","title":"Pre-requisites"},{"location":"000020125/#resolution","text":"Get to the cluster yaml in Rancher by editing the cluster and selecting \"edit as yaml\" or by opening the RKE cluster.yml file. Modify the kube-api section to resemble the following and hit save or running rke up : services: kube-api: extra_args: runtime-config: authorization.k8s.io/v1beta1=true,settings.k8s.io/v1alpha1=true enable-admission-plugins: PodPreset,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize Notice that settings.k8s.io/v1alpha1/podpreset and PodPreset is added to the runtime-config and admission plugins.","title":"Resolution"},{"location":"000020125/#further-reading","text":"You can test the ability to use pod presets with this guide . More details can be found in the kubernetes docs on pod presets .","title":"Further reading"},{"location":"000020125/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020126/","text":"How to use nginx /dbg This document (000020126) is provided subject to the disclaimer at the end of this document. Situation What is the /dbg command /dbg is a program included in the ingress-nginx container image that can be used to show information about the nginx environment and the resulting nginx configuration, which can be helpful when debugging ingress issues in Kubernetes. Requirements A Kubernetes cluster that has ingress enabled with ingress-nginx as the ingress controller A cluster with Linux nodes, nginx will not run on Windows kubectl configured Using /dbg This command needs to be run from inside one of the ingress-nginx pods, so first determine the pod to run it in. > kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE default-http-backend-67cf578fc4-54jlz 1/1 Running 0 5d nginx-ingress-controller-56nss 1/1 Running 0 5d nginx-ingress-controller-hscfg 1/1 Running 0 4d21h nginx-ingress-controller-n4p22 1/1 Running 0 5d > export NGINX_POD=nginx-ingress-controller-n4p22 If you are diagnosing specific connection issues, you can determine which controller is receiving the traffic by looking through the logs of each. Viewing ingress-controller status /dbg general will show the count of running controllers. > kubectl exec -n ingress-nginx $NGINX_POD /dbg general { \"controllerPodsCount\": 3 } Viewing backend configuration /dbg backends list will list the discovered backends: ``` kubectl exec -n ingress-nginx $NGINX_POD /dbg backends list cattle-system-rancher-80 upstream-default-backend ``` /dbg backends get will show the configuration for the named backend: > kubectl exec -n ingress-nginx $NGINX_POD /dbg backends get cattle-system-rancher-80 Viewing ingress certificate data /dbg certs will dump the x509 cert and key for a certificate that nginx has discovered from k8s secrets for the given hostname: > kubectl exec -n ingress-nginx $NGINX_POD /dbg certs get <fqdn> Viewing dynamically generated nginx configuration /dbg conf will dump the dynamically generated nginx configuration. To view the configuration for a specific ingress hostname, you could run /dbg conf and then grep for the server_name: > kubectl exec -n ingress-nginx $NGINX_POD /dbg conf | grep \"server_name example.com\" -B2 -A20 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to use nginx /dbg"},{"location":"000020126/#how-to-use-nginx-dbg","text":"This document (000020126) is provided subject to the disclaimer at the end of this document.","title":"How to use nginx /dbg"},{"location":"000020126/#situation","text":"","title":"Situation"},{"location":"000020126/#what-is-the-dbg-command","text":"/dbg is a program included in the ingress-nginx container image that can be used to show information about the nginx environment and the resulting nginx configuration, which can be helpful when debugging ingress issues in Kubernetes.","title":"What is the /dbg command"},{"location":"000020126/#requirements","text":"A Kubernetes cluster that has ingress enabled with ingress-nginx as the ingress controller A cluster with Linux nodes, nginx will not run on Windows kubectl configured","title":"Requirements"},{"location":"000020126/#using-dbg","text":"This command needs to be run from inside one of the ingress-nginx pods, so first determine the pod to run it in. > kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE default-http-backend-67cf578fc4-54jlz 1/1 Running 0 5d nginx-ingress-controller-56nss 1/1 Running 0 5d nginx-ingress-controller-hscfg 1/1 Running 0 4d21h nginx-ingress-controller-n4p22 1/1 Running 0 5d > export NGINX_POD=nginx-ingress-controller-n4p22 If you are diagnosing specific connection issues, you can determine which controller is receiving the traffic by looking through the logs of each.","title":"Using /dbg"},{"location":"000020126/#viewing-ingress-controller-status","text":"/dbg general will show the count of running controllers. > kubectl exec -n ingress-nginx $NGINX_POD /dbg general { \"controllerPodsCount\": 3 }","title":"Viewing ingress-controller status"},{"location":"000020126/#viewing-backend-configuration","text":"/dbg backends list will list the discovered backends: ``` kubectl exec -n ingress-nginx $NGINX_POD /dbg backends list cattle-system-rancher-80 upstream-default-backend ``` /dbg backends get will show the configuration for the named backend: > kubectl exec -n ingress-nginx $NGINX_POD /dbg backends get cattle-system-rancher-80","title":"Viewing backend configuration"},{"location":"000020126/#viewing-ingress-certificate-data","text":"/dbg certs will dump the x509 cert and key for a certificate that nginx has discovered from k8s secrets for the given hostname: > kubectl exec -n ingress-nginx $NGINX_POD /dbg certs get <fqdn>","title":"Viewing ingress certificate data"},{"location":"000020126/#viewing-dynamically-generated-nginx-configuration","text":"/dbg conf will dump the dynamically generated nginx configuration. To view the configuration for a specific ingress hostname, you could run /dbg conf and then grep for the server_name: > kubectl exec -n ingress-nginx $NGINX_POD /dbg conf | grep \"server_name example.com\" -B2 -A20","title":"Viewing dynamically generated nginx configuration"},{"location":"000020126/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020127/","text":"Where is SUSE Rancher Hosted hosted? This document (000020127) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted is hosted in the Cloud. You can have your choice of regions available in North America, EMEA, and APAC geographies. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Where is SUSE Rancher Hosted hosted?"},{"location":"000020127/#where-is-suse-rancher-hosted-hosted","text":"This document (000020127) is provided subject to the disclaimer at the end of this document.","title":"Where is SUSE Rancher Hosted hosted?"},{"location":"000020127/#resolution","text":"SUSE Rancher Hosted is hosted in the Cloud. You can have your choice of regions available in North America, EMEA, and APAC geographies.","title":"Resolution"},{"location":"000020127/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020128/","text":"How is uptime measured for my SUSE Rancher Hosted environment? This document (000020128) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted currently uses Pingdom for measuring uptime. Your Hosted Rancher endpoint is tested every one minute and is considered down if a response is not returned within five seconds. Pingdom tests endpoints from over 100 locations across the world. Uptime for the month is calculated by dividing the number of uptime minutes by the total number of minutes in the month. For example, in the month of May, there are 44,640 minutes (60 X 24 X 31). If there were 5 minutes of downtime and 44,635 minutes of uptime, the uptime measurement would be 44,635 / 44,640 = 99.989%. Please also refer to your Service Agreement for the legal definitions for uptime. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How is uptime measured for my SUSE Rancher Hosted environment?"},{"location":"000020128/#how-is-uptime-measured-for-my-suse-rancher-hosted-environment","text":"This document (000020128) is provided subject to the disclaimer at the end of this document.","title":"How is uptime measured for my SUSE Rancher Hosted environment?"},{"location":"000020128/#resolution","text":"SUSE Rancher Hosted currently uses Pingdom for measuring uptime. Your Hosted Rancher endpoint is tested every one minute and is considered down if a response is not returned within five seconds. Pingdom tests endpoints from over 100 locations across the world. Uptime for the month is calculated by dividing the number of uptime minutes by the total number of minutes in the month. For example, in the month of May, there are 44,640 minutes (60 X 24 X 31). If there were 5 minutes of downtime and 44,635 minutes of uptime, the uptime measurement would be 44,635 / 44,640 = 99.989%. Please also refer to your Service Agreement for the legal definitions for uptime.","title":"Resolution"},{"location":"000020128/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020129/","text":"Does SUSE Rancher Hosted offer an uptime SLA? This document (000020129) is provided subject to the disclaimer at the end of this document. Resolution Yes, Hosted Rancher offers a 99.9% uptime Service Level Agreement (SLA). For the details on our SLA, check your master service agreement or contact your Account Executive. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Does SUSE Rancher Hosted offer an uptime SLA?"},{"location":"000020129/#does-suse-rancher-hosted-offer-an-uptime-sla","text":"This document (000020129) is provided subject to the disclaimer at the end of this document.","title":"Does SUSE Rancher Hosted offer an uptime SLA?"},{"location":"000020129/#resolution","text":"Yes, Hosted Rancher offers a 99.9% uptime Service Level Agreement (SLA). For the details on our SLA, check your master service agreement or contact your Account Executive.","title":"Resolution"},{"location":"000020129/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020130/","text":"Can I have more than one SUSE Rancher Hosted environment? This document (000020130) is provided subject to the disclaimer at the end of this document. Resolution Yes, you can have multiple SUSE Rancher Hosted environments. Check with your Account Executive for pricing details. There are several use cases where this is preferred, such as having separate environments for development, quality assurance, and production. You will also want multiple SUSE Rancher Hosted environments if you need to manage Kubernetes clusters in separate geographies, such as North America, Europe, and Asia. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can I have more than one SUSE Rancher Hosted environment?"},{"location":"000020130/#can-i-have-more-than-one-suse-rancher-hosted-environment","text":"This document (000020130) is provided subject to the disclaimer at the end of this document.","title":"Can I have more than one SUSE Rancher Hosted environment?"},{"location":"000020130/#resolution","text":"Yes, you can have multiple SUSE Rancher Hosted environments. Check with your Account Executive for pricing details. There are several use cases where this is preferred, such as having separate environments for development, quality assurance, and production. You will also want multiple SUSE Rancher Hosted environments if you need to manage Kubernetes clusters in separate geographies, such as North America, Europe, and Asia.","title":"Resolution"},{"location":"000020130/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020131/","text":"Does SUSE Rancher Hosted offer a support SLA? This document (000020131) is provided subject to the disclaimer at the end of this document. Resolution Yes, SUSE Rancher Hosted offers the same support Service Level Agreement (SLA) to both SUSE Rancher Hosted and customers who manage their own Rancher server instance. Support cases can be opened on the Support Portal . Details of the support offering can be found on the SUSE support page . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Does SUSE Rancher Hosted offer a support SLA?"},{"location":"000020131/#does-suse-rancher-hosted-offer-a-support-sla","text":"This document (000020131) is provided subject to the disclaimer at the end of this document.","title":"Does SUSE Rancher Hosted offer a support SLA?"},{"location":"000020131/#resolution","text":"Yes, SUSE Rancher Hosted offers the same support Service Level Agreement (SLA) to both SUSE Rancher Hosted and customers who manage their own Rancher server instance. Support cases can be opened on the Support Portal . Details of the support offering can be found on the SUSE support page .","title":"Resolution"},{"location":"000020131/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020132/","text":"How long are SUSE Rancher Hosted backups retained? This document (000020132) is provided subject to the disclaimer at the end of this document. Resolution Backups on SUSE Rancher Rancher are taken hourly and retained for up to 1 year. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How long are SUSE Rancher Hosted backups retained?"},{"location":"000020132/#how-long-are-suse-rancher-hosted-backups-retained","text":"This document (000020132) is provided subject to the disclaimer at the end of this document.","title":"How long are SUSE Rancher Hosted backups retained?"},{"location":"000020132/#resolution","text":"Backups on SUSE Rancher Rancher are taken hourly and retained for up to 1 year.","title":"Resolution"},{"location":"000020132/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020133/","text":"How to configure the CoreDNS Autoscaler in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster This document (000020133) is provided subject to the disclaimer at the end of this document. Situation Task During the life of a cluster, you may need to adjust the scaling parameters for the kube-dns or CoreDNS autoscaler. The autoscaler runs as an independant Deployment in the cluster, using the cluster-proportional-autoscaler container to scale up and down the related kube-dns or CoreDNS Deployment, using a linear or ladder pattern. Pre-requisites A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster The cluster is configured with either the kube-dns or coredns provider (enabled by default) Note When running rke up commands, ensure the .rkestate file for the cluster is present in the working directory as per the documentation here . Steps Four approaches are provided, depending on the Rancher or RKE version in use. Note : When making the changes, the coredns-autoscaler or kube-dns-autoscaler` pod will be restarted with updated command arguments, this will not cause any disruption to DNS resolution. Note : Check the logs of the kube-dns-autoscaler , or coredns-autoscaler pod after making changes to confirm they have taken effect. A Rancher provisioned cluster managed by Rancher versions after v2.4.x Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'. Click 'Edit as YAML'. Locate or add the dns field, using the below as an example to add the desired parameters below: rancher_kubernetes_engine_config: [...] dns: linear_autoscaler_params: cores_per_replica: 128 max: 0 min: 1 nodes_per_replica: 4 prevent_single_point_failure: true Click 'Save' to update the cluster with the new configuration. A Rancher provisioned cluster managed by Rancher versions before v2.4.x Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'. Click 'Edit as YAML'. Locate or add the addons field, using the below as an example to add the desired parameters: rancher_kubernetes_engine_config: [...] addons: |- apiVersion: v1 data: linear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}' kind: ConfigMap metadata: name: coredns-autoscaler namespace: kube-system Click 'Save' to update the cluster with the new configuration. An RKE provisioned cluster managed by RKE versions after v1.1.0 Edit the cluster configuration YAML file to configure the dns field, using the below as an example to add the desired parameters below: dns: linear_autoscaler_params: cores_per_replica: 128 max: 0 min: 1 nodes_per_replica: 4 prevent_single_point_failure: true Invoke rke up --config <cluster configuration YAML file> to update the cluster. An RKE provisioned cluster managed by RKE versions before v1.1.0 Edit the cluster configuration YAML file to configure the ConfigMap addon, using the below as an example to add the desired parameters below: addons: |- apiVersion: v1 data: linear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}' kind: ConfigMap metadata: name: coredns-autoscaler namespace: kube-system Invoke rke up --config <cluster configuration YAML file> to update the cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to configure the CoreDNS Autoscaler in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020133/#how-to-configure-the-coredns-autoscaler-in-a-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-kubernetes-cluster","text":"This document (000020133) is provided subject to the disclaimer at the end of this document.","title":"How to configure the CoreDNS Autoscaler in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster"},{"location":"000020133/#situation","text":"","title":"Situation"},{"location":"000020133/#task","text":"During the life of a cluster, you may need to adjust the scaling parameters for the kube-dns or CoreDNS autoscaler. The autoscaler runs as an independant Deployment in the cluster, using the cluster-proportional-autoscaler container to scale up and down the related kube-dns or CoreDNS Deployment, using a linear or ladder pattern.","title":"Task"},{"location":"000020133/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster The cluster is configured with either the kube-dns or coredns provider (enabled by default) Note When running rke up commands, ensure the .rkestate file for the cluster is present in the working directory as per the documentation here .","title":"Pre-requisites"},{"location":"000020133/#steps","text":"Four approaches are provided, depending on the Rancher or RKE version in use. Note : When making the changes, the coredns-autoscaler or kube-dns-autoscaler` pod will be restarted with updated command arguments, this will not cause any disruption to DNS resolution. Note : Check the logs of the kube-dns-autoscaler , or coredns-autoscaler pod after making changes to confirm they have taken effect.","title":"Steps"},{"location":"000020133/#a-rancher-provisioned-cluster-managed-by-rancher-versions-after-v24x","text":"Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'. Click 'Edit as YAML'. Locate or add the dns field, using the below as an example to add the desired parameters below: rancher_kubernetes_engine_config: [...] dns: linear_autoscaler_params: cores_per_replica: 128 max: 0 min: 1 nodes_per_replica: 4 prevent_single_point_failure: true Click 'Save' to update the cluster with the new configuration.","title":"A Rancher provisioned cluster managed by Rancher versions after v2.4.x"},{"location":"000020133/#a-rancher-provisioned-cluster-managed-by-rancher-versions-before-v24x","text":"Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'. Click 'Edit as YAML'. Locate or add the addons field, using the below as an example to add the desired parameters: rancher_kubernetes_engine_config: [...] addons: |- apiVersion: v1 data: linear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}' kind: ConfigMap metadata: name: coredns-autoscaler namespace: kube-system Click 'Save' to update the cluster with the new configuration.","title":"A Rancher provisioned cluster managed by Rancher versions before v2.4.x"},{"location":"000020133/#an-rke-provisioned-cluster-managed-by-rke-versions-after-v110","text":"Edit the cluster configuration YAML file to configure the dns field, using the below as an example to add the desired parameters below: dns: linear_autoscaler_params: cores_per_replica: 128 max: 0 min: 1 nodes_per_replica: 4 prevent_single_point_failure: true Invoke rke up --config <cluster configuration YAML file> to update the cluster.","title":"An RKE provisioned cluster managed by RKE versions after v1.1.0"},{"location":"000020133/#an-rke-provisioned-cluster-managed-by-rke-versions-before-v110","text":"Edit the cluster configuration YAML file to configure the ConfigMap addon, using the below as an example to add the desired parameters below: addons: |- apiVersion: v1 data: linear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}' kind: ConfigMap metadata: name: coredns-autoscaler namespace: kube-system Invoke rke up --config <cluster configuration YAML file> to update the cluster.","title":"An RKE provisioned cluster managed by RKE versions before v1.1.0"},{"location":"000020133/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020134/","text":"Do SUSE employees have a login account for my SUSE Rancher Hosted environment? This document (000020134) is provided subject to the disclaimer at the end of this document. Resolution When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment. At your discretion, you can create an account for SUSE employees, with the permissions you feel comfortable with, to allow SUSE staff to perform troubleshooting activities. SUSE can also do a Teams or Zoom screen-sharing session to help troubleshoot any issues you have with SUSE Rancher Hosted. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Do SUSE employees have a login account for my SUSE Rancher Hosted environment?"},{"location":"000020134/#do-suse-employees-have-a-login-account-for-my-suse-rancher-hosted-environment","text":"This document (000020134) is provided subject to the disclaimer at the end of this document.","title":"Do SUSE employees have a login account for my SUSE Rancher Hosted environment?"},{"location":"000020134/#resolution","text":"When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment. At your discretion, you can create an account for SUSE employees, with the permissions you feel comfortable with, to allow SUSE staff to perform troubleshooting activities. SUSE can also do a Teams or Zoom screen-sharing session to help troubleshoot any issues you have with SUSE Rancher Hosted.","title":"Resolution"},{"location":"000020134/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020135/","text":"Do SUSE employees have the credentials to my \u201cadmin\u201d account? This document (000020135) is provided subject to the disclaimer at the end of this document. Resolution When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Do SUSE employees have the credentials to my \u201cadmin\u201d account?"},{"location":"000020135/#do-suse-employees-have-the-credentials-to-my-admin-account","text":"This document (000020135) is provided subject to the disclaimer at the end of this document.","title":"Do SUSE employees have the credentials to my \u201cadmin\u201d account?"},{"location":"000020135/#resolution","text":"When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment.","title":"Resolution"},{"location":"000020135/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020136/","text":"What type of cluster is SUSE Rancher Hosted running on? This document (000020136) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted runs on top of SUSE Rancher's k3s which is a fully compliant, lightweight Kubernetes distribution. The cluster consists of two k3s server nodes with a MySQL cluster backend for the datastore. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What type of cluster is SUSE Rancher Hosted running on?"},{"location":"000020136/#what-type-of-cluster-is-suse-rancher-hosted-running-on","text":"This document (000020136) is provided subject to the disclaimer at the end of this document.","title":"What type of cluster is SUSE Rancher Hosted running on?"},{"location":"000020136/#resolution","text":"SUSE Rancher Hosted runs on top of SUSE Rancher's k3s which is a fully compliant, lightweight Kubernetes distribution. The cluster consists of two k3s server nodes with a MySQL cluster backend for the datastore.","title":"Resolution"},{"location":"000020136/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020137/","text":"Why does `kubectl get` show a different API group for a resource to the group originally applied in the resource specification? This document (000020137) is provided subject to the disclaimer at the end of this document. Situation Question The Kubernetes API group and version returned for a resource from the Kubernetes API (using for example the kubectl CLI) may show as different to the original group and version defined in the resource specification via the apiVersion . For example, when creating a Deployment resource in the v1 version of the apps API group, the output of kubectl get deployment -o yaml may show the Deployment resource in the v1beta1 version of the extensions API group, per the below: Original Deployment YAML: apiVersion: apps/v1 kind: Deployment [...] Output of kubectl get deployment -o yaml for this Deployment resource post-creation: apiVersion: extensions/v1beta1 kind: Deployment [...] This article explains the cause of this behaviour and how to ensure resources are returned by the API under a specific API group and version. Answer A Kubernetes resource type, such as Deployment, can exist within multiple API groups. Where this is the case, and no API group and version is specified in the command, kubectl will use the first group listed in the discovery docs published by the Kubernetes API server that you are querying. In the instance of the above example, the Deployment resource exists under boths the apps/v1 and extensions/v1beta1 API groups, but for backwards compatability the API server lists this first under the extensions/v1beta1 group. To ensure that the resource retrieved is in a particular API group, you should fully qualify the resource type in the kubectl command, i.e. to query Deployment resources in the apps API group run kubectl get deployments.apps -o yaml . Additionally you can provide an explicit version of the API group, i.e. to query Deployment resources in the v1 version of the apps API group run kubectl get deployments.v1.apps -o yaml . Further Reading You can find a good discussion on this behaviour in the Kubernetes GitHub Issue #58131 . The Kubernetes developer documentation on API resource versioning can be found here . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Why does \\`kubectl get\\` show a different API group for a resource to the group originally applied in the resource specification?"},{"location":"000020137/#why-does-kubectl-get-show-a-different-api-group-for-a-resource-to-the-group-originally-applied-in-the-resource-specification","text":"This document (000020137) is provided subject to the disclaimer at the end of this document.","title":"Why does `kubectl get` show a different API group for a resource to the group originally applied in the resource specification?"},{"location":"000020137/#situation","text":"","title":"Situation"},{"location":"000020137/#question","text":"The Kubernetes API group and version returned for a resource from the Kubernetes API (using for example the kubectl CLI) may show as different to the original group and version defined in the resource specification via the apiVersion . For example, when creating a Deployment resource in the v1 version of the apps API group, the output of kubectl get deployment -o yaml may show the Deployment resource in the v1beta1 version of the extensions API group, per the below: Original Deployment YAML: apiVersion: apps/v1 kind: Deployment [...] Output of kubectl get deployment -o yaml for this Deployment resource post-creation: apiVersion: extensions/v1beta1 kind: Deployment [...] This article explains the cause of this behaviour and how to ensure resources are returned by the API under a specific API group and version.","title":"Question"},{"location":"000020137/#answer","text":"A Kubernetes resource type, such as Deployment, can exist within multiple API groups. Where this is the case, and no API group and version is specified in the command, kubectl will use the first group listed in the discovery docs published by the Kubernetes API server that you are querying. In the instance of the above example, the Deployment resource exists under boths the apps/v1 and extensions/v1beta1 API groups, but for backwards compatability the API server lists this first under the extensions/v1beta1 group. To ensure that the resource retrieved is in a particular API group, you should fully qualify the resource type in the kubectl command, i.e. to query Deployment resources in the apps API group run kubectl get deployments.apps -o yaml . Additionally you can provide an explicit version of the API group, i.e. to query Deployment resources in the v1 version of the apps API group run kubectl get deployments.v1.apps -o yaml .","title":"Answer"},{"location":"000020137/#further-reading","text":"You can find a good discussion on this behaviour in the Kubernetes GitHub Issue #58131 . The Kubernetes developer documentation on API resource versioning can be found here .","title":"Further Reading"},{"location":"000020137/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020138/","text":"How can I audit or examine RBAC Roles for different accounts within a Kubernetes cluster? This document (000020138) is provided subject to the disclaimer at the end of this document. Situation Question Access to different resources within Kubernetes is handled by role-based access control (RBAC) . These resources are referenced by the resource name and API group, for example pods within the core/v1 Kubernetes API group or clusters within the management.cattle.io/v3 API group. A role can be applied (or bound) to different subjects, like a user, group or service account via role bindings, to grant varying degress of access to these resource types at a cluster or namespace level. The access a role grants on a particular resource type is defined by verbs, e.g. get, create, list, watch, delete, and patch etc. This article details methods by which you can audit or examine role-based access control (RBAC) roles for different accounts within a Kubernetes cluster. Pre-requisites A Kubernetes cluster kubectl access to the cluster Answer To audit a specific account, the kubectl command can use the can-i option with the impersonation API to examine what verbs a user has access to, given a specific namespace. Basic Usage Basic usage of the kubectl can-i option takes the following form: kubectl auth can-i <verb> <resource> --as account --namespace=<namespace> Can my user perform all verbs on all resources? Am I an admin? kuboectl auth can-i \"*\" \"*\" Can the helm serviceaccount delete pods in the current namespace or cluster-wide? kubectl auth can-i delete pods --as helm Is user1234 an admin in the \"testing\" namespace? Can they perform all verbs on all resources? kubectl auth can-i \"*\" \"*\" --namespace=testing --as user1234 List option gives insight into permissions for a user or account kubectl auth can-i --list --namespace=testing --as user1234 Additional tools for querying RBAC Other open-source third-party tools exist for auditing RBAC, many of which use the Krew plugin framework : access-matrix - output a CLI matrix of what users or roles have permissions rbac-lookup - perform lookups given subject queries who-can - see \"who-can\" perform a certain verb on a resource, like an opposite view of \"can-i\" Third-party tools also exist for creating visualizations of the RBAC configuration: RBack - parse the output from the kubectl commands as json, import into visualization in different formats RBAC-view - visualizing RBAC relationships via a dashboard interface Further Reading Offical Kubernetes RBAC documentation CNCF RBAC Blog post NCCGROUP Examples Krew Plugin Framework RBAC-View RBack who-can rakksess, acess-matrix plugin rbac-lookup Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How can I audit or examine RBAC Roles for different accounts within a Kubernetes cluster?"},{"location":"000020138/#how-can-i-audit-or-examine-rbac-roles-for-different-accounts-within-a-kubernetes-cluster","text":"This document (000020138) is provided subject to the disclaimer at the end of this document.","title":"How can I audit or examine RBAC Roles for different accounts within a Kubernetes cluster?"},{"location":"000020138/#situation","text":"","title":"Situation"},{"location":"000020138/#question","text":"Access to different resources within Kubernetes is handled by role-based access control (RBAC) . These resources are referenced by the resource name and API group, for example pods within the core/v1 Kubernetes API group or clusters within the management.cattle.io/v3 API group. A role can be applied (or bound) to different subjects, like a user, group or service account via role bindings, to grant varying degress of access to these resource types at a cluster or namespace level. The access a role grants on a particular resource type is defined by verbs, e.g. get, create, list, watch, delete, and patch etc. This article details methods by which you can audit or examine role-based access control (RBAC) roles for different accounts within a Kubernetes cluster.","title":"Question"},{"location":"000020138/#pre-requisites","text":"A Kubernetes cluster kubectl access to the cluster","title":"Pre-requisites"},{"location":"000020138/#answer","text":"To audit a specific account, the kubectl command can use the can-i option with the impersonation API to examine what verbs a user has access to, given a specific namespace.","title":"Answer"},{"location":"000020138/#basic-usage","text":"Basic usage of the kubectl can-i option takes the following form: kubectl auth can-i <verb> <resource> --as account --namespace=<namespace>","title":"Basic Usage"},{"location":"000020138/#can-my-user-perform-all-verbs-on-all-resources-am-i-an-admin","text":"kuboectl auth can-i \"*\" \"*\"","title":"Can my user perform all verbs on all resources? Am I an admin?"},{"location":"000020138/#can-the-helm-serviceaccount-delete-pods-in-the-current-namespace-or-cluster-wide","text":"kubectl auth can-i delete pods --as helm","title":"Can the helm serviceaccount delete pods in the current namespace or cluster-wide?"},{"location":"000020138/#is-user1234-an-admin-in-the-testing-namespace-can-they-perform-all-verbs-on-all-resources","text":"kubectl auth can-i \"*\" \"*\" --namespace=testing --as user1234","title":"Is user1234 an admin in the \"testing\" namespace? Can they perform all verbs on all resources?"},{"location":"000020138/#list-option-gives-insight-into-permissions-for-a-user-or-account","text":"kubectl auth can-i --list --namespace=testing --as user1234","title":"List option gives insight into permissions for a user or account"},{"location":"000020138/#additional-tools-for-querying-rbac","text":"Other open-source third-party tools exist for auditing RBAC, many of which use the Krew plugin framework : access-matrix - output a CLI matrix of what users or roles have permissions rbac-lookup - perform lookups given subject queries who-can - see \"who-can\" perform a certain verb on a resource, like an opposite view of \"can-i\" Third-party tools also exist for creating visualizations of the RBAC configuration: RBack - parse the output from the kubectl commands as json, import into visualization in different formats RBAC-view - visualizing RBAC relationships via a dashboard interface","title":"Additional tools for querying RBAC"},{"location":"000020138/#further-reading","text":"Offical Kubernetes RBAC documentation CNCF RBAC Blog post NCCGROUP Examples Krew Plugin Framework RBAC-View RBack who-can rakksess, acess-matrix plugin rbac-lookup","title":"Further Reading"},{"location":"000020138/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020139/","text":"What is the kube_config_cluster.yml file that is created after provisioning a cluster with the Rancher Kubernetes Engine (RKE) CLI? This document (000020139) is provided subject to the disclaimer at the end of this document. Situation Question The Rancher Kubernetes Engine (RKE) documentation references a file kube_config_cluster.yml that is generated after running rke up , this article explains what this file is and how to use it. Pre-requisites A Rancher Kubernetes Engine (RKE) CLI provisioned Kubernetes cluster kubectl installed Answer When you provision a Kubernetes cluster using RKE, a kubeconfig file is automatically generated for your cluster. This file is created and saved as kube_config_<cluster>.yml , where <cluster> is the filename of your cluster configuration YAML file. This kubeconfig defines the connection and authentication details to interact with your cluster, using tools such as kubectl . By default, kubectl checks ~/.kube/config for a kubeconfig file, but you can specify a different kubeconfig file using the --kubeconfig flag. For example: kubectl --kubeconfig /custom/path/rke/kube_config_cluster.yml get pods Or you can export the config path into the KUBECONFIG environment variable, removing the requirement to specify the --kubeconfig flag each time you run kubectl: export KUBECONFIG=\"/custom/path/rke/kube_config_cluster.yml\" Further Reading RKE Documentation on the kubeconfig Kubernetes Documentation on kubeconfig files Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What is the kube\\_config\\_cluster.yml file that is created after provisioning a cluster with the Rancher Kubernetes Engine (RKE) CLI?"},{"location":"000020139/#what-is-the-kube_config_clusteryml-file-that-is-created-after-provisioning-a-cluster-with-the-rancher-kubernetes-engine-rke-cli","text":"This document (000020139) is provided subject to the disclaimer at the end of this document.","title":"What is the kube_config_cluster.yml file that is created after provisioning a cluster with the Rancher Kubernetes Engine (RKE) CLI?"},{"location":"000020139/#situation","text":"","title":"Situation"},{"location":"000020139/#question","text":"The Rancher Kubernetes Engine (RKE) documentation references a file kube_config_cluster.yml that is generated after running rke up , this article explains what this file is and how to use it.","title":"Question"},{"location":"000020139/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) CLI provisioned Kubernetes cluster kubectl installed","title":"Pre-requisites"},{"location":"000020139/#answer","text":"When you provision a Kubernetes cluster using RKE, a kubeconfig file is automatically generated for your cluster. This file is created and saved as kube_config_<cluster>.yml , where <cluster> is the filename of your cluster configuration YAML file. This kubeconfig defines the connection and authentication details to interact with your cluster, using tools such as kubectl . By default, kubectl checks ~/.kube/config for a kubeconfig file, but you can specify a different kubeconfig file using the --kubeconfig flag. For example: kubectl --kubeconfig /custom/path/rke/kube_config_cluster.yml get pods Or you can export the config path into the KUBECONFIG environment variable, removing the requirement to specify the --kubeconfig flag each time you run kubectl: export KUBECONFIG=\"/custom/path/rke/kube_config_cluster.yml\"","title":"Answer"},{"location":"000020139/#further-reading","text":"RKE Documentation on the kubeconfig Kubernetes Documentation on kubeconfig files","title":"Further Reading"},{"location":"000020139/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020140/","text":"What permissions are required for the API token when configuring the Rancher2 Terraform Provider? This document (000020140) is provided subject to the disclaimer at the end of this document. Situation Question When configuring the Rancher2 Terraform Provider , what permissions are required for the API token configured to authenticate with Rancher (as in the below example)? provider \"rancher2\" { api_url = \"https://rancher.my-domain.com\" access_key = \"${var.rancher2_access_key}\" secret_key = \"${var.rancher2_secret_key}\" } Pre-requisites A Rancher v2.x instance The Rancher2 Terraform Provider Answer The user account for which you generate the API token, to configure the Terraform provider, will need permissions granted on any resources that you intend to configure and manage via Terraform. Further Reading Rancher2 Terraform Provider Documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What permissions are required for the API token when configuring the Rancher2 Terraform Provider?"},{"location":"000020140/#what-permissions-are-required-for-the-api-token-when-configuring-the-rancher2-terraform-provider","text":"This document (000020140) is provided subject to the disclaimer at the end of this document.","title":"What permissions are required for the API token when configuring the Rancher2 Terraform Provider?"},{"location":"000020140/#situation","text":"","title":"Situation"},{"location":"000020140/#question","text":"When configuring the Rancher2 Terraform Provider , what permissions are required for the API token configured to authenticate with Rancher (as in the below example)? provider \"rancher2\" { api_url = \"https://rancher.my-domain.com\" access_key = \"${var.rancher2_access_key}\" secret_key = \"${var.rancher2_secret_key}\" }","title":"Question"},{"location":"000020140/#pre-requisites","text":"A Rancher v2.x instance The Rancher2 Terraform Provider","title":"Pre-requisites"},{"location":"000020140/#answer","text":"The user account for which you generate the API token, to configure the Terraform provider, will need permissions granted on any resources that you intend to configure and manage via Terraform.","title":"Answer"},{"location":"000020140/#further-reading","text":"Rancher2 Terraform Provider Documentation","title":"Further Reading"},{"location":"000020140/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020141/","text":"How to configure container log rotation for the Docker daemon This document (000020141) is provided subject to the disclaimer at the end of this document. Situation Task As the default setting on Docker is to log using the json-file log driver, without a container log limit, this can lead to disk-fill events on nodes. This article provides steps to configure any nodes running Docker to have a limited container log size and rotate out older container logs. Pre-requisites Node(s) running Docker, using the json-file log driver Permission to edit the /etc/docker/daemon.json and to restart the Docker daemon Warning You must restart the Docker daemon for the changes to take effect for newly created containers N.B. As the container logging configuration for pre-existing container is immutable, existing containers do not use the new logging configuration and would need to be redeployed to take on this new configuration. Resolution Edit the Docker daemon configuration file: $ vim /etc/docker/daemon.json Add the following lines to the file, to configure a maximum container log file size of 10MB and maintain only 10 of these before deleting the oldest: { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"10m\", \"max-file\": \"10\" } } Restart the docker daemon to apply the settings to new containers (see Warnings above): $ systemctl restart docker Tips You could include this Docker daemon container log rotation configuration in your build/connfiguration management systems, to ensure this is automatically applied to nodes on provisioning, removing any requirement for manual configuration. Further reading Docker JSON file log driver documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to configure container log rotation for the Docker daemon"},{"location":"000020141/#how-to-configure-container-log-rotation-for-the-docker-daemon","text":"This document (000020141) is provided subject to the disclaimer at the end of this document.","title":"How to configure container log rotation for the Docker daemon"},{"location":"000020141/#situation","text":"","title":"Situation"},{"location":"000020141/#task","text":"As the default setting on Docker is to log using the json-file log driver, without a container log limit, this can lead to disk-fill events on nodes. This article provides steps to configure any nodes running Docker to have a limited container log size and rotate out older container logs.","title":"Task"},{"location":"000020141/#pre-requisites","text":"Node(s) running Docker, using the json-file log driver Permission to edit the /etc/docker/daemon.json and to restart the Docker daemon","title":"Pre-requisites"},{"location":"000020141/#warning","text":"You must restart the Docker daemon for the changes to take effect for newly created containers N.B. As the container logging configuration for pre-existing container is immutable, existing containers do not use the new logging configuration and would need to be redeployed to take on this new configuration.","title":"Warning"},{"location":"000020141/#resolution","text":"Edit the Docker daemon configuration file: $ vim /etc/docker/daemon.json Add the following lines to the file, to configure a maximum container log file size of 10MB and maintain only 10 of these before deleting the oldest: { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"10m\", \"max-file\": \"10\" } } Restart the docker daemon to apply the settings to new containers (see Warnings above): $ systemctl restart docker","title":"Resolution"},{"location":"000020141/#tips","text":"You could include this Docker daemon container log rotation configuration in your build/connfiguration management systems, to ensure this is automatically applied to nodes on provisioning, removing any requirement for manual configuration.","title":"Tips"},{"location":"000020141/#further-reading","text":"Docker JSON file log driver documentation","title":"Further reading"},{"location":"000020141/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020142/","text":"How to configure an internal Elastic Load Balancer (ELB) or Network Load Balancer (NLB) with an Istio Ingress Gateway in Rancher v2.3+ This document (000020142) is provided subject to the disclaimer at the end of this document. Situation Task When configuring an Istio Ingress Gateway, a LoadBalancer type service is commonly configured to provide external access to the cluster. By default Kubernetes will provision an internet-facing Classic Load Balancer (CLB). The below steps provide guidance on the annotations needed to configure an internal CLB or Network Load Balancer (NLB) using private subnets. Pre-requisites A Rancher v2.3+ managed Kubernetes cluster, runnning in AWS, with the AWS cloud provider configured Istio enabled in the cluster Tagging configured for the VPC and Subnets that will be used for the ELB or NLB Note : When using Load Balancers with the AWS cloud provider, it is important tag the private and public subnets in the VPC so that kube-controller-manager can correctly discover the specific subnets intended for use. For example the kubernetes.io/role/internal-elb and kubernetes.io/role/elb keys configured respectively, with the value of 1 . Steps Enable the Istio Ingress Gateway If the not already enabled, enable the Istio Ingress Gateway . In the drop down list for 'Service Type of Ingress Gateway', select LoadBalancer . Use an internal Load Balancer When editing the Istio Ingress Gateway, click the drop down for Custom Answers. Paste the below in the Variable field, this will automatically populate the value: gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-internal\" = \"true\" Use an NLB To use an NLB, click 'Add Answer' and paste the below in the Variable field: gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-type\" = nlb Note: An NLB can be used as an internet-facing loadbancer by using only the above annotation, without adding the aws-load-balancer-internal annotation. References Istio install options documentation Kubernetes load balancer documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to configure an internal Elastic Load Balancer (ELB) or Network Load Balancer (NLB) with an Istio Ingress Gateway in Rancher v2.3+"},{"location":"000020142/#how-to-configure-an-internal-elastic-load-balancer-elb-or-network-load-balancer-nlb-with-an-istio-ingress-gateway-in-rancher-v23","text":"This document (000020142) is provided subject to the disclaimer at the end of this document.","title":"How to configure an internal Elastic Load Balancer (ELB) or Network Load Balancer (NLB) with an Istio Ingress Gateway in Rancher v2.3+"},{"location":"000020142/#situation","text":"","title":"Situation"},{"location":"000020142/#task","text":"When configuring an Istio Ingress Gateway, a LoadBalancer type service is commonly configured to provide external access to the cluster. By default Kubernetes will provision an internet-facing Classic Load Balancer (CLB). The below steps provide guidance on the annotations needed to configure an internal CLB or Network Load Balancer (NLB) using private subnets.","title":"Task"},{"location":"000020142/#pre-requisites","text":"A Rancher v2.3+ managed Kubernetes cluster, runnning in AWS, with the AWS cloud provider configured Istio enabled in the cluster Tagging configured for the VPC and Subnets that will be used for the ELB or NLB Note : When using Load Balancers with the AWS cloud provider, it is important tag the private and public subnets in the VPC so that kube-controller-manager can correctly discover the specific subnets intended for use. For example the kubernetes.io/role/internal-elb and kubernetes.io/role/elb keys configured respectively, with the value of 1 .","title":"Pre-requisites"},{"location":"000020142/#steps","text":"","title":"Steps"},{"location":"000020142/#enable-the-istio-ingress-gateway","text":"If the not already enabled, enable the Istio Ingress Gateway . In the drop down list for 'Service Type of Ingress Gateway', select LoadBalancer .","title":"Enable the Istio Ingress Gateway"},{"location":"000020142/#use-an-internal-load-balancer","text":"When editing the Istio Ingress Gateway, click the drop down for Custom Answers. Paste the below in the Variable field, this will automatically populate the value: gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-internal\" = \"true\"","title":"Use an internal Load Balancer"},{"location":"000020142/#use-an-nlb","text":"To use an NLB, click 'Add Answer' and paste the below in the Variable field: gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-type\" = nlb Note: An NLB can be used as an internet-facing loadbancer by using only the above annotation, without adding the aws-load-balancer-internal annotation.","title":"Use an NLB"},{"location":"000020142/#references","text":"Istio install options documentation Kubernetes load balancer documentation","title":"References"},{"location":"000020142/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020143/","text":"How to set server-tokens to false, to disable the the NGINX header in ingress-nginx responses, within a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster This document (000020143) is provided subject to the disclaimer at the end of this document. Situation Task The ingress-nginx server-tokens option controls display of the NGINX server header, including version information, in the response to ingress requests. By default this header is enabled; however, due to security concerns in exposing version information, a user might want to disable this on the nginx-ingress-controllers of their Kubernetes cluster(s). This article details how to disable the header, via the server-tokens option, in Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters. Pre-requisites A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster Resolution RKE provisioned clusters Add the server-tokens: \"false\" option for nginx into the cluster configuration YAML file as follows: ingress: provider: nginx options: server-tokens: \"false\" Example: nodes: - address: x.x.x.x internal_address: x.x.x.x user: ubuntu role: [controlplane,worker,etcd] ingress: provider: nginx options: server-tokens: \"false\" services: etcd: snapshot: true creation: 6h retention: 24h Execute rke up to update the cluster with the new configuration. N.B. Ensure the .rkestate file for the cluster is present in the working directory when invoking rke up per the documentation here : rke up --config <cluster configuration YAML file> Rancher v2.x provisioned clusters Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'. Click 'Edit as YAML'. Add the server-tokens: \"false\" option for nginx into the cluster configuration YAML file as follows: rancher_kubernetes_engine_config: [...] ingress: provider: nginx options: server-tokens: \"false\" Click 'Save' to update the cluster with the new configuration. Further reading ingress-nginx documentation on the server-tokens options Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to set server-tokens to false, to disable the the NGINX header in ingress-nginx responses, within a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster"},{"location":"000020143/#how-to-set-server-tokens-to-false-to-disable-the-the-nginx-header-in-ingress-nginx-responses-within-a-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-cluster","text":"This document (000020143) is provided subject to the disclaimer at the end of this document.","title":"How to set server-tokens to false, to disable the the NGINX header in ingress-nginx responses, within a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster"},{"location":"000020143/#situation","text":"","title":"Situation"},{"location":"000020143/#task","text":"The ingress-nginx server-tokens option controls display of the NGINX server header, including version information, in the response to ingress requests. By default this header is enabled; however, due to security concerns in exposing version information, a user might want to disable this on the nginx-ingress-controllers of their Kubernetes cluster(s). This article details how to disable the header, via the server-tokens option, in Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters.","title":"Task"},{"location":"000020143/#pre-requisites","text":"A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","title":"Pre-requisites"},{"location":"000020143/#resolution","text":"","title":"Resolution"},{"location":"000020143/#rke-provisioned-clusters","text":"Add the server-tokens: \"false\" option for nginx into the cluster configuration YAML file as follows: ingress: provider: nginx options: server-tokens: \"false\" Example: nodes: - address: x.x.x.x internal_address: x.x.x.x user: ubuntu role: [controlplane,worker,etcd] ingress: provider: nginx options: server-tokens: \"false\" services: etcd: snapshot: true creation: 6h retention: 24h Execute rke up to update the cluster with the new configuration. N.B. Ensure the .rkestate file for the cluster is present in the working directory when invoking rke up per the documentation here : rke up --config <cluster configuration YAML file>","title":"RKE provisioned clusters"},{"location":"000020143/#rancher-v2x-provisioned-clusters","text":"Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'. Click 'Edit as YAML'. Add the server-tokens: \"false\" option for nginx into the cluster configuration YAML file as follows: rancher_kubernetes_engine_config: [...] ingress: provider: nginx options: server-tokens: \"false\" Click 'Save' to update the cluster with the new configuration.","title":"Rancher v2.x provisioned clusters"},{"location":"000020143/#further-reading","text":"ingress-nginx documentation on the server-tokens options","title":"Further reading"},{"location":"000020143/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020144/","text":"How to enable debug level logging for the Rancher Cluster/Project Alerting Alertmanager instance, in a Rancher v2.x managed cluster? This document (000020144) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to enable debug level logging on the Alertmanager instance in a Rancher v2.x managed Kubernetes cluster, which may assist when troubleshooting cluster or project alerting . Pre-requisites A Rancher v2.x managed Kubernetes cluster Cluster or project alerting configured Resolution Within the Rancher UI navigate to the System Project of the relevant cluster and click on the Apps view. Click 'Upgrade' on the cluster-alerting app. In the Answers section click 'Add Answer' and add the variable alertmanager.logLevel with a value of debug . Click upgrade to save the change and update the Alertmanager instance with the debug log level. Navigate to the cattle-prometheus namespace within the System Project for the cluster, and view the logs of the alertmanager-cluster-alerting-0 Pod running for the alertmanager-cluster-alerting StatefulSet. You should see level=debug log messages, such as in the following example, confirming debug level logging has been successfully configured: plaintext level=debug ts=2019-07-09T15:03:37.511451301Z caller=dispatch.go:104 component=dispatcher msg=\"Received alert\" alert=[433a194][active] level=debug ts=2019-07-09T15:03:38.511774835Z caller=dispatch.go:430 component=dispatcher aggrGroup=\"{}/{group_id=\\\"c-5h85q:event-alert\\\"}/{rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\"}:{event_message=\\\"Scaled up replica set mynginx2-7994cd84ff to 1\\\", resource_kind=\\\"Deployment\\\", rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\", target_name=\\\"mynginx2\\\", target_namespace=\\\"default\\\"}\" msg=flushing alerts=[[433a194][active]] Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable debug level logging for the Rancher Cluster/Project Alerting Alertmanager instance, in a Rancher v2.x managed cluster?"},{"location":"000020144/#how-to-enable-debug-level-logging-for-the-rancher-clusterproject-alerting-alertmanager-instance-in-a-rancher-v2x-managed-cluster","text":"This document (000020144) is provided subject to the disclaimer at the end of this document.","title":"How to enable debug level logging for the Rancher Cluster/Project Alerting Alertmanager instance, in a Rancher v2.x managed cluster?"},{"location":"000020144/#situation","text":"","title":"Situation"},{"location":"000020144/#task","text":"This article details how to enable debug level logging on the Alertmanager instance in a Rancher v2.x managed Kubernetes cluster, which may assist when troubleshooting cluster or project alerting .","title":"Task"},{"location":"000020144/#pre-requisites","text":"A Rancher v2.x managed Kubernetes cluster Cluster or project alerting configured","title":"Pre-requisites"},{"location":"000020144/#resolution","text":"Within the Rancher UI navigate to the System Project of the relevant cluster and click on the Apps view. Click 'Upgrade' on the cluster-alerting app. In the Answers section click 'Add Answer' and add the variable alertmanager.logLevel with a value of debug . Click upgrade to save the change and update the Alertmanager instance with the debug log level. Navigate to the cattle-prometheus namespace within the System Project for the cluster, and view the logs of the alertmanager-cluster-alerting-0 Pod running for the alertmanager-cluster-alerting StatefulSet. You should see level=debug log messages, such as in the following example, confirming debug level logging has been successfully configured: plaintext level=debug ts=2019-07-09T15:03:37.511451301Z caller=dispatch.go:104 component=dispatcher msg=\"Received alert\" alert=[433a194][active] level=debug ts=2019-07-09T15:03:38.511774835Z caller=dispatch.go:430 component=dispatcher aggrGroup=\"{}/{group_id=\\\"c-5h85q:event-alert\\\"}/{rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\"}:{event_message=\\\"Scaled up replica set mynginx2-7994cd84ff to 1\\\", resource_kind=\\\"Deployment\\\", rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\", target_name=\\\"mynginx2\\\", target_namespace=\\\"default\\\"}\" msg=flushing alerts=[[433a194][active]]","title":"Resolution"},{"location":"000020144/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020145/","text":"How to generate a Longhorn Support Bundle This document (000020145) is provided subject to the disclaimer at the end of this document. Situation Task When troubleshooting an issue with Longhorn, Rancher Support may request a Longhorn Support Bundle, which can be generated via the Longhorn UI, and contains system information and logs. Pre-requisites A Rancher v2.x managed Kubernetes cluster with Longhorn deployed Steps Generate the Support Bundle Log in into the Rancher UI. Select the cluster with Longhorn depoyed. Select the Project where Longhorn is deployed (typically under the System project). Click on \"Apps\" button. Find the Longhorn system app and click on the index.html button Click on the \"Generate Support Bundle\" in the bottom left of the screen Type in a description and click generate (issue url is optional) Upload the Support Bundle Generally Longhorn Support Bundles files are small in size; however, if the pack is too large to upload directly to the ticket, please request a temporary upload location. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to generate a Longhorn Support Bundle"},{"location":"000020145/#how-to-generate-a-longhorn-support-bundle","text":"This document (000020145) is provided subject to the disclaimer at the end of this document.","title":"How to generate a Longhorn Support Bundle"},{"location":"000020145/#situation","text":"","title":"Situation"},{"location":"000020145/#task","text":"When troubleshooting an issue with Longhorn, Rancher Support may request a Longhorn Support Bundle, which can be generated via the Longhorn UI, and contains system information and logs.","title":"Task"},{"location":"000020145/#pre-requisites","text":"A Rancher v2.x managed Kubernetes cluster with Longhorn deployed","title":"Pre-requisites"},{"location":"000020145/#steps","text":"","title":"Steps"},{"location":"000020145/#generate-the-support-bundle","text":"Log in into the Rancher UI. Select the cluster with Longhorn depoyed. Select the Project where Longhorn is deployed (typically under the System project). Click on \"Apps\" button. Find the Longhorn system app and click on the index.html button Click on the \"Generate Support Bundle\" in the bottom left of the screen Type in a description and click generate (issue url is optional)","title":"Generate the Support Bundle"},{"location":"000020145/#upload-the-support-bundle","text":"Generally Longhorn Support Bundles files are small in size; however, if the pack is too large to upload directly to the ticket, please request a temporary upload location.","title":"Upload the Support Bundle"},{"location":"000020145/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020146/","text":"How to add additional scrape configs to a Rancher cluster or project monitoring prometheus This document (000020146) is provided subject to the disclaimer at the end of this document. Situation Task The Rancher cluster and project monitoring tools , allow you to monitor cluster components and nodes, as well as workloads and custom metrics from any HTTP or TCP/UDP metrics endpoint that these workloads expose. This article will detail how to manually define additional scrape configs for either the cluster or project monitoring prometheus instance, where you want to scrape other metrics. Whether to define the additional scrape config at the cluster or project level would depend on the desired scope for the metrics and possible alerts. If you wish to scope the metrics scraped, and thus possible alerts configured for these metrics, to a project, you could configure the additional scrape config at the project monitoring level. If you wish to scope the metrics at the cluster level, so only those with cluster admin access could see the metrics or configure alerts, you could configure the additional scrape config at the cluster monitoring level. Pre-requisites A Rancher v2.2.x, v2.3.x or v2.4.x managed cluster, with cluster monitoring enabled (and optionally project monitoring enabled, if you wish to configure the additonal scrape config at the project scope). Resolution For both cluster and project monitoring the additional scrape config(s) are defined in the Answers section of the Monitoring configuration. This can be found as follows: Cluster Monitoring: As a user with permissions to edit cluster monitoring (global admins and cluster owners by default), navigate to the cluster view and click Tools -> Monitoring from the menu bar. Click 'Show advanced options' at the bottom right. Project Monitoring: As a user with permissions to edit project monitoring (global admins, cluster owners and project owners by default), navigate to the project and click Tools -> Monitoring from the menu bar. Click 'Show advanced options' at the bottom right. You can add an array of prometheus.additionalScrapeConfigs in the Answers section here. For example to define a scrape job of the following: - job_name: \"prometheus\" static_configs: - targets: - \"localhost:9090\" You would add the following two definitions to the Answers section: prometheus.additionalScrapeConfigs[0].job_name = prometheus prometheus.additionalScrapeConfigs[0].static_configs[0].targets[0] = localhost:9090 After adding the answers, click 'Save' and you should now be able to view the target and its status within the Prometheus UI under Status -> Targets. Further reading Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to add additional scrape configs to a Rancher cluster or project monitoring prometheus"},{"location":"000020146/#how-to-add-additional-scrape-configs-to-a-rancher-cluster-or-project-monitoring-prometheus","text":"This document (000020146) is provided subject to the disclaimer at the end of this document.","title":"How to add additional scrape configs to a Rancher cluster or project monitoring prometheus"},{"location":"000020146/#situation","text":"","title":"Situation"},{"location":"000020146/#task","text":"The Rancher cluster and project monitoring tools , allow you to monitor cluster components and nodes, as well as workloads and custom metrics from any HTTP or TCP/UDP metrics endpoint that these workloads expose. This article will detail how to manually define additional scrape configs for either the cluster or project monitoring prometheus instance, where you want to scrape other metrics. Whether to define the additional scrape config at the cluster or project level would depend on the desired scope for the metrics and possible alerts. If you wish to scope the metrics scraped, and thus possible alerts configured for these metrics, to a project, you could configure the additional scrape config at the project monitoring level. If you wish to scope the metrics at the cluster level, so only those with cluster admin access could see the metrics or configure alerts, you could configure the additional scrape config at the cluster monitoring level.","title":"Task"},{"location":"000020146/#pre-requisites","text":"A Rancher v2.2.x, v2.3.x or v2.4.x managed cluster, with cluster monitoring enabled (and optionally project monitoring enabled, if you wish to configure the additonal scrape config at the project scope).","title":"Pre-requisites"},{"location":"000020146/#resolution","text":"For both cluster and project monitoring the additional scrape config(s) are defined in the Answers section of the Monitoring configuration. This can be found as follows: Cluster Monitoring: As a user with permissions to edit cluster monitoring (global admins and cluster owners by default), navigate to the cluster view and click Tools -> Monitoring from the menu bar. Click 'Show advanced options' at the bottom right. Project Monitoring: As a user with permissions to edit project monitoring (global admins, cluster owners and project owners by default), navigate to the project and click Tools -> Monitoring from the menu bar. Click 'Show advanced options' at the bottom right. You can add an array of prometheus.additionalScrapeConfigs in the Answers section here. For example to define a scrape job of the following: - job_name: \"prometheus\" static_configs: - targets: - \"localhost:9090\" You would add the following two definitions to the Answers section: prometheus.additionalScrapeConfigs[0].job_name = prometheus prometheus.additionalScrapeConfigs[0].static_configs[0].targets[0] = localhost:9090 After adding the answers, click 'Save' and you should now be able to view the target and its status within the Prometheus UI under Status -> Targets.","title":"Resolution"},{"location":"000020146/#further-reading","text":"Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here .","title":"Further reading"},{"location":"000020146/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020147/","text":"How to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters This document (000020147) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters. Pre-requisites A Kubernetes cluster provisioned by the Rancher Kubernetes Enginer (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced. For Rancher v.2x provisioned clusters, you will require cluster owner or global admin permissions in Rancher Resolution Configuration for RKE provisioned clusters Edit the cluster configuration YAML file to include the enable-ssl-passthrough: true option for the ingress, as follows: ingress: provider: nginx extra_args: enable-ssl-passthrough: true Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Recycle the nginx pods in-order to pick up new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done Verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" > /dev/null 2>&1 && echo \"Good\" || echo \"Bad\"; done Edit the ingress to include the new annotations: kubectl -n default edit ingress hello-world-lb Example: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" name: hello-world-lb namespace: default Configuration for Rancher provisioned clusters Login into the Rancher UI. Go to Global -> Clusters -> <>. From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Enclude the enable-ssl-passthrough: true option for the ingress, as follows: yaml ingress: provider: nginx extra_args: enable-ssl-passthrough: true Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument: bash for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done 9. Run the following inside the kubectl CLI to verify the new argument: bash for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" > /dev/null 2>&1 && echo \"Good\" || echo \"Bad\"; done Browse to the ingress in question and click edit. Expand \"Labels & Annotations\". Click \"Add annotation\" and add nginx.ingress.kubernetes.io/ssl-passthrough=true under \"Annotations\". Click \"Save\". Verification Steps Run the following command to verify new certificate: ```bash curl --insecure -v https://<<APP URL>> 2>&1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }' ``` Example output: * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server did not agree to a protocol * Server certificate: * subject: OU=Domain Control Validated; CN=*.rancher.tools * start date: Jul 2 00:42:01 2019 GMT * expire date: May 2 00:19:41 2020 GMT * issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Mark bundle as not supporting multiuse * Connection #0 to host lab.rancher.tools left intact N.B. Some browsers will cache the certificate, as a result you might need to close and re-open the browser in order to get the new certificate. How to clear the SSL state in a browser. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters"},{"location":"000020147/#how-to-enable-ssl-passthrough-on-the-nginx-ingress-controller-in-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-kubernetes-clusters","text":"This document (000020147) is provided subject to the disclaimer at the end of this document.","title":"How to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters"},{"location":"000020147/#situation","text":"","title":"Situation"},{"location":"000020147/#task","text":"This article details how to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","title":"Task"},{"location":"000020147/#pre-requisites","text":"A Kubernetes cluster provisioned by the Rancher Kubernetes Enginer (RKE) CLI or Rancher v2.x For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML , rkestate file and kubectl access with the kubeconfig for the cluster sourced. For Rancher v.2x provisioned clusters, you will require cluster owner or global admin permissions in Rancher","title":"Pre-requisites"},{"location":"000020147/#resolution","text":"","title":"Resolution"},{"location":"000020147/#configuration-for-rke-provisioned-clusters","text":"Edit the cluster configuration YAML file to include the enable-ssl-passthrough: true option for the ingress, as follows: ingress: provider: nginx extra_args: enable-ssl-passthrough: true Apply the changes to the cluster, by invoking rke up : rke up --config <cluster configuration yaml file> Recycle the nginx pods in-order to pick up new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done Verify the new argument: for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" > /dev/null 2>&1 && echo \"Good\" || echo \"Bad\"; done Edit the ingress to include the new annotations: kubectl -n default edit ingress hello-world-lb Example: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" name: hello-world-lb namespace: default","title":"Configuration for RKE provisioned clusters"},{"location":"000020147/#configuration-for-rancher-provisioned-clusters","text":"Login into the Rancher UI. Go to Global -> Clusters -> <>. From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit. Click \"Edit as YAML\". Enclude the enable-ssl-passthrough: true option for the ingress, as follows: yaml ingress: provider: nginx extra_args: enable-ssl-passthrough: true Click \"Save\" at the bottom of the page. Wait for cluster to finish upgrading. Go back to the Cluster Dashboard and click \"Launch kubectl\". Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument: bash for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done 9. Run the following inside the kubectl CLI to verify the new argument: bash for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" > /dev/null 2>&1 && echo \"Good\" || echo \"Bad\"; done Browse to the ingress in question and click edit. Expand \"Labels & Annotations\". Click \"Add annotation\" and add nginx.ingress.kubernetes.io/ssl-passthrough=true under \"Annotations\". Click \"Save\".","title":"Configuration for Rancher provisioned clusters"},{"location":"000020147/#verification-steps","text":"Run the following command to verify new certificate: ```bash curl --insecure -v https://<<APP URL>> 2>&1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }' ``` Example output: * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server did not agree to a protocol * Server certificate: * subject: OU=Domain Control Validated; CN=*.rancher.tools * start date: Jul 2 00:42:01 2019 GMT * expire date: May 2 00:19:41 2020 GMT * issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Mark bundle as not supporting multiuse * Connection #0 to host lab.rancher.tools left intact N.B. Some browsers will cache the certificate, as a result you might need to close and re-open the browser in order to get the new certificate. How to clear the SSL state in a browser.","title":"Verification Steps"},{"location":"000020147/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020148/","text":"Istio fails to deploy with restricted PodSecurityPolicy in Rancher v2.3 and v2.4 This document (000020148) is provided subject to the disclaimer at the end of this document. Situation Issue Attempting to enable Istio in a Rancher v2.3 or v2.4 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the istio-galley, istio-pilot, istio-policy, istio-sidecar-injector and istio-telemtry Deployments in a CrashLoopBackOff, with log messages of the formats below: fatal validation admission webhook ListenAndServeTLS failed: listen tcp :443: bind: permission denied or nginx: [emerg] chown(\"/tmp/nginx\", 101) failed (1: Operation not permitted) In addition in namespaces with Istio sidecar auto injection enabled, an error of the following format will show for Pods upon scheduling: Pods \"nginx-7f4c54479d-\" is forbidden: unable to validate against any pod security policy: [spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_ADMIN\": capability may not be added spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_RAW\": capability may not be added] This is a result of the system capabilities required by the Istio system components ( CHOWN and NET_BIND_SERVICE ), as well as the Istio sidecar containers ( NET_ADMIN and NET_RAW ), in the default Istio configuration and which are blocked by the restricted PSP. Pre-requisites Rancher v2.3.x or v2.4.x with a restricted PSP configured as the default and Istio enabled Resolution The steps to configure Istio in a cluster with restrictive Pod Security Policies enabled can be found in the Rancher documentation \"Enable Istio with Pod Security Policies\" . Futher Reading Rancher Documentation on Istio Kubernetes Documentation on PSP Capabilities Istio Requirements Documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Istio fails to deploy with restricted PodSecurityPolicy in Rancher v2.3 and v2.4"},{"location":"000020148/#istio-fails-to-deploy-with-restricted-podsecuritypolicy-in-rancher-v23-and-v24","text":"This document (000020148) is provided subject to the disclaimer at the end of this document.","title":"Istio fails to deploy with restricted PodSecurityPolicy in Rancher v2.3 and v2.4"},{"location":"000020148/#situation","text":"","title":"Situation"},{"location":"000020148/#issue","text":"Attempting to enable Istio in a Rancher v2.3 or v2.4 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the istio-galley, istio-pilot, istio-policy, istio-sidecar-injector and istio-telemtry Deployments in a CrashLoopBackOff, with log messages of the formats below: fatal validation admission webhook ListenAndServeTLS failed: listen tcp :443: bind: permission denied or nginx: [emerg] chown(\"/tmp/nginx\", 101) failed (1: Operation not permitted) In addition in namespaces with Istio sidecar auto injection enabled, an error of the following format will show for Pods upon scheduling: Pods \"nginx-7f4c54479d-\" is forbidden: unable to validate against any pod security policy: [spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_ADMIN\": capability may not be added spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_RAW\": capability may not be added] This is a result of the system capabilities required by the Istio system components ( CHOWN and NET_BIND_SERVICE ), as well as the Istio sidecar containers ( NET_ADMIN and NET_RAW ), in the default Istio configuration and which are blocked by the restricted PSP.","title":"Issue"},{"location":"000020148/#pre-requisites","text":"Rancher v2.3.x or v2.4.x with a restricted PSP configured as the default and Istio enabled","title":"Pre-requisites"},{"location":"000020148/#resolution","text":"The steps to configure Istio in a cluster with restrictive Pod Security Policies enabled can be found in the Rancher documentation \"Enable Istio with Pod Security Policies\" .","title":"Resolution"},{"location":"000020148/#futher-reading","text":"Rancher Documentation on Istio Kubernetes Documentation on PSP Capabilities Istio Requirements Documentation","title":"Futher Reading"},{"location":"000020148/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020149/","text":"Resolving conntrack table full error messages: 'nf_conntrack: table full, dropping packets' This document (000020149) is provided subject to the disclaimer at the end of this document. Situation Issue When investigating a connectivity issue, you may experience errors like the below in the system logs: nf_conntrack: table full, dropping packets This error indicates the connection tracking table size has been exhausted. This can manifest with different symptoms, such as intermittent or consistent network timeouts. The conntrack table keeps state on open connections that the kernel is translating. This occurs often in a Kubernetes cluster when pods access an external endpoint, or another service within the cluster. These scenarios use NAT and stateful firewall rules which are maintained as entries in the conntrack table. Investigation By default, the table size is calculated based on the memory allocated to the node. This does not fit all workloads demands, for example in a microservice environment typically a higher number of inter-service connections could be expected without consuming a high amount of memory. To output the current max table size: cat /proc/sys/net/netfilter/nf_conntrack_max To get a point in time count of the current entries in the table: cat /proc/sys/net/netfilter/nf_conntrack_count Note: With the conntrack package installed, you can also use conntrack -C If the nf_conntrack_count and nf_conntrack_max are close, it is indicating that the current workload requires a larger table size. If the current number of entries are not approaching the table size, this could indicate that a burst of workload was experienced historically, in a containerized environment this can be common. For example, if the high-traffic Pods may now running on different nodes. Resolution Increasing the conntrack table size is achieved with sysctl . Calculate a higher value, this can be applied to the node immediately with: sysctl -w net.netfilter.nf_conntrack_max=<value> To persist through reboot, add the tunable to either /etc/sysctl.conf , or a specific config file in /etc/sysctl.d . For example, if your Linux distribution follows the /etc/sysctl.d/ directory structure: echo \"net.netfilter.nf_conntrack_max=<value>\" > /etc/sysctl.d/10-conntrack-max.conf sysctl -p /etc/sysctl.d/10-conntrack-max.conf This creates a new config file to set the table size at each boot. Additionally, if you configure nodes with configuration management, UserData, or build custom images etc., you may wish to add this to your usual approach to configure this for future nodes. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Resolving conntrack table full error messages: 'nf\\_conntrack: table full, dropping packets'"},{"location":"000020149/#resolving-conntrack-table-full-error-messages-nf_conntrack-table-full-dropping-packets","text":"This document (000020149) is provided subject to the disclaimer at the end of this document.","title":"Resolving conntrack table full error messages: 'nf_conntrack: table full, dropping packets'"},{"location":"000020149/#situation","text":"","title":"Situation"},{"location":"000020149/#issue","text":"When investigating a connectivity issue, you may experience errors like the below in the system logs: nf_conntrack: table full, dropping packets This error indicates the connection tracking table size has been exhausted. This can manifest with different symptoms, such as intermittent or consistent network timeouts. The conntrack table keeps state on open connections that the kernel is translating. This occurs often in a Kubernetes cluster when pods access an external endpoint, or another service within the cluster. These scenarios use NAT and stateful firewall rules which are maintained as entries in the conntrack table.","title":"Issue"},{"location":"000020149/#investigation","text":"By default, the table size is calculated based on the memory allocated to the node. This does not fit all workloads demands, for example in a microservice environment typically a higher number of inter-service connections could be expected without consuming a high amount of memory. To output the current max table size: cat /proc/sys/net/netfilter/nf_conntrack_max To get a point in time count of the current entries in the table: cat /proc/sys/net/netfilter/nf_conntrack_count Note: With the conntrack package installed, you can also use conntrack -C If the nf_conntrack_count and nf_conntrack_max are close, it is indicating that the current workload requires a larger table size. If the current number of entries are not approaching the table size, this could indicate that a burst of workload was experienced historically, in a containerized environment this can be common. For example, if the high-traffic Pods may now running on different nodes.","title":"Investigation"},{"location":"000020149/#resolution","text":"Increasing the conntrack table size is achieved with sysctl . Calculate a higher value, this can be applied to the node immediately with: sysctl -w net.netfilter.nf_conntrack_max=<value> To persist through reboot, add the tunable to either /etc/sysctl.conf , or a specific config file in /etc/sysctl.d . For example, if your Linux distribution follows the /etc/sysctl.d/ directory structure: echo \"net.netfilter.nf_conntrack_max=<value>\" > /etc/sysctl.d/10-conntrack-max.conf sysctl -p /etc/sysctl.d/10-conntrack-max.conf This creates a new config file to set the table size at each boot. Additionally, if you configure nodes with configuration management, UserData, or build custom images etc., you may wish to add this to your usual approach to configure this for future nodes.","title":"Resolution"},{"location":"000020149/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020150/","text":"Does SUSE Rancher Hosted provide downstream clusters? This document (000020150) is provided subject to the disclaimer at the end of this document. Resolution No, currently our SUSE Rancher Hosted service only includes the Rancher multi-cluster management software. Downstream clusters are not provided as part of the service. You will either need to use Rancher to provision Kubernetes clusters on-premise or in a cloud provider account that you own. You can also import existing Kubernetes clusters into SUSE Rancher Hosted. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Does SUSE Rancher Hosted provide downstream clusters?"},{"location":"000020150/#does-suse-rancher-hosted-provide-downstream-clusters","text":"This document (000020150) is provided subject to the disclaimer at the end of this document.","title":"Does SUSE Rancher Hosted provide downstream clusters?"},{"location":"000020150/#resolution","text":"No, currently our SUSE Rancher Hosted service only includes the Rancher multi-cluster management software. Downstream clusters are not provided as part of the service. You will either need to use Rancher to provision Kubernetes clusters on-premise or in a cloud provider account that you own. You can also import existing Kubernetes clusters into SUSE Rancher Hosted.","title":"Resolution"},{"location":"000020150/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020151/","text":"How to grant users access to Grafana with minimal permissions This document (000020151) is provided subject to the disclaimer at the end of this document. Situation Task You can follow these directions to create a new user and grant minimal permissions to view cluster monitoring and Grafana graphs in your Kubernetes cluster. Requirements Rancher v2.x Monitoring enabled in your cluster Background You may have a use case to grant permissions to a user to view cluster monitoring metrics and graphs, but don't want that same user to be able to see other information or perform any actions on your cluster. This how-to guide will show you how to achieve this. Solution If you have not already, create a new user in Rancher. Go to the Global view and click on the Users menu. Click the Add Users button in the top right corner. Select the desired Username, Password, and Display Name. For Global Permissions, select User-Base and leave all Custom permissions unchecked. Click the Create button at the bottom of the form. Let's assume we are using the username johndoe . Go to the Security menu and select Roles. Select the Projects tab and click the Add Project Role button. In the name field, enter Services Proxy. Under Grant Resources, click the + Add Resource button. Check the Get and List boxes and enter services/proxy in the Resource field. Note, you'll see it changes this to serivces/proxy (Custom) which is normal. Click the Create button at the bottom to create the new project role. Next, go to the cluster view for your cluster and select Members from the menu. Click the Add Members button in the top right corner. In the Members dropdown, select johndoe and select Member for Cluster Permissions. Click the Create button at the bottom of the form. Now navigate to the System project in your cluster. Go to the Members menu and click the Add Member button. Enter johndoe in the Member field and select Services Proxy under Project Permissions. Click the Create button at the bottom of the form. The johndoe user should now be able to log into Rancher and see the cluster dashboard with the Grafana icons. Clicking the Grafana icons should open a new browser window that will show the user various graphs and statistics for the cluster. This user should not be able to perform other operations, like view or launch new workloads in the cluster. Further Reading For more detailed information on how RBAC works in Rancher and Kubernetes, see the following links: Role-Based Access Control (RBAC) in Rancher Using RBAC Authorization in Kubernetes Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to grant users access to Grafana with minimal permissions"},{"location":"000020151/#how-to-grant-users-access-to-grafana-with-minimal-permissions","text":"This document (000020151) is provided subject to the disclaimer at the end of this document.","title":"How to grant users access to Grafana with minimal permissions"},{"location":"000020151/#situation","text":"","title":"Situation"},{"location":"000020151/#task","text":"You can follow these directions to create a new user and grant minimal permissions to view cluster monitoring and Grafana graphs in your Kubernetes cluster.","title":"Task"},{"location":"000020151/#requirements","text":"Rancher v2.x Monitoring enabled in your cluster","title":"Requirements"},{"location":"000020151/#background","text":"You may have a use case to grant permissions to a user to view cluster monitoring metrics and graphs, but don't want that same user to be able to see other information or perform any actions on your cluster. This how-to guide will show you how to achieve this.","title":"Background"},{"location":"000020151/#solution","text":"If you have not already, create a new user in Rancher. Go to the Global view and click on the Users menu. Click the Add Users button in the top right corner. Select the desired Username, Password, and Display Name. For Global Permissions, select User-Base and leave all Custom permissions unchecked. Click the Create button at the bottom of the form. Let's assume we are using the username johndoe . Go to the Security menu and select Roles. Select the Projects tab and click the Add Project Role button. In the name field, enter Services Proxy. Under Grant Resources, click the + Add Resource button. Check the Get and List boxes and enter services/proxy in the Resource field. Note, you'll see it changes this to serivces/proxy (Custom) which is normal. Click the Create button at the bottom to create the new project role. Next, go to the cluster view for your cluster and select Members from the menu. Click the Add Members button in the top right corner. In the Members dropdown, select johndoe and select Member for Cluster Permissions. Click the Create button at the bottom of the form. Now navigate to the System project in your cluster. Go to the Members menu and click the Add Member button. Enter johndoe in the Member field and select Services Proxy under Project Permissions. Click the Create button at the bottom of the form. The johndoe user should now be able to log into Rancher and see the cluster dashboard with the Grafana icons. Clicking the Grafana icons should open a new browser window that will show the user various graphs and statistics for the cluster. This user should not be able to perform other operations, like view or launch new workloads in the cluster.","title":"Solution"},{"location":"000020151/#further-reading","text":"For more detailed information on how RBAC works in Rancher and Kubernetes, see the following links: Role-Based Access Control (RBAC) in Rancher Using RBAC Authorization in Kubernetes","title":"Further Reading"},{"location":"000020151/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020152/","text":"Updating SSL cert in Rancher v2.x with the same CA This document (000020152) is provided subject to the disclaimer at the end of this document. Situation Task How do I renew my SSL/TLS certificate for Rancher? Pre-requisites Running Rancher v2.x Rancher on a Kubernetes Cluster see documentation for more information The new certificate must have the same root CA as the current certificate. Used the option \"Bring your own certificate\" when installing Rancher Doc A copy of the certificate and private key in Base64 format Doc A copy of the root and intermediate CA certificate (Sometimes called the certificate chain). Assumptions kubectl access to the Rancher local cluster The certificate is stored as server.crt The private key is stored as tls.key The root CA is stored as root-ca.crt The intermediate CA is stored as intermediate-ca.crt Resolution Install Steps Verify private key doesn't have a passphrase using the command listed below. If the following command asks for a passphrase then it is password protected and this must be removed. openssl rsa -in tls.key -noout Remove the passphrase (skip this step if the previous command didn't ask for a passphrase): mv tls.key tls-pass.key openssl rsa -in tls-pass.key -out tls.key Enter your passphrase here Create the certificate chain. If you have additional intermediate certs please add them at this step. NB : Order is important! cat server.crt intermediate-ca.crt root-ca.crt > tls.crt Backup the current certificate: kubectl -n cattle-system get secret tls-rancher-ingress -o yaml > tls-rancher-ingress-bk.yaml Remove the current certificate: kubectl -n cattle-system delete secret tls-rancher-ingress Install the new certificate: kubectl -n cattle-system create secret tls tls-rancher-ingress \\ --cert=tls.crt \\ --key=tls.key Verification Steps Run the following command to verify the new certificate. (Replace Rancher with your Rancher URL): curl --insecure -v https://<<Rancher>> 2>&1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }' Example output: * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server did not agree to a protocol * Server certificate: * subject: OU=Domain Control Validated; CN=*.rancher.tools * start date: Jul 2 00:42:01 2019 GMT * expire date: May 2 00:19:41 2020 GMT * issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Mark bundle as not supporting multiuse * Connection #0 to host lab.rancher.tools left intact NOTE: Some browsers will cache the certificate. So you might to close the browser and reopen in order to get the new certificate. How to clear the SSL state in a browser . Rollback Steps Backup the new certificate: kubectl -n cattle-system get secret tls-rancher-ingress -o yaml > tls-rancher-ingress-new.yaml Remove the new certificate: kubectl -n cattle-system delete secret tls-rancher-ingress Re-install the old certificate: kubectl -n cattle-system apply -f tls-rancher-ingress-bk.yaml Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Updating SSL cert in Rancher v2.x with the same CA"},{"location":"000020152/#updating-ssl-cert-in-rancher-v2x-with-the-same-ca","text":"This document (000020152) is provided subject to the disclaimer at the end of this document.","title":"Updating SSL cert in Rancher v2.x with the same CA"},{"location":"000020152/#situation","text":"","title":"Situation"},{"location":"000020152/#task","text":"How do I renew my SSL/TLS certificate for Rancher?","title":"Task"},{"location":"000020152/#pre-requisites","text":"Running Rancher v2.x Rancher on a Kubernetes Cluster see documentation for more information The new certificate must have the same root CA as the current certificate. Used the option \"Bring your own certificate\" when installing Rancher Doc A copy of the certificate and private key in Base64 format Doc A copy of the root and intermediate CA certificate (Sometimes called the certificate chain).","title":"Pre-requisites"},{"location":"000020152/#assumptions","text":"kubectl access to the Rancher local cluster The certificate is stored as server.crt The private key is stored as tls.key The root CA is stored as root-ca.crt The intermediate CA is stored as intermediate-ca.crt","title":"Assumptions"},{"location":"000020152/#resolution","text":"","title":"Resolution"},{"location":"000020152/#install-steps","text":"Verify private key doesn't have a passphrase using the command listed below. If the following command asks for a passphrase then it is password protected and this must be removed. openssl rsa -in tls.key -noout Remove the passphrase (skip this step if the previous command didn't ask for a passphrase): mv tls.key tls-pass.key openssl rsa -in tls-pass.key -out tls.key Enter your passphrase here Create the certificate chain. If you have additional intermediate certs please add them at this step. NB : Order is important! cat server.crt intermediate-ca.crt root-ca.crt > tls.crt Backup the current certificate: kubectl -n cattle-system get secret tls-rancher-ingress -o yaml > tls-rancher-ingress-bk.yaml Remove the current certificate: kubectl -n cattle-system delete secret tls-rancher-ingress Install the new certificate: kubectl -n cattle-system create secret tls tls-rancher-ingress \\ --cert=tls.crt \\ --key=tls.key","title":"Install Steps"},{"location":"000020152/#verification-steps","text":"Run the following command to verify the new certificate. (Replace Rancher with your Rancher URL): curl --insecure -v https://<<Rancher>> 2>&1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }' Example output: * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 * ALPN, server did not agree to a protocol * Server certificate: * subject: OU=Domain Control Validated; CN=*.rancher.tools * start date: Jul 2 00:42:01 2019 GMT * expire date: May 2 00:19:41 2020 GMT * issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2 * SSL certificate verify ok. * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Mark bundle as not supporting multiuse * Connection #0 to host lab.rancher.tools left intact NOTE: Some browsers will cache the certificate. So you might to close the browser and reopen in order to get the new certificate. How to clear the SSL state in a browser .","title":"Verification Steps"},{"location":"000020152/#rollback-steps","text":"Backup the new certificate: kubectl -n cattle-system get secret tls-rancher-ingress -o yaml > tls-rancher-ingress-new.yaml Remove the new certificate: kubectl -n cattle-system delete secret tls-rancher-ingress Re-install the old certificate: kubectl -n cattle-system apply -f tls-rancher-ingress-bk.yaml","title":"Rollback Steps"},{"location":"000020152/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020154/","text":"How to troubleshoot SNI enabled endpoints with curl and openssl This document (000020154) is provided subject to the disclaimer at the end of this document. Situation Issue A modern webserver hosting or proxying to multiple backend domain names will often be configured to use SNI (Server Name Indication). SNI allows multiple SSL-protected domains to be hosted on the same IP address, and is commonly used in Kubernetes with ingress controllers , for example, the nginx ingress controller. As the SNI extension requires a slight change to the conversation between client and server - the hostname must be provided in the Hello message to correctly access the associated domain name. This can present an issue when troubleshooting a node or pod directly, where an IP address is used. Pre-requisites The curl and/or openssl command installed Network access to the endpoint you wish to troubleshoot Steps To perform an SNI-compliant request using an IP address, use the following commands replacing the domain name and IP address. Using the curl command: curl -v --resolve domain.com:443:<ip address> https://domain.com Using openssl can be useful to obtain details about the certificate configured: openssl s_client -showcerts -servername domain.com -connect <ip address>:443 Further reading More information on SNI can be found here . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to troubleshoot SNI enabled endpoints with curl and openssl"},{"location":"000020154/#how-to-troubleshoot-sni-enabled-endpoints-with-curl-and-openssl","text":"This document (000020154) is provided subject to the disclaimer at the end of this document.","title":"How to troubleshoot SNI enabled endpoints with curl and openssl"},{"location":"000020154/#situation","text":"","title":"Situation"},{"location":"000020154/#issue","text":"A modern webserver hosting or proxying to multiple backend domain names will often be configured to use SNI (Server Name Indication). SNI allows multiple SSL-protected domains to be hosted on the same IP address, and is commonly used in Kubernetes with ingress controllers , for example, the nginx ingress controller. As the SNI extension requires a slight change to the conversation between client and server - the hostname must be provided in the Hello message to correctly access the associated domain name. This can present an issue when troubleshooting a node or pod directly, where an IP address is used.","title":"Issue"},{"location":"000020154/#pre-requisites","text":"The curl and/or openssl command installed Network access to the endpoint you wish to troubleshoot","title":"Pre-requisites"},{"location":"000020154/#steps","text":"To perform an SNI-compliant request using an IP address, use the following commands replacing the domain name and IP address. Using the curl command: curl -v --resolve domain.com:443:<ip address> https://domain.com Using openssl can be useful to obtain details about the certificate configured: openssl s_client -showcerts -servername domain.com -connect <ip address>:443","title":"Steps"},{"location":"000020154/#further-reading","text":"More information on SNI can be found here .","title":"Further reading"},{"location":"000020154/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020155/","text":"What is SUSE Rancher Hosted? This document (000020155) is provided subject to the disclaimer at the end of this document. Resolution SUSE Rancher Hosted is a software-as-a-service offering from SUSE. As the name implies, Rancher is completely hosted for you in the cloud. SUSE takes care of the installation, upgrade, and day-to-day operations of your Rancher control plane. Using Rancher, you can then add your own cloud-based, on-premise, AKS, GKE, or EKS clusters. For more information, see the Rancher v2.4 announcement , blog announcement , or SUSE Rancher Hosted product page . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What is SUSE Rancher Hosted?"},{"location":"000020155/#what-is-suse-rancher-hosted","text":"This document (000020155) is provided subject to the disclaimer at the end of this document.","title":"What is SUSE Rancher Hosted?"},{"location":"000020155/#resolution","text":"SUSE Rancher Hosted is a software-as-a-service offering from SUSE. As the name implies, Rancher is completely hosted for you in the cloud. SUSE takes care of the installation, upgrade, and day-to-day operations of your Rancher control plane. Using Rancher, you can then add your own cloud-based, on-premise, AKS, GKE, or EKS clusters. For more information, see the Rancher v2.4 announcement , blog announcement , or SUSE Rancher Hosted product page .","title":"Resolution"},{"location":"000020155/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020156/","text":"Can I move an existing Kubernetes cluster to SUSE Rancher Hosted? This document (000020156) is provided subject to the disclaimer at the end of this document. Resolution Existing Kubernetes clusters can be imported into SUSE Rancher Hosted. However, you cannot currently move a cluster that is already managed by Rancher to SUSE Rancher Hosted. We are currently looking to enhance our management capabilities to allow users to move clusters between Rancher clusters. See GitHub issue 16471 . As a workaround, you can redeploy workloads running in an existing cluster over to a new SUSE Rancher Hosted managed cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can I move an existing Kubernetes cluster to SUSE Rancher Hosted?"},{"location":"000020156/#can-i-move-an-existing-kubernetes-cluster-to-suse-rancher-hosted","text":"This document (000020156) is provided subject to the disclaimer at the end of this document.","title":"Can I move an existing Kubernetes cluster to SUSE Rancher Hosted?"},{"location":"000020156/#resolution","text":"Existing Kubernetes clusters can be imported into SUSE Rancher Hosted. However, you cannot currently move a cluster that is already managed by Rancher to SUSE Rancher Hosted. We are currently looking to enhance our management capabilities to allow users to move clusters between Rancher clusters. See GitHub issue 16471 . As a workaround, you can redeploy workloads running in an existing cluster over to a new SUSE Rancher Hosted managed cluster.","title":"Resolution"},{"location":"000020156/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020157/","text":"Where are the links for Prometheus and Grafana in Rancher v2.x Cluster and Project Monitoring? This document (000020157) is provided subject to the disclaimer at the end of this document. Situation Question Using Rancher v2.x, from v2.2.4 and above, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through the use of the built-in Rancher cluster and project monitoring. Rancher monitoring deploys the open-source Grafana and Prometheus projects, and this article details how to access the UI for these components, so you can view the monitoring dashboards and query metrics. Pre-requisites A Kubernetes cluster managed by Rancher v2.x, from v2.2.4 and above Cluster monitoring enabled : Go to your Cluster --> Tools --> Monitoring --> Enable (Optionally) Project monitoring enabled : Go to your Project --> Tools --> Monitoring --> Enable Answer Cluster level Grafana To access the Grafana UI for Rancher cluster monitoring, from the main dashboard for your cluster, click the three-dot option menu in the top-right and click the button \"Go to Grafana\". You should also see Grafana logos next to the system components on the main dashboard for your cluster. Click on any of them to take you to the Grafana dashboard for that particular component. Prometheus To access the Prometheus UI for the cluster monitoring, navigate to the Apps page in the System project for your cluster. You will see an app called \"cluster-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus. Project level Grafana and Prometheus To access the Grafana and Prometheus UIs, for a project with project monitoring enabled, navigate to the Apps page within the project. You will see an app called \"project-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus. Further Reading Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Where are the links for Prometheus and Grafana in Rancher v2.x Cluster and Project Monitoring?"},{"location":"000020157/#where-are-the-links-for-prometheus-and-grafana-in-rancher-v2x-cluster-and-project-monitoring","text":"This document (000020157) is provided subject to the disclaimer at the end of this document.","title":"Where are the links for Prometheus and Grafana in Rancher v2.x Cluster and Project Monitoring?"},{"location":"000020157/#situation","text":"","title":"Situation"},{"location":"000020157/#question","text":"Using Rancher v2.x, from v2.2.4 and above, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through the use of the built-in Rancher cluster and project monitoring. Rancher monitoring deploys the open-source Grafana and Prometheus projects, and this article details how to access the UI for these components, so you can view the monitoring dashboards and query metrics.","title":"Question"},{"location":"000020157/#pre-requisites","text":"A Kubernetes cluster managed by Rancher v2.x, from v2.2.4 and above Cluster monitoring enabled : Go to your Cluster --> Tools --> Monitoring --> Enable (Optionally) Project monitoring enabled : Go to your Project --> Tools --> Monitoring --> Enable","title":"Pre-requisites"},{"location":"000020157/#answer","text":"","title":"Answer"},{"location":"000020157/#cluster-level","text":"","title":"Cluster level"},{"location":"000020157/#grafana","text":"To access the Grafana UI for Rancher cluster monitoring, from the main dashboard for your cluster, click the three-dot option menu in the top-right and click the button \"Go to Grafana\". You should also see Grafana logos next to the system components on the main dashboard for your cluster. Click on any of them to take you to the Grafana dashboard for that particular component.","title":"Grafana"},{"location":"000020157/#prometheus","text":"To access the Prometheus UI for the cluster monitoring, navigate to the Apps page in the System project for your cluster. You will see an app called \"cluster-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus.","title":"Prometheus"},{"location":"000020157/#project-level","text":"","title":"Project level"},{"location":"000020157/#grafana-and-prometheus","text":"To access the Grafana and Prometheus UIs, for a project with project monitoring enabled, navigate to the Apps page within the project. You will see an app called \"project-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus.","title":"Grafana and Prometheus"},{"location":"000020157/#further-reading","text":"Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here .","title":"Further Reading"},{"location":"000020157/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020158/","text":"How to use the calicoctl CLI in an RKE or Rancher provisioned Kubernetes cluster This document (000020158) is provided subject to the disclaimer at the end of this document. Situation Task The calicoctl CLI provides an interface for managing calico network and security policy. In Kubernetes clusters provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, and which use the Calico or Canal Container Networking Interface (CNI) Plugin, calicoctl can be used to configure Calico GlobalNetworkPolicy and NetworkPolicy resources. Pre-requisites A Kubernetes cluster provisioned with Rancher Kubernetes Engine (RKE) v0.x.x or v1.x.x, or Rancher v2.x.x The Calico or Canal Container Networking Interface (CNI) Plugin (Canal is the default in both RKE and Rancher provisioned clusters). A cluster-admin level kube config sourced via $KUBECONFIG on a host running Docker Resolution N.B. The commands in this section should be run from a host running Docker, with a cluster-admin level kube config sourced. For the purpose of this example, we will demonstrate creating an empty GlobalNetworkPolicy resource via calicoctl . Set $KUBECONFIG environment variable to the cluster-admin kube config With the cluster-admin level kube config file present on the host, execute export KUBECONFIG=<full path to cluster-admin kube config> replacing with the full path of the kube config. Create the desired resource in the working directory Create a YAML file in the working directory with the NetworkPolicy resource definition(s) you want to apply to the cluster. For this example create a file named globalpolicy.yaml in the working directory with the following contents: apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: name: allow-tcp-port-6379 Determine the calico-node version of the cluster First get the version of the calico-node container running in the cluster. In a cluster with the Canal CNI Network Provider, run the following, with the admin kube config sourced: CALICOVERSION=`kubectl -n kube-system get daemonset canal -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3` echo $CALICOVERSION In a cluster with the Calico CNI Network Provider, run the following, with the admin kube config sourced: CALICOVERSION=`kubectl -n kube-system get daemonset calico-node -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3` echo $CALICOVERSION Run calicoctl With the calico-node version determined and now set in the variable $CALICOVERSION , calicoctl can be invoked. This is done by running the calico/ctl image, with the version matching the calico-node . The kube config file is mounted into the container, as is the present working directory (at the path /host ), so that the desired resource (in this example in the file globalpolicy.yaml) is available. To execute calicoctl run the following command, altering the filename as applicable to the resource you have created in the working directory: docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION apply -f /host/globalpolicy.yaml We can now view the GlobalNetworkPolicy resource by using calicoctl get as follows: docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION get globalnetworkpolicy allow-tcp-port-6379 -o yaml This should return output similar to the following: apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: creationTimestamp: \"2020-04-08T15:12:45Z\" name: allow-tcp-port-6379 resourceVersion: \"9033\" uid: df2875a6-1142-4fe0-9f0c-5dc1372bd2c5 spec: types: - Ingress Further reading Calico's Get started with Calico network policy . The calicoctl user reference documentation . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to use the calicoctl CLI in an RKE or Rancher provisioned Kubernetes cluster"},{"location":"000020158/#how-to-use-the-calicoctl-cli-in-an-rke-or-rancher-provisioned-kubernetes-cluster","text":"This document (000020158) is provided subject to the disclaimer at the end of this document.","title":"How to use the calicoctl CLI in an RKE or Rancher provisioned Kubernetes cluster"},{"location":"000020158/#situation","text":"","title":"Situation"},{"location":"000020158/#task","text":"The calicoctl CLI provides an interface for managing calico network and security policy. In Kubernetes clusters provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, and which use the Calico or Canal Container Networking Interface (CNI) Plugin, calicoctl can be used to configure Calico GlobalNetworkPolicy and NetworkPolicy resources.","title":"Task"},{"location":"000020158/#pre-requisites","text":"A Kubernetes cluster provisioned with Rancher Kubernetes Engine (RKE) v0.x.x or v1.x.x, or Rancher v2.x.x The Calico or Canal Container Networking Interface (CNI) Plugin (Canal is the default in both RKE and Rancher provisioned clusters). A cluster-admin level kube config sourced via $KUBECONFIG on a host running Docker","title":"Pre-requisites"},{"location":"000020158/#resolution","text":"N.B. The commands in this section should be run from a host running Docker, with a cluster-admin level kube config sourced. For the purpose of this example, we will demonstrate creating an empty GlobalNetworkPolicy resource via calicoctl .","title":"Resolution"},{"location":"000020158/#set-kubeconfig-environment-variable-to-the-cluster-admin-kube-config","text":"With the cluster-admin level kube config file present on the host, execute export KUBECONFIG=<full path to cluster-admin kube config> replacing with the full path of the kube config.","title":"Set $KUBECONFIG environment variable to the cluster-admin kube config"},{"location":"000020158/#create-the-desired-resource-in-the-working-directory","text":"Create a YAML file in the working directory with the NetworkPolicy resource definition(s) you want to apply to the cluster. For this example create a file named globalpolicy.yaml in the working directory with the following contents: apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: name: allow-tcp-port-6379","title":"Create the desired resource in the working directory"},{"location":"000020158/#determine-the-calico-node-version-of-the-cluster","text":"First get the version of the calico-node container running in the cluster. In a cluster with the Canal CNI Network Provider, run the following, with the admin kube config sourced: CALICOVERSION=`kubectl -n kube-system get daemonset canal -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3` echo $CALICOVERSION In a cluster with the Calico CNI Network Provider, run the following, with the admin kube config sourced: CALICOVERSION=`kubectl -n kube-system get daemonset calico-node -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3` echo $CALICOVERSION","title":"Determine the calico-node version of the cluster"},{"location":"000020158/#run-calicoctl","text":"With the calico-node version determined and now set in the variable $CALICOVERSION , calicoctl can be invoked. This is done by running the calico/ctl image, with the version matching the calico-node . The kube config file is mounted into the container, as is the present working directory (at the path /host ), so that the desired resource (in this example in the file globalpolicy.yaml) is available. To execute calicoctl run the following command, altering the filename as applicable to the resource you have created in the working directory: docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION apply -f /host/globalpolicy.yaml We can now view the GlobalNetworkPolicy resource by using calicoctl get as follows: docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION get globalnetworkpolicy allow-tcp-port-6379 -o yaml This should return output similar to the following: apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: creationTimestamp: \"2020-04-08T15:12:45Z\" name: allow-tcp-port-6379 resourceVersion: \"9033\" uid: df2875a6-1142-4fe0-9f0c-5dc1372bd2c5 spec: types: - Ingress","title":"Run calicoctl"},{"location":"000020158/#further-reading","text":"Calico's Get started with Calico network policy . The calicoctl user reference documentation .","title":"Further reading"},{"location":"000020158/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020159/","text":"How to switch from private mirror to bundled system-charts in Rancher v2.3 and v2.4 This document (000020159) is provided subject to the disclaimer at the end of this document. Situation Task The Rancher system-charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS. In air gapped installations prior to Rancher v2.3.0 it was necessary to host a private mirror of this repository. However, since v2.3.0 a copy of these charts has been bundled with the Rancher image. This article outlines how to switch from using a private mirror to the bundled system-charts that are included in the Rancher v2.3 and v2.4 images, removing the requirement to host a private mirror. Pre-requisites A Rancher v2.3.x or v2.4.x instance, provisioned as a Highly Available (HA) install on a Kubernetes cluster, or running as a Single Node install using Docker. Resolution Rancher HA Install For a Rancher HA install this follows the same steps as the upgrade documentation . Get your current helm deployment values with helm get values rancher . Example output: helm get values rancher hostname: rancher.my.org Append --set useBundledSystemChart=true to your values, set the --version value to your current Rancher version, e.g. 2.4.2, (to pin the version and prevent an actual upgrade) and run the helm upgrade command. Example: helm upgrade rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set useBundledSystemChart=true \\ --version 2.4.2 At this point your HA Rancher should be using the bundled charts for the system-charts. Rancher Single Node Install Using Docker To accomplish this for single node installations using Docker, you will be re-creating the Rancher container, with its current data, to add the CATTLE_SYSTEM_CATALOG=bundled environment variable. This closely follows the upgrade documentation . Create a copy of the data from your Rancher server container. <RANCHER_CONTAINER_NAME> is the name of your container as shown with docker ps and the <RANCHER_CONTAINER_TAG> is the version of Rancher ( v2.3.0 for example): docker stop <RANCHER_CONTAINER_NAME> docker create --volumes-from <RANCHER_CONTAINER_NAME> --name rancher-data rancher/rancher:<RANCHER_CONTAINER_TAG> Create a backup tarball: docker run --volumes-from rancher-data -v $PWD:/backup busybox tar zcvf /backup rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz /var/lib/rancher Start a new Rancher container with the added environment variable. The important thing to note here is that you use all of the same flags as when you initially started Rancher and append -e CATTLE_SYSTEM_CATALOG=bundled before the Rancher image. Example: docker run -d --volumes-from rancher-data \\ --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -e CATTLE_SYSTEM_CATALOG=bundled \\ rancher/rancher:<RANCHER_VERSION_TAG> At this point your single node Rancher installation using Docker should be using the bundled charts for the system-charts. Further reading Documentation on Setting up Local System Charts for Air Gapped Installations Documentation for Rancher HA Installs Rancher HA Upgrade Documentation Rancher HA Helm Chart Options Documentation Documentation for Single Node Installs Rancher Single Node Upgrade Documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to switch from private mirror to bundled system-charts in Rancher v2.3 and v2.4"},{"location":"000020159/#how-to-switch-from-private-mirror-to-bundled-system-charts-in-rancher-v23-and-v24","text":"This document (000020159) is provided subject to the disclaimer at the end of this document.","title":"How to switch from private mirror to bundled system-charts in Rancher v2.3 and v2.4"},{"location":"000020159/#situation","text":"","title":"Situation"},{"location":"000020159/#task","text":"The Rancher system-charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS. In air gapped installations prior to Rancher v2.3.0 it was necessary to host a private mirror of this repository. However, since v2.3.0 a copy of these charts has been bundled with the Rancher image. This article outlines how to switch from using a private mirror to the bundled system-charts that are included in the Rancher v2.3 and v2.4 images, removing the requirement to host a private mirror.","title":"Task"},{"location":"000020159/#pre-requisites","text":"A Rancher v2.3.x or v2.4.x instance, provisioned as a Highly Available (HA) install on a Kubernetes cluster, or running as a Single Node install using Docker.","title":"Pre-requisites"},{"location":"000020159/#resolution","text":"","title":"Resolution"},{"location":"000020159/#rancher-ha-install","text":"For a Rancher HA install this follows the same steps as the upgrade documentation . Get your current helm deployment values with helm get values rancher . Example output: helm get values rancher hostname: rancher.my.org Append --set useBundledSystemChart=true to your values, set the --version value to your current Rancher version, e.g. 2.4.2, (to pin the version and prevent an actual upgrade) and run the helm upgrade command. Example: helm upgrade rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set useBundledSystemChart=true \\ --version 2.4.2 At this point your HA Rancher should be using the bundled charts for the system-charts.","title":"Rancher HA Install"},{"location":"000020159/#rancher-single-node-install-using-docker","text":"To accomplish this for single node installations using Docker, you will be re-creating the Rancher container, with its current data, to add the CATTLE_SYSTEM_CATALOG=bundled environment variable. This closely follows the upgrade documentation . Create a copy of the data from your Rancher server container. <RANCHER_CONTAINER_NAME> is the name of your container as shown with docker ps and the <RANCHER_CONTAINER_TAG> is the version of Rancher ( v2.3.0 for example): docker stop <RANCHER_CONTAINER_NAME> docker create --volumes-from <RANCHER_CONTAINER_NAME> --name rancher-data rancher/rancher:<RANCHER_CONTAINER_TAG> Create a backup tarball: docker run --volumes-from rancher-data -v $PWD:/backup busybox tar zcvf /backup rancher-data-backup-<RANCHER_VERSION>-<DATE>.tar.gz /var/lib/rancher Start a new Rancher container with the added environment variable. The important thing to note here is that you use all of the same flags as when you initially started Rancher and append -e CATTLE_SYSTEM_CATALOG=bundled before the Rancher image. Example: docker run -d --volumes-from rancher-data \\ --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ -e CATTLE_SYSTEM_CATALOG=bundled \\ rancher/rancher:<RANCHER_VERSION_TAG> At this point your single node Rancher installation using Docker should be using the bundled charts for the system-charts.","title":"Rancher Single Node Install Using Docker"},{"location":"000020159/#further-reading","text":"Documentation on Setting up Local System Charts for Air Gapped Installations","title":"Further reading"},{"location":"000020159/#documentation-for-rancher-ha-installs","text":"Rancher HA Upgrade Documentation Rancher HA Helm Chart Options Documentation","title":"Documentation for Rancher HA Installs"},{"location":"000020159/#documentation-for-single-node-installs","text":"Rancher Single Node Upgrade Documentation","title":"Documentation for Single Node Installs"},{"location":"000020159/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020160/","text":"How to run multiple ingress controllers This document (000020160) is provided subject to the disclaimer at the end of this document. Situation Why use multiple ingress controllers? At large numbers of ingresses and related workloads, a single ingress-controller can be a bottleneck in both throughput and reliability. It is recommended to shard ingresses across multiple ingress controllers in these scenarios. Requirements A Kubernetes cluster created by Rancher v2.x or RKE A Linux cluster, Windows is currently not supported Helm installed and configured Overview At a high level, the process for sharding ingresses is to build out one or more extra ingress controllers and logically separate your ingresses to evenly split the load between your ingress controllers. This separation is handled through annotations on the ingresses. When an nginx-ingress-controller pod starts up with an ingressClass set, it will only try to satisfy ingresses that are annotated with the same ingressClass. This allows you to run as many ingress-controllers as needed to satisfy your ingress needs. Creating extra nginx-ingress-controller charts It is recommended to use the community nginx-ingress helm chart to install the extra ingress-controllers with NodePort services. This deployment method allows you to run multiple ingress controllers on a single node, as there are no conflicting ports. You are required to route traffic to the correct ingress controller ports through an external load balancer. Deploy a second default backend and ingress-controller from the nginx-ingress helm chart with the following values: controller.ingressClass - unique name of the ingress class, such as ingress-nginx-2 controller.service.type=NodePort controller.service.nodePorts.http - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned controller.service.nodePorts.https - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned controller.kind=DaemonSet For more configuration options, see the chart readme . An example daemonset install would be: helm repo add stable https://kubernetes-charts.storage.googleapis.com helm install nginx-ingress-second -n ingress-nginx stable/nginx-ingress --set controller.ingressClass=\"ingress-class-2\" --set controller.service.type=NodePort --set controller.kind=DaemonSet This will create an ingress-nginx daemonset and service. This ingress controller will handle any ingress routed to it tagged with the annotation kubernetes.io/ingress.class: ingress-class-2 Sharding Ingresses It is recommended to shard (split) your ingresses in a way that evenly splits load and configuration size between ingress controllers. Sharding in this way does mean changing dns and ingress hosts so that traffic for ingresses is sent to the correct ingress controllers, typically through an external load balancer. The process for sharding ingresses is to tag each ingress with the ingressClass for the ingress controller you want to route them through. For example: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: app_1_ingress annotations: kubernetes.io/ingress.class: \"ingress-class-2\" spec: Once annotated with an ingressClass, these ingresses are now only handled by the ingress-controller that has that ingressClass. In the default configuration, the Rancher-provided nginx-ingress-controller will only handle ingresses that either have the default ingress.class annotation of nginx or do not have an ingress.class annotation at all. Next steps From here it is just a matter of ensuring that the traffic for each ingress is routed to the correct nodePort on the nodes that the daemonset is targeted against. If you did not specify a nodePort when deploying the chart, you can determine the nodePort that was assigned by checking the service created: $ kubectl describe svc -n ingress-nginx nginx-ingress-second Name: nginx-ingress-second-controller Namespace: ingress-nginx Labels: app=nginx-ingress chart=nginx-ingress-1.35.0 component=controller heritage=Helm release=nginx-ingress-second Annotations: field.cattle.io/publicEndpoints: [{\"addresses\":[\"13.210.157.241\"],\"port\":30155,\"protocol\":\"TCP\",\"serviceName\":\"ingress-nginx:nginx-ingress-second-controller\",\"allNodes\":tr... Selector: app.kubernetes.io/component=controller,app=nginx-ingress,release=nginx-ingress-second Type: NodePort IP: 10.43.139.23 Port: http 80/TCP TargetPort: http/TCP NodePort: http 30155/TCP Endpoints: <none> Port: https 443/TCP TargetPort: https/TCP NodePort: https 30636/TCP Endpoints: <none> Session Affinity: None External Traffic Policy: Cluster Events: <none> In this example, the service is exposed on every node on ports 30155 for http and 30636 for https Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to run multiple ingress controllers"},{"location":"000020160/#how-to-run-multiple-ingress-controllers","text":"This document (000020160) is provided subject to the disclaimer at the end of this document.","title":"How to run multiple ingress controllers"},{"location":"000020160/#situation","text":"","title":"Situation"},{"location":"000020160/#why-use-multiple-ingress-controllers","text":"At large numbers of ingresses and related workloads, a single ingress-controller can be a bottleneck in both throughput and reliability. It is recommended to shard ingresses across multiple ingress controllers in these scenarios.","title":"Why use multiple ingress controllers?"},{"location":"000020160/#requirements","text":"A Kubernetes cluster created by Rancher v2.x or RKE A Linux cluster, Windows is currently not supported Helm installed and configured","title":"Requirements"},{"location":"000020160/#overview","text":"At a high level, the process for sharding ingresses is to build out one or more extra ingress controllers and logically separate your ingresses to evenly split the load between your ingress controllers. This separation is handled through annotations on the ingresses. When an nginx-ingress-controller pod starts up with an ingressClass set, it will only try to satisfy ingresses that are annotated with the same ingressClass. This allows you to run as many ingress-controllers as needed to satisfy your ingress needs.","title":"Overview"},{"location":"000020160/#creating-extra-nginx-ingress-controller-charts","text":"It is recommended to use the community nginx-ingress helm chart to install the extra ingress-controllers with NodePort services. This deployment method allows you to run multiple ingress controllers on a single node, as there are no conflicting ports. You are required to route traffic to the correct ingress controller ports through an external load balancer. Deploy a second default backend and ingress-controller from the nginx-ingress helm chart with the following values: controller.ingressClass - unique name of the ingress class, such as ingress-nginx-2 controller.service.type=NodePort controller.service.nodePorts.http - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned controller.service.nodePorts.https - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned controller.kind=DaemonSet For more configuration options, see the chart readme . An example daemonset install would be: helm repo add stable https://kubernetes-charts.storage.googleapis.com helm install nginx-ingress-second -n ingress-nginx stable/nginx-ingress --set controller.ingressClass=\"ingress-class-2\" --set controller.service.type=NodePort --set controller.kind=DaemonSet This will create an ingress-nginx daemonset and service. This ingress controller will handle any ingress routed to it tagged with the annotation kubernetes.io/ingress.class: ingress-class-2","title":"Creating extra nginx-ingress-controller charts"},{"location":"000020160/#sharding-ingresses","text":"It is recommended to shard (split) your ingresses in a way that evenly splits load and configuration size between ingress controllers. Sharding in this way does mean changing dns and ingress hosts so that traffic for ingresses is sent to the correct ingress controllers, typically through an external load balancer. The process for sharding ingresses is to tag each ingress with the ingressClass for the ingress controller you want to route them through. For example: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: app_1_ingress annotations: kubernetes.io/ingress.class: \"ingress-class-2\" spec: Once annotated with an ingressClass, these ingresses are now only handled by the ingress-controller that has that ingressClass. In the default configuration, the Rancher-provided nginx-ingress-controller will only handle ingresses that either have the default ingress.class annotation of nginx or do not have an ingress.class annotation at all.","title":"Sharding Ingresses"},{"location":"000020160/#next-steps","text":"From here it is just a matter of ensuring that the traffic for each ingress is routed to the correct nodePort on the nodes that the daemonset is targeted against. If you did not specify a nodePort when deploying the chart, you can determine the nodePort that was assigned by checking the service created: $ kubectl describe svc -n ingress-nginx nginx-ingress-second Name: nginx-ingress-second-controller Namespace: ingress-nginx Labels: app=nginx-ingress chart=nginx-ingress-1.35.0 component=controller heritage=Helm release=nginx-ingress-second Annotations: field.cattle.io/publicEndpoints: [{\"addresses\":[\"13.210.157.241\"],\"port\":30155,\"protocol\":\"TCP\",\"serviceName\":\"ingress-nginx:nginx-ingress-second-controller\",\"allNodes\":tr... Selector: app.kubernetes.io/component=controller,app=nginx-ingress,release=nginx-ingress-second Type: NodePort IP: 10.43.139.23 Port: http 80/TCP TargetPort: http/TCP NodePort: http 30155/TCP Endpoints: <none> Port: https 443/TCP TargetPort: https/TCP NodePort: https 30636/TCP Endpoints: <none> Session Affinity: None External Traffic Policy: Cluster Events: <none> In this example, the service is exposed on every node on ports 30155 for http and 30636 for https","title":"Next steps"},{"location":"000020160/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020161/","text":"Why does the kubelet certificate still show as expired after performing a cluster certificate rotation in an Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster? This document (000020161) is provided subject to the disclaimer at the end of this document. Situation Question Why is Kubelet certificate still indicating expired after performing a cluster certificate rotation ? Pre-requisite A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster Answer Before Rancher v2.3.3 and RKE v1.0.0, cluster provisioning did not supply the --tls-cert-file and --tls-private-key-file arguments to the Kubelet container. As a result, the kubelet automatically generates the kubelet.crt , and kubelet.key files under the /var/lib/kubelet/pki \u200bdirectory and the certificate is not rotated during the certificate rotation . How to verify the Kubelet certificate openssl s_client -connect <NODE IP>:10250 | openssl x509 -text curl -vk https://<NODE IP>:10250 Resolution You can rotate the kubelet certificate in RKE and Rancher provisioned clusters as follows: How to rotate the kubelet certificate in Rancher v2.2.0 - v2.3.0 and RKE v0.2.0 - v0.3.2 provisioned clusters For clusters provisioned and managed by Rancher prior to v2.3.3 or RKE prior to v1.0.0, you will need to manually delete the kubelet.crt and kubelet.key in /var/lib/kubelet/pki and restart the Kubelet container: docker exec kubelet rm /var/lib/kubelet/pki/kubelet.crt docker exec kubelet rm /var/lib/kubelet/pki/kubelet.key docker restart kubelet How to rotate the kubelet certificate in Rancher v2.3.2+ provisioned clusters For Rancher provisioned clusters managed by Rancher v2.3.3 and above, you can set the generate_serving_certificate kubelet option to true in the cluster configuration YAML to rotate the kubelet certificate. N.B. If hostname_override is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding. For the affected cluster click 'Edit Cluster' from within the Rancher UI cluster view. Click 'Edit as YAML'. Set the generate_serving_certificate option to true for the kubelet, per the below: services: kubelet: generate_serving_certificate: true Click 'Save' to intitate a cluster reconciliation and trigger rotation of the kubelet certificate. How to rotate the kubelet certificate in RKE v1.0.0+ provisioned clusters For clusters managed by RKE v1.0.0 and above, you can set the generate_serving_certificate kubelet option to true in the cluster configuration YAML and invoke rke up to rotate the kubelet certificate. N.B. If hostname_override is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding. Set the generate_serving_certificate option to true for the kubelet, within the cluster configuration YAML file, per the below: services: kubelet: generate_serving_certificate: true Invoke rke up --config <cluster configuration yaml> to update the cluster configuration with the new kubelet option and trigger rotation of the kubelet certificate. Further Reading RKE Certificate Rotation Documentation . Rancher v2.x Certificate Rotation Documentation . Kubelet Service Certificate Requirements Documentation . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Why does the kubelet certificate still show as expired after performing a cluster certificate rotation in an Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster?"},{"location":"000020161/#why-does-the-kubelet-certificate-still-show-as-expired-after-performing-a-cluster-certificate-rotation-in-an-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-cluster","text":"This document (000020161) is provided subject to the disclaimer at the end of this document.","title":"Why does the kubelet certificate still show as expired after performing a cluster certificate rotation in an Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster?"},{"location":"000020161/#situation","text":"","title":"Situation"},{"location":"000020161/#question","text":"Why is Kubelet certificate still indicating expired after performing a cluster certificate rotation ?","title":"Question"},{"location":"000020161/#pre-requisite","text":"A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","title":"Pre-requisite"},{"location":"000020161/#answer","text":"Before Rancher v2.3.3 and RKE v1.0.0, cluster provisioning did not supply the --tls-cert-file and --tls-private-key-file arguments to the Kubelet container. As a result, the kubelet automatically generates the kubelet.crt , and kubelet.key files under the /var/lib/kubelet/pki \u200bdirectory and the certificate is not rotated during the certificate rotation .","title":"Answer"},{"location":"000020161/#how-to-verify-the-kubelet-certificate","text":"openssl s_client -connect <NODE IP>:10250 | openssl x509 -text curl -vk https://<NODE IP>:10250","title":"How to verify the Kubelet certificate"},{"location":"000020161/#resolution","text":"You can rotate the kubelet certificate in RKE and Rancher provisioned clusters as follows:","title":"Resolution"},{"location":"000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v220-v230-and-rke-v020-v032-provisioned-clusters","text":"For clusters provisioned and managed by Rancher prior to v2.3.3 or RKE prior to v1.0.0, you will need to manually delete the kubelet.crt and kubelet.key in /var/lib/kubelet/pki and restart the Kubelet container: docker exec kubelet rm /var/lib/kubelet/pki/kubelet.crt docker exec kubelet rm /var/lib/kubelet/pki/kubelet.key docker restart kubelet","title":"How to rotate the kubelet certificate in Rancher v2.2.0 - v2.3.0 and RKE v0.2.0 - v0.3.2 provisioned clusters"},{"location":"000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v232-provisioned-clusters","text":"For Rancher provisioned clusters managed by Rancher v2.3.3 and above, you can set the generate_serving_certificate kubelet option to true in the cluster configuration YAML to rotate the kubelet certificate. N.B. If hostname_override is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding. For the affected cluster click 'Edit Cluster' from within the Rancher UI cluster view. Click 'Edit as YAML'. Set the generate_serving_certificate option to true for the kubelet, per the below: services: kubelet: generate_serving_certificate: true Click 'Save' to intitate a cluster reconciliation and trigger rotation of the kubelet certificate.","title":"How to rotate the kubelet certificate in Rancher v2.3.2+ provisioned clusters"},{"location":"000020161/#how-to-rotate-the-kubelet-certificate-in-rke-v100-provisioned-clusters","text":"For clusters managed by RKE v1.0.0 and above, you can set the generate_serving_certificate kubelet option to true in the cluster configuration YAML and invoke rke up to rotate the kubelet certificate. N.B. If hostname_override is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding. Set the generate_serving_certificate option to true for the kubelet, within the cluster configuration YAML file, per the below: services: kubelet: generate_serving_certificate: true Invoke rke up --config <cluster configuration yaml> to update the cluster configuration with the new kubelet option and trigger rotation of the kubelet certificate.","title":"How to rotate the kubelet certificate in RKE v1.0.0+ provisioned clusters"},{"location":"000020161/#further-reading","text":"RKE Certificate Rotation Documentation . Rancher v2.x Certificate Rotation Documentation . Kubelet Service Certificate Requirements Documentation .","title":"Further Reading"},{"location":"000020161/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020162/","text":"How to clean a Rancher 2.x node This document (000020162) is provided subject to the disclaimer at the end of this document. Situation Task At times a node may need to be cleaned of all state to ensure it is consistent for further use in a cluster. This article and script are for Rancher 2.x. Please note, this script will delete all containers, volumes, images, network interfaces, and directories that relate to Rancher and Kubernetes. It can also optionally flush all iptables rules and delete container images. It is important to perform pre-checks, and backup the node as needed before proceeding with any steps below. Pre-requisites A node provisioned with the RKE distribution using Rancher or the RKE CLI. The node should no longer be a member of any cluster. A copy of the cleanup script , and root/sudo access. Check the running containers or Pods, these will be forcefully deleted in the following steps. Confirm you are on the correct node and are ready to proceed with cleaning all containers and all data specific to Kubernetes and Rancher/RKE. Note , for RKE2 and K3s use the uninstall script deployed on the node during install. Resolution The below steps use a script to automate the clean of a node, the commands used can be run manually as needed, follow the steps below cleaning a node that has been used previously in a cluster. Login to the node and download the cleanup script: curl -sLO https://github.com/rancherlabs/support-tools/raw/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh You should now have a copy of the script in the current directory. Run the script: sudo bash extended-cleanup-rancher2.sh If desired, the optional -f and -i flags can be used together or individually to flush iptables (-f) and delete container images (-i). sudo bash extended-cleanup-rancher2.sh -f -i Restart the node The node is now in a clean consistent state to be reused in a cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to clean a Rancher 2.x node"},{"location":"000020162/#how-to-clean-a-rancher-2x-node","text":"This document (000020162) is provided subject to the disclaimer at the end of this document.","title":"How to clean a Rancher 2.x node"},{"location":"000020162/#situation","text":"","title":"Situation"},{"location":"000020162/#task","text":"At times a node may need to be cleaned of all state to ensure it is consistent for further use in a cluster. This article and script are for Rancher 2.x. Please note, this script will delete all containers, volumes, images, network interfaces, and directories that relate to Rancher and Kubernetes. It can also optionally flush all iptables rules and delete container images. It is important to perform pre-checks, and backup the node as needed before proceeding with any steps below.","title":"Task"},{"location":"000020162/#pre-requisites","text":"A node provisioned with the RKE distribution using Rancher or the RKE CLI. The node should no longer be a member of any cluster. A copy of the cleanup script , and root/sudo access. Check the running containers or Pods, these will be forcefully deleted in the following steps. Confirm you are on the correct node and are ready to proceed with cleaning all containers and all data specific to Kubernetes and Rancher/RKE. Note , for RKE2 and K3s use the uninstall script deployed on the node during install.","title":"Pre-requisites"},{"location":"000020162/#resolution","text":"The below steps use a script to automate the clean of a node, the commands used can be run manually as needed, follow the steps below cleaning a node that has been used previously in a cluster. Login to the node and download the cleanup script: curl -sLO https://github.com/rancherlabs/support-tools/raw/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh You should now have a copy of the script in the current directory. Run the script: sudo bash extended-cleanup-rancher2.sh If desired, the optional -f and -i flags can be used together or individually to flush iptables (-f) and delete container images (-i). sudo bash extended-cleanup-rancher2.sh -f -i Restart the node The node is now in a clean consistent state to be reused in a cluster.","title":"Resolution"},{"location":"000020162/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020163/","text":"How to troubleshoot using the namespace of a container This document (000020163) is provided subject to the disclaimer at the end of this document. Situation Task When troubleshooting an issue, often a faithful reproduction and exact environment are needed. This can be a challenge in a containerized environment, where tools and a shell environment may not be easily available within containers of a Pod. Steps There are two approaches that can be taken: Sidecar container By running a container in the same namespaces as another, it's possible to use that container for troubleshooting. The sidecar container can be started using the same network and PID namespaces while attaching the same volumes: Set the ID or name of the container you wish to troubleshoot: ID=<container ID or name> Run the sidecar container using the network, PID and volumes docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh It is now possible to troubleshoot with commands from the alpine container, within the context of the container or Pod with the issue. For example, if you were experiencing a network issue from this Pod, it is now possible to use tools available in the sidecar container to simulate the connection, view the network configuration and troubleshoot interactively. Substitute the alpine container as needed with an image of your choice. Note, this will attach the same volumes as the parent container, but the parent container read/write layers will not be accesible - to access the same container filesystem, see the nsenter example below. Use the host tools with nsenter Alternatively you can use tools available on the host for the same usecase with the nsenter command. The nsenter command is standard on most Linux distributions, for example on Ubuntu it is provided by the util-linux package. Set the ID or name of the container you wish to troubleshoot: ID=<container ID or name> Obtain the first process in the container (PID 1): PID=$(docker inspect --format '{{ .State.Pid }}' $ID) Run commands available on the node within the context of all of the container/Pod namespaces with nsenter: nsenter -a -t $PID <command> For example, if troubleshooting a network issue, tools from the node like tcpdump, curl, dig and mtr can be used to troubleshoot the issue interactively. Note, the -a flag is available in recent versions of nsenter , if this does not succeed, use a flag for a specific namespace, check the nsenter --help output. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to troubleshoot using the namespace of a container"},{"location":"000020163/#how-to-troubleshoot-using-the-namespace-of-a-container","text":"This document (000020163) is provided subject to the disclaimer at the end of this document.","title":"How to troubleshoot using the namespace of a container"},{"location":"000020163/#situation","text":"","title":"Situation"},{"location":"000020163/#task","text":"When troubleshooting an issue, often a faithful reproduction and exact environment are needed. This can be a challenge in a containerized environment, where tools and a shell environment may not be easily available within containers of a Pod.","title":"Task"},{"location":"000020163/#steps","text":"There are two approaches that can be taken:","title":"Steps"},{"location":"000020163/#sidecar-container","text":"By running a container in the same namespaces as another, it's possible to use that container for troubleshooting. The sidecar container can be started using the same network and PID namespaces while attaching the same volumes: Set the ID or name of the container you wish to troubleshoot: ID=<container ID or name> Run the sidecar container using the network, PID and volumes docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh It is now possible to troubleshoot with commands from the alpine container, within the context of the container or Pod with the issue. For example, if you were experiencing a network issue from this Pod, it is now possible to use tools available in the sidecar container to simulate the connection, view the network configuration and troubleshoot interactively. Substitute the alpine container as needed with an image of your choice. Note, this will attach the same volumes as the parent container, but the parent container read/write layers will not be accesible - to access the same container filesystem, see the nsenter example below.","title":"Sidecar container"},{"location":"000020163/#use-the-host-tools-with-nsenter","text":"Alternatively you can use tools available on the host for the same usecase with the nsenter command. The nsenter command is standard on most Linux distributions, for example on Ubuntu it is provided by the util-linux package. Set the ID or name of the container you wish to troubleshoot: ID=<container ID or name> Obtain the first process in the container (PID 1): PID=$(docker inspect --format '{{ .State.Pid }}' $ID) Run commands available on the node within the context of all of the container/Pod namespaces with nsenter: nsenter -a -t $PID <command> For example, if troubleshooting a network issue, tools from the node like tcpdump, curl, dig and mtr can be used to troubleshoot the issue interactively. Note, the -a flag is available in recent versions of nsenter , if this does not succeed, use a flag for a specific namespace, check the nsenter --help output.","title":"Use the host tools with nsenter"},{"location":"000020163/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020164/","text":"How to troubleshoot HTTP request performance with curl statistics This document (000020164) is provided subject to the disclaimer at the end of this document. Situation Task When troubleshooting a performance issue with a web-based endpoint, it's important to have metrics that assist in understanding what areas are related. This is where using a lightweight tool like curl, and it's ability to write out the statistics of a request can be very useful. Pre-requisites You will just need curl installed and available from the location performing the test. Steps Download the format file to use with curl: curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/files/curl-format.txt You should now have a curl-format.txt file locally in the current directory. Using the file and the -w flag, perform the desired request to the service, the example below displays the headers and statistics. curl -I -w \"@curl-format.txt\" https://rancher.com Timing statistics will be output with each run of the command, measurements are recorded in seconds. Note: run the command from a location that provides an accurate reproduction of the issue, to simulate the issue as closely as possible. Using the same request parameters are important - like the path and headers that might be used by client applications. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to troubleshoot HTTP request performance with curl statistics"},{"location":"000020164/#how-to-troubleshoot-http-request-performance-with-curl-statistics","text":"This document (000020164) is provided subject to the disclaimer at the end of this document.","title":"How to troubleshoot HTTP request performance with curl statistics"},{"location":"000020164/#situation","text":"","title":"Situation"},{"location":"000020164/#task","text":"When troubleshooting a performance issue with a web-based endpoint, it's important to have metrics that assist in understanding what areas are related. This is where using a lightweight tool like curl, and it's ability to write out the statistics of a request can be very useful.","title":"Task"},{"location":"000020164/#pre-requisites","text":"You will just need curl installed and available from the location performing the test.","title":"Pre-requisites"},{"location":"000020164/#steps","text":"Download the format file to use with curl: curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/files/curl-format.txt You should now have a curl-format.txt file locally in the current directory. Using the file and the -w flag, perform the desired request to the service, the example below displays the headers and statistics. curl -I -w \"@curl-format.txt\" https://rancher.com Timing statistics will be output with each run of the command, measurements are recorded in seconds. Note: run the command from a location that provides an accurate reproduction of the issue, to simulate the issue as closely as possible. Using the same request parameters are important - like the path and headers that might be used by client applications.","title":"Steps"},{"location":"000020164/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020165/","text":"How to recover an RKE v0.2.x, v0.3.x or v1.x.x cluster after restoration with an incorrect or missing rkestate file This document (000020165) is provided subject to the disclaimer at the end of this document. Situation Issue When using RKE (Rancher Kubernetes Engine) v0.2.x, v0.3.x, v1.0.x or v1.1.0, if you have restored a cluster with the incorrect or missing rkestate file you will end up in a state where your infrastructure pods will not start. This includes all pods in the kube-system, cattle-system and ingress-nginx namespaces. As a result of these stopped infrastructure pods, workload pods will not function correctly. If you find yourself in this situation you can use the directions below to fix the cluster. For more information about the cluster state file, please see RKE documentation on Kubernetes Cluster State . Pre-requisites RKE v0.2.x, v0.3.x, v1.0.x or v1.1.0 A cluster restoration performed with the incorrect or missing rkestate file Workaround Delete all service-account-token secrets in kube-system, cattle-system and ingress-nginx namespaces: kubectl get secret -n cattle-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cattle-system delete secret \" $1) }' kubectl get secret -n kube-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n kube-system delete secret \" $1) }' kubectl get secret -n ingress-nginx | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n ingress-nginx delete secret \" $1) }' kubectl get secret -n cert-manager | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cert-manager delete secret \" $1) }' Restart Docker on all nodes in the cluster currently: systemctl restart docker Force delete all pods stuck in a CrashLoopBackOff, Terminating, Error and Evicted state: kubectl get po --all-namespaces | awk '{ if ($4 ==\"CrashLoopBackOff\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' kubectl get po --all-namespaces | awk '{ if ($4 ==\"Terminating\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' kubectl get po --all-namespaces | awk '{ if ($4 ==\"Error\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' kubectl get po --all-namespaces | awk '{ if ($4 ==\"Evicted\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' Once your force delete has finished, restart Docker again to clear out any stale containers from the above force delete command: systemctl restart docker You may have to delete service account tokens more than once or delete pods more than once. After you go through the guide once, monitor pod statuses with a watch command in one terminal as shown below. watch -n1 'kubectl get po --all-namespaces | grep -i \"cattle-system\\|kube-system\\|ingress-nginx\\|cert-manager\"' If you see any pods still in an error state, you can describe them to get idea of what is wrong. Most likely you'll see an error like the following which indicates that you need to delete its service account tokens again. Warning FailedMount 7m23s (x126 over 4h7m) kubelet, 18.219.82.148 MountVolume.SetUp failed for volume \"rancher-token-tksxr\" : secret \"rancher-token-tksxr\" not found Warning FailedMount 114s (x119 over 4h5m) kubelet, 18.219.82.148 Unable to attach or mount volumes: unmounted volumes=[rancher-token-tksxr], unattached volumes=[rancher-token-tksxr]: timed out waiting for the condition Delete the service account tokens again for that one namespace so that pods in other namespaces don't have to be disturbed if they are good. Once the service account tokens are deleted, run a delete pod command for just the namespace with pods still in an error state. cattle-node-agent and cattle-cluster-agent depend on the Rancher pod to be online, so you can ignore those until the very end. Once Rancher pods are stable, go back in and delete all the agents again to get them to restart more quickly. Resolution An update to enable successful restoration of an RKE provisioned cluster without the correct rkestate file is targetted for an RKE v1.1.x patch release. For more information please see RKE GitHub issue #1336 . Further Reading RKE documentation on Kubernetes Cluster State RKE documentation on etcd snapshots Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to recover an RKE v0.2.x, v0.3.x or v1.x.x cluster after restoration with an incorrect or missing rkestate file"},{"location":"000020165/#how-to-recover-an-rke-v02x-v03x-or-v1xx-cluster-after-restoration-with-an-incorrect-or-missing-rkestate-file","text":"This document (000020165) is provided subject to the disclaimer at the end of this document.","title":"How to recover an RKE v0.2.x, v0.3.x or v1.x.x cluster after restoration with an incorrect or missing rkestate file"},{"location":"000020165/#situation","text":"","title":"Situation"},{"location":"000020165/#issue","text":"When using RKE (Rancher Kubernetes Engine) v0.2.x, v0.3.x, v1.0.x or v1.1.0, if you have restored a cluster with the incorrect or missing rkestate file you will end up in a state where your infrastructure pods will not start. This includes all pods in the kube-system, cattle-system and ingress-nginx namespaces. As a result of these stopped infrastructure pods, workload pods will not function correctly. If you find yourself in this situation you can use the directions below to fix the cluster. For more information about the cluster state file, please see RKE documentation on Kubernetes Cluster State .","title":"Issue"},{"location":"000020165/#pre-requisites","text":"RKE v0.2.x, v0.3.x, v1.0.x or v1.1.0 A cluster restoration performed with the incorrect or missing rkestate file","title":"Pre-requisites"},{"location":"000020165/#workaround","text":"Delete all service-account-token secrets in kube-system, cattle-system and ingress-nginx namespaces: kubectl get secret -n cattle-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cattle-system delete secret \" $1) }' kubectl get secret -n kube-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n kube-system delete secret \" $1) }' kubectl get secret -n ingress-nginx | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n ingress-nginx delete secret \" $1) }' kubectl get secret -n cert-manager | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cert-manager delete secret \" $1) }' Restart Docker on all nodes in the cluster currently: systemctl restart docker Force delete all pods stuck in a CrashLoopBackOff, Terminating, Error and Evicted state: kubectl get po --all-namespaces | awk '{ if ($4 ==\"CrashLoopBackOff\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' kubectl get po --all-namespaces | awk '{ if ($4 ==\"Terminating\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' kubectl get po --all-namespaces | awk '{ if ($4 ==\"Error\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' kubectl get po --all-namespaces | awk '{ if ($4 ==\"Evicted\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }' Once your force delete has finished, restart Docker again to clear out any stale containers from the above force delete command: systemctl restart docker You may have to delete service account tokens more than once or delete pods more than once. After you go through the guide once, monitor pod statuses with a watch command in one terminal as shown below. watch -n1 'kubectl get po --all-namespaces | grep -i \"cattle-system\\|kube-system\\|ingress-nginx\\|cert-manager\"' If you see any pods still in an error state, you can describe them to get idea of what is wrong. Most likely you'll see an error like the following which indicates that you need to delete its service account tokens again. Warning FailedMount 7m23s (x126 over 4h7m) kubelet, 18.219.82.148 MountVolume.SetUp failed for volume \"rancher-token-tksxr\" : secret \"rancher-token-tksxr\" not found Warning FailedMount 114s (x119 over 4h5m) kubelet, 18.219.82.148 Unable to attach or mount volumes: unmounted volumes=[rancher-token-tksxr], unattached volumes=[rancher-token-tksxr]: timed out waiting for the condition Delete the service account tokens again for that one namespace so that pods in other namespaces don't have to be disturbed if they are good. Once the service account tokens are deleted, run a delete pod command for just the namespace with pods still in an error state. cattle-node-agent and cattle-cluster-agent depend on the Rancher pod to be online, so you can ignore those until the very end. Once Rancher pods are stable, go back in and delete all the agents again to get them to restart more quickly.","title":"Workaround"},{"location":"000020165/#resolution","text":"An update to enable successful restoration of an RKE provisioned cluster without the correct rkestate file is targetted for an RKE v1.1.x patch release. For more information please see RKE GitHub issue #1336 .","title":"Resolution"},{"location":"000020165/#further-reading","text":"RKE documentation on Kubernetes Cluster State RKE documentation on etcd snapshots","title":"Further Reading"},{"location":"000020165/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020166/","text":"Pod network connectivity non-functional as a result of sysctl net.ipv4.ip_forward=0 This document (000020166) is provided subject to the disclaimer at the end of this document. Situation Issue If the sysctl net.ipv4.ip_forward is set to 0 (disabled) on a Linux host, then IPv4 packet forwarding is disabled. As a result, on a Kubernetes nodes this will prevent Pod networking from functioning. You can confirm the current value of this sysctl on a Linux host, if you are experiencing a network issue, with the following: sysctl net.ipv4.ip_forward The output should show 1 , for enabled. Pre-requisites A Kubernetes cluster with a CNI (Container Network Interface) plugin configure, e.g. an RKE (Rancher Kubernetes Engine) or Rancher launched cluster. The systctl net.ipv4.ip_forward set to 0 (disabled) on the cluster hosts. Resolution Check if the kernel parameter net.ipv4.ip_forward is set to 1 with: sysctl net.ipv4.ip_forward If the current value of net.ipv4.ip_forward is 0, then set to this to 1 with the following: sysctl net.ipv4.ip_forward=1 To make it permanent across reboot, add the following line in /etc/sysctl.conf : net.ipv4.ip_forward=1 With this sysctl correctly enabled, Pod ingress and egress will be able to function as expected. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Pod network connectivity non-functional as a result of sysctl net.ipv4.ip\\_forward=0"},{"location":"000020166/#pod-network-connectivity-non-functional-as-a-result-of-sysctl-netipv4ip_forward0","text":"This document (000020166) is provided subject to the disclaimer at the end of this document.","title":"Pod network connectivity non-functional as a result of sysctl net.ipv4.ip_forward=0"},{"location":"000020166/#situation","text":"","title":"Situation"},{"location":"000020166/#issue","text":"If the sysctl net.ipv4.ip_forward is set to 0 (disabled) on a Linux host, then IPv4 packet forwarding is disabled. As a result, on a Kubernetes nodes this will prevent Pod networking from functioning. You can confirm the current value of this sysctl on a Linux host, if you are experiencing a network issue, with the following: sysctl net.ipv4.ip_forward The output should show 1 , for enabled.","title":"Issue"},{"location":"000020166/#pre-requisites","text":"A Kubernetes cluster with a CNI (Container Network Interface) plugin configure, e.g. an RKE (Rancher Kubernetes Engine) or Rancher launched cluster. The systctl net.ipv4.ip_forward set to 0 (disabled) on the cluster hosts.","title":"Pre-requisites"},{"location":"000020166/#resolution","text":"Check if the kernel parameter net.ipv4.ip_forward is set to 1 with: sysctl net.ipv4.ip_forward If the current value of net.ipv4.ip_forward is 0, then set to this to 1 with the following: sysctl net.ipv4.ip_forward=1 To make it permanent across reboot, add the following line in /etc/sysctl.conf : net.ipv4.ip_forward=1 With this sysctl correctly enabled, Pod ingress and egress will be able to function as expected.","title":"Resolution"},{"location":"000020166/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020167/","text":"How to setup your network CIDR for a large cluster This document (000020167) is provided subject to the disclaimer at the end of this document. Situation Task If you are expecting to use Rancher to deploy a Kubernetes cluster with more than 256 nodes, you'll need to make sure you adjust the default cluster CIDR settings. The default settings only allows clusters of 256 nodes or less. Requirements Rancher v2.x A lot of hardware or VMs! Background Kubernetes provides each pod with an IP address and each node with a block of IP addresses. Each cluster is also provided a block of IP addresses that is distributed to each node. This is controlled by two settings, the cluster_cidr block and node-cidr-mask-size . By default, the cluster_cidr block is 10.42.0.0/16 and the node-cidr-mask-size is 24. This gives the cluster 256 blocks of /24 networks to distribute out to the pool of nodes. For example, node1 will get 10.24.0.0/24, node2 will get 10.42.1.0/24, node3 will get 10.42.2.0/24 and so on. Solution To support more than 256 nodes, you will need to use a larger cluster_cidr block, a smaller node-cidr-mask-size, or adjust both. For example, if you want to support up to 512 nodes you can set: cluster_cidr to 10.40.0.0/15 node-cidr-mask-size to 24 OR cluster_cidr to 10.42.0.0/16 node-cidr-mask-size to 25 To support up to 1024 nodes, you can use a larger cluster_cidr , smaller node-cidr-mask-size , or combination of both: cluster_cidr to 10.38.0.0/14 node-cidr-mask-size to 24 OR cluster_cidr to 10.42.0.0/16 node-cidr-mask-size to 26 OR cluster_cidr to 10.40.0.0/15 node-cidr-mask-size to 25 You should be aware of the following caveats when specifying your cluster_cidr and node-cidr-mask-size settings: Make sure you don't set your cluster_cidr to overlap with the default cluster service network of 10.43.0.0/16. That's why the examples above used 10.40.0.0/15 and 10.38.0.0/14. A CIDR of 10.42.0.0/15 will clash with the default cluster service CIDR. Make sure you don't set your cluster_cidr to overlap with IP address ranges already used in your enterprise infrastructure such as your node IPs, firewalls, load balancers, DNS, or other internal networks. Make sure your node-cidr-mask-size is large enough to accommodate the number of pods you want to run on each node. A size of 24 will give enough IP addresses for about 250 pods per node, which is well above the 110 maximum. However a size of 26 will only give you about 60 IPs, which is below the 110 maximum. If you plan to raise the default pod per node limit beyond 110, make sure sure your node-cidr-mask-size is large enough to support it. Note that pods that have hostNetwork: true do not count toward this total. Set it right the first time! Once your cluster has been deployed, these values cannot change. You'll need to decommission your cluster and start over again if you don't set it right. As of v1.17, Kubernetes supports clusters up to 5000 nodes. If you plan to go beyond this, you're venturing into unknown territory. For the latest large cluster best practices, see https://kubernetes.io/docs/setup/best-practices/cluster-large/ Setting these values can be done when first creating the cluster. You'll need to click on the Edit as YAML button and merge in the following YAML: rancher_kubernetes_engine_config: services: kube-controller: cluster_cidr: 10.40.0.0/15 extra_args: node-cidr-mask-size: 25 The above configuration should allow you to have about 120 pods per node and 1024 nodes in your cluster. That's over 100,000 pods, wow! Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to setup your network CIDR for a large cluster"},{"location":"000020167/#how-to-setup-your-network-cidr-for-a-large-cluster","text":"This document (000020167) is provided subject to the disclaimer at the end of this document.","title":"How to setup your network CIDR for a large cluster"},{"location":"000020167/#situation","text":"","title":"Situation"},{"location":"000020167/#task","text":"If you are expecting to use Rancher to deploy a Kubernetes cluster with more than 256 nodes, you'll need to make sure you adjust the default cluster CIDR settings. The default settings only allows clusters of 256 nodes or less.","title":"Task"},{"location":"000020167/#requirements","text":"Rancher v2.x A lot of hardware or VMs!","title":"Requirements"},{"location":"000020167/#background","text":"Kubernetes provides each pod with an IP address and each node with a block of IP addresses. Each cluster is also provided a block of IP addresses that is distributed to each node. This is controlled by two settings, the cluster_cidr block and node-cidr-mask-size . By default, the cluster_cidr block is 10.42.0.0/16 and the node-cidr-mask-size is 24. This gives the cluster 256 blocks of /24 networks to distribute out to the pool of nodes. For example, node1 will get 10.24.0.0/24, node2 will get 10.42.1.0/24, node3 will get 10.42.2.0/24 and so on.","title":"Background"},{"location":"000020167/#solution","text":"To support more than 256 nodes, you will need to use a larger cluster_cidr block, a smaller node-cidr-mask-size, or adjust both. For example, if you want to support up to 512 nodes you can set: cluster_cidr to 10.40.0.0/15 node-cidr-mask-size to 24 OR cluster_cidr to 10.42.0.0/16 node-cidr-mask-size to 25 To support up to 1024 nodes, you can use a larger cluster_cidr , smaller node-cidr-mask-size , or combination of both: cluster_cidr to 10.38.0.0/14 node-cidr-mask-size to 24 OR cluster_cidr to 10.42.0.0/16 node-cidr-mask-size to 26 OR cluster_cidr to 10.40.0.0/15 node-cidr-mask-size to 25 You should be aware of the following caveats when specifying your cluster_cidr and node-cidr-mask-size settings: Make sure you don't set your cluster_cidr to overlap with the default cluster service network of 10.43.0.0/16. That's why the examples above used 10.40.0.0/15 and 10.38.0.0/14. A CIDR of 10.42.0.0/15 will clash with the default cluster service CIDR. Make sure you don't set your cluster_cidr to overlap with IP address ranges already used in your enterprise infrastructure such as your node IPs, firewalls, load balancers, DNS, or other internal networks. Make sure your node-cidr-mask-size is large enough to accommodate the number of pods you want to run on each node. A size of 24 will give enough IP addresses for about 250 pods per node, which is well above the 110 maximum. However a size of 26 will only give you about 60 IPs, which is below the 110 maximum. If you plan to raise the default pod per node limit beyond 110, make sure sure your node-cidr-mask-size is large enough to support it. Note that pods that have hostNetwork: true do not count toward this total. Set it right the first time! Once your cluster has been deployed, these values cannot change. You'll need to decommission your cluster and start over again if you don't set it right. As of v1.17, Kubernetes supports clusters up to 5000 nodes. If you plan to go beyond this, you're venturing into unknown territory. For the latest large cluster best practices, see https://kubernetes.io/docs/setup/best-practices/cluster-large/ Setting these values can be done when first creating the cluster. You'll need to click on the Edit as YAML button and merge in the following YAML: rancher_kubernetes_engine_config: services: kube-controller: cluster_cidr: 10.40.0.0/15 extra_args: node-cidr-mask-size: 25 The above configuration should allow you to have about 120 pods per node and 1024 nodes in your cluster. That's over 100,000 pods, wow!","title":"Solution"},{"location":"000020167/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020168/","text":"Update self signed certificate on single install of Rancher 2.x This document (000020168) is provided subject to the disclaimer at the end of this document. Situation Task Update/renew self signed certificates to ten year expiration on Single Server Install of Rancher 2.x Pre-requisites rancher-single-tool Resolution Download Rancher single tool on the server that is running your Rancher container: curl -LO https://github.com/patrick0057/rancher-single-tool/raw/master/rancher-single-tool.sh Run script so that it upgrades your installation (you can upgrade to the same version) and pass flags to indicate that you want to regenerate your self signed certificate. The most reliable way is to just specify all of your options on the command line but the script does have an easy to use automated system as well as shown in option b. a. Specify all flags on command line, including any rancher options you had and docker options. Option -s is required for generating new 10 year self signed SSL certificates. bash rancher-single-tool.sh -f -c'<container_id>' -t'upgrade' -v'<rancher_version>' -d'<docker_options>' -r'<rancher_options>' -s'<self_signed_ssl_hostname>' For example: bash rancher-single-tool.sh -f -c'984f2fe62f6a' -t'upgrade' -v'v2.2.4' -d'-d --restart=unless-stopped -p 80:80 -p 443:443' -r'none' -s'company.domain.com' b. Let the script prompt you for answers and autodetect docker and rancher options when asked to. bash rancher-single-tool.sh -s'<self_signed_ssl_hostname>' For example: bash rancher-single-tool.sh -s'company.domain.com' In order to see the new SSL you need to completely quit your browser and start it back up, otherwise it might still show you the old certificate. Alternatively you can consistently check this using openssl instead of using your browser. openssl s_client -connect company.domain.com:443 | openssl x509 -noout -text -startdate -enddate If you have any downstream clusters attached to this Rancher installation you will need to update their Rancher agent deployment which will be covered in https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Update self signed certificate on single install of Rancher 2.x"},{"location":"000020168/#update-self-signed-certificate-on-single-install-of-rancher-2x","text":"This document (000020168) is provided subject to the disclaimer at the end of this document.","title":"Update self signed certificate on single install of Rancher 2.x"},{"location":"000020168/#situation","text":"","title":"Situation"},{"location":"000020168/#task","text":"Update/renew self signed certificates to ten year expiration on Single Server Install of Rancher 2.x","title":"Task"},{"location":"000020168/#pre-requisites","text":"rancher-single-tool","title":"Pre-requisites"},{"location":"000020168/#resolution","text":"Download Rancher single tool on the server that is running your Rancher container: curl -LO https://github.com/patrick0057/rancher-single-tool/raw/master/rancher-single-tool.sh Run script so that it upgrades your installation (you can upgrade to the same version) and pass flags to indicate that you want to regenerate your self signed certificate. The most reliable way is to just specify all of your options on the command line but the script does have an easy to use automated system as well as shown in option b. a. Specify all flags on command line, including any rancher options you had and docker options. Option -s is required for generating new 10 year self signed SSL certificates. bash rancher-single-tool.sh -f -c'<container_id>' -t'upgrade' -v'<rancher_version>' -d'<docker_options>' -r'<rancher_options>' -s'<self_signed_ssl_hostname>' For example: bash rancher-single-tool.sh -f -c'984f2fe62f6a' -t'upgrade' -v'v2.2.4' -d'-d --restart=unless-stopped -p 80:80 -p 443:443' -r'none' -s'company.domain.com' b. Let the script prompt you for answers and autodetect docker and rancher options when asked to. bash rancher-single-tool.sh -s'<self_signed_ssl_hostname>' For example: bash rancher-single-tool.sh -s'company.domain.com' In order to see the new SSL you need to completely quit your browser and start it back up, otherwise it might still show you the old certificate. Alternatively you can consistently check this using openssl instead of using your browser. openssl s_client -connect company.domain.com:443 | openssl x509 -noout -text -startdate -enddate If you have any downstream clusters attached to this Rancher installation you will need to update their Rancher agent deployment which will be covered in https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool","title":"Resolution"},{"location":"000020168/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020169/","text":"How to troubleshoot IPsec stability issues in a Rancher v1.6 cluster This document (000020169) is provided subject to the disclaimer at the end of this document. Situation Troubleshooting If you are experiencing issues with containers communicating to each other in your Rancher 1.6 environment, your ipsec might be having some issues. In this article I will go over common troubleshooting steps and procedures to correct the problem. exec into one of your ipsec-router containers and run the following ipsec test for i in `curl -s rancher-metadata/latest/self/service/containers/| cut -f1 -d=` ; do ping -c2 `curl -s curl rancher-metadata/latest/self/service/containers/$i/primary_ip` ; done If all containers or a majority are not responding then there is likely an issue with ipsec that needs to be addressed. Usually when there are ipsec issues, it is because metadata is having issues getting in sync. To confirm this, check your metadata logs (Infrastructure stacks> network-services> metadata>) and look at the \"Download and reload in\" time. If it is hovering around 10 seconds or greater then this is most likely your problem. We generally want this value to be 1-2 seconds. Below is a sample of what this looks like. Information The metadata container is a database that runs on every host in an environment. Infrastructure containers on each host rely on their local metadata database for information that allows them to run correctly. The data that is retrieved by metadata is serialized, so if it detects that it is out of date it will grab the data again until it is in sync. On a system that downloads and reloads in 10 seconds, the metadata container will be stuck in a perpetual loop of not having the correct data. This will result in infrastructure containers on that host to not work as expected. Repair IPsec usually has issues when there are more than 50 hosts in an environment. Rancher's official recommendation is that you have no more than 50 hosts in an environment. If you need more, we recommend scaling your hosts vertically or creating a separate environment. If you are still having issues or cannot for some reason scale down your environment right away then you can try increasing the CPU allowance to the metadata stack. To check metadata CPU usage, we need to go to infrastructure stacks then click on network-services. In network-services click \"Up to date\" in the top right corner. Then select the latest template version in the drop down menu to reveal the settings. You should see settings similar to the screenshot below. The number on the left is the CPU Period which indicates a number that represents a full CPU core. The number on the right is the CPU quota which indicates how much CPU we want to allow metadata to use. By default we only allow metadata to use 1/2 of a core. In larger environments you can increase this value to correct ipsec issues. To increase 1/2 core to 2 cores for example, you could change the above CPU Quota number from 200000 to 800000. Once you save changes the containers will go through a rolling upgrade, this can take a while depending on how overloaded your environment is and how many hosts are in it. Once the rolling update is complete, test your ipsec connectivity again to ensure that it is working as expected. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to troubleshoot IPsec stability issues in a Rancher v1.6 cluster"},{"location":"000020169/#how-to-troubleshoot-ipsec-stability-issues-in-a-rancher-v16-cluster","text":"This document (000020169) is provided subject to the disclaimer at the end of this document.","title":"How to troubleshoot IPsec stability issues in a Rancher v1.6 cluster"},{"location":"000020169/#situation","text":"","title":"Situation"},{"location":"000020169/#troubleshooting","text":"If you are experiencing issues with containers communicating to each other in your Rancher 1.6 environment, your ipsec might be having some issues. In this article I will go over common troubleshooting steps and procedures to correct the problem. exec into one of your ipsec-router containers and run the following ipsec test for i in `curl -s rancher-metadata/latest/self/service/containers/| cut -f1 -d=` ; do ping -c2 `curl -s curl rancher-metadata/latest/self/service/containers/$i/primary_ip` ; done If all containers or a majority are not responding then there is likely an issue with ipsec that needs to be addressed. Usually when there are ipsec issues, it is because metadata is having issues getting in sync. To confirm this, check your metadata logs (Infrastructure stacks> network-services> metadata>) and look at the \"Download and reload in\" time. If it is hovering around 10 seconds or greater then this is most likely your problem. We generally want this value to be 1-2 seconds. Below is a sample of what this looks like.","title":"Troubleshooting"},{"location":"000020169/#information","text":"The metadata container is a database that runs on every host in an environment. Infrastructure containers on each host rely on their local metadata database for information that allows them to run correctly. The data that is retrieved by metadata is serialized, so if it detects that it is out of date it will grab the data again until it is in sync. On a system that downloads and reloads in 10 seconds, the metadata container will be stuck in a perpetual loop of not having the correct data. This will result in infrastructure containers on that host to not work as expected.","title":"Information"},{"location":"000020169/#repair","text":"IPsec usually has issues when there are more than 50 hosts in an environment. Rancher's official recommendation is that you have no more than 50 hosts in an environment. If you need more, we recommend scaling your hosts vertically or creating a separate environment. If you are still having issues or cannot for some reason scale down your environment right away then you can try increasing the CPU allowance to the metadata stack. To check metadata CPU usage, we need to go to infrastructure stacks then click on network-services. In network-services click \"Up to date\" in the top right corner. Then select the latest template version in the drop down menu to reveal the settings. You should see settings similar to the screenshot below. The number on the left is the CPU Period which indicates a number that represents a full CPU core. The number on the right is the CPU quota which indicates how much CPU we want to allow metadata to use. By default we only allow metadata to use 1/2 of a core. In larger environments you can increase this value to correct ipsec issues. To increase 1/2 core to 2 cores for example, you could change the above CPU Quota number from 200000 to 800000. Once you save changes the containers will go through a rolling upgrade, this can take a while depending on how overloaded your environment is and how many hosts are in it. Once the rolling update is complete, test your ipsec connectivity again to ensure that it is working as expected.","title":"Repair"},{"location":"000020169/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020170/","text":"How to upgrade Docker using Rancher's install script This document (000020170) is provided subject to the disclaimer at the end of this document. Situation Task Rancher provides quick scripts for installing Docker, which are available for the most recent versions of Docker https://rancher.com/docs/rancher/v2.x/en/installation/requirements/installing-docker/ Upgrading Docker on your machine using these scripts is equally as simple Pre-requisites A supported node with a version of Docker needing to be upgraded Curl or Wget installed Resolution Just run the script with the version number you are trying to upgrade to. Let's say you're running 18.09 and want to upgrade to 19.03. Simply provide the version number as the name of the script to run. For example: curl https://releases.rancher.com/install-docker/19.03.sh | sh or wget -O- https://releases.rancher.com/install-docker/19.03.sh | sh This will throw a warning that Docker is already installed, stop the running Docker engine, and upgrade your version. Note that restarting Docker will also stop any running container or workloads running on this host. This procedure does not apply to RancherOS Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to upgrade Docker using Rancher's install script"},{"location":"000020170/#how-to-upgrade-docker-using-ranchers-install-script","text":"This document (000020170) is provided subject to the disclaimer at the end of this document.","title":"How to upgrade Docker using Rancher's install script"},{"location":"000020170/#situation","text":"","title":"Situation"},{"location":"000020170/#task","text":"Rancher provides quick scripts for installing Docker, which are available for the most recent versions of Docker https://rancher.com/docs/rancher/v2.x/en/installation/requirements/installing-docker/ Upgrading Docker on your machine using these scripts is equally as simple","title":"Task"},{"location":"000020170/#pre-requisites","text":"A supported node with a version of Docker needing to be upgraded Curl or Wget installed","title":"Pre-requisites"},{"location":"000020170/#resolution","text":"Just run the script with the version number you are trying to upgrade to. Let's say you're running 18.09 and want to upgrade to 19.03. Simply provide the version number as the name of the script to run. For example: curl https://releases.rancher.com/install-docker/19.03.sh | sh or wget -O- https://releases.rancher.com/install-docker/19.03.sh | sh This will throw a warning that Docker is already installed, stop the running Docker engine, and upgrade your version. Note that restarting Docker will also stop any running container or workloads running on this host. This procedure does not apply to RancherOS","title":"Resolution"},{"location":"000020170/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020171/","text":"Information to provide when logging a support case This document (000020171) is provided subject to the disclaimer at the end of this document. Situation Context To allow us to provide quick and efficient support, we ask that customers provide as much information as possible when logging a ticket. Below are some things that we find we typically need from customers when diagnosing. Assumptions You have ssh access to the affected nodes and access to the Rancher UI Information to provide When did you first notice the issue? Is the issue related to Rancher or a downstream cluster managed by Rancher? What is the version of Rancher server and the affected cluster? The Rancher version can be found at the bottom left of the Rancher UI or by inspecting the image version of the rancher container. The k8s version can be found by navigating to the relevant cluster in the UI and looking for the Kubernetes Version string or can be retrieved with RKE: rke version --config <path to cluster.yml> If the issue is related to an upgrade of Rancher or k8s, what versions did you come from? If the issue is related to a downstream cluster, how was the cluster built? rke, hosted provider, imported or custom? Are there any Github issues or previous support tickets you think are related? Are there any events that you think may correlate with the issue? e.g., infrastructure outage, host reboot, os upgrade, docker upgrade, network changes, configuration changes? Have you taken any corrective action and if so, what? Logs to assist in debugging: System logs are almost always required when diagnosing an issue, you can generate these using the Rancher log collector scripts . For some issues it can help to capture traffic from the browser to the Rancher UI, as per the process found here . If any of the files above are too big to be uploaded to Zendesk, we can provide an alternate location Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Information to provide when logging a support case"},{"location":"000020171/#information-to-provide-when-logging-a-support-case","text":"This document (000020171) is provided subject to the disclaimer at the end of this document.","title":"Information to provide when logging a support case"},{"location":"000020171/#situation","text":"","title":"Situation"},{"location":"000020171/#context","text":"To allow us to provide quick and efficient support, we ask that customers provide as much information as possible when logging a ticket. Below are some things that we find we typically need from customers when diagnosing.","title":"Context"},{"location":"000020171/#assumptions","text":"You have ssh access to the affected nodes and access to the Rancher UI","title":"Assumptions"},{"location":"000020171/#information-to-provide","text":"When did you first notice the issue? Is the issue related to Rancher or a downstream cluster managed by Rancher? What is the version of Rancher server and the affected cluster? The Rancher version can be found at the bottom left of the Rancher UI or by inspecting the image version of the rancher container. The k8s version can be found by navigating to the relevant cluster in the UI and looking for the Kubernetes Version string or can be retrieved with RKE: rke version --config <path to cluster.yml> If the issue is related to an upgrade of Rancher or k8s, what versions did you come from? If the issue is related to a downstream cluster, how was the cluster built? rke, hosted provider, imported or custom? Are there any Github issues or previous support tickets you think are related? Are there any events that you think may correlate with the issue? e.g., infrastructure outage, host reboot, os upgrade, docker upgrade, network changes, configuration changes? Have you taken any corrective action and if so, what? Logs to assist in debugging: System logs are almost always required when diagnosing an issue, you can generate these using the Rancher log collector scripts . For some issues it can help to capture traffic from the browser to the Rancher UI, as per the process found here . If any of the files above are too big to be uploaded to Zendesk, we can provide an alternate location","title":"Information to provide"},{"location":"000020171/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020172/","text":"How to setup Rancher 2.x with Active Directory external authentication This document (000020172) is provided subject to the disclaimer at the end of this document. Situation Overview and Intention This is a quick guide aiming to get Rancher v2.x using external authentication via Active Directory with the least amount of effort. Of course there is much more to consider and configure in a production enterprise environment. For more detail on this function please refer to this Rancher article and fine tune as required. Configuring Active Directory Pre-requisites A running instance of Rancher v2.x, either a single node instance or High Availability (HA) cluster . Local account to log onto the Rancher Server (usually admin) A Windows Server running Active Directory Name of the domain you wish to join A restricted account that Rancher can use to bind and query the Directory with (Security Recommendations at the bottom of article) A standard user account that will be used to test and enable the authentication (i.e your domain account) Knowledge of where the users are in the Active Directory OU (Organisational Unit) structure Network connectivity from the Rancher worker nodes to the Active Directory Servers (There is probably more than one, run nslookup on the domain name) Also good to test ports 389 or 636 (TLS) as these need to be allowed Steps on How To Get Rancher Talking to AD Quickly (Tested with AD running Windows Server 2016/2019) In this example I have used the below examples (yours will be different): my domain is 'rancher.local' All or my users are located under the Users OU in AD my bind account is 'svc-rancher' For more detail refer to Configuring Active Directory : Log into the Rancher UI using the initial local admin account. From the Global view, navigate to Security > Authentication Select Active Directory. The Configure an AD server form will be displayed. Add in the Hostname or IP address into the Hostname field Add 'rancher/svc-rancher' to the Service Account Username field Add 'cn=users,dc=rancher,dc=local' to the User Search Base Goto Section 3 add your domain account username and password Click 'Authenticate with Active Directory' Security tips and Best Practices WARNING: Once enabled all users in the Search base will be able to log into Rancher. Once auth is configured in Rancher change the relaxed default setting from 'Allow any valid Users' to login to 'only allow members of Cluster, Projects' to login. Access must now be specified instead of allowing any User onto the cluster. Under 'Global, Security, Roles' It is best to drop 'New User Default' setting from 'User' to 'User Base' which provide less privleges to new users and must increased as required not as a default. The bind account is critical for ongoing authentication so locking the account will break functionality. If this account gets locked or the password changes your AD authentication will be broken. Setting the account and the password not to expire and removing lockout policies prevent disruption. Remove interactive logon abilites as this account doesn't need to logon to a server and control it Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to setup Rancher 2.x with Active Directory external authentication"},{"location":"000020172/#how-to-setup-rancher-2x-with-active-directory-external-authentication","text":"This document (000020172) is provided subject to the disclaimer at the end of this document.","title":"How to setup Rancher 2.x with Active Directory external authentication"},{"location":"000020172/#situation","text":"","title":"Situation"},{"location":"000020172/#overview-and-intention","text":"This is a quick guide aiming to get Rancher v2.x using external authentication via Active Directory with the least amount of effort. Of course there is much more to consider and configure in a production enterprise environment. For more detail on this function please refer to this Rancher article and fine tune as required. Configuring Active Directory","title":"Overview and Intention"},{"location":"000020172/#pre-requisites","text":"A running instance of Rancher v2.x, either a single node instance or High Availability (HA) cluster . Local account to log onto the Rancher Server (usually admin) A Windows Server running Active Directory Name of the domain you wish to join A restricted account that Rancher can use to bind and query the Directory with (Security Recommendations at the bottom of article) A standard user account that will be used to test and enable the authentication (i.e your domain account) Knowledge of where the users are in the Active Directory OU (Organisational Unit) structure Network connectivity from the Rancher worker nodes to the Active Directory Servers (There is probably more than one, run nslookup on the domain name) Also good to test ports 389 or 636 (TLS) as these need to be allowed","title":"Pre-requisites"},{"location":"000020172/#steps-on-how-to-get-rancher-talking-to-ad-quickly","text":"","title":"Steps on How To Get Rancher Talking to AD Quickly"},{"location":"000020172/#tested-with-ad-running-windows-server-20162019","text":"In this example I have used the below examples (yours will be different): my domain is 'rancher.local' All or my users are located under the Users OU in AD my bind account is 'svc-rancher' For more detail refer to Configuring Active Directory : Log into the Rancher UI using the initial local admin account. From the Global view, navigate to Security > Authentication Select Active Directory. The Configure an AD server form will be displayed. Add in the Hostname or IP address into the Hostname field Add 'rancher/svc-rancher' to the Service Account Username field Add 'cn=users,dc=rancher,dc=local' to the User Search Base Goto Section 3 add your domain account username and password Click 'Authenticate with Active Directory'","title":"(Tested with AD running Windows Server 2016/2019)"},{"location":"000020172/#security-tips-and-best-practices","text":"WARNING: Once enabled all users in the Search base will be able to log into Rancher. Once auth is configured in Rancher change the relaxed default setting from 'Allow any valid Users' to login to 'only allow members of Cluster, Projects' to login. Access must now be specified instead of allowing any User onto the cluster. Under 'Global, Security, Roles' It is best to drop 'New User Default' setting from 'User' to 'User Base' which provide less privleges to new users and must increased as required not as a default. The bind account is critical for ongoing authentication so locking the account will break functionality. If this account gets locked or the password changes your AD authentication will be broken. Setting the account and the password not to expire and removing lockout policies prevent disruption. Remove interactive logon abilites as this account doesn't need to logon to a server and control it","title":"Security tips and Best Practices"},{"location":"000020172/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020173/","text":"How to change Rancher 2.x server-url This document (000020173) is provided subject to the disclaimer at the end of this document. Situation Task Changing the server URL on Rancher 2.x. Pre-requisites rancher-single-tool for Single Server Rancher Installations cluster-agent-tool for both HA and Single Server Rancher Installtions Resolution Single Server Installation During this tutorial it is recommended to use the rancher-single-tool for Rancher single server installations. It isn't required but it makes the process much easier. As a result this guide will be based on using that tool. Download the rancher-single-tool to the node that is running your rancher server container. curl -LO https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh wget https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh Backup your Rancher installation. bash rancher-single-tool.sh -t'backup' Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url. Now we need to upgrade your Rancher container to reflect new certs. This is required in most cases with the exception of already using a wildcard that also encompasses the new server-url. a. To generate a new self signed certificate for your new URL use the following upgrade command. Follow the prompts to finish the upgrade. bash rancher-single-tool.sh -t'upgrade' -s'newhostname.company.com' b. To generate a new Let's Encrypt certificate you will need to change the Rancher server options to reflect this. You could do this with the following command. bash rancher-single-tool.sh -t'upgrade' -r'--acme-domain newhostname.company.com' c. If you were using certificates signed by a recognized CA before and just need to replace them, you should modify the docker options to reflect this change. Keep in mind that if you just replaced the cert files on the host path and the filenames didn't change, you can just restart the docker container. However if the filenames did change, I'm providing the example below of how you would do upgrade the container to see this change. bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem' d. If you were using certificates signed by a private CA or you want to use your own self signed certifiactes (certificates not created by rancher-single-tool option -s). Below is an example of how you would do that. The same rule applies from option c. If the filenames have not changed you don't need to upgrade, you can just restart the container. bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem --volume=/etc/rancherssl/certs/ca.pem:/etc/rancher/ssl/cacerts.pem' Once your Rancher container is backup and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool HA Installation Ensure that you have current etcd backups for your local rancher cluster. Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url. Log into a box where you have helm and kubectl installed. You will need your local Rancher cluster kubeconfig, ensure that it is set to the default config by either placing it in ~/.kube/config or by setting your KUBECONFIG environment variable. Check current helm chart options: helm get values rancher -n cattle-system hostname: rancher.company.com rancherImageTag: v2.3.5 Craft an upgrade command based on the values provided in the previous step and then modify the hostname to match the new server hostname/url. helm upgrade rancher-stable/rancher --name rancher --namespace cattle-system --set hostname=newrancher.company.com --set rancherImageTag=v2.3.5 Run the upgrade command then wait for rollout to complete. kubectl -n cattle-system rollout status deploy/rancher Once your Rancher deployment is back up and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. You also need to login to one of the controlplane nodes of your local Rancher cluster and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to change Rancher 2.x server-url"},{"location":"000020173/#how-to-change-rancher-2x-server-url","text":"This document (000020173) is provided subject to the disclaimer at the end of this document.","title":"How to change Rancher 2.x server-url"},{"location":"000020173/#situation","text":"","title":"Situation"},{"location":"000020173/#task","text":"Changing the server URL on Rancher 2.x.","title":"Task"},{"location":"000020173/#pre-requisites","text":"rancher-single-tool for Single Server Rancher Installations cluster-agent-tool for both HA and Single Server Rancher Installtions","title":"Pre-requisites"},{"location":"000020173/#resolution","text":"","title":"Resolution"},{"location":"000020173/#single-server-installation","text":"During this tutorial it is recommended to use the rancher-single-tool for Rancher single server installations. It isn't required but it makes the process much easier. As a result this guide will be based on using that tool. Download the rancher-single-tool to the node that is running your rancher server container. curl -LO https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh wget https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh Backup your Rancher installation. bash rancher-single-tool.sh -t'backup' Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url. Now we need to upgrade your Rancher container to reflect new certs. This is required in most cases with the exception of already using a wildcard that also encompasses the new server-url. a. To generate a new self signed certificate for your new URL use the following upgrade command. Follow the prompts to finish the upgrade. bash rancher-single-tool.sh -t'upgrade' -s'newhostname.company.com' b. To generate a new Let's Encrypt certificate you will need to change the Rancher server options to reflect this. You could do this with the following command. bash rancher-single-tool.sh -t'upgrade' -r'--acme-domain newhostname.company.com' c. If you were using certificates signed by a recognized CA before and just need to replace them, you should modify the docker options to reflect this change. Keep in mind that if you just replaced the cert files on the host path and the filenames didn't change, you can just restart the docker container. However if the filenames did change, I'm providing the example below of how you would do upgrade the container to see this change. bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem' d. If you were using certificates signed by a private CA or you want to use your own self signed certifiactes (certificates not created by rancher-single-tool option -s). Below is an example of how you would do that. The same rule applies from option c. If the filenames have not changed you don't need to upgrade, you can just restart the container. bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem --volume=/etc/rancherssl/certs/ca.pem:/etc/rancher/ssl/cacerts.pem' Once your Rancher container is backup and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool","title":"Single Server Installation"},{"location":"000020173/#ha-installation","text":"Ensure that you have current etcd backups for your local rancher cluster. Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url. Log into a box where you have helm and kubectl installed. You will need your local Rancher cluster kubeconfig, ensure that it is set to the default config by either placing it in ~/.kube/config or by setting your KUBECONFIG environment variable. Check current helm chart options: helm get values rancher -n cattle-system hostname: rancher.company.com rancherImageTag: v2.3.5 Craft an upgrade command based on the values provided in the previous step and then modify the hostname to match the new server hostname/url. helm upgrade rancher-stable/rancher --name rancher --namespace cattle-system --set hostname=newrancher.company.com --set rancherImageTag=v2.3.5 Run the upgrade command then wait for rollout to complete. kubectl -n cattle-system rollout status deploy/rancher Once your Rancher deployment is back up and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. You also need to login to one of the controlplane nodes of your local Rancher cluster and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool","title":"HA Installation"},{"location":"000020173/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020174/","text":"How to setup Nodelocal DNS cache on Rancher 2.x This document (000020174) is provided subject to the disclaimer at the end of this document. Situation Why use Nodelocal DNS cache? Like many applications in a containerised architecture, CoreDNS or kube-dns runs in a distributed fashion. In certain circumstances, DNS reliability and latency can be impacted with this approach. The causes of this relate notably to conntrack race conditions or exhaustion, cloud provider limits, and the unreliable nature of the UDP protocol. A number of workarounds exist, however long term mitigation of these and other issues has resulted in a redesign of the Kubernetes DNS architecture, and the result being the Nodelocal DNS cache project . Requirements A Kubernetes cluster of v1.15 or greater created by Rancher v2.x or RKE A Linux cluster, Windows is currently not supported Access to the cluster Resolution Installing There are two installation approaches, both approaches should be non-invasive, pods that are currently running will not be modified. The DNS configuration will take effect for pods started after the install is complete. RKE1: Using a Rancher version after v2.4.x, or RKE version after v1.1.0 Update the cluster using 'Edit as YAML' in the Rancher UI. With RKE, edit the cluster.yaml file instead. Note : Updating the cluster using the below will create the node-local-dns Daemonset, and restart the kubelet container on each node. As in the documentation , update or add the dns.nodelocal.ip_address field using the following as an example: dns: [..] nodelocal: ip_address: \"169.254.20.10\" New pods created after the change will configure the node-local-dns link-local address as the nameserver in /etc/resolv.conf . Note : No further action is needed to use node-local-dns (as in the option A/B below), the changes to /etc/resolv.conf will take effect for pods started from this point onwards. RKE1: Using a Rancher version before v2.4.x, or RKE version before v1.1.0 Installing the YAML manifest by navigating to the cluster, and clicking the Launch kubectl button in the Rancher UI. This command can also be run from a terminal where a kubeconfig for the cluster is currently configured. Environment variables are replaced before applying the manifest, one assumption is that the cluster service discovery domain name is cluster.local (default), adjust the command if needed. curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml \\ | sed -e 's/__PILLAR__DNS__DOMAIN__/cluster.local/g' \\ | sed -e \"s/__PILLAR__DNS__SERVER__/$(kubectl get service --namespace kube-system kube-dns -o jsonpath='{.spec.clusterIP}')/g\" \\ | sed -e 's/__PILLAR__LOCAL__DNS__/169.254.20.10/g' \\ | kubectl apply -f - Ensure the node-local-dns pods start successfully, a pod should start on each control plane and worker node. kubectl get -n kube-system pod -l k8s-app=node-local-dns When deploying the YAML manifest there are two options to configure the cluster to use the new node-local-dns configuration, please choose from option A or B below. Option A - Configure the Kubelet By default, the Kubelet will configure the /etc/resolv.conf of pods with the kube-dns Service ClusterIP as the nameserver. Configuring all new pods to query node-local-dns will require updating the Kubelet arguments. Note : Updating the arguments using the below will restart the kubelet container on each node. If the cluster was provisioned by Rancher, edit the cluster in the UI and click on Edit as YAML . If the cluster was provisioned by RKE, edit the cluster.yml file directly. Update the kubelet service with the cluster-dns argument and IP Address. Click save, or run an rke up to put this change into effect. services: kubelet: extra_args: cluster-dns: \"169.254.20.10\" New pods created after the change will configure the node-local-dns link-local address as the nameserver in /etc/resolv.conf . Option B - Configure Workloads Alternatively, node-local-dns can be configured on a per-workload basis by updating the workload with a dnsConfig and dnsConfig . If using the Rancher UI, edit the workload, navigate to Show advanced options > Networking > DNS Nameservers and add 169.254.20.10 . Additionally, adjust the DNS Policy to None . If configuring by YAML, patch in the following to the pod spec to adjust the dnsPolicy and dnsConfig : spec: dnsPolicy: \"None\" dnsConfig: nameservers: - 169.254.20.10 RKE2: Using any RKE2 Kubernetes version Update the default HelmChart for CoreDNS, the nodelocal.enabled: true value will install node-local-dns in the cluster. Please see the documentation here for more details. Testing Once installed, start a new pod to test DNS queries. kubectl run --restart=Never --rm -it --image=tutum/dnsutils dns-test -- dig google.com Unless Option B was used to install node-local-dns, you should expect to see 169.254.20.10 as the server, and a successful answer to the query. To verify a pod or container is using node-local-dns by checking the /etc/resolv.conf file, for example: kubectl exec -it <pod name> -- grep nameserver /etc/resolv.conf nameserver 169.254.20.10 Removing Nodelocal DNS cache To remove from a cluster, the reverse steps are needed. Note : Pods created with the node-local-dns nameserver in /etc/resolv.conf will need to be started again to use the kube-dns service as a nameserver again. Using a Rancher version after v2.4.x, or RKE version after v1.1.0 Remove the dns.nodelocal configuration from the cluster YAML Using a Rancher version before v2.4.x, or RKE version before v1.1.0 Remove the Kubelet configuration (Option A), or remove the dnsConfig from workloads (Option B). If Option A was taken, delete any pods in workloads that were started since the Kubelet configuration change so that they are started with the kube-dns ClusterIP again. Remove the node-local-dns objects with the following command: curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml | kubectl delete -f - Note : it is important to perform these steps in order, and only complete step 3 once the pods using node-local-dns have been started with the kube-dns ClusterIP configured in /etc/resolv.conf again. Additional Information Troubleshooting Node-local-dns will perform external lookups on behalf of pods, this lookup occurs from the node-local-dns DaemonSet pod running on the same node as the pod. For internal lookups, CoreDNS will be used, node-local-dns will cache successful queries (30s), and negative queries (5s) by default. For an architecture overview please see the diagram here . In no specific order, the following can help understand a DNS issue further. Check all kube-dns and node-local-dns objects Ensure there are no obvious issues with scheduling CoreDNS and node-local-dns pods in the cluster. kubectl get all -n kube-system -l k8s-app=node-local-dns kubectl get all -n kube-system -l k8s-app=kube-dns All node-local-dns and kube-dns pods should be ready and running, the kube-dns Service should exist. Check the events if needed to locate any warning or failed event messages. kubectl describe ds -n kube-system -l k8s-app=node-local-dns kubectl describe rs -n kube-system -l k8s-app=kube-dns Check the logs and ConfigMap of kube-dns and node-local-dns pods kubectl logs -n kube-system -l k8s-app=kube-dns kubectl logs -n kube-system -l k8s-app=node-local-dns kubectl get configmap -n kube-system coredns -o yaml kubectl get configmap -n kube-system node-local-dns -o yaml Enable logging and perform a DNS test Note, query logging can increase the log output from CoreDNS, enabling this temporarily while investigating is suggested. Enable query logging to understand the pattern from workloads Run a DaemonSet to perform queries from a pod running on each node in the cluster Ask questions to further eliminate the issue Is it only DNS that is affected, or is all connectivity affected? Are internal, external or all DNS queries failing? Are all nodes and workloads experiencing the issue, or a specific node or workload? * Nodes use the upstream DNS configured in /etc/resolv.conf , queries failing from a node could indicate the issue is with upstream DNS What is the error reported by applications? * If logs are aggregated, queries can be performed on the logs to identify timelines and impact Is the issue intermittent or constantly occuring? * If the issue is intermittent, configure monitoring or a loop to identify when the issue occurs, when it does - are internal, external or all queries affected? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to setup Nodelocal DNS cache on Rancher 2.x"},{"location":"000020174/#how-to-setup-nodelocal-dns-cache-on-rancher-2x","text":"This document (000020174) is provided subject to the disclaimer at the end of this document.","title":"How to setup Nodelocal DNS cache on Rancher 2.x"},{"location":"000020174/#situation","text":"","title":"Situation"},{"location":"000020174/#why-use-nodelocal-dns-cache","text":"Like many applications in a containerised architecture, CoreDNS or kube-dns runs in a distributed fashion. In certain circumstances, DNS reliability and latency can be impacted with this approach. The causes of this relate notably to conntrack race conditions or exhaustion, cloud provider limits, and the unreliable nature of the UDP protocol. A number of workarounds exist, however long term mitigation of these and other issues has resulted in a redesign of the Kubernetes DNS architecture, and the result being the Nodelocal DNS cache project .","title":"Why use Nodelocal DNS cache?"},{"location":"000020174/#requirements","text":"A Kubernetes cluster of v1.15 or greater created by Rancher v2.x or RKE A Linux cluster, Windows is currently not supported Access to the cluster","title":"Requirements"},{"location":"000020174/#resolution","text":"","title":"Resolution"},{"location":"000020174/#installing","text":"There are two installation approaches, both approaches should be non-invasive, pods that are currently running will not be modified. The DNS configuration will take effect for pods started after the install is complete.","title":"Installing"},{"location":"000020174/#rke1-using-a-rancher-version-after-v24x-or-rke-version-after-v110","text":"Update the cluster using 'Edit as YAML' in the Rancher UI. With RKE, edit the cluster.yaml file instead. Note : Updating the cluster using the below will create the node-local-dns Daemonset, and restart the kubelet container on each node. As in the documentation , update or add the dns.nodelocal.ip_address field using the following as an example: dns: [..] nodelocal: ip_address: \"169.254.20.10\" New pods created after the change will configure the node-local-dns link-local address as the nameserver in /etc/resolv.conf . Note : No further action is needed to use node-local-dns (as in the option A/B below), the changes to /etc/resolv.conf will take effect for pods started from this point onwards.","title":"RKE1: Using a Rancher version after v2.4.x, or RKE version after v1.1.0"},{"location":"000020174/#rke1-using-a-rancher-version-before-v24x-or-rke-version-before-v110","text":"Installing the YAML manifest by navigating to the cluster, and clicking the Launch kubectl button in the Rancher UI. This command can also be run from a terminal where a kubeconfig for the cluster is currently configured. Environment variables are replaced before applying the manifest, one assumption is that the cluster service discovery domain name is cluster.local (default), adjust the command if needed. curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml \\ | sed -e 's/__PILLAR__DNS__DOMAIN__/cluster.local/g' \\ | sed -e \"s/__PILLAR__DNS__SERVER__/$(kubectl get service --namespace kube-system kube-dns -o jsonpath='{.spec.clusterIP}')/g\" \\ | sed -e 's/__PILLAR__LOCAL__DNS__/169.254.20.10/g' \\ | kubectl apply -f - Ensure the node-local-dns pods start successfully, a pod should start on each control plane and worker node. kubectl get -n kube-system pod -l k8s-app=node-local-dns When deploying the YAML manifest there are two options to configure the cluster to use the new node-local-dns configuration, please choose from option A or B below.","title":"RKE1: Using a Rancher version before v2.4.x, or RKE version before v1.1.0"},{"location":"000020174/#option-a-configure-the-kubelet","text":"By default, the Kubelet will configure the /etc/resolv.conf of pods with the kube-dns Service ClusterIP as the nameserver. Configuring all new pods to query node-local-dns will require updating the Kubelet arguments. Note : Updating the arguments using the below will restart the kubelet container on each node. If the cluster was provisioned by Rancher, edit the cluster in the UI and click on Edit as YAML . If the cluster was provisioned by RKE, edit the cluster.yml file directly. Update the kubelet service with the cluster-dns argument and IP Address. Click save, or run an rke up to put this change into effect. services: kubelet: extra_args: cluster-dns: \"169.254.20.10\" New pods created after the change will configure the node-local-dns link-local address as the nameserver in /etc/resolv.conf .","title":"Option A - Configure the Kubelet"},{"location":"000020174/#option-b-configure-workloads","text":"Alternatively, node-local-dns can be configured on a per-workload basis by updating the workload with a dnsConfig and dnsConfig . If using the Rancher UI, edit the workload, navigate to Show advanced options > Networking > DNS Nameservers and add 169.254.20.10 . Additionally, adjust the DNS Policy to None . If configuring by YAML, patch in the following to the pod spec to adjust the dnsPolicy and dnsConfig : spec: dnsPolicy: \"None\" dnsConfig: nameservers: - 169.254.20.10","title":"Option B - Configure Workloads"},{"location":"000020174/#rke2-using-any-rke2-kubernetes-version","text":"Update the default HelmChart for CoreDNS, the nodelocal.enabled: true value will install node-local-dns in the cluster. Please see the documentation here for more details.","title":"RKE2: Using any RKE2 Kubernetes version"},{"location":"000020174/#testing","text":"Once installed, start a new pod to test DNS queries. kubectl run --restart=Never --rm -it --image=tutum/dnsutils dns-test -- dig google.com Unless Option B was used to install node-local-dns, you should expect to see 169.254.20.10 as the server, and a successful answer to the query. To verify a pod or container is using node-local-dns by checking the /etc/resolv.conf file, for example: kubectl exec -it <pod name> -- grep nameserver /etc/resolv.conf nameserver 169.254.20.10","title":"Testing"},{"location":"000020174/#removing-nodelocal-dns-cache","text":"To remove from a cluster, the reverse steps are needed. Note : Pods created with the node-local-dns nameserver in /etc/resolv.conf will need to be started again to use the kube-dns service as a nameserver again.","title":"Removing Nodelocal DNS cache"},{"location":"000020174/#using-a-rancher-version-after-v24x-or-rke-version-after-v110","text":"Remove the dns.nodelocal configuration from the cluster YAML","title":"Using a Rancher version after v2.4.x, or RKE version after v1.1.0"},{"location":"000020174/#using-a-rancher-version-before-v24x-or-rke-version-before-v110","text":"Remove the Kubelet configuration (Option A), or remove the dnsConfig from workloads (Option B). If Option A was taken, delete any pods in workloads that were started since the Kubelet configuration change so that they are started with the kube-dns ClusterIP again. Remove the node-local-dns objects with the following command: curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml | kubectl delete -f - Note : it is important to perform these steps in order, and only complete step 3 once the pods using node-local-dns have been started with the kube-dns ClusterIP configured in /etc/resolv.conf again.","title":"Using a Rancher version before v2.4.x, or RKE version before v1.1.0"},{"location":"000020174/#additional-information","text":"","title":"Additional Information"},{"location":"000020174/#troubleshooting","text":"Node-local-dns will perform external lookups on behalf of pods, this lookup occurs from the node-local-dns DaemonSet pod running on the same node as the pod. For internal lookups, CoreDNS will be used, node-local-dns will cache successful queries (30s), and negative queries (5s) by default. For an architecture overview please see the diagram here . In no specific order, the following can help understand a DNS issue further.","title":"Troubleshooting"},{"location":"000020174/#check-all-kube-dns-and-node-local-dns-objects","text":"Ensure there are no obvious issues with scheduling CoreDNS and node-local-dns pods in the cluster. kubectl get all -n kube-system -l k8s-app=node-local-dns kubectl get all -n kube-system -l k8s-app=kube-dns All node-local-dns and kube-dns pods should be ready and running, the kube-dns Service should exist. Check the events if needed to locate any warning or failed event messages. kubectl describe ds -n kube-system -l k8s-app=node-local-dns kubectl describe rs -n kube-system -l k8s-app=kube-dns","title":"Check all kube-dns and node-local-dns objects"},{"location":"000020174/#check-the-logs-and-configmap-of-kube-dns-and-node-local-dns-pods","text":"kubectl logs -n kube-system -l k8s-app=kube-dns kubectl logs -n kube-system -l k8s-app=node-local-dns kubectl get configmap -n kube-system coredns -o yaml kubectl get configmap -n kube-system node-local-dns -o yaml","title":"Check the logs and ConfigMap of kube-dns and node-local-dns pods"},{"location":"000020174/#enable-logging-and-perform-a-dns-test","text":"Note, query logging can increase the log output from CoreDNS, enabling this temporarily while investigating is suggested. Enable query logging to understand the pattern from workloads Run a DaemonSet to perform queries from a pod running on each node in the cluster","title":"Enable logging and perform a DNS test"},{"location":"000020174/#ask-questions-to-further-eliminate-the-issue","text":"Is it only DNS that is affected, or is all connectivity affected? Are internal, external or all DNS queries failing? Are all nodes and workloads experiencing the issue, or a specific node or workload? * Nodes use the upstream DNS configured in /etc/resolv.conf , queries failing from a node could indicate the issue is with upstream DNS What is the error reported by applications? * If logs are aggregated, queries can be performed on the logs to identify timelines and impact Is the issue intermittent or constantly occuring? * If the issue is intermittent, configure monitoring or a loop to identify when the issue occurs, when it does - are internal, external or all queries affected?","title":"Ask questions to further eliminate the issue"},{"location":"000020174/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020175/","text":"How to setup HAProxy for Rancher v2.x This document (000020175) is provided subject to the disclaimer at the end of this document. Situation Task Setup HAProxy as a frontend load balancer for Rancher v2.x. Overview Install HAProxy Ubuntu apt update apt install -y haproxy systemctl enable haproxy systemctl start haproxy CentOS / RedHat yum update yum install haproxy -y systemctl enable haproxy systemctl start haproxy Example HAProxy Config Option A - Full SSL Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/ Verify Rancher URL works when connecting directly to a Rancher node. For example: curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping Copy cert and key into a single file called /etc/haproxy/cert.pem Add frontend to /etc/haproxy/haproxy.cfg: frontend www-http bind *:80 reqadd X-Forwarded-Proto:\\ http default_backend rancher-http frontend www-https bind *:443 ssl crt /etc/haproxy/cert.pem reqadd X-Forwarded-Proto:\\ https default_backend rancher-https Add backends to /etc/haproxy/haproxy.cfg: backend rancher-http mode http option httpchk HEAD /healthz HTTP/1.0 server rancher01 192.168.1.103:80 check weight 1 maxconn 1024 server rancher02 192.168.1.104:80 check weight 1 maxconn 1024 server rancher03 192.168.1.105:80 check weight 1 maxconn 1024 backend rancher-https mode http option httpchk HEAD /healthz HTTP/1.0 server rancher01 192.168.1.103:443 check weight 1 maxconn 1024 ssl verify none server rancher02 192.168.1.104:443 check weight 1 maxconn 1024 ssl verify none server rancher03 192.168.1.105:443 check weight 1 maxconn 1024 ssl verify none Test the configuration: haproxy -f /etc/haproxy/haproxy.cfg -c Reload HAProxy: systemctl reload haproxy Example config Option B - External TLS Termination Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/options/chart-options/#external-tls-termination Verify Rancher URL works went connecting directly to a Rancher node. For example: curl --header \"Host: rancher.example.com\" http://192.168.1.103/ping Copy cert and key into a single file called /etc/haproxy/cert.pem Create frontends: frontend www-http bind *:80 reqadd X-Forwarded-Proto:\\ http default_backend rancher-http frontend www-https bind *:443 ssl crt /etc/haproxy/cert.pem reqadd X-Forwarded-Proto:\\ https default_backend rancher-http Create backends: backend rancher-http mode http option httpchk HEAD /healthz HTTP/1.0 server rancher01 192.168.1.103:80 check weight 1 maxconn 1024 server rancher02 192.168.1.104:80 check weight 1 maxconn 1024 server rancher03 192.168.1.105:80 check weight 1 maxconn 1024 Test the configuration: haproxy -f /etc/haproxy/haproxy.cfg -c Reload HAProxy: systemctl reload haproxy Example config Option C - TCP pass-through Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/ Verify Rancher URL works when connecting directly to a Rancher node. For example: curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping NOTE: The default gateway for all 3 Rancher nodes must be the load balancer. Doc: https://www.haproxy.com/blog/howto-transparent-proxying-and-binding-with-haproxy-and-aloha-load-balancer/ Create frontends: frontend www-http bind *:80 mode tcp option tcplog tcp-request inspect-delay 5s default_backend rancher-http frontend www-https bind *:443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend rancher-https Create backends: backend rancher-http mode tcp balance roundrobin source 0.0.0.0 usesrc client server rancher01 192.168.1.103:80 server rancher02 192.168.1.104:80 server rancher03 192.168.1.105:80 backend rancher-https mode tcp balance roundrobin source 0.0.0.0 usesrc client server rancher01 192.168.1.103:443 server rancher02 192.168.1.104:443 server rancher03 192.168.1.105:443 Test the configuration: haproxy -f /etc/haproxy/haproxy.cfg -c Reload HAProxy: systemctl reload haproxy Example config Troubleshooting Add the following to /etc/haproxy/haproxy.cfg before the frontend section. listen stats bind :9000 mode http stats enable stats hide-version stats realm Haproxy\\ Statistics stats uri / stats auth admin:admin Go to http://load01.example.com:9000/ Username/Password: admin/admin If there are firewall rules blocking port 9000, use ssh tunneling to proxy the connection: ssh -f -N -L 9000:127.0.0.1:9000 root@192.168.1.101 Go to http://localhost:9000/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to setup HAProxy for Rancher v2.x"},{"location":"000020175/#how-to-setup-haproxy-for-rancher-v2x","text":"This document (000020175) is provided subject to the disclaimer at the end of this document.","title":"How to setup HAProxy for Rancher v2.x"},{"location":"000020175/#situation","text":"","title":"Situation"},{"location":"000020175/#task","text":"Setup HAProxy as a frontend load balancer for Rancher v2.x.","title":"Task"},{"location":"000020175/#overview","text":"","title":"Overview"},{"location":"000020175/#install-haproxy","text":"","title":"Install HAProxy"},{"location":"000020175/#ubuntu","text":"apt update apt install -y haproxy systemctl enable haproxy systemctl start haproxy","title":"Ubuntu"},{"location":"000020175/#centos-redhat","text":"yum update yum install haproxy -y systemctl enable haproxy systemctl start haproxy","title":"CentOS / RedHat"},{"location":"000020175/#example-haproxy-config","text":"","title":"Example HAProxy Config"},{"location":"000020175/#option-a-full-ssl","text":"Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/ Verify Rancher URL works when connecting directly to a Rancher node. For example: curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping Copy cert and key into a single file called /etc/haproxy/cert.pem Add frontend to /etc/haproxy/haproxy.cfg: frontend www-http bind *:80 reqadd X-Forwarded-Proto:\\ http default_backend rancher-http frontend www-https bind *:443 ssl crt /etc/haproxy/cert.pem reqadd X-Forwarded-Proto:\\ https default_backend rancher-https Add backends to /etc/haproxy/haproxy.cfg: backend rancher-http mode http option httpchk HEAD /healthz HTTP/1.0 server rancher01 192.168.1.103:80 check weight 1 maxconn 1024 server rancher02 192.168.1.104:80 check weight 1 maxconn 1024 server rancher03 192.168.1.105:80 check weight 1 maxconn 1024 backend rancher-https mode http option httpchk HEAD /healthz HTTP/1.0 server rancher01 192.168.1.103:443 check weight 1 maxconn 1024 ssl verify none server rancher02 192.168.1.104:443 check weight 1 maxconn 1024 ssl verify none server rancher03 192.168.1.105:443 check weight 1 maxconn 1024 ssl verify none Test the configuration: haproxy -f /etc/haproxy/haproxy.cfg -c Reload HAProxy: systemctl reload haproxy Example config","title":"Option A - Full SSL"},{"location":"000020175/#option-b-external-tls-termination","text":"Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/options/chart-options/#external-tls-termination Verify Rancher URL works went connecting directly to a Rancher node. For example: curl --header \"Host: rancher.example.com\" http://192.168.1.103/ping Copy cert and key into a single file called /etc/haproxy/cert.pem Create frontends: frontend www-http bind *:80 reqadd X-Forwarded-Proto:\\ http default_backend rancher-http frontend www-https bind *:443 ssl crt /etc/haproxy/cert.pem reqadd X-Forwarded-Proto:\\ https default_backend rancher-http Create backends: backend rancher-http mode http option httpchk HEAD /healthz HTTP/1.0 server rancher01 192.168.1.103:80 check weight 1 maxconn 1024 server rancher02 192.168.1.104:80 check weight 1 maxconn 1024 server rancher03 192.168.1.105:80 check weight 1 maxconn 1024 Test the configuration: haproxy -f /etc/haproxy/haproxy.cfg -c Reload HAProxy: systemctl reload haproxy Example config","title":"Option B - External TLS Termination"},{"location":"000020175/#option-c-tcp-pass-through","text":"Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/ Verify Rancher URL works when connecting directly to a Rancher node. For example: curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping NOTE: The default gateway for all 3 Rancher nodes must be the load balancer. Doc: https://www.haproxy.com/blog/howto-transparent-proxying-and-binding-with-haproxy-and-aloha-load-balancer/ Create frontends: frontend www-http bind *:80 mode tcp option tcplog tcp-request inspect-delay 5s default_backend rancher-http frontend www-https bind *:443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend rancher-https Create backends: backend rancher-http mode tcp balance roundrobin source 0.0.0.0 usesrc client server rancher01 192.168.1.103:80 server rancher02 192.168.1.104:80 server rancher03 192.168.1.105:80 backend rancher-https mode tcp balance roundrobin source 0.0.0.0 usesrc client server rancher01 192.168.1.103:443 server rancher02 192.168.1.104:443 server rancher03 192.168.1.105:443 Test the configuration: haproxy -f /etc/haproxy/haproxy.cfg -c Reload HAProxy: systemctl reload haproxy Example config","title":"Option C - TCP pass-through"},{"location":"000020175/#troubleshooting","text":"Add the following to /etc/haproxy/haproxy.cfg before the frontend section. listen stats bind :9000 mode http stats enable stats hide-version stats realm Haproxy\\ Statistics stats uri / stats auth admin:admin Go to http://load01.example.com:9000/ Username/Password: admin/admin If there are firewall rules blocking port 9000, use ssh tunneling to proxy the connection: ssh -f -N -L 9000:127.0.0.1:9000 root@192.168.1.101 Go to http://localhost:9000/","title":"Troubleshooting"},{"location":"000020175/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020176/","text":"How to collect a trace and heap from nginx ingress This document (000020176) is provided subject to the disclaimer at the end of this document. Situation Task When troubleshooting an ingress-nginx issue, collecting the trace and heap dump from ingress-nginx Pods may be requested. This can assist with understanding issues like excessive memory consumption. Pre-requisites Access to the node(s) where the ingress-nginx Pods are experiencing the issue, or access to the node on 10254/TCP from a workstation. To collect the output, the below commands use curl , you may need to install the package. If needed, wget could be used instead. The date command is used to provide a consistent timestamp for the files, this could be changed or removed if the date command on the node doesn't support these flags. The issue should be occurring at the time for the collection to be useful when investigating. Steps SSH to the node(s), use the following commands to collect the trace and heap dump. If the issue is intermittent or fluctuating, repeat the commands as necessary to capture the collection when the issue is ocurring. Heap curl -s http://localhost:10254/debug/pprof/trace?seconds=5 --output /tmp/nginx-trace.$(date -u --iso-8601=seconds) Trace curl -s http://localhost:10254/debug/pprof/heap --output /tmp/nginx-heap.$(date -u --iso-8601=seconds) Note : if accessing the node on 10254/TCP instead, be sure to update localhost with the IP Address of the node. If the files are too large to upload to the ticket, please request or use the provided temporary upload location. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to collect a trace and heap from nginx ingress"},{"location":"000020176/#how-to-collect-a-trace-and-heap-from-nginx-ingress","text":"This document (000020176) is provided subject to the disclaimer at the end of this document.","title":"How to collect a trace and heap from nginx ingress"},{"location":"000020176/#situation","text":"","title":"Situation"},{"location":"000020176/#task","text":"When troubleshooting an ingress-nginx issue, collecting the trace and heap dump from ingress-nginx Pods may be requested. This can assist with understanding issues like excessive memory consumption.","title":"Task"},{"location":"000020176/#pre-requisites","text":"Access to the node(s) where the ingress-nginx Pods are experiencing the issue, or access to the node on 10254/TCP from a workstation. To collect the output, the below commands use curl , you may need to install the package. If needed, wget could be used instead. The date command is used to provide a consistent timestamp for the files, this could be changed or removed if the date command on the node doesn't support these flags. The issue should be occurring at the time for the collection to be useful when investigating.","title":"Pre-requisites"},{"location":"000020176/#steps","text":"SSH to the node(s), use the following commands to collect the trace and heap dump. If the issue is intermittent or fluctuating, repeat the commands as necessary to capture the collection when the issue is ocurring.","title":"Steps"},{"location":"000020176/#heap","text":"curl -s http://localhost:10254/debug/pprof/trace?seconds=5 --output /tmp/nginx-trace.$(date -u --iso-8601=seconds)","title":"Heap"},{"location":"000020176/#trace","text":"curl -s http://localhost:10254/debug/pprof/heap --output /tmp/nginx-heap.$(date -u --iso-8601=seconds) Note : if accessing the node on 10254/TCP instead, be sure to update localhost with the IP Address of the node. If the files are too large to upload to the ticket, please request or use the provided temporary upload location.","title":"Trace"},{"location":"000020176/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020177/","text":"How to generate a HAR file This document (000020177) is provided subject to the disclaimer at the end of this document. Situation Task When troubleshooting an issue that is reproducible in a browser, it is sometimes necessary to have additional information about the requests and responses. You may be requested to generate a HAR file recording to capture this and attach this to a ticket for analysis. Please note, the information collected in a HAR file can contain sensitive data like content, headers, and cookies. This is not always the case, and some information is transient only. However, please check and santise the information as necessary before uploading. Pre-requisites A browser that can reproduce the issue, we've covered Chrome and Firefox in this article. The issue should be occuring or reproducible at the time of the collection to contain an example of the issue. Steps Open your browser ready to reproduce the issue. Chrome From the menu, select View > Developer > Developer Tools From the pane, click on the Network tab Locate the Preserve log setting in the upper left and ensure it is checked Locate the record button, it should be a red circle to indicate that it is currently recording, if it is grey, click it once to start recording Follow any steps needed to reproduce the issue during the recording Once the issue has occurred, right click in the pane and select Save as HAR with Content Firefox From the menu, select Tools > Web Developer > Network The recording to start automatically with any further navigation in the browser Follow any steps needed to reproduce the issue with the network pane open Once the issue has occurred, right click in the pane and select Save all as HAR Upload the HAR file Generally, HAR files are small in size, however if the file are too large to upload directly to the ticket, please request or use the provided temporary upload location. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to generate a HAR file"},{"location":"000020177/#how-to-generate-a-har-file","text":"This document (000020177) is provided subject to the disclaimer at the end of this document.","title":"How to generate a HAR file"},{"location":"000020177/#situation","text":"","title":"Situation"},{"location":"000020177/#task","text":"When troubleshooting an issue that is reproducible in a browser, it is sometimes necessary to have additional information about the requests and responses. You may be requested to generate a HAR file recording to capture this and attach this to a ticket for analysis. Please note, the information collected in a HAR file can contain sensitive data like content, headers, and cookies. This is not always the case, and some information is transient only. However, please check and santise the information as necessary before uploading.","title":"Task"},{"location":"000020177/#pre-requisites","text":"A browser that can reproduce the issue, we've covered Chrome and Firefox in this article. The issue should be occuring or reproducible at the time of the collection to contain an example of the issue.","title":"Pre-requisites"},{"location":"000020177/#steps","text":"Open your browser ready to reproduce the issue.","title":"Steps"},{"location":"000020177/#chrome","text":"From the menu, select View > Developer > Developer Tools From the pane, click on the Network tab Locate the Preserve log setting in the upper left and ensure it is checked Locate the record button, it should be a red circle to indicate that it is currently recording, if it is grey, click it once to start recording Follow any steps needed to reproduce the issue during the recording Once the issue has occurred, right click in the pane and select Save as HAR with Content Firefox From the menu, select Tools > Web Developer > Network The recording to start automatically with any further navigation in the browser Follow any steps needed to reproduce the issue with the network pane open Once the issue has occurred, right click in the pane and select Save all as HAR","title":"Chrome"},{"location":"000020177/#upload-the-har-file","text":"Generally, HAR files are small in size, however if the file are too large to upload directly to the ticket, please request or use the provided temporary upload location.","title":"Upload the HAR file"},{"location":"000020177/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020178/","text":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x This document (000020178) is provided subject to the disclaimer at the end of this document. Situation How to conduct CIS hardening benchmark scanning for Rancher v2.3.x CIS Benchmarks are best practices for the secure configuration of a target system. Available for more than 140 technologies, CIS Benchmarks are developed through a unique consensus-based process comprised of cybersecurity professionals and subject matter experts around the world. CIS Benchmarks are the only consensus-based, best-practice security configuration guides both developed and accepted by government, business, industry, and academia. This script is based on CIS Benchmark Rancher Self-Assessment Guide v2.3 https://rancher.com/docs/rancher/v2.x/en/security/benchmark-2.3, which was derived from CIS Kubernetes Benchmark v1.4.1 . Pre-requisites Rancher version 2.3.x Kubernetes version 1.15 jq , grep , awk and kubectl installed on target node Steps Clone the script into the target node git clone https://github.com/nickngch/rancher-hardening.git Access the folder cd rancher-hardening Execute the script based on the node's role For Control Plane - sudo bash ./master.sh 2.3 cp For Control Plane + ETCD - sudo bash ./master.sh 2.3 all For ETCD - sudo bash ./master.sh 2.3 etcd For worker node - sudo ./worker.sh 2.3 Limitation Section 1.6 and 1.7 in master node require manual verification. Further reading https://www.cisecurity.org/cis-benchmarks/cis-benchmarks-faq/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x"},{"location":"000020178/#how-to-conduct-cis-hardening-benchmark-scanning-for-rancher-v23x","text":"This document (000020178) is provided subject to the disclaimer at the end of this document.","title":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x"},{"location":"000020178/#situation","text":"","title":"Situation"},{"location":"000020178/#how-to-conduct-cis-hardening-benchmark-scanning-for-rancher-v23x_1","text":"CIS Benchmarks are best practices for the secure configuration of a target system. Available for more than 140 technologies, CIS Benchmarks are developed through a unique consensus-based process comprised of cybersecurity professionals and subject matter experts around the world. CIS Benchmarks are the only consensus-based, best-practice security configuration guides both developed and accepted by government, business, industry, and academia. This script is based on CIS Benchmark Rancher Self-Assessment Guide v2.3 https://rancher.com/docs/rancher/v2.x/en/security/benchmark-2.3, which was derived from CIS Kubernetes Benchmark v1.4.1 .","title":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x"},{"location":"000020178/#pre-requisites","text":"Rancher version 2.3.x Kubernetes version 1.15 jq , grep , awk and kubectl installed on target node","title":"Pre-requisites"},{"location":"000020178/#steps","text":"Clone the script into the target node git clone https://github.com/nickngch/rancher-hardening.git Access the folder cd rancher-hardening Execute the script based on the node's role For Control Plane - sudo bash ./master.sh 2.3 cp For Control Plane + ETCD - sudo bash ./master.sh 2.3 all For ETCD - sudo bash ./master.sh 2.3 etcd For worker node - sudo ./worker.sh 2.3","title":"Steps"},{"location":"000020178/#limitation","text":"Section 1.6 and 1.7 in master node require manual verification.","title":"Limitation"},{"location":"000020178/#further-reading","text":"https://www.cisecurity.org/cis-benchmarks/cis-benchmarks-faq/","title":"Further reading"},{"location":"000020178/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020179/","text":"How to conduct performance testing with Clusterloader2 This document (000020179) is provided subject to the disclaimer at the end of this document. Situation How to conduct performance testing with Clusterloader2 Clusterloader is an opensource performance testing tool to measure the performance metrics of your Kubernetes cluster. Pre-requisites Linux or Mac machine that has Golang and kubectl installed SSH key of the Kubernetes master node Kubeconfig file of the target cluster Steps Create a folder named k8s.io under ~/go/src/ : mkdir ~/go/src/k8s.io Clone the perf-test under k8s.io folder: cd ~/go/src/k8s.io && git clone https://github.com/galal-hussein/perf-tests.git Navigate to the clusterloader2 directory: cd ~/go/src/k8s.io/perf-tests/clusterloader2 Edit the testconfig according to the environment: vim testing/load/config.yaml Execute the clusterloader2 with appropriate options: KUBE_SSH_USER=<SSH USERNAME> LOCAL_SSH_KEY=<SSH KEY PATH> go run cmd/clusterloader.go --nodes 3 --mastername=<MASTER NODE NAME> --kubeconfig=<KUBECONFIG FILE PATH> --provider=local --masterip=<MASTER NODE IP ADDRESS> --testconfig=testing/<TESTING SUBJECT>/config.yaml --report-dir=/tmp/reports 2>&1 | tee /tmp/tmp.log The results of the testing will be stored in the /tmp/reports directory. FAQ Errors: [config reading error: decoding failed: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal number -4611686018427388 into Go struct field Phase.Steps.Phases.ReplicasPerNamespace of type int32]\" Change the value of {{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}} to match the number of the nodes in the config.yaml file level=warning msg=\"Got errors during step execution: [measurement call APIResponsiveness - APIResponsiveness error: unexpected response: \\\"# HELP aggregator_openapi_v2_regeneration_count [ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason. Comment out the APIResponsiveness section in config.yaml: measurements: - Identifier: APIResponsiveness Method: APIResponsiveness Params: action: reset Further reading https://github.com/kubernetes/perf-tests Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to conduct performance testing with Clusterloader2"},{"location":"000020179/#how-to-conduct-performance-testing-with-clusterloader2","text":"This document (000020179) is provided subject to the disclaimer at the end of this document.","title":"How to conduct performance testing with Clusterloader2"},{"location":"000020179/#situation","text":"","title":"Situation"},{"location":"000020179/#how-to-conduct-performance-testing-with-clusterloader2_1","text":"Clusterloader is an opensource performance testing tool to measure the performance metrics of your Kubernetes cluster.","title":"How to conduct performance testing with Clusterloader2"},{"location":"000020179/#pre-requisites","text":"Linux or Mac machine that has Golang and kubectl installed SSH key of the Kubernetes master node Kubeconfig file of the target cluster","title":"Pre-requisites"},{"location":"000020179/#steps","text":"Create a folder named k8s.io under ~/go/src/ : mkdir ~/go/src/k8s.io Clone the perf-test under k8s.io folder: cd ~/go/src/k8s.io && git clone https://github.com/galal-hussein/perf-tests.git Navigate to the clusterloader2 directory: cd ~/go/src/k8s.io/perf-tests/clusterloader2 Edit the testconfig according to the environment: vim testing/load/config.yaml Execute the clusterloader2 with appropriate options: KUBE_SSH_USER=<SSH USERNAME> LOCAL_SSH_KEY=<SSH KEY PATH> go run cmd/clusterloader.go --nodes 3 --mastername=<MASTER NODE NAME> --kubeconfig=<KUBECONFIG FILE PATH> --provider=local --masterip=<MASTER NODE IP ADDRESS> --testconfig=testing/<TESTING SUBJECT>/config.yaml --report-dir=/tmp/reports 2>&1 | tee /tmp/tmp.log The results of the testing will be stored in the /tmp/reports directory.","title":"Steps"},{"location":"000020179/#faq","text":"Errors: [config reading error: decoding failed: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal number -4611686018427388 into Go struct field Phase.Steps.Phases.ReplicasPerNamespace of type int32]\" Change the value of {{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}} to match the number of the nodes in the config.yaml file level=warning msg=\"Got errors during step execution: [measurement call APIResponsiveness - APIResponsiveness error: unexpected response: \\\"# HELP aggregator_openapi_v2_regeneration_count [ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason. Comment out the APIResponsiveness section in config.yaml: measurements: - Identifier: APIResponsiveness Method: APIResponsiveness Params: action: reset","title":"FAQ"},{"location":"000020179/#further-reading","text":"https://github.com/kubernetes/perf-tests","title":"Further reading"},{"location":"000020179/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020180/","text":"How do I edit my cluster using RKE Templates? This document (000020180) is provided subject to the disclaimer at the end of this document. Situation Question After converting / managing a cluster using RKE Templates, when trying to make changes under \"Edit Cluster\" the 'Edit' button is gone and removed features such as the Kubernetes version dropdown menu. Where did this go? Pre-requisites Kubernetes clusters managed by the RKE Template feature Answer If your Kubernetes cluster now has an RKE Template attached, you now need to make changes to your cluster in the RKE Template section Navigate to Global --> Tools ---> RKE Tempates Click the three-dot menu to make a new revision Here is where you will make changes to the cluster configuration and save it as a new version. However it won't take effect immediately. After saving the revision, navigate back to your cluster, click Edit. Under \"Cluster Options\", there will be a drop down menu to select which version of your template you want to use. Select your new version and Save. Further Reading https://rancher.com/docs/rancher/v2.x/en/admin-settings/rke-templates/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How do I edit my cluster using RKE Templates?"},{"location":"000020180/#how-do-i-edit-my-cluster-using-rke-templates","text":"This document (000020180) is provided subject to the disclaimer at the end of this document.","title":"How do I edit my cluster using RKE Templates?"},{"location":"000020180/#situation","text":"","title":"Situation"},{"location":"000020180/#question","text":"After converting / managing a cluster using RKE Templates, when trying to make changes under \"Edit Cluster\" the 'Edit' button is gone and removed features such as the Kubernetes version dropdown menu. Where did this go?","title":"Question"},{"location":"000020180/#pre-requisites","text":"Kubernetes clusters managed by the RKE Template feature","title":"Pre-requisites"},{"location":"000020180/#answer","text":"If your Kubernetes cluster now has an RKE Template attached, you now need to make changes to your cluster in the RKE Template section Navigate to Global --> Tools ---> RKE Tempates Click the three-dot menu to make a new revision Here is where you will make changes to the cluster configuration and save it as a new version. However it won't take effect immediately. After saving the revision, navigate back to your cluster, click Edit. Under \"Cluster Options\", there will be a drop down menu to select which version of your template you want to use. Select your new version and Save.","title":"Answer"},{"location":"000020180/#further-reading","text":"https://rancher.com/docs/rancher/v2.x/en/admin-settings/rke-templates/","title":"Further Reading"},{"location":"000020180/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020181/","text":"How to configure Okta Auth with Rancher HA This document (000020181) is provided subject to the disclaimer at the end of this document. Situation Issue When configuring Okta Authentication using the Rancher Official Documentation in a Rancher HA environment you encounter 501 errors when trying to verify and enable the configuration. Cause For Rancher to fully enable Okta Authenication it requires a succesful test of your configuration to verify the information is correct. When the test request is sent from one of your Rancher Servers to Okta the returned verification is routed through a Load Balancer to a different Rancher Server in the cluster. As the recipient has not yet been configured to service Okta Authentication it will return a 501 for the request and the Rancher Server that acted as a requester will fail to enable as it could not complete the verification. Resolution Assumptions You have appropriately configured Okta Authentication according to the Rancher Official Documentation. Steps to Resolve Using the Nodes Tab in your Rancher Management Cluster cordon off the nodes you are not currently connected to, this will force traffic to be returned to the Requester. Run the test and enable procedure for Okta Configuration from Rancher and verify you can now login successfully. Uncordon the other Nodes and the settings will be synced across the cluster automatically. Verify the cluster is working as expected by logging in using an Okta sign-in. (Optional) To verify the settings have been synced to all nodes in the cluster you can cordon off all but another Node, not the one you used to configure, and attempt logging in using Okta. This process can be repeated for each node. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to configure Okta Auth with Rancher HA"},{"location":"000020181/#how-to-configure-okta-auth-with-rancher-ha","text":"This document (000020181) is provided subject to the disclaimer at the end of this document.","title":"How to configure Okta Auth with Rancher HA"},{"location":"000020181/#situation","text":"","title":"Situation"},{"location":"000020181/#issue","text":"When configuring Okta Authentication using the Rancher Official Documentation in a Rancher HA environment you encounter 501 errors when trying to verify and enable the configuration.","title":"Issue"},{"location":"000020181/#cause","text":"For Rancher to fully enable Okta Authenication it requires a succesful test of your configuration to verify the information is correct. When the test request is sent from one of your Rancher Servers to Okta the returned verification is routed through a Load Balancer to a different Rancher Server in the cluster. As the recipient has not yet been configured to service Okta Authentication it will return a 501 for the request and the Rancher Server that acted as a requester will fail to enable as it could not complete the verification.","title":"Cause"},{"location":"000020181/#resolution","text":"","title":"Resolution"},{"location":"000020181/#assumptions","text":"You have appropriately configured Okta Authentication according to the Rancher Official Documentation.","title":"Assumptions"},{"location":"000020181/#steps-to-resolve","text":"Using the Nodes Tab in your Rancher Management Cluster cordon off the nodes you are not currently connected to, this will force traffic to be returned to the Requester. Run the test and enable procedure for Okta Configuration from Rancher and verify you can now login successfully. Uncordon the other Nodes and the settings will be synced across the cluster automatically. Verify the cluster is working as expected by logging in using an Okta sign-in. (Optional) To verify the settings have been synced to all nodes in the cluster you can cordon off all but another Node, not the one you used to configure, and attempt logging in using Okta. This process can be repeated for each node.","title":"Steps to Resolve"},{"location":"000020181/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020182/","text":"Rancher pre v1.6.22 \"Hosts stuck Reconnecting\" Rancher server logs show 'Cursor returned more than one result' This document (000020182) is provided subject to the disclaimer at the end of this document. Situation Issue Hosts getting stuck in either state Reconnecting or Finishing-Reconnect and Rancher server logs include errors like the following: 2019-02-26 12:05:55,265 ERROR [51e1303d-21b2-409f-ba2e-7542e8de4941:9663402] [healthcheckInstanceHostMap:445975] [healthcheckinstancehostmap.remove] [] [ecutorService-3] [c.p.e.p.i.DefaultProcessInstanceImpl] Unknown exception org.jooq.exception.InvalidResultException: Cursor returned more than one result Pre-requisites Rancher version lower than 1.6.22 Workaround In the Rancher MySQL database, find all the duplicates by checking column 3 for entries with more than a count of 1 in the return from the following query: select host_id,healthcheck_instance_id,count(*) from healthcheck_instance_host_map where removed is null group by host_id,healthcheck_instance_id order by 3; For each healthcheck_instance_id in any row with more than 1 in column 3, run the following command: update healthcheck_instance_host_map set state='removed', removed=now(), remove_time=now() where healthcheck_instance_id='<INSERT_HEALTHCHECK_ID>'; Wait and watch the hosts view. The hosts should all finish reconnecting and instances should update. Resolution Upgrade to 1.6.22+ or 2.x Further reading https://github.com/rancher/rancher/issues/15284 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher pre v1.6.22 \"Hosts stuck Reconnecting\" Rancher server logs show 'Cursor returned more than one result'"},{"location":"000020182/#rancher-pre-v1622-hosts-stuck-reconnecting-rancher-server-logs-show-cursor-returned-more-than-one-result","text":"This document (000020182) is provided subject to the disclaimer at the end of this document.","title":"Rancher pre v1.6.22 \"Hosts stuck Reconnecting\" Rancher server logs show 'Cursor returned more than one result'"},{"location":"000020182/#situation","text":"","title":"Situation"},{"location":"000020182/#issue","text":"Hosts getting stuck in either state Reconnecting or Finishing-Reconnect and Rancher server logs include errors like the following: 2019-02-26 12:05:55,265 ERROR [51e1303d-21b2-409f-ba2e-7542e8de4941:9663402] [healthcheckInstanceHostMap:445975] [healthcheckinstancehostmap.remove] [] [ecutorService-3] [c.p.e.p.i.DefaultProcessInstanceImpl] Unknown exception org.jooq.exception.InvalidResultException: Cursor returned more than one result","title":"Issue"},{"location":"000020182/#pre-requisites","text":"Rancher version lower than 1.6.22","title":"Pre-requisites"},{"location":"000020182/#workaround","text":"In the Rancher MySQL database, find all the duplicates by checking column 3 for entries with more than a count of 1 in the return from the following query: select host_id,healthcheck_instance_id,count(*) from healthcheck_instance_host_map where removed is null group by host_id,healthcheck_instance_id order by 3; For each healthcheck_instance_id in any row with more than 1 in column 3, run the following command: update healthcheck_instance_host_map set state='removed', removed=now(), remove_time=now() where healthcheck_instance_id='<INSERT_HEALTHCHECK_ID>'; Wait and watch the hosts view. The hosts should all finish reconnecting and instances should update.","title":"Workaround"},{"location":"000020182/#resolution","text":"Upgrade to 1.6.22+ or 2.x","title":"Resolution"},{"location":"000020182/#further-reading","text":"https://github.com/rancher/rancher/issues/15284","title":"Further reading"},{"location":"000020182/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020183/","text":"Admins cannot edit or see node templates created by another user in Rancher v2.0.0-v2.3.2. This document (000020183) is provided subject to the disclaimer at the end of this document. Situation Issue Admins cannot edit or see node templates created by another user in Rancher v2.0.0 to v2.3.2. As a result, they are also unable to edit a cluster with a user that did not create it, even if that user is an admin. Workaround Change node template owner This script will change your node template owner in Rancher 2.x. You can run this script as a Docker image or directly as a bash script. You'll need the cluster ID and the user ID you want to change the ownership to. To obtain the cluster ID in the Rancher user interface, Navigate to Global > \"Your Cluster Name\", then grab the cluster ID from your address bar. I have listed an example of the URL and a cluster ID derrived from the URL below. - Example URL: https://<RANCHER URL>/c/c-48x9z/monitoring - Derived cluster ID from above URL: c-48x9z Now we need the user ID of the user to become the new node template owner, navigate to Global > Users to find the ID. To run the script using a docker image, make sure your $KUBECONFIG is set to the full path of your Rancher local cluster kube config then run the following command. docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -c <cluster-id> -n <user-id> To run the script directly, just download the change-nodetemplate-owner.sh script , make sure your $KUBECONFIG or ~/.kube/config is pointing to the correct Rancher local cluster then run the following command: curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh bash change-nodetemplate-owner.sh -c <cluster-id> -n <user-id> Assign a node template to a cluster's node pool Assign a node template to a cluster's node pool. This is useful for situations where the original owner of a cluster has been deleted which also deletes their node templates. To use this task successfully it is recommended that you create a new node template in the UI before using it. Make sure the node template matches the original ones as closely as possible. You will be shown options to choose from and prompted for confirmation. Run script with docker image: docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -t changenodetemplate -c <cluster-id> Run script from bash command line: curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh bash change-nodetemplate-owner.sh -t changenodetemplate -c <cluster-id> Resolution Upgrade to Rancher v2.3.3 or newer to receive the fix to this issue. More information on this bug can be found at the GitHub issue #12186 . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Admins cannot edit or see node templates created by another user in Rancher v2.0.0-v2.3.2."},{"location":"000020183/#admins-cannot-edit-or-see-node-templates-created-by-another-user-in-rancher-v200-v232","text":"This document (000020183) is provided subject to the disclaimer at the end of this document.","title":"Admins cannot edit or see node templates created by another user in Rancher v2.0.0-v2.3.2."},{"location":"000020183/#situation","text":"","title":"Situation"},{"location":"000020183/#issue","text":"Admins cannot edit or see node templates created by another user in Rancher v2.0.0 to v2.3.2. As a result, they are also unable to edit a cluster with a user that did not create it, even if that user is an admin.","title":"Issue"},{"location":"000020183/#workaround","text":"","title":"Workaround"},{"location":"000020183/#change-node-template-owner","text":"This script will change your node template owner in Rancher 2.x. You can run this script as a Docker image or directly as a bash script. You'll need the cluster ID and the user ID you want to change the ownership to. To obtain the cluster ID in the Rancher user interface, Navigate to Global > \"Your Cluster Name\", then grab the cluster ID from your address bar. I have listed an example of the URL and a cluster ID derrived from the URL below. - Example URL: https://<RANCHER URL>/c/c-48x9z/monitoring - Derived cluster ID from above URL: c-48x9z Now we need the user ID of the user to become the new node template owner, navigate to Global > Users to find the ID. To run the script using a docker image, make sure your $KUBECONFIG is set to the full path of your Rancher local cluster kube config then run the following command. docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -c <cluster-id> -n <user-id> To run the script directly, just download the change-nodetemplate-owner.sh script , make sure your $KUBECONFIG or ~/.kube/config is pointing to the correct Rancher local cluster then run the following command: curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh bash change-nodetemplate-owner.sh -c <cluster-id> -n <user-id>","title":"Change node template owner"},{"location":"000020183/#assign-a-node-template-to-a-clusters-node-pool","text":"Assign a node template to a cluster's node pool. This is useful for situations where the original owner of a cluster has been deleted which also deletes their node templates. To use this task successfully it is recommended that you create a new node template in the UI before using it. Make sure the node template matches the original ones as closely as possible. You will be shown options to choose from and prompted for confirmation. Run script with docker image: docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -t changenodetemplate -c <cluster-id> Run script from bash command line: curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh bash change-nodetemplate-owner.sh -t changenodetemplate -c <cluster-id>","title":"Assign a node template to a cluster's node pool"},{"location":"000020183/#resolution","text":"Upgrade to Rancher v2.3.3 or newer to receive the fix to this issue. More information on this bug can be found at the GitHub issue #12186 .","title":"Resolution"},{"location":"000020183/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020184/","text":"When will my cluster certificates expire? This document (000020184) is provided subject to the disclaimer at the end of this document. Situation Question I have a cluster which is displaying the message \"This cluster has certs that are expiring or have expired\". Is there any way to determine when the certs will expire? Pre-requisites Kubernetes clusters provisioned via the RKE, or Rancher launched Kubernetes clusters Answer Dates on when particular certificates will expire are located in the Rancher API. To view them, from the \"Clusters\" page, click the three-dot menu on your cluster and \"View in API\" Scroll down or search for the section called \"certificatesExpiration\" There you can see the expiration date for each kubernetes component FYI, as of Rancher 2.3.5, there is a known issue that old, removed nodes still appear in this list. This is cosmetic and won't affect your running cluster: https://github.com/rancher/rancher/issues/24333 Further Reading https://rancher.com/blog/2019/kubernetes-certificate-expiry-and-rotation-in-rancher-kubernetes-clusters Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"When will my cluster certificates expire?"},{"location":"000020184/#when-will-my-cluster-certificates-expire","text":"This document (000020184) is provided subject to the disclaimer at the end of this document.","title":"When will my cluster certificates expire?"},{"location":"000020184/#situation","text":"","title":"Situation"},{"location":"000020184/#question","text":"I have a cluster which is displaying the message \"This cluster has certs that are expiring or have expired\". Is there any way to determine when the certs will expire?","title":"Question"},{"location":"000020184/#pre-requisites","text":"Kubernetes clusters provisioned via the RKE, or Rancher launched Kubernetes clusters","title":"Pre-requisites"},{"location":"000020184/#answer","text":"Dates on when particular certificates will expire are located in the Rancher API. To view them, from the \"Clusters\" page, click the three-dot menu on your cluster and \"View in API\" Scroll down or search for the section called \"certificatesExpiration\" There you can see the expiration date for each kubernetes component FYI, as of Rancher 2.3.5, there is a known issue that old, removed nodes still appear in this list. This is cosmetic and won't affect your running cluster: https://github.com/rancher/rancher/issues/24333","title":"Answer"},{"location":"000020184/#further-reading","text":"https://rancher.com/blog/2019/kubernetes-certificate-expiry-and-rotation-in-rancher-kubernetes-clusters","title":"Further Reading"},{"location":"000020184/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020185/","text":"Why does a cluster or node show requested memory with a milli (m) unit? This document (000020185) is provided subject to the disclaimer at the end of this document. Situation Question Why does a cluster or node show requested memory with a milli (m) unit? Pre-requisites provisioned Kubernetes cluster Answer CPU resources in Kubernetes are measured in millicpus, or 1/1000th of a CPU core, and 1 CPU Core = 1000m. The API will change any request for a decimal point into millicpus. For example 0.1 is converted into 100m. One hyperthread is considered one core, or 1000m. Memory resources in Kubernetes are mesured in bytes, and can be expressed as an integer with one of these suffixes: E, P, T, G, M, K - decimal suffixes, or Ei, Pi, Ti, Gi, Mi, Ki - binary suffixes (more commonly used for memory), or omit the suffix altogether. Lowercase \"m\" notation is not a recommended suffix for memory. The \"m\" notation for memory might indicate a misconfiguration, where CPU units are recommended to use that suffix (example: 200m), and Memory units are recommended to use \"Mi\" (example: 128Mi). Further Reading Explaination of Kubernetes CPU resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu ) Explaination of Kubernetes memory resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory ) Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Why does a cluster or node show requested memory with a milli (m) unit?"},{"location":"000020185/#why-does-a-cluster-or-node-show-requested-memory-with-a-milli-m-unit","text":"This document (000020185) is provided subject to the disclaimer at the end of this document.","title":"Why does a cluster or node show requested memory with a milli (m) unit?"},{"location":"000020185/#situation","text":"","title":"Situation"},{"location":"000020185/#question","text":"Why does a cluster or node show requested memory with a milli (m) unit?","title":"Question"},{"location":"000020185/#pre-requisites","text":"provisioned Kubernetes cluster","title":"Pre-requisites"},{"location":"000020185/#answer","text":"CPU resources in Kubernetes are measured in millicpus, or 1/1000th of a CPU core, and 1 CPU Core = 1000m. The API will change any request for a decimal point into millicpus. For example 0.1 is converted into 100m. One hyperthread is considered one core, or 1000m. Memory resources in Kubernetes are mesured in bytes, and can be expressed as an integer with one of these suffixes: E, P, T, G, M, K - decimal suffixes, or Ei, Pi, Ti, Gi, Mi, Ki - binary suffixes (more commonly used for memory), or omit the suffix altogether. Lowercase \"m\" notation is not a recommended suffix for memory. The \"m\" notation for memory might indicate a misconfiguration, where CPU units are recommended to use that suffix (example: 200m), and Memory units are recommended to use \"Mi\" (example: 128Mi).","title":"Answer"},{"location":"000020185/#further-reading","text":"Explaination of Kubernetes CPU resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu ) Explaination of Kubernetes memory resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory )","title":"Further Reading"},{"location":"000020185/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020186/","text":"How To Update CoreDNS's Resolver Policy This document (000020186) is provided subject to the disclaimer at the end of this document. Situation Task This article outlines how to change CoreDNS's forward resolver policy. Pre-requisites A custom cluster provisioned by Rancher or RKE with CoreDNS. The CoreDNS docs explain the various configurations. In this case, we are concerned with the policy. Which defaults to random . Resolution To change the policy to sequential , edit your clusters yaml. For RKE provisioned clusters this will be your cluster.yaml and for Rancher provisioned custom clusters this will be found by editing the cluster. For RKE add the following to the end of the file, but for Rancher provisioned clusters, nest this in the rancher_kubernetes_engine_config section. addons: |- --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy sequential } cache 30 loop reload loadbalance } The lines of note here are the following, which are changed from just forward . \"/etc/resolv.conf\" . forward . \"/etc/resolv.conf\" { policy sequential } At this point you should be able to just hit save in Rancher or run rke up and the change will be pushed to the cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How To Update CoreDNS's Resolver Policy"},{"location":"000020186/#how-to-update-corednss-resolver-policy","text":"This document (000020186) is provided subject to the disclaimer at the end of this document.","title":"How To Update CoreDNS's Resolver Policy"},{"location":"000020186/#situation","text":"","title":"Situation"},{"location":"000020186/#task","text":"This article outlines how to change CoreDNS's forward resolver policy.","title":"Task"},{"location":"000020186/#pre-requisites","text":"A custom cluster provisioned by Rancher or RKE with CoreDNS. The CoreDNS docs explain the various configurations. In this case, we are concerned with the policy. Which defaults to random .","title":"Pre-requisites"},{"location":"000020186/#resolution","text":"To change the policy to sequential , edit your clusters yaml. For RKE provisioned clusters this will be your cluster.yaml and for Rancher provisioned custom clusters this will be found by editing the cluster. For RKE add the following to the end of the file, but for Rancher provisioned clusters, nest this in the rancher_kubernetes_engine_config section. addons: |- --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy sequential } cache 30 loop reload loadbalance } The lines of note here are the following, which are changed from just forward . \"/etc/resolv.conf\" . forward . \"/etc/resolv.conf\" { policy sequential } At this point you should be able to just hit save in Rancher or run rke up and the change will be pushed to the cluster.","title":"Resolution"},{"location":"000020186/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020187/","text":"How to update your etcd space alerts for better etcd monitoring This document (000020187) is provided subject to the disclaimer at the end of this document. Situation Task An alert you may have seen is Database usage close to the quota 500M in your Rancher 2.x cluster. This is a default etcd alert built into Rancher, you can find more info on the default alerts here: Rancher v2.x Default Alerts Upon further examination of the alert, you see the description of A warning alert is triggered when the size of etcd exceeds 500M. This alert is somewhat misleading as the default etcd size is 2GB. This alert is also at a severity level of Warning. Below, we suggest configuring your etcd alerts to better utilize the alert thresholds and to avoid the default alert constantly going off. Pre-requisites Running Rancher v2.x Resolution You can access the alerts by going to Tools -> Alerts at the cluster level. From here, clone the default alert three times . Once the 3 clones are created, disable the default alert so you always retain a clean reference alert. Alert for 1GB usage - INFO Alert for 1.5GB usage - WARNING Alert for 1.75GB usage - CRITICAL If you are running out of space, please reference the following two links. There will be an upcoming KB article walking through the resize process for etcd. Rancher v2.x etcd options etcd space quota documentation Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to update your etcd space alerts for better etcd monitoring"},{"location":"000020187/#how-to-update-your-etcd-space-alerts-for-better-etcd-monitoring","text":"This document (000020187) is provided subject to the disclaimer at the end of this document.","title":"How to update your etcd space alerts for better etcd monitoring"},{"location":"000020187/#situation","text":"","title":"Situation"},{"location":"000020187/#task","text":"An alert you may have seen is Database usage close to the quota 500M in your Rancher 2.x cluster. This is a default etcd alert built into Rancher, you can find more info on the default alerts here: Rancher v2.x Default Alerts Upon further examination of the alert, you see the description of A warning alert is triggered when the size of etcd exceeds 500M. This alert is somewhat misleading as the default etcd size is 2GB. This alert is also at a severity level of Warning. Below, we suggest configuring your etcd alerts to better utilize the alert thresholds and to avoid the default alert constantly going off.","title":"Task"},{"location":"000020187/#pre-requisites","text":"Running Rancher v2.x","title":"Pre-requisites"},{"location":"000020187/#resolution","text":"You can access the alerts by going to Tools -> Alerts at the cluster level. From here, clone the default alert three times . Once the 3 clones are created, disable the default alert so you always retain a clean reference alert. Alert for 1GB usage - INFO Alert for 1.5GB usage - WARNING Alert for 1.75GB usage - CRITICAL","title":"Resolution"},{"location":"000020187/#if-you-are-running-out-of-space-please-reference-the-following-two-links-there-will-be-an-upcoming-kb-article-walking-through-the-resize-process-for-etcd","text":"Rancher v2.x etcd options etcd space quota documentation","title":"If you are running out of space, please reference the following two links. There will be an upcoming KB article walking through the resize process for etcd."},{"location":"000020187/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020188/","text":"The RancherOS log collector script This document (000020188) is provided subject to the disclaimer at the end of this document. Situation RancherOS log collection Host logs and information can be collected from a node running RancherOS using the RancherOS log collector script . The script needs to be downloaded and run directly on the host using sudo, as follows: wget https://raw.githubusercontent.com/rancher/os/master/scripts/tools/collect_rancheros_info.sh | sudo sh The output will be written to /tmp to a tar file named rancheros_export_<datetime>.tar Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"The RancherOS log collector script"},{"location":"000020188/#the-rancheros-log-collector-script","text":"This document (000020188) is provided subject to the disclaimer at the end of this document.","title":"The RancherOS log collector script"},{"location":"000020188/#situation","text":"","title":"Situation"},{"location":"000020188/#rancheros-log-collection","text":"Host logs and information can be collected from a node running RancherOS using the RancherOS log collector script . The script needs to be downloaded and run directly on the host using sudo, as follows: wget https://raw.githubusercontent.com/rancher/os/master/scripts/tools/collect_rancheros_info.sh | sudo sh The output will be written to /tmp to a tar file named rancheros_export_<datetime>.tar","title":"RancherOS log collection"},{"location":"000020188/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020189/","text":"How to test websocket connections to Rancher v2.x This document (000020189) is provided subject to the disclaimer at the end of this document. Situation Task Rancher depends heavily on websocket support for UI and CLI features within Rancher as well as managing and interacting with downstream clusters. This article provides a quick test to determine if websocket connections are working from a potential downstream node or client to the Rancher server cluster. Pre-requisites A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster . Executing the test First you will need to create an API token to authenticate against Rancher. Start by logging into the Rancher UI. Once logged in, navigate to the API & Keys section by clicking the user icon in the top right of the pane, then click on the API & Keys menu item. Generate a new key by clicking the Add Key button, providing a name for the token and clicking Create. Copy the bearer token to a safe location. In a Linux shell from the desired test node execute the following, substituting the bearer token and fully qualified domain name of your Rancher endpoint with these environmental variables: export TOKEN=<your token here> export FQDN=<your Rancher fully qualified domain name here> Next execute the test using the following command: curl -s -i -N \\ --http1.1 \\ -H \"Connection: Upgrade\" \\ -H \"Upgrade: websocket\" \\ -H \"Sec-WebSocket-Key: SGVsbG8sIHdvcmxkIQ==\" \\ -H \"Sec-WebSocket-Version: 13\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Host: $FQDN\" \\ -k https://$FQDN/v3/subscribe If websockets work this will successfully connect to the Rancher server and print a steady stream of json output reflecting configuration items being sent from the server. In the event of a failed connection this should print a meaningful error you can act upon to get websockets working between your client and Rancher server. The below is an example of the output from the test upon a successfully established websocket: HTTP/1.1 101 Switching Protocols Date: Tue, 21 Jan 2020 04:54:05 GMT Connection: upgrade Server: openresty/1.15.8.1 Upgrade: websocket Sec-WebSocket-Accept: qGEgH3En71di5rrssAZTmtRTyFk= {\"name\":\"resource.change\",\"data\":{\"baseType\":\"listenConfig\",\"created\":\"2020-01-04T22:34:26Z\",\"createdTS\":1578177266000,\"creatorId\":null,\"enabled\":true,\"generatedCerts\":{\"local/10.42.0.7\":\"*CERT_CONTENTS_REDACTED*\"},\"id\":\"cli-config\",\"keySize\":0,\"knownIps\":[\"10.42.0.7\",\"10.42.0.8\"],\"labels\":{\"cattle.io/creator\":\"norman\"},\"links\":{\"remove\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"self\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"update\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\"},\"mode\":\"https\",\"tos\":\"auto\",\"type\":\"listenConfig\",\"uuid\":\"511129ca-aa2c-4d16-a8e5-2d77cb171d61\",\"version\":0} Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to test websocket connections to Rancher v2.x"},{"location":"000020189/#how-to-test-websocket-connections-to-rancher-v2x","text":"This document (000020189) is provided subject to the disclaimer at the end of this document.","title":"How to test websocket connections to Rancher v2.x"},{"location":"000020189/#situation","text":"","title":"Situation"},{"location":"000020189/#task","text":"Rancher depends heavily on websocket support for UI and CLI features within Rancher as well as managing and interacting with downstream clusters. This article provides a quick test to determine if websocket connections are working from a potential downstream node or client to the Rancher server cluster.","title":"Task"},{"location":"000020189/#pre-requisites","text":"A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster .","title":"Pre-requisites"},{"location":"000020189/#executing-the-test","text":"First you will need to create an API token to authenticate against Rancher. Start by logging into the Rancher UI. Once logged in, navigate to the API & Keys section by clicking the user icon in the top right of the pane, then click on the API & Keys menu item. Generate a new key by clicking the Add Key button, providing a name for the token and clicking Create. Copy the bearer token to a safe location. In a Linux shell from the desired test node execute the following, substituting the bearer token and fully qualified domain name of your Rancher endpoint with these environmental variables: export TOKEN=<your token here> export FQDN=<your Rancher fully qualified domain name here> Next execute the test using the following command: curl -s -i -N \\ --http1.1 \\ -H \"Connection: Upgrade\" \\ -H \"Upgrade: websocket\" \\ -H \"Sec-WebSocket-Key: SGVsbG8sIHdvcmxkIQ==\" \\ -H \"Sec-WebSocket-Version: 13\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -H \"Host: $FQDN\" \\ -k https://$FQDN/v3/subscribe If websockets work this will successfully connect to the Rancher server and print a steady stream of json output reflecting configuration items being sent from the server. In the event of a failed connection this should print a meaningful error you can act upon to get websockets working between your client and Rancher server. The below is an example of the output from the test upon a successfully established websocket: HTTP/1.1 101 Switching Protocols Date: Tue, 21 Jan 2020 04:54:05 GMT Connection: upgrade Server: openresty/1.15.8.1 Upgrade: websocket Sec-WebSocket-Accept: qGEgH3En71di5rrssAZTmtRTyFk= {\"name\":\"resource.change\",\"data\":{\"baseType\":\"listenConfig\",\"created\":\"2020-01-04T22:34:26Z\",\"createdTS\":1578177266000,\"creatorId\":null,\"enabled\":true,\"generatedCerts\":{\"local/10.42.0.7\":\"*CERT_CONTENTS_REDACTED*\"},\"id\":\"cli-config\",\"keySize\":0,\"knownIps\":[\"10.42.0.7\",\"10.42.0.8\"],\"labels\":{\"cattle.io/creator\":\"norman\"},\"links\":{\"remove\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"self\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"update\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\"},\"mode\":\"https\",\"tos\":\"auto\",\"type\":\"listenConfig\",\"uuid\":\"511129ca-aa2c-4d16-a8e5-2d77cb171d61\",\"version\":0}","title":"Executing the test"},{"location":"000020189/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020190/","text":"The Rancher v1.6 log collector script This document (000020190) is provided subject to the disclaimer at the end of this document. Situation Rancher v1.6 log collection Logs can be collected from a node within a Rancher v1.6 cluster using the Rancher v1.6 log collector script . The script needs to be downloaded and run directly on the host using the root user or using sudo, as follows: wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v1.6/logs-collector/rancher16_logs_collector.sh | sudo bash -s The output will be written to /tmp to a gziped tar file named <hostname>-<datetime>.tar.gz Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"The Rancher v1.6 log collector script"},{"location":"000020190/#the-rancher-v16-log-collector-script","text":"This document (000020190) is provided subject to the disclaimer at the end of this document.","title":"The Rancher v1.6 log collector script"},{"location":"000020190/#situation","text":"","title":"Situation"},{"location":"000020190/#rancher-v16-log-collection","text":"Logs can be collected from a node within a Rancher v1.6 cluster using the Rancher v1.6 log collector script . The script needs to be downloaded and run directly on the host using the root user or using sudo, as follows: wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v1.6/logs-collector/rancher16_logs_collector.sh | sudo bash -s The output will be written to /tmp to a gziped tar file named <hostname>-<datetime>.tar.gz","title":"Rancher v1.6 log collection"},{"location":"000020190/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020191/","text":"The Rancher v2.x Linux log collector script This document (000020191) is provided subject to the disclaimer at the end of this document. Situation Rancher v2.x Linux log collector Logs can be collected from a Linux node using the Rancher v2.x log collector script . Note: This script is intended to collect logs from Rancher Kubernetes Engine (RKE) CLI provisioned clusters, K3s clusters , RKE2 clusters , Rancher provisioned Custom , and Node Driver clusters. This script may not collect all necessary information when run on nodes in Hosted Kubernetes Provider clusters . The script needs to be downloaded and run directly on the node, using the root user or sudo. Output will be written to /tmp as a tar.gz archive named <hostname>-<date>.tar.gz , the default output directory can be changed with the -d flag. Download and run the script Download the script as: rancher2_logs_collector.sh Using wget : wget https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh Using curl : curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh Run the script: sudo bash rancher2_logs_collector.sh Optional: Download and run the script in one command curl -Ls rnch.io/rancher2_logs | sudo bash Note: This command requires curl to be installed, and internet access from the node. Options The available flags that can be passed to the script can be found in the Rancher v2.x log collector script README . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"The Rancher v2.x Linux log collector script"},{"location":"000020191/#the-rancher-v2x-linux-log-collector-script","text":"This document (000020191) is provided subject to the disclaimer at the end of this document.","title":"The Rancher v2.x Linux log collector script"},{"location":"000020191/#situation","text":"","title":"Situation"},{"location":"000020191/#rancher-v2x-linux-log-collector","text":"Logs can be collected from a Linux node using the Rancher v2.x log collector script . Note: This script is intended to collect logs from Rancher Kubernetes Engine (RKE) CLI provisioned clusters, K3s clusters , RKE2 clusters , Rancher provisioned Custom , and Node Driver clusters. This script may not collect all necessary information when run on nodes in Hosted Kubernetes Provider clusters . The script needs to be downloaded and run directly on the node, using the root user or sudo. Output will be written to /tmp as a tar.gz archive named <hostname>-<date>.tar.gz , the default output directory can be changed with the -d flag.","title":"Rancher v2.x Linux log collector"},{"location":"000020191/#download-and-run-the-script","text":"Download the script as: rancher2_logs_collector.sh Using wget : wget https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh Using curl : curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh Run the script: sudo bash rancher2_logs_collector.sh","title":"Download and run the script"},{"location":"000020191/#optional-download-and-run-the-script-in-one-command","text":"curl -Ls rnch.io/rancher2_logs | sudo bash Note: This command requires curl to be installed, and internet access from the node.","title":"Optional: Download and run the script in one command"},{"location":"000020191/#options","text":"The available flags that can be passed to the script can be found in the Rancher v2.x log collector script README .","title":"Options"},{"location":"000020191/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020192/","text":"The Rancher v2.x systems summary script This document (000020192) is provided subject to the disclaimer at the end of this document. Situation Rancher v2.x systems summary Understanding your cluster/node distribution on an on-going basis assists Rancher in sending you any prescriptive advisories related to scale and performance. System information can be collected from a Rancher v2.x server node using the Rancher v2.x systems summary script . The script needs to be downloaded and run directly on a host running a Rancher server container, either as a single node install or a Rancher Pod as part of a High Availability install. The script needs to be run by a user with access to the Docker socket or using sudo, as follows: wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sudo bash -s Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"The Rancher v2.x systems summary script"},{"location":"000020192/#the-rancher-v2x-systems-summary-script","text":"This document (000020192) is provided subject to the disclaimer at the end of this document.","title":"The Rancher v2.x systems summary script"},{"location":"000020192/#situation","text":"","title":"Situation"},{"location":"000020192/#rancher-v2x-systems-summary","text":"Understanding your cluster/node distribution on an on-going basis assists Rancher in sending you any prescriptive advisories related to scale and performance. System information can be collected from a Rancher v2.x server node using the Rancher v2.x systems summary script . The script needs to be downloaded and run directly on a host running a Rancher server container, either as a single node install or a Rancher Pod as part of a High Availability install. The script needs to be run by a user with access to the Docker socket or using sudo, as follows: wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sudo bash -s","title":"Rancher v2.x systems summary"},{"location":"000020192/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020193/","text":"What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster? This document (000020193) is provided subject to the disclaimer at the end of this document. Situation Question What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster? Pre-requisites Running Rancher v2.0.x - v2.3.x. Note, Kubernetes upgrades will be changing in v2.4.x, see Further Reading below. OR RKE CLI v0.2.x+ Answer Rancher, either through the UI or API, can be used to upgrade a Kubernetes cluster that was provisioned using the \"Custom\" option or on cloud infrastructure such as AWS EC2 or Azure. This can be accomplished by editing the cluster and selecting the desired Kubernetes version. Clusters provisioned with the RKE CLI can also be upgraded by editing the kubernetes_version key in the cluster YAML file. This will trigger an update of all the Kubernetes components in the order listed below: Etcd plane Each etcd container is updated, one node at a time. If the etcd version has not changed between versions of Kubernetes, no action is taken. The process consists of: Downloading etcd image Stopping and renaming old etcd container (backend datastore is preserved on host) Creating and starting new etcd container Running etcd health check Removing old etcd container For RKE CLI provisioned clusters, the etcd-rolling-snapshot container is also upgraded if a new version is available. Control plane Every Kubernetes update will require the control plane components to be updated. All control plane nodes are updated in parallel. The process consists of: Downloading hyperkube image, which is used by all control plane components. Stopping and renaming old kube-apiserver container Creating and starting new kube-apiserver container Running kube-apiserver health check Removing old kube-apiserver container Stopping and renaming old kube-controller-manager container Creating and starting new kube-controller-manager container Running kube-controller-manager health check Removing old kube-controller-manager container Stopping and renaming old kube-scheduler container Creating and starting new kube-scheduler container Running kube-scheduler health check Removing old kube-scheduler container Worker plane Every Kubernetes update will require the worker components to be updated. These components run on all nodes, including the control plane and etcd. Nodes are updating in parallel. The process consists of: Downloading hyperkube image (if not already present) Stopping and renaming old kubelet container Creating and starting new kubelet container Running kubelet health check Removing old kubelet container Stopping and renaming old kube-proxy container Creating and starting new kube-proxy container Running kube-proxy health check Removing old kube-proxy container Addons & user workloads Once Kubernetes etcd, control plane, and worker components have been updated, the latest manifests for addons are applied. This includes, but is not limited to KubeDNS/CoreDNS, Nginx Ingress, Metrics Server, and CNI plugin (Calico, Weave, Flannel, Canal). Depending on the manifest deltas and the upgrade strategy defined in the manifest, pods and their corresponding containers may or may not be removed and recreated. Please be aware that some of these addons are critical for your cluster to operator correctly and you may experience brief outages if these workloads are restarted. For example, when KubeDNS/CoreDNS is restarted, you could have issues resolving hostname to IP addresses. When the Nginx Ingress is restarted, layer 7 http/https traffic from outside your cluster to your workloads may get interrupted. When your CNI plugin is restarted on each node, the workloads running on the node may temporarily not be able to reach workloads running on other nodes. The best way to minimize outages or disruptions is to make sure you have proper fault tolerance in your cluster. The kubelet automatically destroys and recreates all user workload pods when the spec hash value is changed. This value will change for a pod if the Kubernetes upgrade involves any field changes in the pod manifest, such as a new field or the removal of a deprecated field. As a best practice, it's best to assume all your pods and containers will be destroyed and recreated during a Kubernetes upgrade. This is more likely to happen for major/minor releases and less likely for patch releases. Further Reading Upgrade refactor in v2.4: https://github.com/rancher/rancher/issues/23038 Kubeadm upgrades: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?"},{"location":"000020193/#what-is-the-process-performed-by-rancher-v2x-when-upgrading-a-rancher-managed-kubernetes-cluster","text":"This document (000020193) is provided subject to the disclaimer at the end of this document.","title":"What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?"},{"location":"000020193/#situation","text":"","title":"Situation"},{"location":"000020193/#question","text":"What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?","title":"Question"},{"location":"000020193/#pre-requisites","text":"Running Rancher v2.0.x - v2.3.x. Note, Kubernetes upgrades will be changing in v2.4.x, see Further Reading below. OR RKE CLI v0.2.x+","title":"Pre-requisites"},{"location":"000020193/#answer","text":"Rancher, either through the UI or API, can be used to upgrade a Kubernetes cluster that was provisioned using the \"Custom\" option or on cloud infrastructure such as AWS EC2 or Azure. This can be accomplished by editing the cluster and selecting the desired Kubernetes version. Clusters provisioned with the RKE CLI can also be upgraded by editing the kubernetes_version key in the cluster YAML file. This will trigger an update of all the Kubernetes components in the order listed below:","title":"Answer"},{"location":"000020193/#etcd-plane","text":"Each etcd container is updated, one node at a time. If the etcd version has not changed between versions of Kubernetes, no action is taken. The process consists of: Downloading etcd image Stopping and renaming old etcd container (backend datastore is preserved on host) Creating and starting new etcd container Running etcd health check Removing old etcd container For RKE CLI provisioned clusters, the etcd-rolling-snapshot container is also upgraded if a new version is available.","title":"Etcd plane"},{"location":"000020193/#control-plane","text":"Every Kubernetes update will require the control plane components to be updated. All control plane nodes are updated in parallel. The process consists of: Downloading hyperkube image, which is used by all control plane components. Stopping and renaming old kube-apiserver container Creating and starting new kube-apiserver container Running kube-apiserver health check Removing old kube-apiserver container Stopping and renaming old kube-controller-manager container Creating and starting new kube-controller-manager container Running kube-controller-manager health check Removing old kube-controller-manager container Stopping and renaming old kube-scheduler container Creating and starting new kube-scheduler container Running kube-scheduler health check Removing old kube-scheduler container","title":"Control plane"},{"location":"000020193/#worker-plane","text":"Every Kubernetes update will require the worker components to be updated. These components run on all nodes, including the control plane and etcd. Nodes are updating in parallel. The process consists of: Downloading hyperkube image (if not already present) Stopping and renaming old kubelet container Creating and starting new kubelet container Running kubelet health check Removing old kubelet container Stopping and renaming old kube-proxy container Creating and starting new kube-proxy container Running kube-proxy health check Removing old kube-proxy container","title":"Worker plane"},{"location":"000020193/#addons-user-workloads","text":"Once Kubernetes etcd, control plane, and worker components have been updated, the latest manifests for addons are applied. This includes, but is not limited to KubeDNS/CoreDNS, Nginx Ingress, Metrics Server, and CNI plugin (Calico, Weave, Flannel, Canal). Depending on the manifest deltas and the upgrade strategy defined in the manifest, pods and their corresponding containers may or may not be removed and recreated. Please be aware that some of these addons are critical for your cluster to operator correctly and you may experience brief outages if these workloads are restarted. For example, when KubeDNS/CoreDNS is restarted, you could have issues resolving hostname to IP addresses. When the Nginx Ingress is restarted, layer 7 http/https traffic from outside your cluster to your workloads may get interrupted. When your CNI plugin is restarted on each node, the workloads running on the node may temporarily not be able to reach workloads running on other nodes. The best way to minimize outages or disruptions is to make sure you have proper fault tolerance in your cluster. The kubelet automatically destroys and recreates all user workload pods when the spec hash value is changed. This value will change for a pod if the Kubernetes upgrade involves any field changes in the pod manifest, such as a new field or the removal of a deprecated field. As a best practice, it's best to assume all your pods and containers will be destroyed and recreated during a Kubernetes upgrade. This is more likely to happen for major/minor releases and less likely for patch releases.","title":"Addons &amp; user workloads"},{"location":"000020193/#further-reading","text":"Upgrade refactor in v2.4: https://github.com/rancher/rancher/issues/23038 Kubeadm upgrades: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/","title":"Further Reading"},{"location":"000020193/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020194/","text":"What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters? This document (000020194) is provided subject to the disclaimer at the end of this document. Situation Question What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters? Pre-requisites Running Rancher v2.x HA deployed using Helm. Answer The bulk of the Rancher HA installation and upgrade are performed by using Helm. The core piece of the Rancher Helm Chart is the Rancher deployment. Please note the following characteristics of this Helm Chart: Deployment is set to a replica of 3. This means Kubernetes will attempt to run and maintain three rancher pods. Deployment is set to do a rolling update with a max surge of 25% and max unavailability of 25%. This means: During an upgrade, pods are updated in chunks, not all at once. During an update, no more than 4 pods will be running at once During an update, no fewer than 2 pods will be available at once Deployment has an anti-affinity for the node's hostname. This means Kubernetes will attempt to place each pod on a separate host. For three pods and three hosts, that means one pod on each host. Rancher will also apply two other important manifests to the Rancher HA cluster as well as all managed clusters. These are described below: cattle-cluster-agent deployment Deployment is set to a replica of 1 Deployment is set to do a rolling update with a max surge of 25% and a max unavailability of 25%. See Rancher's deployment description above for the behavior of these settings. cattle-node-agent daemonset Daemonset will deploy one agent per node Daemonset is set to a rolling update with max unavailable of 1 pod. That means during an update, one pod is updated at a time. Given the information above on how the manifests are defined, below is the expected sequence of events during a Rancher upgrade: Rancher HA cluster A new rancher pod is created An old rancher pod is terminated A new second rancher pod is created A second old rancher pod is terminated A new third rancher pod is created A third old rancher pod is terminated The latest versions of the cattle-cluster-agent and cattle-node-agent manifests are updated and deployed on the cluster. These deployments are triggered in parallel and will result in a new cattle-cluster-agent and new cattle-node-agents running on the cluster. Downstream clusters Once Rancher is upgraded, Rancher will check each cluster it manages to make sure the cattle-cluster-agent and cattle-node-agents are up to date. If the cluster is not in a \"Provisioning\" state, meaning another cluster update is in progress, it will deploy the latest cattle-cluster-agent and cattle-node-agent manifests into the cluster. All managed clusters are updated in parallel and not sequentially. Other workloads running in the cluster should not be impacted. Further Reading Kubernetes deployments - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ Kubernetes daemonsets - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/ Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?"},{"location":"000020194/#what-is-the-process-performed-during-a-rancher-v2x-upgrade-and-what-is-the-impact-to-the-managed-downstream-kubernetes-clusters","text":"This document (000020194) is provided subject to the disclaimer at the end of this document.","title":"What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?"},{"location":"000020194/#situation","text":"","title":"Situation"},{"location":"000020194/#question","text":"What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?","title":"Question"},{"location":"000020194/#pre-requisites","text":"Running Rancher v2.x HA deployed using Helm.","title":"Pre-requisites"},{"location":"000020194/#answer","text":"The bulk of the Rancher HA installation and upgrade are performed by using Helm. The core piece of the Rancher Helm Chart is the Rancher deployment. Please note the following characteristics of this Helm Chart: Deployment is set to a replica of 3. This means Kubernetes will attempt to run and maintain three rancher pods. Deployment is set to do a rolling update with a max surge of 25% and max unavailability of 25%. This means: During an upgrade, pods are updated in chunks, not all at once. During an update, no more than 4 pods will be running at once During an update, no fewer than 2 pods will be available at once Deployment has an anti-affinity for the node's hostname. This means Kubernetes will attempt to place each pod on a separate host. For three pods and three hosts, that means one pod on each host. Rancher will also apply two other important manifests to the Rancher HA cluster as well as all managed clusters. These are described below:","title":"Answer"},{"location":"000020194/#cattle-cluster-agent-deployment","text":"Deployment is set to a replica of 1 Deployment is set to do a rolling update with a max surge of 25% and a max unavailability of 25%. See Rancher's deployment description above for the behavior of these settings.","title":"cattle-cluster-agent deployment"},{"location":"000020194/#cattle-node-agent-daemonset","text":"Daemonset will deploy one agent per node Daemonset is set to a rolling update with max unavailable of 1 pod. That means during an update, one pod is updated at a time. Given the information above on how the manifests are defined, below is the expected sequence of events during a Rancher upgrade:","title":"cattle-node-agent daemonset"},{"location":"000020194/#rancher-ha-cluster","text":"A new rancher pod is created An old rancher pod is terminated A new second rancher pod is created A second old rancher pod is terminated A new third rancher pod is created A third old rancher pod is terminated The latest versions of the cattle-cluster-agent and cattle-node-agent manifests are updated and deployed on the cluster. These deployments are triggered in parallel and will result in a new cattle-cluster-agent and new cattle-node-agents running on the cluster.","title":"Rancher HA cluster"},{"location":"000020194/#downstream-clusters","text":"Once Rancher is upgraded, Rancher will check each cluster it manages to make sure the cattle-cluster-agent and cattle-node-agents are up to date. If the cluster is not in a \"Provisioning\" state, meaning another cluster update is in progress, it will deploy the latest cattle-cluster-agent and cattle-node-agent manifests into the cluster. All managed clusters are updated in parallel and not sequentially. Other workloads running in the cluster should not be impacted.","title":"Downstream clusters"},{"location":"000020194/#further-reading","text":"Kubernetes deployments - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ Kubernetes daemonsets - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/","title":"Further Reading"},{"location":"000020194/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020195/","text":"How to rollback the Kubernetes version of a Rancher v2.x provisioned cluster This document (000020195) is provided subject to the disclaimer at the end of this document. Situation Task This article details how to rollback the Kubernetes version of a Rancher v2.x provisioned cluster. Important Note: A Kubernetes Cluster Rollback will most definitely cause downtime in the cluster, as you are restoring a snapshot from before the upgrade and the cluster will have to reconcile state. Pre-requisites In order to rollback your Kubernetes cluster version upgrade, you need to have first taken an etcd snapshot from before the upgrade. You should keep the reference to the snapshot name that was created as your \"pre-upgrade\" snapshot. In my cluster which has cluster ID: c-q8st7 , my snapshot name was c-q8st7-ml-qdxdh . Our example upgrade is from v1.14.9-rancher1-2 to v1.15.7-rancher1-1 . Rollback operation In order to rollback, you must: Edit Cluster Edit as YAML Set kubernetes_version back to v1.14.9-rancher1-2 (or whatever your desired restore version is) Find the restore key in the YAML. You will need to update the following configuration: rancher_kubernetes_engine_config: restore: restore: false You'll want to closely model the following: rancher_kubernetes_engine_config: restore: restore: true snapshot_name: \"c-q8st7:c-q8st7-ml-qdxdh\" Note the snapshot_name has the cluster ID prefixed to it with a : . Finally, you can save the cluster, and observe the snapshot restore + K8s version rollback occur. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to rollback the Kubernetes version of a Rancher v2.x provisioned cluster"},{"location":"000020195/#how-to-rollback-the-kubernetes-version-of-a-rancher-v2x-provisioned-cluster","text":"This document (000020195) is provided subject to the disclaimer at the end of this document.","title":"How to rollback the Kubernetes version of a Rancher v2.x provisioned cluster"},{"location":"000020195/#situation","text":"","title":"Situation"},{"location":"000020195/#task","text":"This article details how to rollback the Kubernetes version of a Rancher v2.x provisioned cluster.","title":"Task"},{"location":"000020195/#important-note","text":"A Kubernetes Cluster Rollback will most definitely cause downtime in the cluster, as you are restoring a snapshot from before the upgrade and the cluster will have to reconcile state.","title":"Important Note:"},{"location":"000020195/#pre-requisites","text":"In order to rollback your Kubernetes cluster version upgrade, you need to have first taken an etcd snapshot from before the upgrade. You should keep the reference to the snapshot name that was created as your \"pre-upgrade\" snapshot. In my cluster which has cluster ID: c-q8st7 , my snapshot name was c-q8st7-ml-qdxdh . Our example upgrade is from v1.14.9-rancher1-2 to v1.15.7-rancher1-1 .","title":"Pre-requisites"},{"location":"000020195/#rollback-operation","text":"In order to rollback, you must: Edit Cluster Edit as YAML Set kubernetes_version back to v1.14.9-rancher1-2 (or whatever your desired restore version is) Find the restore key in the YAML. You will need to update the following configuration: rancher_kubernetes_engine_config: restore: restore: false You'll want to closely model the following: rancher_kubernetes_engine_config: restore: restore: true snapshot_name: \"c-q8st7:c-q8st7-ml-qdxdh\" Note the snapshot_name has the cluster ID prefixed to it with a : . Finally, you can save the cluster, and observe the snapshot restore + K8s version rollback occur.","title":"Rollback operation"},{"location":"000020195/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020196/","text":"HTTP 401 \"clusterID does not match\" error using cluster-scoped Rancher API token in Rancher v2.x This document (000020196) is provided subject to the disclaimer at the end of this document. Situation Issue When attempting to perform operations against the Rancher v2.x API, with a cluster-scoped API token, you receive a HTTP 401 response code with a body of the following format: { \"type\":\"error\", \"status\":\"401\", \"message\":\"clusterID does not match\" } Pre-requisites A Rancher v2.x instance A cluster-scoped Rancher API token Root cause The primary purpose of cluster-scoped API tokens is to permit access to the Kubernetes API for a specific cluster via Rancher, i.e. via the endpoint https://<rancher_url>/k8s/clusters/<cluster_id> for the matching cluster. Cluster-scoped tokens can be used to interact directly with the Kubernetes API of clusters configured with an Authorized Cluster Endpoint . In addition, a cluster-scoped token also works for resources under the Rancher v3 API endpoint for that cluster, at https://<rancher_url>/v3/clusters/<cluster_id> . The token is not valid for the other available API endpoints, nor for other clusters. Attempts to perform API operations on other clusters or endpoints with a cluster-scoped token will result in the HTTP 401 \"clusterID does not match\" error. Resolution Only use a cluster-scoped API token where you wish to restrict usage of the token to the Kubernetes API for that cluster, or the Rancher v3 cluster endpoint. To permit access to other API endpoints, or to use a token for API access to multiple clusters, create a Rancher API token that is not cluster-scoped. Further reading You can read more on the Rancher v2.x API within the API documentation . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"HTTP 401 \"clusterID does not match\" error using cluster-scoped Rancher API token in Rancher v2.x"},{"location":"000020196/#http-401-clusterid-does-not-match-error-using-cluster-scoped-rancher-api-token-in-rancher-v2x","text":"This document (000020196) is provided subject to the disclaimer at the end of this document.","title":"HTTP 401 \"clusterID does not match\" error using cluster-scoped Rancher API token in Rancher v2.x"},{"location":"000020196/#situation","text":"","title":"Situation"},{"location":"000020196/#issue","text":"When attempting to perform operations against the Rancher v2.x API, with a cluster-scoped API token, you receive a HTTP 401 response code with a body of the following format: { \"type\":\"error\", \"status\":\"401\", \"message\":\"clusterID does not match\" }","title":"Issue"},{"location":"000020196/#pre-requisites","text":"A Rancher v2.x instance A cluster-scoped Rancher API token","title":"Pre-requisites"},{"location":"000020196/#root-cause","text":"The primary purpose of cluster-scoped API tokens is to permit access to the Kubernetes API for a specific cluster via Rancher, i.e. via the endpoint https://<rancher_url>/k8s/clusters/<cluster_id> for the matching cluster. Cluster-scoped tokens can be used to interact directly with the Kubernetes API of clusters configured with an Authorized Cluster Endpoint . In addition, a cluster-scoped token also works for resources under the Rancher v3 API endpoint for that cluster, at https://<rancher_url>/v3/clusters/<cluster_id> . The token is not valid for the other available API endpoints, nor for other clusters. Attempts to perform API operations on other clusters or endpoints with a cluster-scoped token will result in the HTTP 401 \"clusterID does not match\" error.","title":"Root cause"},{"location":"000020196/#resolution","text":"Only use a cluster-scoped API token where you wish to restrict usage of the token to the Kubernetes API for that cluster, or the Rancher v3 cluster endpoint. To permit access to other API endpoints, or to use a token for API access to multiple clusters, create a Rancher API token that is not cluster-scoped.","title":"Resolution"},{"location":"000020196/#further-reading","text":"You can read more on the Rancher v2.x API within the API documentation .","title":"Further reading"},{"location":"000020196/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020197/","text":"Provisioning of Kubernetes clusters in Rancher v2.x, prior to v2.3.3, using nodes in an infrastructure provider, does not respect NO_PROXY entries in CIDR format This document (000020197) is provided subject to the disclaimer at the end of this document. Situation Issue Attempting to provision a Kubernetes cluster with the vSphere node-driver , in a Rancher v2.x environment, prior to v2.3.3, using a HTTP proxy configuration results in an error of the following format: Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect In addition, the Rancher logs show an error message of the following format: ... 2019/12/06 10:23:51 [INFO] [node-controller-docker-machine] (vsphere-all1) Waiting for VMware Tools to come online... 2019/12/06 10:25:49 [INFO] [node-controller-docker-machine] (vsphere-all1) Provisioning certs and ssh keys... 2019/12/06 10:27:35 http: TLS handshake error from 127.0.0.1:41746: EOF 2019/12/06 10:28:03 [INFO] [node-controller-docker-machine] The default lines below are for a sh/bash shell, you can specify the shell you're using, with the --shell flag. 2019/12/06 10:28:03 [INFO] [node-controller-docker-machine] 2019/12/06 10:28:04 [INFO] Generating and uploading node config vsphere-all1 2019/12/06 10:28:04 [ERROR] NodeController c-f6xbs/m-fsl6t [node-controller] failed with : Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect ... Pre-requisites A Rancher v2.x instance, prior to Rancher v2.3.3. A HTTP Proxy configured on Rancher, per the documentation for a single node or High Availability (HA) install of Rancher, in which the vSphere datacenter ESXi hosts are not reachable via the proxy. A Rancher provisioned Kubernetes cluster, using the the vSphere node-driver . The IP Range containing the ESXi hosts within the vSphere datacenter configured in CIDR notation within the Rancher NO_PROXY configuration. Root cause This issue was caused by the Go version used to build the docker-machine driver that provides the Rancher node driver capabilities, including the vSphere node driver. Support for NO_PROXY entries in CIDR notation was introduced in Go v1.10.x; however, the docker-machine version in Rancher v2.x, prior to v2.3.3, was built using an earlier version of Go. As a result, NO_PROXY entries in CIDR notation did not take effect during cluster provisioning via node drivers, even though these same NO_PROXY entries were observed by the Rancher server itself, built with a later version of Go. Workaround To workaround this issue in Rancher v2.x versions before v2.3.3, you should ensure that the vSphere server address, and all ESXi hosts within the vSphere datacenter in which you are provisioning the cluster, are listed as individual IPs within the Rancher NO_PROXY configuration. Resolution This issue was tracked in Rancher GitHub issue #21674 and a fix, bumping the Go version of the docker-machine driver to v1.12.9, was released in Rancher v2.3.3. Users can therefore upgrade to Rancher v2.3.3, or above, to resolve this issue. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Provisioning of Kubernetes clusters in Rancher v2.x, prior to v2.3.3, using nodes in an infrastructure provider, does not respect NO\\_PROXY entries in CIDR format"},{"location":"000020197/#provisioning-of-kubernetes-clusters-in-rancher-v2x-prior-to-v233-using-nodes-in-an-infrastructure-provider-does-not-respect-no_proxy-entries-in-cidr-format","text":"This document (000020197) is provided subject to the disclaimer at the end of this document.","title":"Provisioning of Kubernetes clusters in Rancher v2.x, prior to v2.3.3, using nodes in an infrastructure provider, does not respect NO_PROXY entries in CIDR format"},{"location":"000020197/#situation","text":"","title":"Situation"},{"location":"000020197/#issue","text":"Attempting to provision a Kubernetes cluster with the vSphere node-driver , in a Rancher v2.x environment, prior to v2.3.3, using a HTTP proxy configuration results in an error of the following format: Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect In addition, the Rancher logs show an error message of the following format: ... 2019/12/06 10:23:51 [INFO] [node-controller-docker-machine] (vsphere-all1) Waiting for VMware Tools to come online... 2019/12/06 10:25:49 [INFO] [node-controller-docker-machine] (vsphere-all1) Provisioning certs and ssh keys... 2019/12/06 10:27:35 http: TLS handshake error from 127.0.0.1:41746: EOF 2019/12/06 10:28:03 [INFO] [node-controller-docker-machine] The default lines below are for a sh/bash shell, you can specify the shell you're using, with the --shell flag. 2019/12/06 10:28:03 [INFO] [node-controller-docker-machine] 2019/12/06 10:28:04 [INFO] Generating and uploading node config vsphere-all1 2019/12/06 10:28:04 [ERROR] NodeController c-f6xbs/m-fsl6t [node-controller] failed with : Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect ...","title":"Issue"},{"location":"000020197/#pre-requisites","text":"A Rancher v2.x instance, prior to Rancher v2.3.3. A HTTP Proxy configured on Rancher, per the documentation for a single node or High Availability (HA) install of Rancher, in which the vSphere datacenter ESXi hosts are not reachable via the proxy. A Rancher provisioned Kubernetes cluster, using the the vSphere node-driver . The IP Range containing the ESXi hosts within the vSphere datacenter configured in CIDR notation within the Rancher NO_PROXY configuration.","title":"Pre-requisites"},{"location":"000020197/#root-cause","text":"This issue was caused by the Go version used to build the docker-machine driver that provides the Rancher node driver capabilities, including the vSphere node driver. Support for NO_PROXY entries in CIDR notation was introduced in Go v1.10.x; however, the docker-machine version in Rancher v2.x, prior to v2.3.3, was built using an earlier version of Go. As a result, NO_PROXY entries in CIDR notation did not take effect during cluster provisioning via node drivers, even though these same NO_PROXY entries were observed by the Rancher server itself, built with a later version of Go.","title":"Root cause"},{"location":"000020197/#workaround","text":"To workaround this issue in Rancher v2.x versions before v2.3.3, you should ensure that the vSphere server address, and all ESXi hosts within the vSphere datacenter in which you are provisioning the cluster, are listed as individual IPs within the Rancher NO_PROXY configuration.","title":"Workaround"},{"location":"000020197/#resolution","text":"This issue was tracked in Rancher GitHub issue #21674 and a fix, bumping the Go version of the docker-machine driver to v1.12.9, was released in Rancher v2.3.3. Users can therefore upgrade to Rancher v2.3.3, or above, to resolve this issue.","title":"Resolution"},{"location":"000020197/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020198/","text":"Launching kubectl for cluster within Rancher UI fails in a cluster after following the CIS Benchmark Hardening Guide for Kubernetes This document (000020198) is provided subject to the disclaimer at the end of this document. Situation Issue Attempting to launch kubectl in the Rancher v2.x UI, for a cluster upon which the Rancher CIS Hardening Guide has been applied, results in a Closed Code: 1006 message. Further, using the browser developer tools to inspect requests when opening this page reveals the API request to initiate the connection (https:///v3/clusters/?shell=true) receiving a HTTP 403 response. Pre-requisites An RKE CLI or Rancher v2.x launched Kubernetes cluster, with the Rancher v2.1.x, v2.2.x or v2.3.x CIS Hardening Guide applied. Root cause This behaviour is caused by CIS Control 1.1.12, which specifies that the DenyEscalatingExec Admission Controller should be enabled on the Kubernetes API Server. The terminal for the Rancher UI is provided by exec'ing into a cattle-node-agent Pod, whilst Pods within this DaemonSet run in Privileged mode. As a result the exec to open the terminal session is denied by the DenyEscalatingExec Admission Controller. Workaround You can workaround the issue by removing DenyEscalatingExec from the list of enable-admission-plugins in extra_args for the kube-api service. Resolution This issue is tracked in the Rancher GitHub issue #19439 . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Launching kubectl for cluster within Rancher UI fails in a cluster after following the CIS Benchmark Hardening Guide for Kubernetes"},{"location":"000020198/#launching-kubectl-for-cluster-within-rancher-ui-fails-in-a-cluster-after-following-the-cis-benchmark-hardening-guide-for-kubernetes","text":"This document (000020198) is provided subject to the disclaimer at the end of this document.","title":"Launching kubectl for cluster within Rancher UI fails in a cluster after following the CIS Benchmark Hardening Guide for Kubernetes"},{"location":"000020198/#situation","text":"","title":"Situation"},{"location":"000020198/#issue","text":"Attempting to launch kubectl in the Rancher v2.x UI, for a cluster upon which the Rancher CIS Hardening Guide has been applied, results in a Closed Code: 1006 message. Further, using the browser developer tools to inspect requests when opening this page reveals the API request to initiate the connection (https:///v3/clusters/?shell=true) receiving a HTTP 403 response.","title":"Issue"},{"location":"000020198/#pre-requisites","text":"An RKE CLI or Rancher v2.x launched Kubernetes cluster, with the Rancher v2.1.x, v2.2.x or v2.3.x CIS Hardening Guide applied.","title":"Pre-requisites"},{"location":"000020198/#root-cause","text":"This behaviour is caused by CIS Control 1.1.12, which specifies that the DenyEscalatingExec Admission Controller should be enabled on the Kubernetes API Server. The terminal for the Rancher UI is provided by exec'ing into a cattle-node-agent Pod, whilst Pods within this DaemonSet run in Privileged mode. As a result the exec to open the terminal session is denied by the DenyEscalatingExec Admission Controller.","title":"Root cause"},{"location":"000020198/#workaround","text":"You can workaround the issue by removing DenyEscalatingExec from the list of enable-admission-plugins in extra_args for the kube-api service.","title":"Workaround"},{"location":"000020198/#resolution","text":"This issue is tracked in the Rancher GitHub issue #19439 .","title":"Resolution"},{"location":"000020198/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020199/","text":"How to increase the log level of Kubernetes components in an RKE CLI or Rancher provisioned Kubernetes cluster This document (000020199) is provided subject to the disclaimer at the end of this document. Situation Task When troubleshooting an issue with an RKE CLI or Rancher provisioned Kubernetes cluster, it may be helpful to increase the verbosity of logging on one or more of the Kubernetes components, above the default level. This article details the process of increasing logging on both those components that use the Kubernetes hyperkube image (kubelet, kube-apiserver, kube-controller-manager, kube-scheduler, kube-proxy) as well as the etcd component. Pre-requisites A Kubernetes cluster provisioned by the RKE CLI or Rancher v2.x Resolution Kubernetes API Server, Controller Manager, Scheduler, Kube Proxy and Kubelet The Kubernetes core components, which run using the Kubernetes hyperkube image, will log ERROR, WARNING and INFO messages. The verbosity of the INFO level log output is controlled by the --v flag, which is set to an integer from 0 to 9. In an RKE CLI or Rancher launched Kubernetes cluster, the --v flag is configured to 2 by default. At this level, the components will log useful steady state information about the service and important log messages that may correlate to significant changes in the system. In order to troubleshoot an issue, it may be useful to increase the verbosity flag to one of the following: VerbosityDescription--v=3Extended information about changes.--v=4Debug level verbosity.--v=6Display requested resources.--v=7Display HTTP request headers.--v=8Display HTTP request contents.--v=9Display HTTP request contents without truncation of contents. Update the --v flag in an RKE CLI launched cluster First set the --v flag for the desired components within the cluster.yml . For each of the services you wish to change the verbosity on, you should add an extra_args option with v: \"<value>\" in the services block, per the example below. The appropriate name for each service within this block can be found within the RKE documentation . N.B. Please see the separate section below for updating the log verbosity of the etcd component services: kube-api: extra_args: v: '9' Having set the flag in the cluster.yml, run rke up --config cluster.yml to update the cluster with the new configuration. Update the --v flag in a Rancher launched cluster Navigate to the cluster within the Rancher UI and click Edit Cluster , then Edit as YAML . For each of the services you wish to change the verbosity on, you should add an extra_args option with v: \"<value>\" in the services block of the cluster, per the example below. N.B. Please see the separate section below for updating the log verbosity of the etcd component. services: kube-api: extra_args: v: '9' The appropriate name for each service within this block can be found within the RKE documentation . Having set the verbosity flag, click Save at the bottom of the page, to update the cluster. etcd The etcd component is configured to log at an INFO level by default, in an RKE CLI or Rancher launched Kubernetes cluster, but this can be set to DEBUG level by setting the --debug=true flag. Update etcd verbosity in an RKE CLI launched cluster First set the --debug=true flag, within the cluster.yml cluster configuration file, under extra_args for the etcd service, per the following example: services: etcd: extra_args: debug: 'true' Having set the flag in the cluster.yml, run rke up --config cluster.yml to update the cluster with the new configuration. Update etcd verbosity in a Rancher launched cluster Navigate to the cluster within the Rancher UI and click Edit Cluster , then Edit as YAML . Set the --debug=true flag under extra_args , for the etcd service, per the following example: services: etcd: extra_args: debug: 'true' Having set the debug flag, click Save at the bottom of the page, to update the cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to increase the log level of Kubernetes components in an RKE CLI or Rancher provisioned Kubernetes cluster"},{"location":"000020199/#how-to-increase-the-log-level-of-kubernetes-components-in-an-rke-cli-or-rancher-provisioned-kubernetes-cluster","text":"This document (000020199) is provided subject to the disclaimer at the end of this document.","title":"How to increase the log level of Kubernetes components in an RKE CLI or Rancher provisioned Kubernetes cluster"},{"location":"000020199/#situation","text":"","title":"Situation"},{"location":"000020199/#task","text":"When troubleshooting an issue with an RKE CLI or Rancher provisioned Kubernetes cluster, it may be helpful to increase the verbosity of logging on one or more of the Kubernetes components, above the default level. This article details the process of increasing logging on both those components that use the Kubernetes hyperkube image (kubelet, kube-apiserver, kube-controller-manager, kube-scheduler, kube-proxy) as well as the etcd component.","title":"Task"},{"location":"000020199/#pre-requisites","text":"A Kubernetes cluster provisioned by the RKE CLI or Rancher v2.x","title":"Pre-requisites"},{"location":"000020199/#resolution","text":"","title":"Resolution"},{"location":"000020199/#kubernetes-api-server-controller-manager-scheduler-kube-proxy-and-kubelet","text":"The Kubernetes core components, which run using the Kubernetes hyperkube image, will log ERROR, WARNING and INFO messages. The verbosity of the INFO level log output is controlled by the --v flag, which is set to an integer from 0 to 9. In an RKE CLI or Rancher launched Kubernetes cluster, the --v flag is configured to 2 by default. At this level, the components will log useful steady state information about the service and important log messages that may correlate to significant changes in the system. In order to troubleshoot an issue, it may be useful to increase the verbosity flag to one of the following: VerbosityDescription--v=3Extended information about changes.--v=4Debug level verbosity.--v=6Display requested resources.--v=7Display HTTP request headers.--v=8Display HTTP request contents.--v=9Display HTTP request contents without truncation of contents.","title":"Kubernetes API Server, Controller Manager, Scheduler, Kube Proxy and Kubelet"},{"location":"000020199/#update-the-v-flag-in-an-rke-cli-launched-cluster","text":"First set the --v flag for the desired components within the cluster.yml . For each of the services you wish to change the verbosity on, you should add an extra_args option with v: \"<value>\" in the services block, per the example below. The appropriate name for each service within this block can be found within the RKE documentation . N.B. Please see the separate section below for updating the log verbosity of the etcd component services: kube-api: extra_args: v: '9' Having set the flag in the cluster.yml, run rke up --config cluster.yml to update the cluster with the new configuration.","title":"Update the --v flag in an RKE CLI launched cluster"},{"location":"000020199/#update-the-v-flag-in-a-rancher-launched-cluster","text":"Navigate to the cluster within the Rancher UI and click Edit Cluster , then Edit as YAML . For each of the services you wish to change the verbosity on, you should add an extra_args option with v: \"<value>\" in the services block of the cluster, per the example below. N.B. Please see the separate section below for updating the log verbosity of the etcd component. services: kube-api: extra_args: v: '9' The appropriate name for each service within this block can be found within the RKE documentation . Having set the verbosity flag, click Save at the bottom of the page, to update the cluster.","title":"Update the --v flag in a Rancher launched cluster"},{"location":"000020199/#etcd","text":"The etcd component is configured to log at an INFO level by default, in an RKE CLI or Rancher launched Kubernetes cluster, but this can be set to DEBUG level by setting the --debug=true flag.","title":"etcd"},{"location":"000020199/#update-etcd-verbosity-in-an-rke-cli-launched-cluster","text":"First set the --debug=true flag, within the cluster.yml cluster configuration file, under extra_args for the etcd service, per the following example: services: etcd: extra_args: debug: 'true' Having set the flag in the cluster.yml, run rke up --config cluster.yml to update the cluster with the new configuration.","title":"Update etcd verbosity in an RKE CLI launched cluster"},{"location":"000020199/#update-etcd-verbosity-in-a-rancher-launched-cluster","text":"Navigate to the cluster within the Rancher UI and click Edit Cluster , then Edit as YAML . Set the --debug=true flag under extra_args , for the etcd service, per the following example: services: etcd: extra_args: debug: 'true' Having set the debug flag, click Save at the bottom of the page, to update the cluster.","title":"Update etcd verbosity in a Rancher launched cluster"},{"location":"000020199/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020200/","text":"In Rancher v2.x, prior to v2.3, nodes in Rancher provisioned clusters deleted via Kubernetes, instead of via Rancher, remain present in Rancher in an 'unavailable' state This document (000020200) is provided subject to the disclaimer at the end of this document. Situation Issue In Rancher v2.x, prior to v2.3, if a node is deleted via Kubernetes, rather than Rancher itself - i.e. via kubectl delete node or another process connecting to the Kubernetes API, such as the use of the cluster-autoscaler - the node will be removed from the Kubernetes cluster, but still be present according to Rancher, remaining in an 'unavailable' state. Kubernetes scheduling and workloads will perform as expected for the removal of the node, as the node is correctly removed from the Kubernetes cluster. However, the view in Rancher will continue to show the node as 'unavailable' until it is manually deleted from within Rancher too. Pre-requisites A Rancher v2.x provisioned Kubernetes cluster, prior to v2.3, using either custom nodes or nodes hosted in an infrastructure provider . Workaround To remove nodes, in a Rancher v2.x provisioned cluster, that have been deleted in Kubernetes, and are no longer present in the output of kubectl get nodes , but remain in Rancher in an 'unavailable' state, you can delete these from within the node list for the cluster within the Rancher UI. Resolution This was tracked in Rancher GitHub issue #14184 and has been resolved since the release of Rancher v2.3. Where a node is deleted via Kubernetes, in Rancher v2.3 and above, this is detected by Rancher and the cluster is reconciled by Rancher to reflect the removal. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"In Rancher v2.x, prior to v2.3, nodes in Rancher provisioned clusters deleted via Kubernetes, instead of via Rancher, remain present in Rancher in an 'unavailable' state"},{"location":"000020200/#in-rancher-v2x-prior-to-v23-nodes-in-rancher-provisioned-clusters-deleted-via-kubernetes-instead-of-via-rancher-remain-present-in-rancher-in-an-unavailable-state","text":"This document (000020200) is provided subject to the disclaimer at the end of this document.","title":"In Rancher v2.x, prior to v2.3, nodes in Rancher provisioned clusters deleted via Kubernetes, instead of via Rancher, remain present in Rancher in an 'unavailable' state"},{"location":"000020200/#situation","text":"","title":"Situation"},{"location":"000020200/#issue","text":"In Rancher v2.x, prior to v2.3, if a node is deleted via Kubernetes, rather than Rancher itself - i.e. via kubectl delete node or another process connecting to the Kubernetes API, such as the use of the cluster-autoscaler - the node will be removed from the Kubernetes cluster, but still be present according to Rancher, remaining in an 'unavailable' state. Kubernetes scheduling and workloads will perform as expected for the removal of the node, as the node is correctly removed from the Kubernetes cluster. However, the view in Rancher will continue to show the node as 'unavailable' until it is manually deleted from within Rancher too.","title":"Issue"},{"location":"000020200/#pre-requisites","text":"A Rancher v2.x provisioned Kubernetes cluster, prior to v2.3, using either custom nodes or nodes hosted in an infrastructure provider .","title":"Pre-requisites"},{"location":"000020200/#workaround","text":"To remove nodes, in a Rancher v2.x provisioned cluster, that have been deleted in Kubernetes, and are no longer present in the output of kubectl get nodes , but remain in Rancher in an 'unavailable' state, you can delete these from within the node list for the cluster within the Rancher UI.","title":"Workaround"},{"location":"000020200/#resolution","text":"This was tracked in Rancher GitHub issue #14184 and has been resolved since the release of Rancher v2.3. Where a node is deleted via Kubernetes, in Rancher v2.3 and above, this is detected by Rancher and the cluster is reconciled by Rancher to reflect the removal.","title":"Resolution"},{"location":"000020200/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020201/","text":"\"log unreadable. It is excluded and would be examined next time.\" warning messages, for kubelet and kube-proxy, in rancher-logging-fluentd Pod logs of worker nodes This document (000020201) is provided subject to the disclaimer at the end of this document. Situation Issue In a Rancher v2.x provisioned Kubernetes cluster, with Rancher Cluster Logging configured, the rancher-logging-fluentd Pod logs on Linux worker role only nodes show warning messages of the following format: 2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kubelet_5c47838dd4af749a7a0d1c457b04a6d7b905e680157718063c8e5d9eb61268fa.log unreadable. It is excluded and would be examined next time. 2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kube-proxy_d2beb2e667eefbd6d95355082af4bc61c367fc4c220d9f1d165d15a8c8be2ab1.log unreadable. It is excluded and would be examined next time. The output of ls /var/lib/rancher/rke/log/ on affected workers shows that these files referenced in the warning log messages are broken symlinks. In addition, docker ps output shows the container ID for the currently running kubelet and kube-proxy containers does not match the IDs in these filenames. Pre-requisites A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider . Rancher Cluster Logging configured on the cluster, with the 'Include System Log' option set. Worker role only nodes in the cluster. Root cause These warning level messages in the rancher-logging-fluentd Pod are the result of the issue tracked in Rancher GitHub issue #22549 . The container log symlinks in /var/lib/rancher/rke/log/ for cluster component containers ( kubelet , kube-proxy , nginx-proxy ) on worker nodes in Rancher launched clusters are not cleaned up when these components are re-created, i.e. due to a Kubernetes version upgrade, or other configuration update for these components. As a result these broken symlinks persist and cause the log unreadable warning messages when the rancher-logging-fluentd Pod attempts to parse files in the /var/lib/rancher/rke/log/ directory. This warning message itself is harmless and can be ignored. Resolution The request to handle automatic clean-up of these log symlinks on worker nodes, in Rancher provisioned clusters, is tracked in Rancher GitHub issue #22549 . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\"log unreadable. It is excluded and would be examined next time.\" warning messages, for kubelet and kube-proxy, in rancher-logging-fluentd Pod logs of worker nodes"},{"location":"000020201/#log-unreadable-it-is-excluded-and-would-be-examined-next-time-warning-messages-for-kubelet-and-kube-proxy-in-rancher-logging-fluentd-pod-logs-of-worker-nodes","text":"This document (000020201) is provided subject to the disclaimer at the end of this document.","title":"\"log unreadable. It is excluded and would be examined next time.\" warning messages, for kubelet and kube-proxy, in rancher-logging-fluentd Pod logs of worker nodes"},{"location":"000020201/#situation","text":"","title":"Situation"},{"location":"000020201/#issue","text":"In a Rancher v2.x provisioned Kubernetes cluster, with Rancher Cluster Logging configured, the rancher-logging-fluentd Pod logs on Linux worker role only nodes show warning messages of the following format: 2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kubelet_5c47838dd4af749a7a0d1c457b04a6d7b905e680157718063c8e5d9eb61268fa.log unreadable. It is excluded and would be examined next time. 2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kube-proxy_d2beb2e667eefbd6d95355082af4bc61c367fc4c220d9f1d165d15a8c8be2ab1.log unreadable. It is excluded and would be examined next time. The output of ls /var/lib/rancher/rke/log/ on affected workers shows that these files referenced in the warning log messages are broken symlinks. In addition, docker ps output shows the container ID for the currently running kubelet and kube-proxy containers does not match the IDs in these filenames.","title":"Issue"},{"location":"000020201/#pre-requisites","text":"A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider . Rancher Cluster Logging configured on the cluster, with the 'Include System Log' option set. Worker role only nodes in the cluster.","title":"Pre-requisites"},{"location":"000020201/#root-cause","text":"These warning level messages in the rancher-logging-fluentd Pod are the result of the issue tracked in Rancher GitHub issue #22549 . The container log symlinks in /var/lib/rancher/rke/log/ for cluster component containers ( kubelet , kube-proxy , nginx-proxy ) on worker nodes in Rancher launched clusters are not cleaned up when these components are re-created, i.e. due to a Kubernetes version upgrade, or other configuration update for these components. As a result these broken symlinks persist and cause the log unreadable warning messages when the rancher-logging-fluentd Pod attempts to parse files in the /var/lib/rancher/rke/log/ directory. This warning message itself is harmless and can be ignored.","title":"Root cause"},{"location":"000020201/#resolution","text":"The request to handle automatic clean-up of these log symlinks on worker nodes, in Rancher provisioned clusters, is tracked in Rancher GitHub issue #22549 .","title":"Resolution"},{"location":"000020201/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020202/","text":"Many rancher-agent containers running on Rancher v2.x provisioned Kubernetes cluster, where stopped containers are regularly deleted on hosts This document (000020202) is provided subject to the disclaimer at the end of this document. Situation Issue On a Rancher v2.x provisioned cluster, a host shows a large number of containers running the rancher-agent image, per the following output of docker ps | grep rancher-agent : $ docker ps | grep rancher-agent ... aeffe9725521 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" About a minute ago Up About a minute sleepy_hopper 130120f49b71 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 6 minutes ago Up 6 minutes stoic_hypatia 498b923d9b6e rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 11 minutes ago Up 11 minutes laughing_elbakyan 3453865e5f70 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 16 minutes ago Up 16 minutes wonderful_gagarin f925209cd16a rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 21 minutes ago Up 21 minutes silly_shannon 7d7fb5d4bf04 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 26 minutes ago Up 26 minutes gifted_elgamal ... A docker inspect <container_id> for these containers, shows the Path and Args are of the following format: \"Path\": \"run.sh\", \"Args\": [ \"--server\", \"https://167.172.96.240\", \"--token\", \"gwrp7zlnwvsnzh2nhbvwcgdw45ccv6cq9pztzdd92j6xlv69xxhvnp\", \"--ca-checksum\", \"bbc8c7ca05c87a7140154554fa1a516178852f2710538c57718f4c874c29533c\", \"--no-register\", \"--only-write-certs\" ], Pre-requisites A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider . Repeated deletion of stopped containers on hosts in the cluster, e.g. use of docker system prune , either manually or as part of an automated process such as a cronjob. Root cause This behaviour is a result of the issue tracked in Rancher GitHub issue #15364 . The share-mnt container is created on a Rancher provisioned Kubernetes cluster, and exits upon completion, but is not removed such that it can be invoked again. Meanwhile, the Rancher node-agent Pod on a host will spawn a new share-mnt container, if the share-mnt is removed. Upon starting, the share-mnt process spawns a rancher-agent container to write certificates. This agent container will run indefinitely until the node-agent is triggered to reconnect to the Rancher server or the node-agent process is restarted. As a result, where the share-mnt container on a host is removed repeatedly, either manually or by an automated process, this will result in multiple running rancher-agent containers. Workaround To trigger automatic removal of the rancher-agent containers, the node-agent container on the host can be restarted. Identifying the running agent container with docker ps | grep k8s_agent_cattle-node restart the container with docker restart <container_id> . In addition, you can prevent further creation of multiple rancher-agent container instances by removing whichever process is triggering the deletion of stopped containers. Resolution An enhancement request, to prevent the creation of multiple long-running rancher-agent containers, in the event of repeated deletion of the share-mnt container, is tracked in Rancher GitHub issue #15364 . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Many rancher-agent containers running on Rancher v2.x provisioned Kubernetes cluster, where stopped containers are regularly deleted on hosts"},{"location":"000020202/#many-rancher-agent-containers-running-on-rancher-v2x-provisioned-kubernetes-cluster-where-stopped-containers-are-regularly-deleted-on-hosts","text":"This document (000020202) is provided subject to the disclaimer at the end of this document.","title":"Many rancher-agent containers running on Rancher v2.x provisioned Kubernetes cluster, where stopped containers are regularly deleted on hosts"},{"location":"000020202/#situation","text":"","title":"Situation"},{"location":"000020202/#issue","text":"On a Rancher v2.x provisioned cluster, a host shows a large number of containers running the rancher-agent image, per the following output of docker ps | grep rancher-agent : $ docker ps | grep rancher-agent ... aeffe9725521 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" About a minute ago Up About a minute sleepy_hopper 130120f49b71 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 6 minutes ago Up 6 minutes stoic_hypatia 498b923d9b6e rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 11 minutes ago Up 11 minutes laughing_elbakyan 3453865e5f70 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 16 minutes ago Up 16 minutes wonderful_gagarin f925209cd16a rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 21 minutes ago Up 21 minutes silly_shannon 7d7fb5d4bf04 rancher/rancher-agent:v2.3.3 \"run.sh --server htt\u2026\" 26 minutes ago Up 26 minutes gifted_elgamal ... A docker inspect <container_id> for these containers, shows the Path and Args are of the following format: \"Path\": \"run.sh\", \"Args\": [ \"--server\", \"https://167.172.96.240\", \"--token\", \"gwrp7zlnwvsnzh2nhbvwcgdw45ccv6cq9pztzdd92j6xlv69xxhvnp\", \"--ca-checksum\", \"bbc8c7ca05c87a7140154554fa1a516178852f2710538c57718f4c874c29533c\", \"--no-register\", \"--only-write-certs\" ],","title":"Issue"},{"location":"000020202/#pre-requisites","text":"A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider . Repeated deletion of stopped containers on hosts in the cluster, e.g. use of docker system prune , either manually or as part of an automated process such as a cronjob.","title":"Pre-requisites"},{"location":"000020202/#root-cause","text":"This behaviour is a result of the issue tracked in Rancher GitHub issue #15364 . The share-mnt container is created on a Rancher provisioned Kubernetes cluster, and exits upon completion, but is not removed such that it can be invoked again. Meanwhile, the Rancher node-agent Pod on a host will spawn a new share-mnt container, if the share-mnt is removed. Upon starting, the share-mnt process spawns a rancher-agent container to write certificates. This agent container will run indefinitely until the node-agent is triggered to reconnect to the Rancher server or the node-agent process is restarted. As a result, where the share-mnt container on a host is removed repeatedly, either manually or by an automated process, this will result in multiple running rancher-agent containers.","title":"Root cause"},{"location":"000020202/#workaround","text":"To trigger automatic removal of the rancher-agent containers, the node-agent container on the host can be restarted. Identifying the running agent container with docker ps | grep k8s_agent_cattle-node restart the container with docker restart <container_id> . In addition, you can prevent further creation of multiple rancher-agent container instances by removing whichever process is triggering the deletion of stopped containers.","title":"Workaround"},{"location":"000020202/#resolution","text":"An enhancement request, to prevent the creation of multiple long-running rancher-agent containers, in the event of repeated deletion of the share-mnt container, is tracked in Rancher GitHub issue #15364 .","title":"Resolution"},{"location":"000020202/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020203/","text":"Is it safe to update the Docker bridge IP range on hosts in an RKE or Rancher v2.x launched Kubernetes cluster? This document (000020203) is provided subject to the disclaimer at the end of this document. Situation Question The docker0 bridge network has a default IP range of 172.17.0.0/16 (with an additional docker-sys bridge for system-docker using 172.18.0.0/16 by default on RancherOS). These ranges will be routed to these interfaces, per the below example of the route output. If the range(s) overlap with the internal IP space usage in your own network, the host will not be able to route packets to other hosts in your network that lie within these ranges. As a result you may wish to change the bridge range(s) to enable successful routing to hosts within these. $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface ... 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 ... Pre-requisites This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x, or Rancher v2.x Answer Updating the docker0 bridge IP range (and docker-sys bridge IP range in RancherOS) is possible in an RKE or Rancher v2.x provisioned Kubernetes cluster, where no cluster containers are in fact running attached to the Docker bridge network. The only impact of the change should be some downtime, as you will be required to restart the Docker daemon for the change to take effect. On RancherOS the bridge IP range ( bip ) can be updated for docker and system-docker per the RancherOS documentation on Configuring Docker or System Docker . You will need to reboot the host for the change to take effect after updating the settings. For other operating systems, where Docker is installed from the upstream Docker repositories, you should update the bip configuration in /etc/docker/daemon.json per the dockerd documentation . On CentOS 7, RHEL 7 and SLES 12 you should also check the configuration in /etc/sysconfig/docker to ensure --bip has not been configured there. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is it safe to update the Docker bridge IP range on hosts in an RKE or Rancher v2.x launched Kubernetes cluster?"},{"location":"000020203/#is-it-safe-to-update-the-docker-bridge-ip-range-on-hosts-in-an-rke-or-rancher-v2x-launched-kubernetes-cluster","text":"This document (000020203) is provided subject to the disclaimer at the end of this document.","title":"Is it safe to update the Docker bridge IP range on hosts in an RKE or Rancher v2.x launched Kubernetes cluster?"},{"location":"000020203/#situation","text":"","title":"Situation"},{"location":"000020203/#question","text":"The docker0 bridge network has a default IP range of 172.17.0.0/16 (with an additional docker-sys bridge for system-docker using 172.18.0.0/16 by default on RancherOS). These ranges will be routed to these interfaces, per the below example of the route output. If the range(s) overlap with the internal IP space usage in your own network, the host will not be able to route packets to other hosts in your network that lie within these ranges. As a result you may wish to change the bridge range(s) to enable successful routing to hosts within these. $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface ... 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 ...","title":"Question"},{"location":"000020203/#pre-requisites","text":"This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x, or Rancher v2.x","title":"Pre-requisites"},{"location":"000020203/#answer","text":"Updating the docker0 bridge IP range (and docker-sys bridge IP range in RancherOS) is possible in an RKE or Rancher v2.x provisioned Kubernetes cluster, where no cluster containers are in fact running attached to the Docker bridge network. The only impact of the change should be some downtime, as you will be required to restart the Docker daemon for the change to take effect. On RancherOS the bridge IP range ( bip ) can be updated for docker and system-docker per the RancherOS documentation on Configuring Docker or System Docker . You will need to reboot the host for the change to take effect after updating the settings. For other operating systems, where Docker is installed from the upstream Docker repositories, you should update the bip configuration in /etc/docker/daemon.json per the dockerd documentation . On CentOS 7, RHEL 7 and SLES 12 you should also check the configuration in /etc/sysconfig/docker to ensure --bip has not been configured there.","title":"Answer"},{"location":"000020203/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020204/","text":"Is it safe to disable inter-container connectivity (icc) on the Docker daemon in an RKE or Rancher v2.x launched Kubernetes cluster? This document (000020204) is provided subject to the disclaimer at the end of this document. Situation Question The Docker daemon provides a configuration option icc which permits a user to disable inter-container connectivity (icc) on the Docker bridge network. Is is safe to disable this Docker daemon option in an RKE or Rancher v2.x launched Kubernetes cluster? Pre-requisites This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x or Rancher v2.x Answer Setting icc to false in the docker daemon.json configuration, or as an argument to to dockerd, is possible but is unnecessary in an RKE or Rancher v2.x provisioned Kubernetes cluster, as containers are not run attached to the Docker bridge network. Therefore, whilst this step is often included in standard 'hardening Docker daemon' guides, it is not relevant to operating an RKE or Rancher launched Kubernetes cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is it safe to disable inter-container connectivity (icc) on the Docker daemon in an RKE or Rancher v2.x launched Kubernetes cluster?"},{"location":"000020204/#is-it-safe-to-disable-inter-container-connectivity-icc-on-the-docker-daemon-in-an-rke-or-rancher-v2x-launched-kubernetes-cluster","text":"This document (000020204) is provided subject to the disclaimer at the end of this document.","title":"Is it safe to disable inter-container connectivity (icc) on the Docker daemon in an RKE or Rancher v2.x launched Kubernetes cluster?"},{"location":"000020204/#situation","text":"","title":"Situation"},{"location":"000020204/#question","text":"The Docker daemon provides a configuration option icc which permits a user to disable inter-container connectivity (icc) on the Docker bridge network. Is is safe to disable this Docker daemon option in an RKE or Rancher v2.x launched Kubernetes cluster?","title":"Question"},{"location":"000020204/#pre-requisites","text":"This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x or Rancher v2.x","title":"Pre-requisites"},{"location":"000020204/#answer","text":"Setting icc to false in the docker daemon.json configuration, or as an argument to to dockerd, is possible but is unnecessary in an RKE or Rancher v2.x provisioned Kubernetes cluster, as containers are not run attached to the Docker bridge network. Therefore, whilst this step is often included in standard 'hardening Docker daemon' guides, it is not relevant to operating an RKE or Rancher launched Kubernetes cluster.","title":"Answer"},{"location":"000020204/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020205/","text":"Users assigned the Project Owner or Member role on a project are able to create namespaces on any project, in the same cluster, to which they have access This document (000020205) is provided subject to the disclaimer at the end of this document. Situation Issue A user assigned the Project Owner or Member role on one project is able to create namespaces on any project, in the same cluster, to which they have access. For example, if a user has been granted the Project Member role on a Project named Dev in a cluster, and the Read-only role on a project named Test in that cluster, they will be able to create namespaces on both the Dev and Test projects. Pre-requisites A cluster managed by Rancher v2.x A user granted the Project Member or Owner role on one project, and access e.g. the Read-only role, on another project Explanation Per the caveat explanation in the Rancher v2.x documentation : Users assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole , meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the owner or member role for a project can create namespaces in other projects they\u2019re assigned to, even with only the Read Only role assigned. Further Reading Read more on Cluster and Project Roles in the Rancher v2.x. documentation . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Users assigned the Project Owner or Member role on a project are able to create namespaces on any project, in the same cluster, to which they have access"},{"location":"000020205/#users-assigned-the-project-owner-or-member-role-on-a-project-are-able-to-create-namespaces-on-any-project-in-the-same-cluster-to-which-they-have-access","text":"This document (000020205) is provided subject to the disclaimer at the end of this document.","title":"Users assigned the Project Owner or Member role on a project are able to create namespaces on any project, in the same cluster, to which they have access"},{"location":"000020205/#situation","text":"","title":"Situation"},{"location":"000020205/#issue","text":"A user assigned the Project Owner or Member role on one project is able to create namespaces on any project, in the same cluster, to which they have access. For example, if a user has been granted the Project Member role on a Project named Dev in a cluster, and the Read-only role on a project named Test in that cluster, they will be able to create namespaces on both the Dev and Test projects.","title":"Issue"},{"location":"000020205/#pre-requisites","text":"A cluster managed by Rancher v2.x A user granted the Project Member or Owner role on one project, and access e.g. the Read-only role, on another project","title":"Pre-requisites"},{"location":"000020205/#explanation","text":"Per the caveat explanation in the Rancher v2.x documentation : Users assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole , meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the owner or member role for a project can create namespaces in other projects they\u2019re assigned to, even with only the Read Only role assigned.","title":"Explanation"},{"location":"000020205/#further-reading","text":"Read more on Cluster and Project Roles in the Rancher v2.x. documentation .","title":"Further Reading"},{"location":"000020205/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020206/","text":"How does session management work in the Rancher v1.6 UI? This document (000020206) is provided subject to the disclaimer at the end of this document. Situation Question This article looks at how session management, and expiry, functions in the Rancher v1.6 UI. Pre-requisites This article is applicable to Rancher v1.6 instances Answer The Rancher user interface uses a token for session management. The token is originally obtained by the client by calling the /v2-beta/token API. This API is triggered by the end-user entering their username and password and clicking the \"Log In\" button. Below is an example request: URL: https://rancher.example.com/v2-beta/token Method: POST Request body (formatted for readability): { \"code\":\"admin:<password here>\", \"authProvider\":\"localauthconfig\" } Upon successful authentication, the server will generate a random 40 character token that is associated with the authenticated user. This token is provided back to the user interface in the jwt field in the JSON response. The token is valid for 16 hours from the time of creation. This expiration is enforced by the server. Below is a sample response (formatted for readability): { \"id\":null, \"type\":\"token\", \"links\":{}, \"baseType\":\"token\", \"actionLinks\":{}, \"accountId\":\"1a1\", \"authProvider\":\"localAuthConfig\", \"code\":null, \"enabled\":true, \"jwt\":\"V1dMyPArix5nN1jxiA6DdzsqdZitDJhZuBR3vZNr\", \"originalLogin\":null, \"redirectUrl\":null, \"security\":true, \"user\":\"admin\", \"userIdentity\": { \"externalId\":\"1a1\", \"profilePicture\":null, \"name\":\"admin\", \"externalIdType\":\"rancher_id\", \"profileUrl\":null, \"login\":\"admin\", \"role\":null, \"projectId\":null, \"user\":false, \"all\":null, \"id\":\"rancher_id:1a1\" }, \"userType\":\"admin\" } The user interface stores the token in a cookie called token and will send this cookie to all subsequent API requests to the server. In addition to a token, the server also sends a CSRF (Cross-Site Request Forgery) cookie which must be sent back on each request. This ensures the request came from the client and not a third party or malicious script. Below is a sequence diagram that demonstrates how a token is created and used. Upon session expiration, the user interface will redirect the user back to the login page. Note, the session token expiration duration is not currently configurable. There is an enhancement request on GitHub to add this functionality, tracked in https://github.com/rancher/rancher/issues/16467 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How does session management work in the Rancher v1.6 UI?"},{"location":"000020206/#how-does-session-management-work-in-the-rancher-v16-ui","text":"This document (000020206) is provided subject to the disclaimer at the end of this document.","title":"How does session management work in the Rancher v1.6 UI?"},{"location":"000020206/#situation","text":"","title":"Situation"},{"location":"000020206/#question","text":"This article looks at how session management, and expiry, functions in the Rancher v1.6 UI.","title":"Question"},{"location":"000020206/#pre-requisites","text":"This article is applicable to Rancher v1.6 instances","title":"Pre-requisites"},{"location":"000020206/#answer","text":"The Rancher user interface uses a token for session management. The token is originally obtained by the client by calling the /v2-beta/token API. This API is triggered by the end-user entering their username and password and clicking the \"Log In\" button. Below is an example request: URL: https://rancher.example.com/v2-beta/token Method: POST Request body (formatted for readability): { \"code\":\"admin:<password here>\", \"authProvider\":\"localauthconfig\" } Upon successful authentication, the server will generate a random 40 character token that is associated with the authenticated user. This token is provided back to the user interface in the jwt field in the JSON response. The token is valid for 16 hours from the time of creation. This expiration is enforced by the server. Below is a sample response (formatted for readability): { \"id\":null, \"type\":\"token\", \"links\":{}, \"baseType\":\"token\", \"actionLinks\":{}, \"accountId\":\"1a1\", \"authProvider\":\"localAuthConfig\", \"code\":null, \"enabled\":true, \"jwt\":\"V1dMyPArix5nN1jxiA6DdzsqdZitDJhZuBR3vZNr\", \"originalLogin\":null, \"redirectUrl\":null, \"security\":true, \"user\":\"admin\", \"userIdentity\": { \"externalId\":\"1a1\", \"profilePicture\":null, \"name\":\"admin\", \"externalIdType\":\"rancher_id\", \"profileUrl\":null, \"login\":\"admin\", \"role\":null, \"projectId\":null, \"user\":false, \"all\":null, \"id\":\"rancher_id:1a1\" }, \"userType\":\"admin\" } The user interface stores the token in a cookie called token and will send this cookie to all subsequent API requests to the server. In addition to a token, the server also sends a CSRF (Cross-Site Request Forgery) cookie which must be sent back on each request. This ensures the request came from the client and not a third party or malicious script. Below is a sequence diagram that demonstrates how a token is created and used. Upon session expiration, the user interface will redirect the user back to the login page. Note, the session token expiration duration is not currently configurable. There is an enhancement request on GitHub to add this functionality, tracked in https://github.com/rancher/rancher/issues/16467","title":"Answer"},{"location":"000020206/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020207/","text":"Node labels and taints reset on reboot with AWS cloudprovider in Kubernetes lower than v1.12.0 This document (000020207) is provided subject to the disclaimer at the end of this document. Situation Issue In a Kubernetes cluster, running on AWS EC2 instances, with the AWS cloudprovider configured, labels and taints for a node are reset when the EC2 instance is rebooted. Pre-requisites Kubernetes version lower than v1.12.0 Cluster running on AWS EC2 instances, with the AWS cloudprovider configured Root Cause This behaviour is caused by the AWS cloudprovider in Kubernetes versions prior to v1.12.0, in which a stopped EC2 instance is deleted from the Kubernetes cluster, and then re-created when started again. As a result of this deletion and re-creation labels and taints on the node are lost during the reboot. Details of the issue and fix can be found in Kubernetes Pull Request #66835 . Resolution In order to resolve this issue, the cluster should be upgraded to Kubernetes version v1.12.0 or above. For clusters provisioned via the RKE CLI, users can upgrade the cluster to a Kubernetes version of v1.12.0 or higher with RKE v0.1.10 or above. For clusters provisioned via Rancher , users can upgrade the cluster to a Kubernetes version of v1.12.6 or higher with Rancher v2.1.7 or above. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Node labels and taints reset on reboot with AWS cloudprovider in Kubernetes lower than v1.12.0"},{"location":"000020207/#node-labels-and-taints-reset-on-reboot-with-aws-cloudprovider-in-kubernetes-lower-than-v1120","text":"This document (000020207) is provided subject to the disclaimer at the end of this document.","title":"Node labels and taints reset on reboot with AWS cloudprovider in Kubernetes lower than v1.12.0"},{"location":"000020207/#situation","text":"","title":"Situation"},{"location":"000020207/#issue","text":"In a Kubernetes cluster, running on AWS EC2 instances, with the AWS cloudprovider configured, labels and taints for a node are reset when the EC2 instance is rebooted.","title":"Issue"},{"location":"000020207/#pre-requisites","text":"Kubernetes version lower than v1.12.0 Cluster running on AWS EC2 instances, with the AWS cloudprovider configured","title":"Pre-requisites"},{"location":"000020207/#root-cause","text":"This behaviour is caused by the AWS cloudprovider in Kubernetes versions prior to v1.12.0, in which a stopped EC2 instance is deleted from the Kubernetes cluster, and then re-created when started again. As a result of this deletion and re-creation labels and taints on the node are lost during the reboot. Details of the issue and fix can be found in Kubernetes Pull Request #66835 .","title":"Root Cause"},{"location":"000020207/#resolution","text":"In order to resolve this issue, the cluster should be upgraded to Kubernetes version v1.12.0 or above. For clusters provisioned via the RKE CLI, users can upgrade the cluster to a Kubernetes version of v1.12.0 or higher with RKE v0.1.10 or above. For clusters provisioned via Rancher , users can upgrade the cluster to a Kubernetes version of v1.12.6 or higher with Rancher v2.1.7 or above.","title":"Resolution"},{"location":"000020207/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020208/","text":"CronJobs fail to run in a Kubernetes v1.14 cluster, with more than 500 Job resources: \"expected type *batchv1.JobList, got type *internalversion.List\" This document (000020208) is provided subject to the disclaimer at the end of this document. Situation Issue In a Kubernetes v1.14 cluster, CronJobs fail to run when there are more than 500 Job resources in the cluster. The Kubernetes controller manager logs show errors of the format {\"log\":\"E0818 18:25:50.081946 1 cronjob_controller.go:117] expected type *batchv1.JobList, got type *internalversion.List\\n\",\"stream\":\"stderr\",\"time\":\"2019-08-18T18:25:50.082127727Z\"} . Pre-requisites A Kubernetes cluster, running Kubernetes v1.14, from v1.14.0 - v1.14.6 More than 500 Job resources in the cluster Workaround To mitigate the issue you should ensure that there are fewer than 500 Job resources in the cluster, you can view all Jobs with kubectl get jobs --all-namespaces -o wide . You should aim to delete completed Jobs to reduce the total number below 500. You can also check and adjust the configured job history limits for CronJobs to reduce the number of Jobs maintained for completed CronJobs per the Kubernetes documentation . Resolution The issue was tracked in Kubernetes GitHub Issue #77465 and a fix was released in Kubernetes v1.14.7 . A Kubernetes v1.14 patch release of v1.14.7 or above, including this fix, is available in Rancher v2.2, starting with v2.2.9 (v1.14.8), and v2.3, starting with v2.3.0 (v1.14.7). Similarly the fix is available via the RKE CLI in v0.2, starting with v0.2.9 (v1.14.8), and v0.3, starting with v0.3.0 (v1.14.7). Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"CronJobs fail to run in a Kubernetes v1.14 cluster, with more than 500 Job resources: \"expected type \\*batchv1.JobList, got type \\*internalversion.List\""},{"location":"000020208/#cronjobs-fail-to-run-in-a-kubernetes-v114-cluster-with-more-than-500-job-resources-expected-type-batchv1joblist-got-type-internalversionlist","text":"This document (000020208) is provided subject to the disclaimer at the end of this document.","title":"CronJobs fail to run in a Kubernetes v1.14 cluster, with more than 500 Job resources: \"expected type *batchv1.JobList, got type *internalversion.List\""},{"location":"000020208/#situation","text":"","title":"Situation"},{"location":"000020208/#issue","text":"In a Kubernetes v1.14 cluster, CronJobs fail to run when there are more than 500 Job resources in the cluster. The Kubernetes controller manager logs show errors of the format {\"log\":\"E0818 18:25:50.081946 1 cronjob_controller.go:117] expected type *batchv1.JobList, got type *internalversion.List\\n\",\"stream\":\"stderr\",\"time\":\"2019-08-18T18:25:50.082127727Z\"} .","title":"Issue"},{"location":"000020208/#pre-requisites","text":"A Kubernetes cluster, running Kubernetes v1.14, from v1.14.0 - v1.14.6 More than 500 Job resources in the cluster","title":"Pre-requisites"},{"location":"000020208/#workaround","text":"To mitigate the issue you should ensure that there are fewer than 500 Job resources in the cluster, you can view all Jobs with kubectl get jobs --all-namespaces -o wide . You should aim to delete completed Jobs to reduce the total number below 500. You can also check and adjust the configured job history limits for CronJobs to reduce the number of Jobs maintained for completed CronJobs per the Kubernetes documentation .","title":"Workaround"},{"location":"000020208/#resolution","text":"The issue was tracked in Kubernetes GitHub Issue #77465 and a fix was released in Kubernetes v1.14.7 . A Kubernetes v1.14 patch release of v1.14.7 or above, including this fix, is available in Rancher v2.2, starting with v2.2.9 (v1.14.8), and v2.3, starting with v2.3.0 (v1.14.7). Similarly the fix is available via the RKE CLI in v0.2, starting with v0.2.9 (v1.14.8), and v0.3, starting with v0.3.0 (v1.14.7).","title":"Resolution"},{"location":"000020208/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020209/","text":"How to configure iptables on RancherOS This document (000020209) is provided subject to the disclaimer at the end of this document. Situation Task How to configure firewall rules using iptables on RancherOS Pre-requisites A RancherOS v1.5.x host Resolution The runcmd option in cloud-config can be used to run commands, such as iptables rules, to set firewall rules on a RancherOS host. For example the following can be used to disable SSH access on port 22. #cloud-config runcmd: - \"iptables -A INPUT -p tcp --destination-port 22 -j DROP\" The above snipet can be placed in /var/lib/rancher/conf/cloud-config.d/xxx.yaml, or added to the initial config while installing RancherOS. It will be executed every time RancherOS is booted. You can use the following iptables command to view the status of the rules: $ iptables -t filter -nv -L INPUT Chain INPUT (policy ACCEPT 321 packets, 41200 bytes) pkts bytes target prot opt in out source destination 9 523 DROP tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 Further reading More information on running command on boot can be found here . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to configure iptables on RancherOS"},{"location":"000020209/#how-to-configure-iptables-on-rancheros","text":"This document (000020209) is provided subject to the disclaimer at the end of this document.","title":"How to configure iptables on RancherOS"},{"location":"000020209/#situation","text":"","title":"Situation"},{"location":"000020209/#task","text":"How to configure firewall rules using iptables on RancherOS","title":"Task"},{"location":"000020209/#pre-requisites","text":"A RancherOS v1.5.x host","title":"Pre-requisites"},{"location":"000020209/#resolution","text":"The runcmd option in cloud-config can be used to run commands, such as iptables rules, to set firewall rules on a RancherOS host. For example the following can be used to disable SSH access on port 22. #cloud-config runcmd: - \"iptables -A INPUT -p tcp --destination-port 22 -j DROP\" The above snipet can be placed in /var/lib/rancher/conf/cloud-config.d/xxx.yaml, or added to the initial config while installing RancherOS. It will be executed every time RancherOS is booted. You can use the following iptables command to view the status of the rules: $ iptables -t filter -nv -L INPUT Chain INPUT (policy ACCEPT 321 packets, 41200 bytes) pkts bytes target prot opt in out source destination 9 523 DROP tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22","title":"Resolution"},{"location":"000020209/#further-reading","text":"More information on running command on boot can be found here .","title":"Further reading"},{"location":"000020209/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020210/","text":"RKE errors connecting to the Docker socket whilst updating clusters with the Aqua Enforcer deployed This document (000020210) is provided subject to the disclaimer at the end of this document. Situation Issue During invocations of rke up via the RKE CLI or whilst modifying Rancher provisioned Kubernetes clusters , the process fails upon attempted creation of a Kubernetes component container with an error of the following format: 2019-04-30T15:19:17.9826528Z time=\"2019-04-30T15:19:17Z\" level=fatal msg=\"[etcd] Failed to bring up Etcd Plane: Failed to create [etcd] container on host [rancher.example.com]: Failed to create [etcd] container on host [rancher.example.com]: error during connect: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create?name=etcd: EOF Pre-requisites A Kubernetes cluster provisioned via the RKE CLI or Rancher The Aqua Enforcer workload deployed in the cluster, with AQUA_RUNC_INTERCEPTION environment variable set to 0 Root cause The issue is caused by Aqua Enforcer's use of the Docker socket to perform runtime enforcement operations preventing RKE from successfully connecting to the Docker socket upon some requests. Resolution To resolve this issue set the AQUA_RUNC_INTERCEPTION environment variable on the Aqua Enforcer daemonset to 1. With this setting the Aqua Enforcer will interact directly with runC to perform runtime enforcement operations, and not with the Docker daemon via the Docker socket. This is the default behaviour in new versions of the Aqua Enforcer, as it brings stability and performance benefits. More information on this setting can be found at https://docs.aquasec.com/docs/40-ga#section-new-aqua-enforcer-architecture-for-enhanced-stability-and-performance Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"RKE errors connecting to the Docker socket whilst updating clusters with the Aqua Enforcer deployed"},{"location":"000020210/#rke-errors-connecting-to-the-docker-socket-whilst-updating-clusters-with-the-aqua-enforcer-deployed","text":"This document (000020210) is provided subject to the disclaimer at the end of this document.","title":"RKE errors connecting to the Docker socket whilst updating clusters with the Aqua Enforcer deployed"},{"location":"000020210/#situation","text":"","title":"Situation"},{"location":"000020210/#issue","text":"During invocations of rke up via the RKE CLI or whilst modifying Rancher provisioned Kubernetes clusters , the process fails upon attempted creation of a Kubernetes component container with an error of the following format: 2019-04-30T15:19:17.9826528Z time=\"2019-04-30T15:19:17Z\" level=fatal msg=\"[etcd] Failed to bring up Etcd Plane: Failed to create [etcd] container on host [rancher.example.com]: Failed to create [etcd] container on host [rancher.example.com]: error during connect: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create?name=etcd: EOF","title":"Issue"},{"location":"000020210/#pre-requisites","text":"A Kubernetes cluster provisioned via the RKE CLI or Rancher The Aqua Enforcer workload deployed in the cluster, with AQUA_RUNC_INTERCEPTION environment variable set to 0","title":"Pre-requisites"},{"location":"000020210/#root-cause","text":"The issue is caused by Aqua Enforcer's use of the Docker socket to perform runtime enforcement operations preventing RKE from successfully connecting to the Docker socket upon some requests.","title":"Root cause"},{"location":"000020210/#resolution","text":"To resolve this issue set the AQUA_RUNC_INTERCEPTION environment variable on the Aqua Enforcer daemonset to 1. With this setting the Aqua Enforcer will interact directly with runC to perform runtime enforcement operations, and not with the Docker daemon via the Docker socket. This is the default behaviour in new versions of the Aqua Enforcer, as it brings stability and performance benefits. More information on this setting can be found at https://docs.aquasec.com/docs/40-ga#section-new-aqua-enforcer-architecture-for-enhanced-stability-and-performance","title":"Resolution"},{"location":"000020210/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020211/","text":"Editing Rancher launched Kubernetes cluster in infrastructure provider restricted to creating user This document (000020211) is provided subject to the disclaimer at the end of this document. Situation Issue When attempting to edit a Rancher launched Kubernetes cluster, hosted on nodes in an infrastructure provider neither the Cluster Options nor Node Pools sections are available and configurable in the edit cluster view, if logged in as a different user to the cluster creator. Pre-requisites A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider Access to the Rancher UI as a user different to the cluster creator Root cause Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider. Node templates are user-scoped and, as a result, where userA creates a node template in Rancher it is not accessible by userB . This prevents Rancher launched Kubernetes clusters, provisioned on nodes in an infrastructure provider by userA from being edited by other users, as only userA has access to the node template configuration. Resolution An enhancement request to enable users, other than the cluster creator, to edit Rancher launched Kubernetes clusters is tracked in Rancher GitHub Issue #12038 . Where it is necessary for another user to edit the cluster, i.e. the original user who created the cluster has left the business, it is possible to re-associate the node template with a different user. If you encounter this situation, please open a ticket with Rancher Support for assistance. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Editing Rancher launched Kubernetes cluster in infrastructure provider restricted to creating user"},{"location":"000020211/#editing-rancher-launched-kubernetes-cluster-in-infrastructure-provider-restricted-to-creating-user","text":"This document (000020211) is provided subject to the disclaimer at the end of this document.","title":"Editing Rancher launched Kubernetes cluster in infrastructure provider restricted to creating user"},{"location":"000020211/#situation","text":"","title":"Situation"},{"location":"000020211/#issue","text":"When attempting to edit a Rancher launched Kubernetes cluster, hosted on nodes in an infrastructure provider neither the Cluster Options nor Node Pools sections are available and configurable in the edit cluster view, if logged in as a different user to the cluster creator.","title":"Issue"},{"location":"000020211/#pre-requisites","text":"A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider Access to the Rancher UI as a user different to the cluster creator","title":"Pre-requisites"},{"location":"000020211/#root-cause","text":"Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider. Node templates are user-scoped and, as a result, where userA creates a node template in Rancher it is not accessible by userB . This prevents Rancher launched Kubernetes clusters, provisioned on nodes in an infrastructure provider by userA from being edited by other users, as only userA has access to the node template configuration.","title":"Root cause"},{"location":"000020211/#resolution","text":"An enhancement request to enable users, other than the cluster creator, to edit Rancher launched Kubernetes clusters is tracked in Rancher GitHub Issue #12038 . Where it is necessary for another user to edit the cluster, i.e. the original user who created the cluster has left the business, it is possible to re-associate the node template with a different user. If you encounter this situation, please open a ticket with Rancher Support for assistance.","title":"Resolution"},{"location":"000020211/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020212/","text":"Blank provider listed for cluster when logged in as user who did not create cluster in Rancher v2.x This document (000020212) is provided subject to the disclaimer at the end of this document. Situation Issue When viewing a Rancher launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider , as a user other than the cluster creator, the infrastructure provider name is blank. Pre-requisites A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider Access to the Rancher UI as a user different to the cluster creator Root cause Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider. Node templates are user-scoped and, as a result, where userA creates a node template in Rancher it is not accessible by userB . Meanwhile, the Rancher v2.x UI determines the provider for a Rancher launched Kubernetes cluster by mapping nodes to node templates to node drivers . The user-scoping of node templates therefore prevents users, other than the creator, from viewing the provider of the cluster. Resolution An enhancement request to enable all users to view the cluster provider is tracked in Rancher GitHub Issue #12038 . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Blank provider listed for cluster when logged in as user who did not create cluster in Rancher v2.x"},{"location":"000020212/#blank-provider-listed-for-cluster-when-logged-in-as-user-who-did-not-create-cluster-in-rancher-v2x","text":"This document (000020212) is provided subject to the disclaimer at the end of this document.","title":"Blank provider listed for cluster when logged in as user who did not create cluster in Rancher v2.x"},{"location":"000020212/#situation","text":"","title":"Situation"},{"location":"000020212/#issue","text":"When viewing a Rancher launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider , as a user other than the cluster creator, the infrastructure provider name is blank.","title":"Issue"},{"location":"000020212/#pre-requisites","text":"A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider Access to the Rancher UI as a user different to the cluster creator","title":"Pre-requisites"},{"location":"000020212/#root-cause","text":"Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider. Node templates are user-scoped and, as a result, where userA creates a node template in Rancher it is not accessible by userB . Meanwhile, the Rancher v2.x UI determines the provider for a Rancher launched Kubernetes cluster by mapping nodes to node templates to node drivers . The user-scoping of node templates therefore prevents users, other than the creator, from viewing the provider of the cluster.","title":"Root cause"},{"location":"000020212/#resolution","text":"An enhancement request to enable all users to view the cluster provider is tracked in Rancher GitHub Issue #12038 .","title":"Resolution"},{"location":"000020212/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020213/","text":"How to recover after deleting the Calico CRDs from a cluster This document (000020213) is provided subject to the disclaimer at the end of this document. Situation Issue Calico uses a number of Custom Resource Definitions (CRDs) in order to store configuration data in Custom Resources. In the event that these CRDs are accidentally deleted from a cluster by a user, the configuration data in these Custom Resources will be deleted, preventing successful programming of pod networking. This article documents how to recreate the CRDs and ensure the configuration data is also re-populated. Pre-requisites A Kubernetes v1.8.x - v1.16.x cluster provisioned by the RKE CLI or Rancher v2.x, running with the Canal or Calico network providers Resolution 1. Re-create the CRDs The first step is to re-create the CRDs. The definitions are dependent upon the Kubernetes version running in the cluster, as well as whether the cluster is running the Canal or Calico network provider. Please refer to the matching network provider and Kubernetes version combination below: Canal Network Provider and Kubernetes version 1.8.x - 1.12.x Download the canal-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-8-to-1-12.yaml Canal Network Provider and Kubernetes version 1.13.x - 1.14.x Download the canal-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-13-to-1-14.yaml Canal Network Provider and Kubernetes version 1.15.x Download the canal-calico-crds-k8s-1-15.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-15.yaml Canal Network Provider and Kubernetes version 1.16.x Download the canal-calico-crds-k8s-1-16.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-16.yaml Calico Network Provider and Kubernetes version 1.8.x - 1.12.x Download the calico-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-8-to-1-12.yaml Calico Network Provider and Kubernetes version 1.13.x - 1.14.x Download the calico-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-13-to-1-14.yaml Calico Network Provider and Kubernetes version 1.15.x Download the calico-calico-crds-k8s-1-15.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-15.yaml Calico Network Provider and Kubernetes version 1.16.x Download the calico-calico-crds-k8s-1-16.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-16.yaml 2. Delete a network pod to trigger re-creation of the Calico custom resources Delete a network provider pod from a single node in the cluster, per the network provider specific instructions below. This will trigger creation of a new pod on that node, and the initialization of this will create the Calico custom resources containing Calico configuration. After this cluster networking should be fully restored. Canal Network Provider Delete one of the canal pods within the kube-system namespace. Calico Network Provider Delete one of the calico-node pods within the kube-system namespace. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to recover after deleting the Calico CRDs from a cluster"},{"location":"000020213/#how-to-recover-after-deleting-the-calico-crds-from-a-cluster","text":"This document (000020213) is provided subject to the disclaimer at the end of this document.","title":"How to recover after deleting the Calico CRDs from a cluster"},{"location":"000020213/#situation","text":"","title":"Situation"},{"location":"000020213/#issue","text":"Calico uses a number of Custom Resource Definitions (CRDs) in order to store configuration data in Custom Resources. In the event that these CRDs are accidentally deleted from a cluster by a user, the configuration data in these Custom Resources will be deleted, preventing successful programming of pod networking. This article documents how to recreate the CRDs and ensure the configuration data is also re-populated.","title":"Issue"},{"location":"000020213/#pre-requisites","text":"A Kubernetes v1.8.x - v1.16.x cluster provisioned by the RKE CLI or Rancher v2.x, running with the Canal or Calico network providers","title":"Pre-requisites"},{"location":"000020213/#resolution","text":"","title":"Resolution"},{"location":"000020213/#1-re-create-the-crds","text":"The first step is to re-create the CRDs. The definitions are dependent upon the Kubernetes version running in the cluster, as well as whether the cluster is running the Canal or Calico network provider. Please refer to the matching network provider and Kubernetes version combination below: Canal Network Provider and Kubernetes version 1.8.x - 1.12.x Download the canal-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-8-to-1-12.yaml Canal Network Provider and Kubernetes version 1.13.x - 1.14.x Download the canal-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-13-to-1-14.yaml Canal Network Provider and Kubernetes version 1.15.x Download the canal-calico-crds-k8s-1-15.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-15.yaml Canal Network Provider and Kubernetes version 1.16.x Download the canal-calico-crds-k8s-1-16.yaml file and apply this to the cluster: kubectl apply -f canal-calico-crds-k8s-1-16.yaml Calico Network Provider and Kubernetes version 1.8.x - 1.12.x Download the calico-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-8-to-1-12.yaml Calico Network Provider and Kubernetes version 1.13.x - 1.14.x Download the calico-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-13-to-1-14.yaml Calico Network Provider and Kubernetes version 1.15.x Download the calico-calico-crds-k8s-1-15.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-15.yaml Calico Network Provider and Kubernetes version 1.16.x Download the calico-calico-crds-k8s-1-16.yaml file and apply this to the cluster: kubectl apply -f calico-calico-crds-k8s-1-16.yaml","title":"1. Re-create the CRDs"},{"location":"000020213/#2-delete-a-network-pod-to-trigger-re-creation-of-the-calico-custom-resources","text":"Delete a network provider pod from a single node in the cluster, per the network provider specific instructions below. This will trigger creation of a new pod on that node, and the initialization of this will create the Calico custom resources containing Calico configuration. After this cluster networking should be fully restored. Canal Network Provider Delete one of the canal pods within the kube-system namespace. Calico Network Provider Delete one of the calico-node pods within the kube-system namespace.","title":"2. Delete a network pod to trigger re-creation of the Calico custom resources"},{"location":"000020213/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020214/","text":"'Error: snapshot missing hash but --skip-hash-check=false' when performing `rke etcd snapshort-restore` with .zip extension included in snapshot name This document (000020214) is provided subject to the disclaimer at the end of this document. Situation Issue When performing an etcd snapshot restore via RKE, including the .zip file extension in the snapshot name parameter, i.e. rke etcd snapshot-restore --name snapshot.zip and a snapshot filename of snapshot.zip , the restoration fails with an error of the following format: FATA[0020] [etcd] Failed to restore etcd snapshot: Failed to run etcd restore container, exit status is: 128, container logs: Error: snapshot missing hash but --skip-hash-check=false Pre-requisites This issue is applicable to RKE CLI v0.2.x, starting with v0.2.5, v0.3.x and v1.0.x Resolution This issue is caused by the incorrect inclusion of the .zip file extension to the snapshot name parameter. The snapshot name parameter ( --name ) should contain the snapshot name, excluding the file extension. In the example of a snapshot filename of snapshot.zip the correct name parameter is therefore just snapshot , i.e. rke etcd snapshot-restore --name snapshot . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"'Error: snapshot missing hash but --skip-hash-check=false' when performing \\`rke etcd snapshort-restore\\` with .zip extension included in snapshot name"},{"location":"000020214/#error-snapshot-missing-hash-but-skip-hash-checkfalse-when-performing-rke-etcd-snapshort-restore-with-zip-extension-included-in-snapshot-name","text":"This document (000020214) is provided subject to the disclaimer at the end of this document.","title":"'Error: snapshot missing hash but --skip-hash-check=false' when performing `rke etcd snapshort-restore` with .zip extension included in snapshot name"},{"location":"000020214/#situation","text":"","title":"Situation"},{"location":"000020214/#issue","text":"When performing an etcd snapshot restore via RKE, including the .zip file extension in the snapshot name parameter, i.e. rke etcd snapshot-restore --name snapshot.zip and a snapshot filename of snapshot.zip , the restoration fails with an error of the following format: FATA[0020] [etcd] Failed to restore etcd snapshot: Failed to run etcd restore container, exit status is: 128, container logs: Error: snapshot missing hash but --skip-hash-check=false","title":"Issue"},{"location":"000020214/#pre-requisites","text":"This issue is applicable to RKE CLI v0.2.x, starting with v0.2.5, v0.3.x and v1.0.x","title":"Pre-requisites"},{"location":"000020214/#resolution","text":"This issue is caused by the incorrect inclusion of the .zip file extension to the snapshot name parameter. The snapshot name parameter ( --name ) should contain the snapshot name, excluding the file extension. In the example of a snapshot filename of snapshot.zip the correct name parameter is therefore just snapshot , i.e. rke etcd snapshot-restore --name snapshot .","title":"Resolution"},{"location":"000020214/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020215/","text":"How to configure the Docker bridge IP range on RancherOS v1.5 This document (000020215) is provided subject to the disclaimer at the end of this document. Situation Task In RancherOS v1.5.x the docker0 bridge network has a default IP range of 172.17.0.0/16 and the docker-sys bridge for system-docker has a default range of 172.18.0.0/16 . This article details how to update these ranges. Pre-requisites A RancherOS v1.5.x host Resolution The docker0 bridge range is set by the rancher.docker.bip argument, whilst the docker-sys bridge is set by rancher.system_docker.bip argument. These can be configured in the cloud-config as follows: rancher: docker: bip: 192.168.0.0/16 system_docker: bip: 172.19.0.0/16 These can also be customized after the host has started with the ros config command, and will take effect after a reboot: ros config set rancher.docker.bip 192.168.0.0/16 ros config set rancher.system_docker.bip 172.19.0.0/16 Further reading You can read more on configuring Docker or System Docker within the RancherOS documentation . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to configure the Docker bridge IP range on RancherOS v1.5"},{"location":"000020215/#how-to-configure-the-docker-bridge-ip-range-on-rancheros-v15","text":"This document (000020215) is provided subject to the disclaimer at the end of this document.","title":"How to configure the Docker bridge IP range on RancherOS v1.5"},{"location":"000020215/#situation","text":"","title":"Situation"},{"location":"000020215/#task","text":"In RancherOS v1.5.x the docker0 bridge network has a default IP range of 172.17.0.0/16 and the docker-sys bridge for system-docker has a default range of 172.18.0.0/16 . This article details how to update these ranges.","title":"Task"},{"location":"000020215/#pre-requisites","text":"A RancherOS v1.5.x host","title":"Pre-requisites"},{"location":"000020215/#resolution","text":"The docker0 bridge range is set by the rancher.docker.bip argument, whilst the docker-sys bridge is set by rancher.system_docker.bip argument. These can be configured in the cloud-config as follows: rancher: docker: bip: 192.168.0.0/16 system_docker: bip: 172.19.0.0/16 These can also be customized after the host has started with the ros config command, and will take effect after a reboot: ros config set rancher.docker.bip 192.168.0.0/16 ros config set rancher.system_docker.bip 172.19.0.0/16","title":"Resolution"},{"location":"000020215/#further-reading","text":"You can read more on configuring Docker or System Docker within the RancherOS documentation .","title":"Further reading"},{"location":"000020215/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020216/","text":"subPath does not work with hostPath volumes in Rancher v2.x or RKE CLI launched Kubernetes clusters This document (000020216) is provided subject to the disclaimer at the end of this document. Situation Issue Attempting to use the subPath option with a hostPath volume in a Rancher Kubernetes Engine (RKE) CLI, or Rancher v2.x, launched Kubernetes fails. The particular failure type depends upon the value of the type field specified on the hostPath volume. type undefined If no type is specified (defined as Anything: do not check the target path within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format Error: lstat /site-data: no such file or directory spec: containers: - name: nginx image: nginx:latest volumeMounts: - mountPath: /volume/nginx name: site-data subPath: nginx volumes: - name: site-data hostPath: path: /site-data type: Directory If the type is specified as Directory (defined as An existing directory within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format MountVolume.SetUp failed for volume \"site-data\" : hostPath type check failed: /site-data is not a directory spec: containers: - name: nginx image: nginx:latest volumeMounts: - mountPath: /volume/nginx name: site-data subPath: nginx volumes: - name: site-data hostPath: path: /site-data type: Directory type: DirectoryOrCreate If the type is specified as DirectoryOrCreate (defined as A directory, or create if it does not exist within the Rancher UI), per the example spec below, then the Pod will start successfully; however, an empty local volume will be mounted in the container at the mountPath , rather than this being bind-mounted to the path on the host as expected. spec: containers: - name: nginx image: nginx:latest volumeMounts: - mountPath: /volume/nginx name: site-data subPath: nginx volumes: - name: site-data hostPath: path: /site-data type: DirectoryOrCreate Pre-requisites Any RKE CLI v0.1.x or v0.2.x, or Rancher v2.x launched Kubernetes cluster (except those launched in hosted Kubernetes providers ) Root Cause This behavior is a result of the containerized kubelet process in RKE and Rancher launched Kubernetes clusters, and is tracked in GitHub issue https://github.com/rancher/rancher/issues/14836 Pending resolution of this issue, you should avoid the use of the subPath option on hostPath volumes, to prevent encountering this. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"subPath does not work with hostPath volumes in Rancher v2.x or RKE CLI launched Kubernetes clusters"},{"location":"000020216/#subpath-does-not-work-with-hostpath-volumes-in-rancher-v2x-or-rke-cli-launched-kubernetes-clusters","text":"This document (000020216) is provided subject to the disclaimer at the end of this document.","title":"subPath does not work with hostPath volumes in Rancher v2.x or RKE CLI launched Kubernetes clusters"},{"location":"000020216/#situation","text":"","title":"Situation"},{"location":"000020216/#issue","text":"Attempting to use the subPath option with a hostPath volume in a Rancher Kubernetes Engine (RKE) CLI, or Rancher v2.x, launched Kubernetes fails. The particular failure type depends upon the value of the type field specified on the hostPath volume.","title":"Issue"},{"location":"000020216/#type-undefined","text":"If no type is specified (defined as Anything: do not check the target path within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format Error: lstat /site-data: no such file or directory spec: containers: - name: nginx image: nginx:latest volumeMounts: - mountPath: /volume/nginx name: site-data subPath: nginx volumes: - name: site-data hostPath: path: /site-data","title":"type undefined"},{"location":"000020216/#type-directory","text":"If the type is specified as Directory (defined as An existing directory within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format MountVolume.SetUp failed for volume \"site-data\" : hostPath type check failed: /site-data is not a directory spec: containers: - name: nginx image: nginx:latest volumeMounts: - mountPath: /volume/nginx name: site-data subPath: nginx volumes: - name: site-data hostPath: path: /site-data type: Directory","title":"type: Directory"},{"location":"000020216/#type-directoryorcreate","text":"If the type is specified as DirectoryOrCreate (defined as A directory, or create if it does not exist within the Rancher UI), per the example spec below, then the Pod will start successfully; however, an empty local volume will be mounted in the container at the mountPath , rather than this being bind-mounted to the path on the host as expected. spec: containers: - name: nginx image: nginx:latest volumeMounts: - mountPath: /volume/nginx name: site-data subPath: nginx volumes: - name: site-data hostPath: path: /site-data type: DirectoryOrCreate","title":"type: DirectoryOrCreate"},{"location":"000020216/#pre-requisites","text":"Any RKE CLI v0.1.x or v0.2.x, or Rancher v2.x launched Kubernetes cluster (except those launched in hosted Kubernetes providers )","title":"Pre-requisites"},{"location":"000020216/#root-cause","text":"This behavior is a result of the containerized kubelet process in RKE and Rancher launched Kubernetes clusters, and is tracked in GitHub issue https://github.com/rancher/rancher/issues/14836 Pending resolution of this issue, you should avoid the use of the subPath option on hostPath volumes, to prevent encountering this.","title":"Root Cause"},{"location":"000020216/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020217/","text":"How to install or upgrade to a specific Rancher v2.x version This document (000020217) is provided subject to the disclaimer at the end of this document. Situation Issue By default the installation and upgrade documentation references the installation of, or upgrade to, the most recently released latest or stable tagged version of Rancher. This article details how to install a specific version, both in a single node and high availability installation. For details on the difference between the latest and stable releases please see the documentation on 'Choosing a Version' . N.B. We strongly recommend you only run product releases tagged \u201cStable\u201d in your production and any other business-critical environments. Any product release with the \u201cLatest\u201d tag should only be used for testing the latest releases.\" Resolution Single Node Install To install or upgrade to a specific Rancher version in a single node install, you can specify the exact version number of the image to run, rancher/rancher:vX.X.X , i.e.: docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ rancher/rancher:v2.2.2 High Availability (HA) Install To install or upgrade to a specific version in a High Availability install, you can specify the --version X.X.X parameter when running the helm install or helm upgrade command, i.e.: helm install rancher-stable/rancher \\ --name rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --version 2.2.2 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to install or upgrade to a specific Rancher v2.x version"},{"location":"000020217/#how-to-install-or-upgrade-to-a-specific-rancher-v2x-version","text":"This document (000020217) is provided subject to the disclaimer at the end of this document.","title":"How to install or upgrade to a specific Rancher v2.x version"},{"location":"000020217/#situation","text":"","title":"Situation"},{"location":"000020217/#issue","text":"By default the installation and upgrade documentation references the installation of, or upgrade to, the most recently released latest or stable tagged version of Rancher. This article details how to install a specific version, both in a single node and high availability installation. For details on the difference between the latest and stable releases please see the documentation on 'Choosing a Version' . N.B. We strongly recommend you only run product releases tagged \u201cStable\u201d in your production and any other business-critical environments. Any product release with the \u201cLatest\u201d tag should only be used for testing the latest releases.\"","title":"Issue"},{"location":"000020217/#resolution","text":"","title":"Resolution"},{"location":"000020217/#single-node-install","text":"To install or upgrade to a specific Rancher version in a single node install, you can specify the exact version number of the image to run, rancher/rancher:vX.X.X , i.e.: docker run -d --restart=unless-stopped \\ -p 80:80 -p 443:443 \\ rancher/rancher:v2.2.2","title":"Single Node Install"},{"location":"000020217/#high-availability-ha-install","text":"To install or upgrade to a specific version in a High Availability install, you can specify the --version X.X.X parameter when running the helm install or helm upgrade command, i.e.: helm install rancher-stable/rancher \\ --name rancher \\ --namespace cattle-system \\ --set hostname=rancher.my.org \\ --version 2.2.2","title":"High Availability (HA) Install"},{"location":"000020217/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020218/","text":"Why are namespaces created via the kubectl CLI not assigned to a project? This document (000020218) is provided subject to the disclaimer at the end of this document. Situation Question When a user creates a Kubernetes namespace via the Rancher UI, API or CLI the namespace is created within a specified Rancher project in the cluster; however, when a user creates a namespace via the kubectl CLI ( kubectl create ns <namespace> ) it is created outside of any project, why is this? Pre-requisites A cluster managed via Rancher v2.x Answer It is expected behaviour that namespaces created via the kubectl CLI are created outside of a Project. Projects are Rancher abstractions and do not exist natively within the Kubernetes, as a result when you create a namespace via the kubectl CLI (or by otherwise POST'ing directly to the kube-apiserver) it is not associated with any project in Rancher. If you wish to create namespaces within a Rancher Project with a command-line tool, then you should use the Rancher CLI ( https://rancher.com/docs/rancher/v2.x/en/cli/ ). Where a namespace has been created outside a project via kubectl, a cluster admin can move the namespace into a project, within the 'Projects/Namespaces' view for the cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Why are namespaces created via the kubectl CLI not assigned to a project?"},{"location":"000020218/#why-are-namespaces-created-via-the-kubectl-cli-not-assigned-to-a-project","text":"This document (000020218) is provided subject to the disclaimer at the end of this document.","title":"Why are namespaces created via the kubectl CLI not assigned to a project?"},{"location":"000020218/#situation","text":"","title":"Situation"},{"location":"000020218/#question","text":"When a user creates a Kubernetes namespace via the Rancher UI, API or CLI the namespace is created within a specified Rancher project in the cluster; however, when a user creates a namespace via the kubectl CLI ( kubectl create ns <namespace> ) it is created outside of any project, why is this?","title":"Question"},{"location":"000020218/#pre-requisites","text":"A cluster managed via Rancher v2.x","title":"Pre-requisites"},{"location":"000020218/#answer","text":"It is expected behaviour that namespaces created via the kubectl CLI are created outside of a Project. Projects are Rancher abstractions and do not exist natively within the Kubernetes, as a result when you create a namespace via the kubectl CLI (or by otherwise POST'ing directly to the kube-apiserver) it is not associated with any project in Rancher. If you wish to create namespaces within a Rancher Project with a command-line tool, then you should use the Rancher CLI ( https://rancher.com/docs/rancher/v2.x/en/cli/ ). Where a namespace has been created outside a project via kubectl, a cluster admin can move the namespace into a project, within the 'Projects/Namespaces' view for the cluster.","title":"Answer"},{"location":"000020218/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020219/","text":"'SIGSEGV: segmentation violation' in prometheus container of the prometheus-project-monitoring-0 Pod when enabling Project monitoring on the System Project This document (000020219) is provided subject to the disclaimer at the end of this document. Situation Issue Enabling project monitoring in a Rancher v2.2 cluster, in which cluster monitoring is enabled, fails with the Prometheus Pod in a CrashLoopBackOff. The prometheus container in the prometheus-project-monitoring StatefulSet fails with an error of the following format: panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x669c12] goroutine 437 [running]: net/http.(*Client).deadline(0x0, 0xc005381070, 0x40bb8f, 0xc0055e3600) /usr/local/go/src/net/http/client.go:187 +0x22 net/http.(*Client).do(0x0, 0xc005cdaa00, 0x0, 0x0, 0x0) /usr/local/go/src/net/http/client.go:527 +0xab net/http.(*Client).Do(0x0, 0xc005cdaa00, 0x23, 0xc002802230, 0x9) /usr/local/go/src/net/http/client.go:509 +0x35 github.com/prometheus/prometheus/scrape.(*targetScraper).scrape(0xc0060fa960, 0x1fd4a60, 0xc00010ec60, 0x1fb2760, 0xc0002eb110, 0x0, 0x0, 0x0, 0x0) /app/scrape/scrape.go:471 +0x111 github.com/prometheus/prometheus/scrape.(*scrapeLoop).run(0xc00616a100, 0xdf8475800, 0x2540be400, 0x0) /app/scrape/scrape.go:813 +0x487 created by github.com/prometheus/prometheus/scrape.(*scrapePool).sync /app/scrape/scrape.go:336 +0x45d Pre-requisites A cluster managed by Rancher v2.2 Cluster monitoring enabled and Project monitoring enabled on the System project Resolution Project monitoring is not compatible with the Rancher System project and should not be enabled in the System project. Starting with Rancher v2.3.0 monitoring of the System project is performed by cluster monitoring, when this is enabled, and the UI prevents enabling of project monitoring on the System project. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"'SIGSEGV: segmentation violation' in prometheus container of the prometheus-project-monitoring-0 Pod when enabling Project monitoring on the System Project"},{"location":"000020219/#sigsegv-segmentation-violation-in-prometheus-container-of-the-prometheus-project-monitoring-0-pod-when-enabling-project-monitoring-on-the-system-project","text":"This document (000020219) is provided subject to the disclaimer at the end of this document.","title":"'SIGSEGV: segmentation violation' in prometheus container of the prometheus-project-monitoring-0 Pod when enabling Project monitoring on the System Project"},{"location":"000020219/#situation","text":"","title":"Situation"},{"location":"000020219/#issue","text":"Enabling project monitoring in a Rancher v2.2 cluster, in which cluster monitoring is enabled, fails with the Prometheus Pod in a CrashLoopBackOff. The prometheus container in the prometheus-project-monitoring StatefulSet fails with an error of the following format: panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x669c12] goroutine 437 [running]: net/http.(*Client).deadline(0x0, 0xc005381070, 0x40bb8f, 0xc0055e3600) /usr/local/go/src/net/http/client.go:187 +0x22 net/http.(*Client).do(0x0, 0xc005cdaa00, 0x0, 0x0, 0x0) /usr/local/go/src/net/http/client.go:527 +0xab net/http.(*Client).Do(0x0, 0xc005cdaa00, 0x23, 0xc002802230, 0x9) /usr/local/go/src/net/http/client.go:509 +0x35 github.com/prometheus/prometheus/scrape.(*targetScraper).scrape(0xc0060fa960, 0x1fd4a60, 0xc00010ec60, 0x1fb2760, 0xc0002eb110, 0x0, 0x0, 0x0, 0x0) /app/scrape/scrape.go:471 +0x111 github.com/prometheus/prometheus/scrape.(*scrapeLoop).run(0xc00616a100, 0xdf8475800, 0x2540be400, 0x0) /app/scrape/scrape.go:813 +0x487 created by github.com/prometheus/prometheus/scrape.(*scrapePool).sync /app/scrape/scrape.go:336 +0x45d","title":"Issue"},{"location":"000020219/#pre-requisites","text":"A cluster managed by Rancher v2.2 Cluster monitoring enabled and Project monitoring enabled on the System project","title":"Pre-requisites"},{"location":"000020219/#resolution","text":"Project monitoring is not compatible with the Rancher System project and should not be enabled in the System project. Starting with Rancher v2.3.0 monitoring of the System project is performed by cluster monitoring, when this is enabled, and the UI prevents enabling of project monitoring on the System project.","title":"Resolution"},{"location":"000020219/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020220/","text":"Is it possible to migrate a Rancher launched Kubernetes cluster between Rancher instances? This document (000020220) is provided subject to the disclaimer at the end of this document. Situation Question Is it possible to migrate a Rancher launched Kubernetes cluster from one Rancher server instance to another, e.g. to launch a custom cluster using one Rancher server, and then at a later time, to migrate this to be managed instead via a different Rancher instance? Pre-requisites A Kubernetes cluster launched and managed by Rancher v2.x Answer No, it is not possible to migrate a cluster between Rancher server instances. A feature request for this is tracked in GitHub Issue #16471 . Currently, if you launch a Kubernetes cluster in one Rancher instance, then later attempt to use the imported cluster feature to import this cluster into another Rancher instance, you will lose any ability to add or remove nodes from the cluster, perform etcd backups or disaster recovery, or to edit any of the cluster configuration. We would therefore strongly recommend against this. Instead, we recommend performing regular Rancher server backups , so that you can recover the Rancher server cluster in a disaster recovery scenario, ensuring successful on-going management of downstream clusters launched by the server. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is it possible to migrate a Rancher launched Kubernetes cluster between Rancher instances?"},{"location":"000020220/#is-it-possible-to-migrate-a-rancher-launched-kubernetes-cluster-between-rancher-instances","text":"This document (000020220) is provided subject to the disclaimer at the end of this document.","title":"Is it possible to migrate a Rancher launched Kubernetes cluster between Rancher instances?"},{"location":"000020220/#situation","text":"","title":"Situation"},{"location":"000020220/#question","text":"Is it possible to migrate a Rancher launched Kubernetes cluster from one Rancher server instance to another, e.g. to launch a custom cluster using one Rancher server, and then at a later time, to migrate this to be managed instead via a different Rancher instance?","title":"Question"},{"location":"000020220/#pre-requisites","text":"A Kubernetes cluster launched and managed by Rancher v2.x","title":"Pre-requisites"},{"location":"000020220/#answer","text":"No, it is not possible to migrate a cluster between Rancher server instances. A feature request for this is tracked in GitHub Issue #16471 . Currently, if you launch a Kubernetes cluster in one Rancher instance, then later attempt to use the imported cluster feature to import this cluster into another Rancher instance, you will lose any ability to add or remove nodes from the cluster, perform etcd backups or disaster recovery, or to edit any of the cluster configuration. We would therefore strongly recommend against this. Instead, we recommend performing regular Rancher server backups , so that you can recover the Rancher server cluster in a disaster recovery scenario, ensuring successful on-going management of downstream clusters launched by the server.","title":"Answer"},{"location":"000020220/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020221/","text":"Is it possible to update the Cluster (Pod) CIDR or Service CIDR for an RKE CLI or Rancher launched cluster post-provisioning? This document (000020221) is provided subject to the disclaimer at the end of this document. Situation Question The Cluster (Pod) CIDR (default 10.42.0.0/16) and Service CIDR (default 10.43.0.0/16) ranges for a cluster can be specified in the cluster configuration YAML when launching a Kubernetes cluster via both the RKE CLI and Rancher v2.x. Is it possible to change these values after the cluster has been provisioned? Pre-requisites A Kubernetes cluster launched by the RKE CLI, or Rancher v2.x Answer Updating either the Cluster (Pod) CIDR or Service CIDR after the cluster has been provisioned is not supported and you should be careful to set these as required when first configuring the cluster. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is it possible to update the Cluster (Pod) CIDR or Service CIDR for an RKE CLI or Rancher launched cluster post-provisioning?"},{"location":"000020221/#is-it-possible-to-update-the-cluster-pod-cidr-or-service-cidr-for-an-rke-cli-or-rancher-launched-cluster-post-provisioning","text":"This document (000020221) is provided subject to the disclaimer at the end of this document.","title":"Is it possible to update the Cluster (Pod) CIDR or Service CIDR for an RKE CLI or Rancher launched cluster post-provisioning?"},{"location":"000020221/#situation","text":"","title":"Situation"},{"location":"000020221/#question","text":"The Cluster (Pod) CIDR (default 10.42.0.0/16) and Service CIDR (default 10.43.0.0/16) ranges for a cluster can be specified in the cluster configuration YAML when launching a Kubernetes cluster via both the RKE CLI and Rancher v2.x. Is it possible to change these values after the cluster has been provisioned?","title":"Question"},{"location":"000020221/#pre-requisites","text":"A Kubernetes cluster launched by the RKE CLI, or Rancher v2.x","title":"Pre-requisites"},{"location":"000020221/#answer","text":"Updating either the Cluster (Pod) CIDR or Service CIDR after the cluster has been provisioned is not supported and you should be careful to set these as required when first configuring the cluster.","title":"Answer"},{"location":"000020221/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020222/","text":"\"fatal: could not read Username for 'http://host:port': No such device or address\" error for pipeline Publish Catalog Template step in Rancher v2.2 when Git URL contains a port This document (000020222) is provided subject to the disclaimer at the end of this document. Situation Issue If a pipeline is configured in Rancher v2.2 with a Publish Catalog Template step, in which the specified Git URL contains a port, the step will fail to execute with an error of the format fatal: could not read Username for 'http://host:port': No such device or address\" . Pre-requisites A cluster managed by Rancher v2.2 A pipeline configured with a Publish Catalog Template step, in which the Git URL contains a port Resolution A patch was developed for this issue, and is available in Rancher v2.3.0 and above. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\"fatal: could not read Username for 'http://host:port': No such device or address\" error for pipeline Publish Catalog Template step in Rancher v2.2 when Git URL contains a port"},{"location":"000020222/#fatal-could-not-read-username-for-httphostport-no-such-device-or-address-error-for-pipeline-publish-catalog-template-step-in-rancher-v22-when-git-url-contains-a-port","text":"This document (000020222) is provided subject to the disclaimer at the end of this document.","title":"\"fatal: could not read Username for 'http://host:port': No such device or address\" error for pipeline Publish Catalog Template step in Rancher v2.2 when Git URL contains a port"},{"location":"000020222/#situation","text":"","title":"Situation"},{"location":"000020222/#issue","text":"If a pipeline is configured in Rancher v2.2 with a Publish Catalog Template step, in which the specified Git URL contains a port, the step will fail to execute with an error of the format fatal: could not read Username for 'http://host:port': No such device or address\" .","title":"Issue"},{"location":"000020222/#pre-requisites","text":"A cluster managed by Rancher v2.2 A pipeline configured with a Publish Catalog Template step, in which the Git URL contains a port","title":"Pre-requisites"},{"location":"000020222/#resolution","text":"A patch was developed for this issue, and is available in Rancher v2.3.0 and above.","title":"Resolution"},{"location":"000020222/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020223/","text":"Groups assigned Project roles do not display under the Edit Project view in the Rancher v2.2.4 UI This document (000020223) is provided subject to the disclaimer at the end of this document. Situation Issue Despite being able to successfully assign user groups a project role, the groups are not listed under Members in the Edit Project view in the Rancher v2.2.4 UI. After adding a new group to the project as follows: The newly added group does not display in the list: However, querying the projectRoleTemplateBindings for the project via the Rancher API confirms it has been successfully added: { \"annotations\": { \"lifecycle.cattle.io/create.cluster-prtb-sync_c-r5gcn\": \"true\", \"lifecycle.cattle.io/create.mgmt-auth-prtb-controller\": \"true\" }, \"baseType\": \"projectRoleTemplateBinding\", \"created\": \"2019-09-19T10:19:08Z\", \"createdTS\": 1568888348000, \"creatorId\": \"user-2qtnq\", \"groupId\": null, \"groupPrincipalId\": \"freeipa_group://cn=developers,cn=groups,cn=accounts,dc=ipa,dc=example,dc=com\", \"id\": \"p-nbhcs:prtb-nq2c9\", \"labels\": { \"cattle.io/creator\": \"norman\" }, \"links\": { \"remove\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\", \"self\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\", \"update\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\" }, \"name\": \"prtb-nq2c9\", \"namespaceId\": null, \"projectId\": \"c-r5gcn:p-nbhcs\", \"roleTemplateId\": \"project-member\", \"type\": \"projectRoleTemplateBinding\", \"userId\": null, \"userPrincipalId\": null, \"uuid\": \"ead04e5d-dac6-11e9-9a87-0242ac110002\" } Pre-requisites This issue only affects Rancher v2.2.4 Root Cause This issue is caused by a UI bug ( GitHub Issue #20760 ) in Rancher v2.2.4 that was patched in Rancher v2.2.5. Resolution Users should upgrade Rancher to v2.2.5 or above to resolve the issue. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Groups assigned Project roles do not display under the Edit Project view in the Rancher v2.2.4 UI"},{"location":"000020223/#groups-assigned-project-roles-do-not-display-under-the-edit-project-view-in-the-rancher-v224-ui","text":"This document (000020223) is provided subject to the disclaimer at the end of this document.","title":"Groups assigned Project roles do not display under the Edit Project view in the Rancher v2.2.4 UI"},{"location":"000020223/#situation","text":"","title":"Situation"},{"location":"000020223/#issue","text":"Despite being able to successfully assign user groups a project role, the groups are not listed under Members in the Edit Project view in the Rancher v2.2.4 UI. After adding a new group to the project as follows: The newly added group does not display in the list: However, querying the projectRoleTemplateBindings for the project via the Rancher API confirms it has been successfully added: { \"annotations\": { \"lifecycle.cattle.io/create.cluster-prtb-sync_c-r5gcn\": \"true\", \"lifecycle.cattle.io/create.mgmt-auth-prtb-controller\": \"true\" }, \"baseType\": \"projectRoleTemplateBinding\", \"created\": \"2019-09-19T10:19:08Z\", \"createdTS\": 1568888348000, \"creatorId\": \"user-2qtnq\", \"groupId\": null, \"groupPrincipalId\": \"freeipa_group://cn=developers,cn=groups,cn=accounts,dc=ipa,dc=example,dc=com\", \"id\": \"p-nbhcs:prtb-nq2c9\", \"labels\": { \"cattle.io/creator\": \"norman\" }, \"links\": { \"remove\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\", \"self\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\", \"update\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\" }, \"name\": \"prtb-nq2c9\", \"namespaceId\": null, \"projectId\": \"c-r5gcn:p-nbhcs\", \"roleTemplateId\": \"project-member\", \"type\": \"projectRoleTemplateBinding\", \"userId\": null, \"userPrincipalId\": null, \"uuid\": \"ead04e5d-dac6-11e9-9a87-0242ac110002\" }","title":"Issue"},{"location":"000020223/#pre-requisites","text":"This issue only affects Rancher v2.2.4","title":"Pre-requisites"},{"location":"000020223/#root-cause","text":"This issue is caused by a UI bug ( GitHub Issue #20760 ) in Rancher v2.2.4 that was patched in Rancher v2.2.5.","title":"Root Cause"},{"location":"000020223/#resolution","text":"Users should upgrade Rancher to v2.2.5 or above to resolve the issue.","title":"Resolution"},{"location":"000020223/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020224/","text":"Cluster Logging (log forwarding) fails to deploy with restricted PodSecurityPolicy in Rancher v2.2 This document (000020224) is provided subject to the disclaimer at the end of this document. Situation Issue Attempting to enable Cluster Logging (log forwarding) in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the rancher-logging-fluentd and rancher-logging-log-aggregator Deployments failing to create Pods. The rancher-logging-fluentd Deployment fails to validate against the restricted PSP, with an error of the following format in events: Error creating: pods \"rancher-logging-fluentd-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[1]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[2]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[3]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[4]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[5]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[6]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[10]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used] The rancher-logging-log-aggregator Deployment fails to validate against the restricted PSP, with and error of the following format in events: Error creating: pods \"rancher-logging-log-aggregator-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed] Pre-requisites A cluster managed by Rancher v2.2 Cluster Logging enabled PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level Root Cause The logging system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, cluster logging will fail to deploy successfully. Workaround To workaround this issue, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with kubectl --config <cluster kubeconfig> apply -f <file> . apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: default-psp-role namespace: cattle-logging rules: - apiGroups: - extensions resourceNames: - default-psp resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-psp-rolebinding namespace: cattle-logging roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: default-psp-role subjects: - kind: Group name: system:serviceaccounts:cattle-logging apiGroup: rbac.authorization.k8s.io Resolution An update to ensure the logging system-charts are compatible with the restricted PodSecurityPolicy and is available in Rancher v2.3.0 and above. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Cluster Logging (log forwarding) fails to deploy with restricted PodSecurityPolicy in Rancher v2.2"},{"location":"000020224/#cluster-logging-log-forwarding-fails-to-deploy-with-restricted-podsecuritypolicy-in-rancher-v22","text":"This document (000020224) is provided subject to the disclaimer at the end of this document.","title":"Cluster Logging (log forwarding) fails to deploy with restricted PodSecurityPolicy in Rancher v2.2"},{"location":"000020224/#situation","text":"","title":"Situation"},{"location":"000020224/#issue","text":"Attempting to enable Cluster Logging (log forwarding) in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the rancher-logging-fluentd and rancher-logging-log-aggregator Deployments failing to create Pods. The rancher-logging-fluentd Deployment fails to validate against the restricted PSP, with an error of the following format in events: Error creating: pods \"rancher-logging-fluentd-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[1]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[2]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[3]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[4]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[5]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[6]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[10]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used] The rancher-logging-log-aggregator Deployment fails to validate against the restricted PSP, with and error of the following format in events: Error creating: pods \"rancher-logging-log-aggregator-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed]","title":"Issue"},{"location":"000020224/#pre-requisites","text":"A cluster managed by Rancher v2.2 Cluster Logging enabled PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level","title":"Pre-requisites"},{"location":"000020224/#root-cause","text":"The logging system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, cluster logging will fail to deploy successfully.","title":"Root Cause"},{"location":"000020224/#workaround","text":"To workaround this issue, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with kubectl --config <cluster kubeconfig> apply -f <file> . apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: default-psp-role namespace: cattle-logging rules: - apiGroups: - extensions resourceNames: - default-psp resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-psp-rolebinding namespace: cattle-logging roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: default-psp-role subjects: - kind: Group name: system:serviceaccounts:cattle-logging apiGroup: rbac.authorization.k8s.io","title":"Workaround"},{"location":"000020224/#resolution","text":"An update to ensure the logging system-charts are compatible with the restricted PodSecurityPolicy and is available in Rancher v2.3.0 and above.","title":"Resolution"},{"location":"000020224/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020225/","text":"How to migrate from CentOS packaged to upstream Docker This document (000020225) is provided subject to the disclaimer at the end of this document. Situation Task This article will describe the process by which you can migrate a CentOS node in a Rancher cluster from running the CentOS packaged Docker package to the upstream package from Docker. In order to perform this migration you will be required to first uninstall the CentOS packaged Docker, before installing the upstream version. This process is destructive, and will remove all container state from the host. As a result the process outlined below, will guide you through first removing the node from the Rancher cluster, before conducting the package migration, then finally re-adding the node to the cluster. Pre-requisites A Kubernetes cluster launched with the Rancher Kubernetes Engine (RKE) CLI , v0.1.x or v0.2.x, or a Rancher v2.x launched Kubernetes cluster on custom nodes Nodes running CentOS 7.x, with Docker installed from the CentOS extras repository. Resolution Cluster launched by the RKE CLI Create a Backup As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the RKE documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster. Perform migration on each cluster node in turn Check if you should first add an additional node to the cluster, to replace the node during its migration: Controlplane or etcd nodes In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, or add the role(s) to an existing node, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node to the cluster configuration YAML and run rke up to provision this. Worker nodes If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node to the cluster configuration YAML and run rke up to provision this. Remove the node which you are migrating from the cluster, to do so remove the node from the cluster configuration YAML and then run rke up to reconcile the cluster. Once the rke up invocation in step 2. completes successfully, run the Extended Rancher 2 cleanup script on the node that you are migrating, to clean up Rancher state. Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section Uninstall old versions here . Add the node back to the cluster configuration YAML and run rke up to provision it. Custom cluster launched by Rancher Create a Backup As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the Rancher documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster, if S3 backups are not configured for the cluster. Perform migration on each cluster node in turn Check if you should first add an additional node to the cluster, to replace the node during its migration: Controlplane or etcd nodes In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the replacement node. Worker nodes If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node by running the Rancher agent command from the 'Edit Cluster' view, with the worker role, on the replacement node. Remove the node which you are migrating from the cluster, to do so delete it from the node list for the cluster within Rancher. Once the cluster reconciliation triggered by step 2. is complete, and the cluster no longer shows as updating within Rancher, run the Extended Rancher 2 cleanup script on the node that you are migrating to clean up Rancher state. Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section Uninstall old versions here . Add the node back by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the node. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to migrate from CentOS packaged to upstream Docker"},{"location":"000020225/#how-to-migrate-from-centos-packaged-to-upstream-docker","text":"This document (000020225) is provided subject to the disclaimer at the end of this document.","title":"How to migrate from CentOS packaged to upstream Docker"},{"location":"000020225/#situation","text":"","title":"Situation"},{"location":"000020225/#task","text":"This article will describe the process by which you can migrate a CentOS node in a Rancher cluster from running the CentOS packaged Docker package to the upstream package from Docker. In order to perform this migration you will be required to first uninstall the CentOS packaged Docker, before installing the upstream version. This process is destructive, and will remove all container state from the host. As a result the process outlined below, will guide you through first removing the node from the Rancher cluster, before conducting the package migration, then finally re-adding the node to the cluster.","title":"Task"},{"location":"000020225/#pre-requisites","text":"A Kubernetes cluster launched with the Rancher Kubernetes Engine (RKE) CLI , v0.1.x or v0.2.x, or a Rancher v2.x launched Kubernetes cluster on custom nodes Nodes running CentOS 7.x, with Docker installed from the CentOS extras repository.","title":"Pre-requisites"},{"location":"000020225/#resolution","text":"","title":"Resolution"},{"location":"000020225/#cluster-launched-by-the-rke-cli","text":"","title":"Cluster launched by the RKE CLI"},{"location":"000020225/#create-a-backup","text":"As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the RKE documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster.","title":"Create a Backup"},{"location":"000020225/#perform-migration-on-each-cluster-node-in-turn","text":"Check if you should first add an additional node to the cluster, to replace the node during its migration: Controlplane or etcd nodes In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, or add the role(s) to an existing node, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node to the cluster configuration YAML and run rke up to provision this. Worker nodes If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node to the cluster configuration YAML and run rke up to provision this. Remove the node which you are migrating from the cluster, to do so remove the node from the cluster configuration YAML and then run rke up to reconcile the cluster. Once the rke up invocation in step 2. completes successfully, run the Extended Rancher 2 cleanup script on the node that you are migrating, to clean up Rancher state. Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section Uninstall old versions here . Add the node back to the cluster configuration YAML and run rke up to provision it.","title":"Perform migration on each cluster node in turn"},{"location":"000020225/#custom-cluster-launched-by-rancher","text":"","title":"Custom cluster launched by Rancher"},{"location":"000020225/#create-a-backup_1","text":"As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the Rancher documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster, if S3 backups are not configured for the cluster.","title":"Create a Backup"},{"location":"000020225/#perform-migration-on-each-cluster-node-in-turn_1","text":"Check if you should first add an additional node to the cluster, to replace the node during its migration: Controlplane or etcd nodes In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the replacement node. Worker nodes If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node by running the Rancher agent command from the 'Edit Cluster' view, with the worker role, on the replacement node. Remove the node which you are migrating from the cluster, to do so delete it from the node list for the cluster within Rancher. Once the cluster reconciliation triggered by step 2. is complete, and the cluster no longer shows as updating within Rancher, run the Extended Rancher 2 cleanup script on the node that you are migrating to clean up Rancher state. Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section Uninstall old versions here . Add the node back by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the node.","title":"Perform migration on each cluster node in turn"},{"location":"000020225/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020226/","text":"How to rotate certificates for clusters launched by RKE v0.1.x or Rancher v2.0.x and v2.1.x This document (000020226) is provided subject to the disclaimer at the end of this document. Situation Task Kubernetes clusters use multiple certificates to provide both encryption of traffic to the Kubernetes components as well as authentication of these requests. These certificates are auto-generated for clusters launched by Rancher and also clusters launched by the Rancher Kubernetes Engine (RKE) CLI . In Rancher v2.0.x and v2.1.x, the auto-generated certificates for Rancher-launched Kubernetes clusters have a validity period of one year, meaning these certificates will expire one year after the cluster is provisioned. The same applies to Kubernetes clusters provisioned by v0.1.x of the Rancher Kubernetes Engine (RKE) CLI . If you created a Rancher-launched or RKE-provisioned Kubernetes cluster about 1 year ago, and have not already rotated the certificates, you need to rotate the certificates. If no action is taken, then when the certificates expire, the cluster will go into an error state and the Kubernetes API for the cluster will become unavailable. Rancher recommends that you rotate the certificates before they expire to avoid an unexpected service interruption. The rotation is a one time operation, and the newly-generated certificates will be valid for the next 10 years. Pre-requisites A Kubernetes cluster launched by RKE CLI v0.1.x, or Rancher v2.0.x and v2.1.x Resolution Full details on who to rotate the certificates for the both RKE and Rancher launched clusters can be found in the Rancher blog post \"Manual Rotation of Certificates in Rancher Kubernetes Clusters\" . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to rotate certificates for clusters launched by RKE v0.1.x or Rancher v2.0.x and v2.1.x"},{"location":"000020226/#how-to-rotate-certificates-for-clusters-launched-by-rke-v01x-or-rancher-v20x-and-v21x","text":"This document (000020226) is provided subject to the disclaimer at the end of this document.","title":"How to rotate certificates for clusters launched by RKE v0.1.x or Rancher v2.0.x and v2.1.x"},{"location":"000020226/#situation","text":"","title":"Situation"},{"location":"000020226/#task","text":"Kubernetes clusters use multiple certificates to provide both encryption of traffic to the Kubernetes components as well as authentication of these requests. These certificates are auto-generated for clusters launched by Rancher and also clusters launched by the Rancher Kubernetes Engine (RKE) CLI . In Rancher v2.0.x and v2.1.x, the auto-generated certificates for Rancher-launched Kubernetes clusters have a validity period of one year, meaning these certificates will expire one year after the cluster is provisioned. The same applies to Kubernetes clusters provisioned by v0.1.x of the Rancher Kubernetes Engine (RKE) CLI . If you created a Rancher-launched or RKE-provisioned Kubernetes cluster about 1 year ago, and have not already rotated the certificates, you need to rotate the certificates. If no action is taken, then when the certificates expire, the cluster will go into an error state and the Kubernetes API for the cluster will become unavailable. Rancher recommends that you rotate the certificates before they expire to avoid an unexpected service interruption. The rotation is a one time operation, and the newly-generated certificates will be valid for the next 10 years.","title":"Task"},{"location":"000020226/#pre-requisites","text":"A Kubernetes cluster launched by RKE CLI v0.1.x, or Rancher v2.0.x and v2.1.x","title":"Pre-requisites"},{"location":"000020226/#resolution","text":"Full details on who to rotate the certificates for the both RKE and Rancher launched clusters can be found in the Rancher blog post \"Manual Rotation of Certificates in Rancher Kubernetes Clusters\" .","title":"Resolution"},{"location":"000020226/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020227/","text":"Cluster and Project monitoring fail to deploy with restricted PodSecurityPolicy in Rancher v2.2 This document (000020227) is provided subject to the disclaimer at the end of this document. Situation Issue Attempting to enable Cluster or Project monitoring, in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the Cluster or Project, fails with the Grafana and Prometheus Pods in a CrashLoopBackOff. The grafana-proxy container in the grafana-cluster-monitoring / grafana-project-monitoring Deployment and the promtheus-proxy container in the prometheus-cluster-monitoring / prometheus-project-monitoring StatefulSet fail with an error of the following format: 2019/09/20 11:54:17 [warn] 1#1: duplicate MIME type \"text/html\" in /var/run/nginx.conf:46 nginx: [warn] duplicate MIME type \"text/html\" in /var/run/nginx.conf:46 2019/09/20 11:54:17 [emerg] 1#1: chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted) nginx: [emerg] chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted) The grafana container in the grafana-cluster-monitoring / grafana-project-monitoring Deployment fails to start with an error of the following format shown in the events for the Pod: Error: failed to start container \"grafana\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"/var/lib/kubelet/pods/6c9bbb63-db9a-11e9-932e-2af11a72a258/volume-subpaths/grafana-static-contents/grafana/1\\\\\\\" to rootfs \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged\\\\\\\" at \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged/usr/share/grafana/public/app/plugins/datasource/prometheus/plugin.json\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type Pre-requisites A cluster managed by Rancher v2.2 Cluster or Project monitoring enabled PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level, or on the Project for which monitoring is enabled Root Cause The monitoring system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, monitoring will fail to deploy successfully. Workaround To workaround the impact to cluster monitoring, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with kubectl --config <cluster kubeconfig> apply -f <file> . apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: default-psp-role namespace: cattle-prometheus rules: - apiGroups: - extensions resourceNames: - default-psp resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-psp-rolebinding namespace: cattle-prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: default-psp-role subjects: - kind: Group name: system:serviceaccounts:cattle-prometheus apiGroup: rbac.authorization.k8s.io Resolution An update to ensure the monitoring system-charts are compatible with the restricted PodSecurityPolicy is available in Rancher v2.3.0 and above. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Cluster and Project monitoring fail to deploy with restricted PodSecurityPolicy in Rancher v2.2"},{"location":"000020227/#cluster-and-project-monitoring-fail-to-deploy-with-restricted-podsecuritypolicy-in-rancher-v22","text":"This document (000020227) is provided subject to the disclaimer at the end of this document.","title":"Cluster and Project monitoring fail to deploy with restricted PodSecurityPolicy in Rancher v2.2"},{"location":"000020227/#situation","text":"","title":"Situation"},{"location":"000020227/#issue","text":"Attempting to enable Cluster or Project monitoring, in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the Cluster or Project, fails with the Grafana and Prometheus Pods in a CrashLoopBackOff. The grafana-proxy container in the grafana-cluster-monitoring / grafana-project-monitoring Deployment and the promtheus-proxy container in the prometheus-cluster-monitoring / prometheus-project-monitoring StatefulSet fail with an error of the following format: 2019/09/20 11:54:17 [warn] 1#1: duplicate MIME type \"text/html\" in /var/run/nginx.conf:46 nginx: [warn] duplicate MIME type \"text/html\" in /var/run/nginx.conf:46 2019/09/20 11:54:17 [emerg] 1#1: chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted) nginx: [emerg] chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted) The grafana container in the grafana-cluster-monitoring / grafana-project-monitoring Deployment fails to start with an error of the following format shown in the events for the Pod: Error: failed to start container \"grafana\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"/var/lib/kubelet/pods/6c9bbb63-db9a-11e9-932e-2af11a72a258/volume-subpaths/grafana-static-contents/grafana/1\\\\\\\" to rootfs \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged\\\\\\\" at \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged/usr/share/grafana/public/app/plugins/datasource/prometheus/plugin.json\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type","title":"Issue"},{"location":"000020227/#pre-requisites","text":"A cluster managed by Rancher v2.2 Cluster or Project monitoring enabled PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level, or on the Project for which monitoring is enabled","title":"Pre-requisites"},{"location":"000020227/#root-cause","text":"The monitoring system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, monitoring will fail to deploy successfully.","title":"Root Cause"},{"location":"000020227/#workaround","text":"To workaround the impact to cluster monitoring, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with kubectl --config <cluster kubeconfig> apply -f <file> . apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: default-psp-role namespace: cattle-prometheus rules: - apiGroups: - extensions resourceNames: - default-psp resources: - podsecuritypolicies verbs: - use --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-psp-rolebinding namespace: cattle-prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: default-psp-role subjects: - kind: Group name: system:serviceaccounts:cattle-prometheus apiGroup: rbac.authorization.k8s.io","title":"Workaround"},{"location":"000020227/#resolution","text":"An update to ensure the monitoring system-charts are compatible with the restricted PodSecurityPolicy is available in Rancher v2.3.0 and above.","title":"Resolution"},{"location":"000020227/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020228/","text":"'[object Object]' error attempting to edit Load Balancing or Service Discovery resource YAMLs via the UI in Rancher v2.1.7 - v2.2.3 This document (000020228) is provided subject to the disclaimer at the end of this document. Situation Issue Selecting View/Edit YAML on Load Balancing (Ingress) or Service Discovery (Service) resources in the Rancher UI for Rancher v2.1.7 - v2.2.3 fails to display the resource YAML and displays an error [object Object] per the screenshot below. Pre-requisites This issue is only applicable to Rancher v2.1.x, starting with v2.1.7, and Rancher v2.2.x, from Rancher v2.2.0 to Rancher v2.2.3 Root Cause Issue was caused by an incorrect content-length header returned by the Rancher API for gzipped responses ( GitHub Issue #19723 ) and was patched in Rancher v2.2.4. Workaround In affected versions of Rancher it is still possible to edit the YAML of Load Balancing (Ingress) or Service Discovery (Service) resources by using the kubectl CLI. Resolution Users should upgrade to Rancher v2.2.4 or above to resolve the issue. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"'\\[object Object\\]' error attempting to edit Load Balancing or Service Discovery resource YAMLs via the UI in Rancher v2.1.7 - v2.2.3"},{"location":"000020228/#object-object-error-attempting-to-edit-load-balancing-or-service-discovery-resource-yamls-via-the-ui-in-rancher-v217-v223","text":"This document (000020228) is provided subject to the disclaimer at the end of this document.","title":"'[object Object]' error attempting to edit Load Balancing or Service Discovery resource YAMLs via the UI in Rancher v2.1.7 - v2.2.3"},{"location":"000020228/#situation","text":"","title":"Situation"},{"location":"000020228/#issue","text":"Selecting View/Edit YAML on Load Balancing (Ingress) or Service Discovery (Service) resources in the Rancher UI for Rancher v2.1.7 - v2.2.3 fails to display the resource YAML and displays an error [object Object] per the screenshot below.","title":"Issue"},{"location":"000020228/#pre-requisites","text":"This issue is only applicable to Rancher v2.1.x, starting with v2.1.7, and Rancher v2.2.x, from Rancher v2.2.0 to Rancher v2.2.3","title":"Pre-requisites"},{"location":"000020228/#root-cause","text":"Issue was caused by an incorrect content-length header returned by the Rancher API for gzipped responses ( GitHub Issue #19723 ) and was patched in Rancher v2.2.4.","title":"Root Cause"},{"location":"000020228/#workaround","text":"In affected versions of Rancher it is still possible to edit the YAML of Load Balancing (Ingress) or Service Discovery (Service) resources by using the kubectl CLI.","title":"Workaround"},{"location":"000020228/#resolution","text":"Users should upgrade to Rancher v2.2.4 or above to resolve the issue.","title":"Resolution"},{"location":"000020228/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020229/","text":"No monitoring for etcd in the local Rancher cluster with cluster monitoring in Rancher v2.2.x This document (000020229) is provided subject to the disclaimer at the end of this document. Situation Issue When enabling cluster monitoring in Rancher v2.2.x, upon the local cluster running the Rancher server itself, no monitoring metrics are available for the etcd instances within the cluster. Pre-requisites An RKE provisioned Rancher HA cluster Rancher v2.2.x Cluster monitoring enabled on the local Rancher cluster Root cause Cluster monitoring was introduced in Rancher v2.2.0 and when enabled for a Rancher launched Kubernetes clusters , i.e. a custom cluster or using node templates for an infrastructure provider, will include etcd monitoring metrics. The local cluster running Rancher, whilst provisioned via RKE, is an imported cluster from the Rancher perspective. As a result etcd monitoring metrics are not collected and displayed for this cluster. Resolution An enhancement request to include etcd monitoring metric collection, for the local Rancher cluster, within Rancher cluster monitoring is tracked in Rancher Issue #18619 . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"No monitoring for etcd in the local Rancher cluster with cluster monitoring in Rancher v2.2.x"},{"location":"000020229/#no-monitoring-for-etcd-in-the-local-rancher-cluster-with-cluster-monitoring-in-rancher-v22x","text":"This document (000020229) is provided subject to the disclaimer at the end of this document.","title":"No monitoring for etcd in the local Rancher cluster with cluster monitoring in Rancher v2.2.x"},{"location":"000020229/#situation","text":"","title":"Situation"},{"location":"000020229/#issue","text":"When enabling cluster monitoring in Rancher v2.2.x, upon the local cluster running the Rancher server itself, no monitoring metrics are available for the etcd instances within the cluster.","title":"Issue"},{"location":"000020229/#pre-requisites","text":"An RKE provisioned Rancher HA cluster Rancher v2.2.x Cluster monitoring enabled on the local Rancher cluster","title":"Pre-requisites"},{"location":"000020229/#root-cause","text":"Cluster monitoring was introduced in Rancher v2.2.0 and when enabled for a Rancher launched Kubernetes clusters , i.e. a custom cluster or using node templates for an infrastructure provider, will include etcd monitoring metrics. The local cluster running Rancher, whilst provisioned via RKE, is an imported cluster from the Rancher perspective. As a result etcd monitoring metrics are not collected and displayed for this cluster.","title":"Root cause"},{"location":"000020229/#resolution","text":"An enhancement request to include etcd monitoring metric collection, for the local Rancher cluster, within Rancher cluster monitoring is tracked in Rancher Issue #18619 .","title":"Resolution"},{"location":"000020229/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020230/","text":"How to collect and share Rancher logs with Support This document (000020230) is provided subject to the disclaimer at the end of this document. Situation Issue When corresponding with Rancher Support to troubleshoot an issue, a common request is to retrieve environment and log data to assist with investigation. To standardise and simplify this data collection, product-specific scripts exist to retrieve the information, per the details below. Once logs are collected from each required node, per the direction of Rancher Support, this output can be uploaded to the Support case. Resolution Rancher v2.x Linux log collector Logs can be collected from a Linux node within a Rancher v2.x cluster using the Linux log collector script as detailed here . Rancher v2.x Windows log collector Logs can be collected from a Windows node within a Rancher v2.x cluster using the Windows log collector script as detailed here . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to collect and share Rancher logs with Support"},{"location":"000020230/#how-to-collect-and-share-rancher-logs-with-support","text":"This document (000020230) is provided subject to the disclaimer at the end of this document.","title":"How to collect and share Rancher logs with Support"},{"location":"000020230/#situation","text":"","title":"Situation"},{"location":"000020230/#issue","text":"When corresponding with Rancher Support to troubleshoot an issue, a common request is to retrieve environment and log data to assist with investigation. To standardise and simplify this data collection, product-specific scripts exist to retrieve the information, per the details below. Once logs are collected from each required node, per the direction of Rancher Support, this output can be uploaded to the Support case.","title":"Issue"},{"location":"000020230/#resolution","text":"","title":"Resolution"},{"location":"000020230/#rancher-v2x-linux-log-collector","text":"Logs can be collected from a Linux node within a Rancher v2.x cluster using the Linux log collector script as detailed here .","title":"Rancher v2.x Linux log collector"},{"location":"000020230/#rancher-v2x-windows-log-collector","text":"Logs can be collected from a Windows node within a Rancher v2.x cluster using the Windows log collector script as detailed here .","title":"Rancher v2.x Windows log collector"},{"location":"000020230/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020231/","text":"Network ingress traffic from 192.168.0.0/16 always SNAT'd in Kubernetes clusters with canal network provider This document (000020231) is provided subject to the disclaimer at the end of this document. Situation Issue Network ingress traffic to a Kubernetes cluster with the canal network provider, from IP addresses in the range 192.168.0.0/16 , is always SNAT'd, even in instances where this is not desired. For example on NodePort services configured with externalTrafficPolicy: Local the source IP should be preserved without SNAT, per the Kubernetes documentation . With this issue the source IP is SNAT'd even in instances of NodePort services configured with externalTrafficPolicy: Local . Pre-requisites A Kubernetes cluster provisioned via the RKE CLI or Rancher, using the canal network provider Root cause When a cluster is provisioned with the canal network provider selected, Flannel is used for networking and Calico for network policy enforcement, and IP address management is therefore managed by Flannel. The calico-node container in the canal pod is still configured with an (un-used) IP pool, which defaults to 192.168.0.0/16 . By default Calico programs iptables rules in the cali-nat-outgoing chain of the nat table on cluster nodes to perform SNAT on traffic from this IP pool. The purpose of these rules is to masquerade egress traffic from pods where Calico is used for networking (and not just network policy). As a result in a canal network provider cluster, where the calico-node container is present for network policy enforcement, these rules are programed and any ingress traffic from the range 192.168.0.0/16 will match and be SNAT'd. Resolution The permanent solution to prevent this issue is to update the RKE deployment templates for the canal daemonset, to set the environment variable CALICO_IPV4POOL_NAT_OUTGOING to 0 for the calico-node container. This will prevent programming of the problematic cali-nat-outgoing iptables rules and is tracked in Rancher Issue #20500 . In order to workaround the issue in existing clusters, the Calico ippool configuration can be edited to disable outgoing nat, which removes programming of the cali-nat-outgoing iptables rules. To implement this workaround run kubectl against the affected to edit the default-ipv4-ippool object: kubectl edit ippools default-ipv4-ippool . Edit the line natOutgoing: true to set natOutgoing: false and save the change. Calico will detect the configuration update and remove the cali-nat-outgoing iptables rules. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Network ingress traffic from 192.168.0.0/16 always SNAT'd in Kubernetes clusters with canal network provider"},{"location":"000020231/#network-ingress-traffic-from-1921680016-always-snatd-in-kubernetes-clusters-with-canal-network-provider","text":"This document (000020231) is provided subject to the disclaimer at the end of this document.","title":"Network ingress traffic from 192.168.0.0/16 always SNAT'd in Kubernetes clusters with canal network provider"},{"location":"000020231/#situation","text":"","title":"Situation"},{"location":"000020231/#issue","text":"Network ingress traffic to a Kubernetes cluster with the canal network provider, from IP addresses in the range 192.168.0.0/16 , is always SNAT'd, even in instances where this is not desired. For example on NodePort services configured with externalTrafficPolicy: Local the source IP should be preserved without SNAT, per the Kubernetes documentation . With this issue the source IP is SNAT'd even in instances of NodePort services configured with externalTrafficPolicy: Local .","title":"Issue"},{"location":"000020231/#pre-requisites","text":"A Kubernetes cluster provisioned via the RKE CLI or Rancher, using the canal network provider","title":"Pre-requisites"},{"location":"000020231/#root-cause","text":"When a cluster is provisioned with the canal network provider selected, Flannel is used for networking and Calico for network policy enforcement, and IP address management is therefore managed by Flannel. The calico-node container in the canal pod is still configured with an (un-used) IP pool, which defaults to 192.168.0.0/16 . By default Calico programs iptables rules in the cali-nat-outgoing chain of the nat table on cluster nodes to perform SNAT on traffic from this IP pool. The purpose of these rules is to masquerade egress traffic from pods where Calico is used for networking (and not just network policy). As a result in a canal network provider cluster, where the calico-node container is present for network policy enforcement, these rules are programed and any ingress traffic from the range 192.168.0.0/16 will match and be SNAT'd.","title":"Root cause"},{"location":"000020231/#resolution","text":"The permanent solution to prevent this issue is to update the RKE deployment templates for the canal daemonset, to set the environment variable CALICO_IPV4POOL_NAT_OUTGOING to 0 for the calico-node container. This will prevent programming of the problematic cali-nat-outgoing iptables rules and is tracked in Rancher Issue #20500 . In order to workaround the issue in existing clusters, the Calico ippool configuration can be edited to disable outgoing nat, which removes programming of the cali-nat-outgoing iptables rules. To implement this workaround run kubectl against the affected to edit the default-ipv4-ippool object: kubectl edit ippools default-ipv4-ippool . Edit the line natOutgoing: true to set natOutgoing: false and save the change. Calico will detect the configuration update and remove the cali-nat-outgoing iptables rules.","title":"Resolution"},{"location":"000020231/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020232/","text":"Is it possible to perform etcd snapshots to an s3 endpoint with a certificate signed by a custom CA? This document (000020232) is provided subject to the disclaimer at the end of this document. Situation Question Is it possible to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom certificate authority (CA)? Pre-requisites Kubernetes clusters provisioned via the RKE CLI v0.2.x, or Rancher launched Kubernetes clusters via Rancher v2.2.x. Answer Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4 In Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4 it is not possible to configure etcd snapshots to use a custom CA. Attempts to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom CA will fail with an error similar to the following: FATA[0002] Failed to take one-time snapshot, exit code [1]: time=\"2019-04-29T08:37:15Z\" level=fatal msg=\"faield to set s3 server: failed to check s3 bucket:rke, err:Get https://s3.example.com/rke/?location=: x509: certificate signed by unknown authority\" Rancher v2.2.5+ In Rancher v2.2.5 and above it is possible to specify a custom CA for the S3 endpoint within the S3 backup options . Expanding 'Show advanced options' under the 'Edit Cluster' view, a 'Custom CA Certificate' field is shown when the s3 backup target is selected, enabling you to enter the certificate or upload this from file. RKE v0.2.5+ With the RKE CLI v0.2.5 and above it also possible to specify a custom CA for the S3 endpoint within the S3 backup options . To do you specify the certificate via the custom_ca field in the s3backupconfig block of the cluster configuration YAML . The cert should be provided as string, with newlines replaced with \\n, per the example below: services: etcd: backup_config: interval_hours: 12 retention: 6 s3backupconfig: access_key: S3_ACCESS_KEY secret_key: S3_SECRET_KEY bucket_name: s3-bucket-name region: \"\" endpoint: s3.amazonaws.com custom_ca: \"-----BEGIN CERTIFICATE-----\\nMIIDazCCAlOgAwIBAgIUMoCmUpa4u2UJWqNIkizFbpeJkwowDQYJKoZIhvcNAQEL\\nBQAwRTELMAkGA1UEBhMCQVUxEzARBgNVBAgMClNvbWUtU3RhdGUxITAfBgNVBAoM\\nGEludGVybmV0IFdpZGdpdHMgUHR5IEx0ZDAeFw0xOTA5MTgwOTI4NDBaFw0yMjA3\\nMDgwOTI4NDBaMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEw\\nHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQwggEiMA0GCSqGSIb3DQEB\\nAQUAA4IBDwAwggEKAoIBAQDIW8aN2vszkiNAqykYvqivZgWPRqEukPSAZz39Qtyx\\nkv2wl3B29chBzw5+vjG6veaUnWufOpGeiwglL2PEBOMI0a62zmmm3ttyJDy1lY+A\\ncuxZ1+hveWjWrA2B2bN69/wdkQTQu6ZLoguk+8mRFBZ7ghu6YTZQfczBsHlDxUpA\\n77qQunE4RmcQzOBHoWmMkSSxSGMBsVIj2rRihtVqpgbrMr3/LtCqzqsF+UcroJPC\\nIIBd8bSFlcgkWLnJdqlSa8s1PUodcKD3q6mbMZPDudraszuRgLyC5pIylGQOk+XF\\nMjf2I8zkkAV4QtfSpgBpNXbZEZ3a6CPhveDZqoZN4rxTAgMBAAGjUzBRMB0GA1Ud\\nDgQWBBTD/EagPfxclAlfViV5kKLq0YwBYzAfBgNVHSMEGDAWgBTD/EagPfxclAlf\\nViV5kKLq0YwBYzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQB0\\nyJ6vjtmuvBEKuNgWwIJLh2CqZubUL+lUQGi1NhdFzkXj7+fLeLjqsmbi2Xj/qQ5n\\nooI/p4MeHfYrUqqS7nqTBIsRZQZDZcKUYTZWzDRBdQZtxvEsB1WUq5+nsCQqVuZO\\n+ICsXQFL45xDKaWOoRMH8z9JksYf2CSKeRWViAFElC/IDwf8d5mtufe17h5vlyPR\\nLaIMJ37vyAosN6h8icztVHRzfcIjp1KLqwaGfaOrNSCv8zja9YsD6kbYL64lKND4\\nHiOJy3oSjjjTNdnXjIO44Ngo7L4TWF1CshFlsRF3a5/Jw+NmsEV46Vq41YcuRX9E\\n5JYZWzGRsPDeG4vrzWrV\\n-----END CERTIFICATE-----\" Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is it possible to perform etcd snapshots to an s3 endpoint with a certificate signed by a custom CA?"},{"location":"000020232/#is-it-possible-to-perform-etcd-snapshots-to-an-s3-endpoint-with-a-certificate-signed-by-a-custom-ca","text":"This document (000020232) is provided subject to the disclaimer at the end of this document.","title":"Is it possible to perform etcd snapshots to an s3 endpoint with a certificate signed by a custom CA?"},{"location":"000020232/#situation","text":"","title":"Situation"},{"location":"000020232/#question","text":"Is it possible to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom certificate authority (CA)?","title":"Question"},{"location":"000020232/#pre-requisites","text":"Kubernetes clusters provisioned via the RKE CLI v0.2.x, or Rancher launched Kubernetes clusters via Rancher v2.2.x.","title":"Pre-requisites"},{"location":"000020232/#answer","text":"","title":"Answer"},{"location":"000020232/#rancher-v220-v224-and-rke-cli-v020-v024","text":"In Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4 it is not possible to configure etcd snapshots to use a custom CA. Attempts to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom CA will fail with an error similar to the following: FATA[0002] Failed to take one-time snapshot, exit code [1]: time=\"2019-04-29T08:37:15Z\" level=fatal msg=\"faield to set s3 server: failed to check s3 bucket:rke, err:Get https://s3.example.com/rke/?location=: x509: certificate signed by unknown authority\"","title":"Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4"},{"location":"000020232/#rancher-v225","text":"In Rancher v2.2.5 and above it is possible to specify a custom CA for the S3 endpoint within the S3 backup options . Expanding 'Show advanced options' under the 'Edit Cluster' view, a 'Custom CA Certificate' field is shown when the s3 backup target is selected, enabling you to enter the certificate or upload this from file.","title":"Rancher v2.2.5+"},{"location":"000020232/#rke-v025","text":"With the RKE CLI v0.2.5 and above it also possible to specify a custom CA for the S3 endpoint within the S3 backup options . To do you specify the certificate via the custom_ca field in the s3backupconfig block of the cluster configuration YAML . The cert should be provided as string, with newlines replaced with \\n, per the example below: services: etcd: backup_config: interval_hours: 12 retention: 6 s3backupconfig: access_key: S3_ACCESS_KEY secret_key: S3_SECRET_KEY bucket_name: s3-bucket-name region: \"\" endpoint: s3.amazonaws.com custom_ca: \"-----BEGIN CERTIFICATE-----\\nMIIDazCCAlOgAwIBAgIUMoCmUpa4u2UJWqNIkizFbpeJkwowDQYJKoZIhvcNAQEL\\nBQAwRTELMAkGA1UEBhMCQVUxEzARBgNVBAgMClNvbWUtU3RhdGUxITAfBgNVBAoM\\nGEludGVybmV0IFdpZGdpdHMgUHR5IEx0ZDAeFw0xOTA5MTgwOTI4NDBaFw0yMjA3\\nMDgwOTI4NDBaMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEw\\nHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQwggEiMA0GCSqGSIb3DQEB\\nAQUAA4IBDwAwggEKAoIBAQDIW8aN2vszkiNAqykYvqivZgWPRqEukPSAZz39Qtyx\\nkv2wl3B29chBzw5+vjG6veaUnWufOpGeiwglL2PEBOMI0a62zmmm3ttyJDy1lY+A\\ncuxZ1+hveWjWrA2B2bN69/wdkQTQu6ZLoguk+8mRFBZ7ghu6YTZQfczBsHlDxUpA\\n77qQunE4RmcQzOBHoWmMkSSxSGMBsVIj2rRihtVqpgbrMr3/LtCqzqsF+UcroJPC\\nIIBd8bSFlcgkWLnJdqlSa8s1PUodcKD3q6mbMZPDudraszuRgLyC5pIylGQOk+XF\\nMjf2I8zkkAV4QtfSpgBpNXbZEZ3a6CPhveDZqoZN4rxTAgMBAAGjUzBRMB0GA1Ud\\nDgQWBBTD/EagPfxclAlfViV5kKLq0YwBYzAfBgNVHSMEGDAWgBTD/EagPfxclAlf\\nViV5kKLq0YwBYzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQB0\\nyJ6vjtmuvBEKuNgWwIJLh2CqZubUL+lUQGi1NhdFzkXj7+fLeLjqsmbi2Xj/qQ5n\\nooI/p4MeHfYrUqqS7nqTBIsRZQZDZcKUYTZWzDRBdQZtxvEsB1WUq5+nsCQqVuZO\\n+ICsXQFL45xDKaWOoRMH8z9JksYf2CSKeRWViAFElC/IDwf8d5mtufe17h5vlyPR\\nLaIMJ37vyAosN6h8icztVHRzfcIjp1KLqwaGfaOrNSCv8zja9YsD6kbYL64lKND4\\nHiOJy3oSjjjTNdnXjIO44Ngo7L4TWF1CshFlsRF3a5/Jw+NmsEV46Vq41YcuRX9E\\n5JYZWzGRsPDeG4vrzWrV\\n-----END CERTIFICATE-----\"","title":"RKE v0.2.5+"},{"location":"000020232/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020233/","text":"Upgrade to Rancher v2.2.4 fails for instances managing OpenStack CloudProvider enabled clusters with a Loadbalancer config: 'cannot unmarshal number into Go value of type string' This document (000020233) is provided subject to the disclaimer at the end of this document. Situation Issue Upon attempting to upgrade to Rancher v2.2.4, where the Rancher instance manages an, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config, the Rancher server fails to start. Logs for the Rancher pods show error messages of the format: E0606 07:39:20.296926 8 reflector.go:134] github.com/rancher/norman/controller/generic_controller.go:175: Failed to list *v3.Cluster: json: cannot unmarshal number into Go value of type string Pre-requisites Upgrading Rancher to v2.2.4 A Rancher launched, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config. Root cause In order to resolve Rancher/14577 , the monitor-delay and monitor-timeout parameters for OpenStack cluster loadbalancer healthchecks were set from an integer type to a string, in Rancher v2.2.4. As the default in the Rancher API framework had configured these values to 0, upon upgrade to Rancher v2.2.4 an error occurs attempting to unmarshal these integer values of 0 to a string type. If these had been manually set to a non-zero integer value, resulting in kubelet failures in the OpenStack cluster itself previously, these will now result in failure of the Rancher pods themselves. Resolution You can apply a one time fix, to workaround this issue, by manually editing the monitor-delay and monitor-timeout values of the cluster Custom Resource of affected clusters, via kubectl run against the Rancher management cluster. Using your RKE generated kube config, perform the following operations: Identify affected clusters by running kubectl get clusters and checking for those with a spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer definition. For affected clusters run kubectl edit <cluster name> , where <cluster name> is the metadata.name value for the cluster and update the spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-delay and spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-timeout fields to a quoted string. Example: if it was 30, change it to \"30s\", if it was 0, change it to \"\". Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Upgrade to Rancher v2.2.4 fails for instances managing OpenStack CloudProvider enabled clusters with a Loadbalancer config: 'cannot unmarshal number into Go value of type string'"},{"location":"000020233/#upgrade-to-rancher-v224-fails-for-instances-managing-openstack-cloudprovider-enabled-clusters-with-a-loadbalancer-config-cannot-unmarshal-number-into-go-value-of-type-string","text":"This document (000020233) is provided subject to the disclaimer at the end of this document.","title":"Upgrade to Rancher v2.2.4 fails for instances managing OpenStack CloudProvider enabled clusters with a Loadbalancer config: 'cannot unmarshal number into Go value of type string'"},{"location":"000020233/#situation","text":"","title":"Situation"},{"location":"000020233/#issue","text":"Upon attempting to upgrade to Rancher v2.2.4, where the Rancher instance manages an, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config, the Rancher server fails to start. Logs for the Rancher pods show error messages of the format: E0606 07:39:20.296926 8 reflector.go:134] github.com/rancher/norman/controller/generic_controller.go:175: Failed to list *v3.Cluster: json: cannot unmarshal number into Go value of type string","title":"Issue"},{"location":"000020233/#pre-requisites","text":"Upgrading Rancher to v2.2.4 A Rancher launched, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config.","title":"Pre-requisites"},{"location":"000020233/#root-cause","text":"In order to resolve Rancher/14577 , the monitor-delay and monitor-timeout parameters for OpenStack cluster loadbalancer healthchecks were set from an integer type to a string, in Rancher v2.2.4. As the default in the Rancher API framework had configured these values to 0, upon upgrade to Rancher v2.2.4 an error occurs attempting to unmarshal these integer values of 0 to a string type. If these had been manually set to a non-zero integer value, resulting in kubelet failures in the OpenStack cluster itself previously, these will now result in failure of the Rancher pods themselves.","title":"Root cause"},{"location":"000020233/#resolution","text":"You can apply a one time fix, to workaround this issue, by manually editing the monitor-delay and monitor-timeout values of the cluster Custom Resource of affected clusters, via kubectl run against the Rancher management cluster. Using your RKE generated kube config, perform the following operations: Identify affected clusters by running kubectl get clusters and checking for those with a spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer definition. For affected clusters run kubectl edit <cluster name> , where <cluster name> is the metadata.name value for the cluster and update the spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-delay and spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-timeout fields to a quoted string. Example: if it was 30, change it to \"30s\", if it was 0, change it to \"\".","title":"Resolution"},{"location":"000020233/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020234/","text":"Rancher 1.6.x - Rancher Server Tuning This document (000020234) is provided subject to the disclaimer at the end of this document. Situation Rancher Server Tuning Refer PDF attachment Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher 1.6.x - Rancher Server Tuning"},{"location":"000020234/#rancher-16x-rancher-server-tuning","text":"This document (000020234) is provided subject to the disclaimer at the end of this document.","title":"Rancher 1.6.x - Rancher Server Tuning"},{"location":"000020234/#situation","text":"Rancher Server Tuning Refer PDF attachment","title":"Situation"},{"location":"000020234/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020235/","text":"Rancher 1.6.x - Production Environment Sizing guide This document (000020235) is provided subject to the disclaimer at the end of this document. Situation Production Environment Sizing Guide Refer PDF attachment Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher 1.6.x - Production Environment Sizing guide"},{"location":"000020235/#rancher-16x-production-environment-sizing-guide","text":"This document (000020235) is provided subject to the disclaimer at the end of this document.","title":"Rancher 1.6.x - Production Environment Sizing guide"},{"location":"000020235/#situation","text":"Production Environment Sizing Guide Refer PDF attachment","title":"Situation"},{"location":"000020235/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020236/","text":"Rancher 1.6.x - MySQL Config Tuning This document (000020236) is provided subject to the disclaimer at the end of this document. Situation MySQL Config Tuning Guide Refer PDF attachment Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher 1.6.x - MySQL Config Tuning"},{"location":"000020236/#rancher-16x-mysql-config-tuning","text":"This document (000020236) is provided subject to the disclaimer at the end of this document.","title":"Rancher 1.6.x - MySQL Config Tuning"},{"location":"000020236/#situation","text":"MySQL Config Tuning Guide Refer PDF attachment","title":"Situation"},{"location":"000020236/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020237/","text":"Rancher 2.1.x - CIS Kubernetes v1.3.0 Benchmark Self Assessment This document (000020237) is provided subject to the disclaimer at the end of this document. Situation Rancher v2.1.x Version 1.0.0 - Nov 2018 Authors Jason Greathouse Overview The following document scores an RKE cluster provisioned according to the Rancher 2.1.x hardening guide against the CIS 1.3.0 Kubernetes benchmark. This document is to be used by Rancher operators, security teams, auditors and decision makers. Download document here: https://releases.rancher.com/documents/security/latest/Rancher_Benchmark_Assessment.pdf Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher 2.1.x - CIS Kubernetes v1.3.0 Benchmark Self Assessment"},{"location":"000020237/#rancher-21x-cis-kubernetes-v130-benchmark-self-assessment","text":"This document (000020237) is provided subject to the disclaimer at the end of this document.","title":"Rancher 2.1.x - CIS Kubernetes v1.3.0 Benchmark Self Assessment"},{"location":"000020237/#situation","text":"","title":"Situation"},{"location":"000020237/#rancher-v21x","text":"Version 1.0.0 - Nov 2018 Authors Jason Greathouse Overview The following document scores an RKE cluster provisioned according to the Rancher 2.1.x hardening guide against the CIS 1.3.0 Kubernetes benchmark. This document is to be used by Rancher operators, security teams, auditors and decision makers. Download document here: https://releases.rancher.com/documents/security/latest/Rancher_Benchmark_Assessment.pdf","title":"Rancher v2.1.x"},{"location":"000020237/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020238/","text":"Rancher 2.1.x - Rancher Hardening Guide This document (000020238) is provided subject to the disclaimer at the end of this document. Situation Rancher v2.1.x Version: 0.1.0 - November 26th 2018 Overview This document provides prescriptive guidance for hardening a production installation of Rancher v2.1.x. It outlines the configurations and controls required to address CIS-Kubernetes benchmark controls. Rancher CIS-Kubernetes self assessment using RKE This document has been created by the Engineering team at Rancher Labs. Profile Definitions The following profile definitions agree with the CIS Benchmarks for Kubernetes. Level 1 Items in this profile intend to: offer practical advice appropriate for the environment; deliver an obvious security benefit; and not alter the functionality or utility of the environment beyond an acceptable margin Level 2 Items in this profile extend the \u201cLevel 1\u201d profile and exhibit one or more of the following characteristics: are intended for use in environments or use cases where security is paramount act as a defense in depth measure may negatively impact the utility or performance of the technology Authors Jason Greathouse Bill Maxwell Download document here: https://releases.rancher.com/documents/security/latest/Rancher_Hardening_Guide.pdf Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher 2.1.x - Rancher Hardening Guide"},{"location":"000020238/#rancher-21x-rancher-hardening-guide","text":"This document (000020238) is provided subject to the disclaimer at the end of this document.","title":"Rancher 2.1.x - Rancher Hardening Guide"},{"location":"000020238/#situation","text":"","title":"Situation"},{"location":"000020238/#rancher-v21x","text":"Version: 0.1.0 - November 26th 2018 Overview This document provides prescriptive guidance for hardening a production installation of Rancher v2.1.x. It outlines the configurations and controls required to address CIS-Kubernetes benchmark controls. Rancher CIS-Kubernetes self assessment using RKE This document has been created by the Engineering team at Rancher Labs. Profile Definitions The following profile definitions agree with the CIS Benchmarks for Kubernetes. Level 1 Items in this profile intend to: offer practical advice appropriate for the environment; deliver an obvious security benefit; and not alter the functionality or utility of the environment beyond an acceptable margin Level 2 Items in this profile extend the \u201cLevel 1\u201d profile and exhibit one or more of the following characteristics: are intended for use in environments or use cases where security is paramount act as a defense in depth measure may negatively impact the utility or performance of the technology Authors Jason Greathouse Bill Maxwell Download document here: https://releases.rancher.com/documents/security/latest/Rancher_Hardening_Guide.pdf","title":"Rancher v2.1.x"},{"location":"000020238/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020239/","text":"How to change the log level for Rancher v2.x This document (000020239) is provided subject to the disclaimer at the end of this document. Situation Task By default the Rancher v2.x server log level is set to info ; however, when investigating an issue it may be helpful to increase the log verbosity to debug . This article details how to control the log verbosity on Rancher v2.x containers. Pre-requisites A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster . Resolution The log verbosity is set within a running Rancher server container by use of the loglevel command: loglevel --set <verbosity> Using kubectl with your cluster's context, you can update the log level of all your Rancher server containers by running the following: kubectl -n cattle-system get pods -l app=rancher --no-headers -o custom-columns=name:.metadata.name | while read rancherpod; do kubectl -n cattle-system exec $rancherpod -c rancher -- loglevel --set debug; done where verbosity is one of error , info or debug . Instructions on how to run this command in either a single node or High Availability installation of Rancher can be found within the Rancher documentation under the \"Logging\" troubleshooting guide. If the log level is increased to debug for troubleshooting purposes, you should be sure to reduce to info after the necessary logs have been captured, in order to reduce disk usage and minimise noise when reading the logs. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to change the log level for Rancher v2.x"},{"location":"000020239/#how-to-change-the-log-level-for-rancher-v2x","text":"This document (000020239) is provided subject to the disclaimer at the end of this document.","title":"How to change the log level for Rancher v2.x"},{"location":"000020239/#situation","text":"","title":"Situation"},{"location":"000020239/#task","text":"By default the Rancher v2.x server log level is set to info ; however, when investigating an issue it may be helpful to increase the log verbosity to debug . This article details how to control the log verbosity on Rancher v2.x containers.","title":"Task"},{"location":"000020239/#pre-requisites","text":"A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster .","title":"Pre-requisites"},{"location":"000020239/#resolution","text":"The log verbosity is set within a running Rancher server container by use of the loglevel command: loglevel --set <verbosity> Using kubectl with your cluster's context, you can update the log level of all your Rancher server containers by running the following: kubectl -n cattle-system get pods -l app=rancher --no-headers -o custom-columns=name:.metadata.name | while read rancherpod; do kubectl -n cattle-system exec $rancherpod -c rancher -- loglevel --set debug; done where verbosity is one of error , info or debug . Instructions on how to run this command in either a single node or High Availability installation of Rancher can be found within the Rancher documentation under the \"Logging\" troubleshooting guide. If the log level is increased to debug for troubleshooting purposes, you should be sure to reduce to info after the necessary logs have been captured, in order to reduce disk usage and minimise noise when reading the logs.","title":"Resolution"},{"location":"000020239/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020241/","text":"Istio-init container fails to start when SElinux is enabled on RHEL 7.x This document (000020241) is provided subject to the disclaimer at the end of this document. Environment OS: RHEL 7.x ISTIO Chart Version: v1.8.300 Rancher Version : v2.5.7 Situation Starting a workload on Istio-enabled namespace fails as the istio-init container failed to start. The istio-init container shows below error; The error is: nat -N ISTIO_INBOUND -N ISTIO_REDIRECT -N ISTIO_IN_REDIRECT -N ISTIO_OUTPUT -A ISTIO_INBOUND -p tcp --dport 15008 -j RETURN -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001 -A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006 -A PREROUTING -p tcp -j ISTIO_INBOUND -A ISTIO_INBOUND -p tcp --dport 22 -j RETURN -A ISTIO_INBOUND -p tcp --dport 15090 -j RETURN -A ISTIO_INBOUND -p tcp --dport 15021 -j RETURN -A ISTIO_INBOUND -p tcp --dport 15020 -j RETURN -A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT -A OUTPUT -p tcp -j ISTIO_OUTPUT -A ISTIO_OUTPUT -o lo -s 127.0.0.6/32 -j RETURN -A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT -A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT -A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN -A ISTIO_OUTPUT -j ISTIO_REDIRECT COMMIT iptables-restore --noflush /tmp/iptables-rules-1618985143596701894.txt019825926 iptables-restore: line 25 failed iptables-save Resolution Execute the modprobe in the below order to fix this without a reboot. modprobe br_netfilter modprobe nf_nat_redirect modprobe xt_REDIRECT modprobe xt_owner To load the modules automatically during boot, create a file inside /etc/modules-load.d/ as shown below. cat >/etc/modules-load.d/istio-iptables.conf <<EOF br_netfilter nf_nat_redirect xt_REDIRECT xt_owner EOF Cause The issue is caused by SELinux which prevents the istio-init to load kernel modules that are needed for the iptables rules. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Istio-init container fails to start when SElinux is enabled on RHEL 7.x"},{"location":"000020241/#istio-init-container-fails-to-start-when-selinux-is-enabled-on-rhel-7x","text":"This document (000020241) is provided subject to the disclaimer at the end of this document.","title":"Istio-init container fails to start when SElinux is enabled on RHEL 7.x"},{"location":"000020241/#environment","text":"OS: RHEL 7.x ISTIO Chart Version: v1.8.300 Rancher Version : v2.5.7","title":"Environment"},{"location":"000020241/#situation","text":"Starting a workload on Istio-enabled namespace fails as the istio-init container failed to start. The istio-init container shows below error; The error is: nat -N ISTIO_INBOUND -N ISTIO_REDIRECT -N ISTIO_IN_REDIRECT -N ISTIO_OUTPUT -A ISTIO_INBOUND -p tcp --dport 15008 -j RETURN -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001 -A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006 -A PREROUTING -p tcp -j ISTIO_INBOUND -A ISTIO_INBOUND -p tcp --dport 22 -j RETURN -A ISTIO_INBOUND -p tcp --dport 15090 -j RETURN -A ISTIO_INBOUND -p tcp --dport 15021 -j RETURN -A ISTIO_INBOUND -p tcp --dport 15020 -j RETURN -A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT -A OUTPUT -p tcp -j ISTIO_OUTPUT -A ISTIO_OUTPUT -o lo -s 127.0.0.6/32 -j RETURN -A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT -A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT -A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN -A ISTIO_OUTPUT -j ISTIO_REDIRECT COMMIT iptables-restore --noflush /tmp/iptables-rules-1618985143596701894.txt019825926 iptables-restore: line 25 failed iptables-save","title":"Situation"},{"location":"000020241/#resolution","text":"Execute the modprobe in the below order to fix this without a reboot. modprobe br_netfilter modprobe nf_nat_redirect modprobe xt_REDIRECT modprobe xt_owner To load the modules automatically during boot, create a file inside /etc/modules-load.d/ as shown below. cat >/etc/modules-load.d/istio-iptables.conf <<EOF br_netfilter nf_nat_redirect xt_REDIRECT xt_owner EOF","title":"Resolution"},{"location":"000020241/#cause","text":"The issue is caused by SELinux which prevents the istio-init to load kernel modules that are needed for the iptables rules.","title":"Cause"},{"location":"000020241/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020243/","text":"Can I receive an application backup of my SUSE Rancher Hosted environment if I decide not to renew at the end of my term? This document (000020243) is provided subject to the disclaimer at the end of this document. Situation Resolution Yes, the SUSE can provide an application backup of your SUSE Rancher Hosted environment. You can follow the Rancher documentation to restore your environment into a Rancher server running in your datacenter or cloud account. To request a backup of your SUSE Rancher Hosted environment, open a support ticket on our support portal . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can I receive an application backup of my SUSE Rancher Hosted environment if I decide not to renew at the end of my term?"},{"location":"000020243/#can-i-receive-an-application-backup-of-my-suse-rancher-hosted-environment-if-i-decide-not-to-renew-at-the-end-of-my-term","text":"This document (000020243) is provided subject to the disclaimer at the end of this document.","title":"Can I receive an application backup of my SUSE Rancher Hosted environment if I decide not to renew at the end of my term?"},{"location":"000020243/#situation","text":"","title":"Situation"},{"location":"000020243/#resolution","text":"Yes, the SUSE can provide an application backup of your SUSE Rancher Hosted environment. You can follow the Rancher documentation to restore your environment into a Rancher server running in your datacenter or cloud account. To request a backup of your SUSE Rancher Hosted environment, open a support ticket on our support portal .","title":"Resolution"},{"location":"000020243/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020265/","text":"Node become NotReady state because of unhealthy PLEG This document (000020265) is provided subject to the disclaimer at the end of this document. Situation The rancher UI displays the following: \"PLEG is not healthy: pleg was last seen active 18h50m17.324752357s ago; threshold is 3m0s \" Kubelet log shows; docker logs kubelet --tail=20 -f Output:- E0511 09:12:59.037051 10851 remote_runtime.go:312] ListContainers with filter &ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216) E0511 09:12:59.037105 10851 kuberuntime_container.go:382] getKubeletContainers failed: rpc error: <b>code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)</b> E0511 09:12:59.037123 10851 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216) Resolution The workaround here is to prune stopped or dead containers to reduce the message size. docker system prune The Paketo Buildpacks fixed this issue already. Please refer GitHub issue #80 for more details. Cause The labels set on the containers build by Paketo Buildpacks are causing a huge message size. This results in the default gRPC message buffer size of 16Mb overflowing in kubelet, causing PLEG to fail. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Node become NotReady state because of unhealthy PLEG"},{"location":"000020265/#node-become-notready-state-because-of-unhealthy-pleg","text":"This document (000020265) is provided subject to the disclaimer at the end of this document.","title":"Node become NotReady state because of unhealthy PLEG"},{"location":"000020265/#situation","text":"The rancher UI displays the following: \"PLEG is not healthy: pleg was last seen active 18h50m17.324752357s ago; threshold is 3m0s \" Kubelet log shows; docker logs kubelet --tail=20 -f Output:- E0511 09:12:59.037051 10851 remote_runtime.go:312] ListContainers with filter &ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216) E0511 09:12:59.037105 10851 kuberuntime_container.go:382] getKubeletContainers failed: rpc error: <b>code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)</b> E0511 09:12:59.037123 10851 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)","title":"Situation"},{"location":"000020265/#resolution","text":"The workaround here is to prune stopped or dead containers to reduce the message size. docker system prune The Paketo Buildpacks fixed this issue already. Please refer GitHub issue #80 for more details.","title":"Resolution"},{"location":"000020265/#cause","text":"The labels set on the containers build by Paketo Buildpacks are causing a huge message size. This results in the default gRPC message buffer size of 16Mb overflowing in kubelet, causing PLEG to fail.","title":"Cause"},{"location":"000020265/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020274/","text":"Rancher Support Migration - Frequently Asked Questions This document (000020274) is provided subject to the disclaimer at the end of this document. Environment Dear SUSE Rancher Customer, This page is specific to customer users who are being migrated from the Rancher Support system on Zendesk to the SUSE Customer Center (SCC). This page is intended to answer some of the Frequently Asked Questions our team has received about the migration process. We hope this gives you and your team some additional insight into what is changing and what is not changing. Will Rancher Support be available during the maintenance window for this migration? Yes. During this migration, our support leadership team will be monitoring an email address for any customers that are in need of immediate assistance. If you find yourself in need of assistance please do not hesitate to email us at RancherSupportNow@suse.com . From here our team will assess the issue and work with you for resolution. For customers with accounts already in SCC, during the migration, you may continue to interact with our team through SCC. For any customers that do not have SCC accounts (please don't create one until you are advised to do so), please use the RancherSupportNow@suse.com to interact with our team during this migration, especially to log Sev1 cases. Once the migration is complete, SCC will be the new system moving forward and how our teams interact. What will happen with my existing tickets in Zendesk after this migration? Ensuring the historical context of your account is important to us. During the migration, our team will be working to ensure that all of your historical (closed) tickets are migrated. Additionally, we will work to ensure that each of the tickets that we are currently working on is migrated. During the migration, you may see a new ticket (in the new customer center) that informs you that your previous Zendsk ticket has been migrated to a new SalesForce ticket. This new ticket will contain the history of that open/working ticket. What will happen to our existing user accounts in Zendesk after the migration? Because we are migrating to a new CRM, each user will need to register for a new account (directions for registration will be sent in future communications). Each of the users that is currently active in Zendesk will be receiving the same messaging that contains the instructions on how they may create a new account in the new system. In the new system, the creation of new accounts is now able to be done by users on the account that have administrator permissions. Each of these users will be able to invite users and set the level of permissions for those users. If a user in your team either doesn't receive information on how to create a new user account or if you simply want to add a new user that did not exist in Zendesk, your account administrators will be able to take control of the account invites. The process of emailing us with Register in the subject will not work, so an administrator will need to be contacted to create the accounts moving forward. How will I access the Knowledgebase articles found at support.rancher.com? During this migration period, we have set all the support documents to be viable by anyone accessing the site, this site no longer requires an agent login. Our team is in the process of migrating these articles to suse.com . Once the migration of articles is complete, they will be accessible at https://www.suse.com/support/kb/ Once our team has cloned all of the articles we will place a redirection on the support.rancher.com page informing you that all of the documents have been moved. May I continue to open and update tickets via email? No, post-migration, it won\u2019t be possible to create a new ticket (support case) by email. After the migration, this email address support@rancher.com will not create new support cases. It will be monitored by our team and there will be an automated response to emails sent to this address on how to create a new support case. While we will continue to monitor that mailbox and offer help on best effort, we request that you create a new support case via the SCC portal. Creating a ticket through the portal will ensure that our engineers are aware and able to assist more quickly. What will happen if I already have a support contract with SUSE, separate from my Rancher account? For customers that have a service account with both, we are working to ensure that the entitlements are maintained. If you already have an account where you are accessing your SUSE entitlements there is no need to create a new account, the (SUSE) Rancher entitlements should be added to your existing account with no issue. After the migration, if you find this not to be the case please email your Customer Success Manager or your Services Delivery Manager. If you have users that you want to How will I know if the migration is successful or failed? Leading up to the migration and on the day of the migration process our team will be sending out messages, when appropriate, to keep you apprised of the process. If the migration is successful, our final message will let you know that and will include steps for how you and your team may set up your new accounts in the new customer center. If the migration is unsuccessful, our team will let you know as well. If the migration is unsuccessful, nothing will change with your current support and you will continue to use Zendesk. We will then continue to work on another try to migrate What if I have issues with my SCC account after the migration occurs? If there are issues you should first check https://suse.okta.com/help/login . If you are having a password or username issue please visit https://suse.okta.com/ , select \u201cNeed Help signing in?\u201d, and complete the steps that work best for you. If neither of these resolves your problem and you are still unable to log in our team will be available to assist with any other issues. Please email your Customer Success Manager and/or RancherSupportNow@suse.com. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher Support Migration - Frequently Asked Questions"},{"location":"000020274/#rancher-support-migration-frequently-asked-questions","text":"This document (000020274) is provided subject to the disclaimer at the end of this document.","title":"Rancher Support Migration - Frequently Asked Questions"},{"location":"000020274/#environment","text":"Dear SUSE Rancher Customer, This page is specific to customer users who are being migrated from the Rancher Support system on Zendesk to the SUSE Customer Center (SCC). This page is intended to answer some of the Frequently Asked Questions our team has received about the migration process. We hope this gives you and your team some additional insight into what is changing and what is not changing.","title":"Environment"},{"location":"000020274/#will-rancher-support-be-available-during-the-maintenance-window-for-this-migration","text":"Yes. During this migration, our support leadership team will be monitoring an email address for any customers that are in need of immediate assistance. If you find yourself in need of assistance please do not hesitate to email us at RancherSupportNow@suse.com . From here our team will assess the issue and work with you for resolution. For customers with accounts already in SCC, during the migration, you may continue to interact with our team through SCC. For any customers that do not have SCC accounts (please don't create one until you are advised to do so), please use the RancherSupportNow@suse.com to interact with our team during this migration, especially to log Sev1 cases. Once the migration is complete, SCC will be the new system moving forward and how our teams interact.","title":"Will Rancher Support be available during the maintenance window for this migration?"},{"location":"000020274/#what-will-happen-with-my-existing-tickets-in-zendesk-after-this-migration","text":"Ensuring the historical context of your account is important to us. During the migration, our team will be working to ensure that all of your historical (closed) tickets are migrated. Additionally, we will work to ensure that each of the tickets that we are currently working on is migrated. During the migration, you may see a new ticket (in the new customer center) that informs you that your previous Zendsk ticket has been migrated to a new SalesForce ticket. This new ticket will contain the history of that open/working ticket.","title":"What will happen with my existing tickets in Zendesk after this migration?"},{"location":"000020274/#what-will-happen-to-our-existing-user-accounts-in-zendesk-after-the-migration","text":"Because we are migrating to a new CRM, each user will need to register for a new account (directions for registration will be sent in future communications). Each of the users that is currently active in Zendesk will be receiving the same messaging that contains the instructions on how they may create a new account in the new system. In the new system, the creation of new accounts is now able to be done by users on the account that have administrator permissions. Each of these users will be able to invite users and set the level of permissions for those users. If a user in your team either doesn't receive information on how to create a new user account or if you simply want to add a new user that did not exist in Zendesk, your account administrators will be able to take control of the account invites. The process of emailing us with Register in the subject will not work, so an administrator will need to be contacted to create the accounts moving forward.","title":"What will happen to our existing user accounts in Zendesk after the migration?"},{"location":"000020274/#how-will-i-access-the-knowledgebase-articles-found-at-supportranchercom","text":"During this migration period, we have set all the support documents to be viable by anyone accessing the site, this site no longer requires an agent login. Our team is in the process of migrating these articles to suse.com . Once the migration of articles is complete, they will be accessible at https://www.suse.com/support/kb/ Once our team has cloned all of the articles we will place a redirection on the support.rancher.com page informing you that all of the documents have been moved.","title":"How will I access the Knowledgebase articles found at\u00a0support.rancher.com?"},{"location":"000020274/#may-i-continue-to-open-and-update-tickets-via-email","text":"No, post-migration, it won\u2019t be possible to create a new ticket (support case) by email. After the migration, this email address support@rancher.com will not create new support cases. It will be monitored by our team and there will be an automated response to emails sent to this address on how to create a new support case. While we will continue to monitor that mailbox and offer help on best effort, we request that you create a new support case via the SCC portal. Creating a ticket through the portal will ensure that our engineers are aware and able to assist more quickly.","title":"May I continue to open and update tickets via email?"},{"location":"000020274/#what-will-happen-if-i-already-have-a-support-contract-with-suse-separate-from-my-rancher-account","text":"For customers that have a service account with both, we are working to ensure that the entitlements are maintained. If you already have an account where you are accessing your SUSE entitlements there is no need to create a new account, the (SUSE) Rancher entitlements should be added to your existing account with no issue. After the migration, if you find this not to be the case please email your Customer Success Manager or your Services Delivery Manager. If you have users that you want to","title":"What will happen if I already have a support contract with SUSE, separate from my Rancher account?"},{"location":"000020274/#how-will-i-know-if-the-migration-is-successful-or-failed","text":"Leading up to the migration and on the day of the migration process our team will be sending out messages, when appropriate, to keep you apprised of the process. If the migration is successful, our final message will let you know that and will include steps for how you and your team may set up your new accounts in the new customer center. If the migration is unsuccessful, our team will let you know as well. If the migration is unsuccessful, nothing will change with your current support and you will continue to use Zendesk. We will then continue to work on another try to migrate","title":"How will I know if the migration is successful or failed?"},{"location":"000020274/#what-if-i-have-issues-with-my-scc-account-after-the-migration-occurs","text":"If there are issues you should first check https://suse.okta.com/help/login . If you are having a password or username issue please visit https://suse.okta.com/ , select \u201cNeed Help signing in?\u201d, and complete the steps that work best for you. If neither of these resolves your problem and you are still unable to log in our team will be available to assist with any other issues. Please email your Customer Success Manager and/or RancherSupportNow@suse.com.","title":"What if I have issues with my SCC account after the migration occurs?"},{"location":"000020274/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020277/","text":"Is SUSE Rancher Hosted data encrypted at rest? This document (000020277) is provided subject to the disclaimer at the end of this document. Environment Hosted Rancher Resolution Yes, the disk volumes of the virtual machines used by SUSE Rancher Hosted are encrypted as well as the disks used by the backend databases. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is SUSE Rancher Hosted data encrypted at rest?"},{"location":"000020277/#is-suse-rancher-hosted-data-encrypted-at-rest","text":"This document (000020277) is provided subject to the disclaimer at the end of this document.","title":"Is SUSE Rancher Hosted data encrypted at rest?"},{"location":"000020277/#environment","text":"Hosted Rancher","title":"Environment"},{"location":"000020277/#resolution","text":"Yes, the disk volumes of the virtual machines used by SUSE Rancher Hosted are encrypted as well as the disks used by the backend databases.","title":"Resolution"},{"location":"000020277/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020296/","text":"[Rancher] How do I change severity level on an open case? This document (000020296) is provided subject to the disclaimer at the end of this document. Situation I have reviewed the severity levels listed below and I would now like to know how I can change the severity level of an open case myself. SeverityDescriptionSev 1 (Critical) The solution is in production or is mission-critical to your business. It is inoperable, and the situation is resulting in a total disruption of work. There is no workaround available. Sev 2 (High)Operations are severely restricted, but work can continue. Core functionalities can operate in a restricted fashion despite important features being unavailable. A workaround is available that ensures no immediate business impact.Sev3 (Medium)The solution does not work as designed, resulting in a minor loss of usage. It may also be a significant software defect that impacts the Customer when performing some actions and has no workaround.Sev 4 (Low)There is no loss of service, and this may be a request for documentation, general information, product enhancement request, Software defects with workarounds or medium or low functionality impact, etc. Resolution During the creation of a support case, you may select the severity of the support case via the drop-down menu. The creation of a Sev1 or a Sev2 support case will trigger our escalation workflows. If during the life of your support case the initial severity is no longer accurate, you may post a new comment with the string bump_to_sev1 or bump_to_sev2 to escalate a ticket to higher severity. This string can be stand-alone as a comment or can be a part of a sentence. There is no need for any special characters (such as \" \" or * *). During the life of the case, you can also recategorize it to sev3 or sev4 by using bump_to_sev3 or bump_to_sev4 . Using these bump_to_ strings for sev1 and sev2 will trigger our escalation workflows, where sev3 and sev4 will update the severity with no escalation workflows. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] How do I change severity level on an open case?"},{"location":"000020296/#rancher-how-do-i-change-severity-level-on-an-open-case","text":"This document (000020296) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] How do I change severity level on an open case?"},{"location":"000020296/#situation","text":"I have reviewed the severity levels listed below and I would now like to know how I can change the severity level of an open case myself. SeverityDescriptionSev 1 (Critical) The solution is in production or is mission-critical to your business. It is inoperable, and the situation is resulting in a total disruption of work. There is no workaround available. Sev 2 (High)Operations are severely restricted, but work can continue. Core functionalities can operate in a restricted fashion despite important features being unavailable. A workaround is available that ensures no immediate business impact.Sev3 (Medium)The solution does not work as designed, resulting in a minor loss of usage. It may also be a significant software defect that impacts the Customer when performing some actions and has no workaround.Sev 4 (Low)There is no loss of service, and this may be a request for documentation, general information, product enhancement request, Software defects with workarounds or medium or low functionality impact, etc.","title":"Situation"},{"location":"000020296/#resolution","text":"During the creation of a support case, you may select the severity of the support case via the drop-down menu. The creation of a Sev1 or a Sev2 support case will trigger our escalation workflows. If during the life of your support case the initial severity is no longer accurate, you may post a new comment with the string bump_to_sev1 or bump_to_sev2 to escalate a ticket to higher severity. This string can be stand-alone as a comment or can be a part of a sentence. There is no need for any special characters (such as \" \" or * *). During the life of the case, you can also recategorize it to sev3 or sev4 by using bump_to_sev3 or bump_to_sev4 . Using these bump_to_ strings for sev1 and sev2 will trigger our escalation workflows, where sev3 and sev4 will update the severity with no escalation workflows.","title":"Resolution"},{"location":"000020296/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020302/","text":"How to add custom labels to Alerts in Monitoring v2 This document (000020302) is provided subject to the disclaimer at the end of this document. Environment Rancher Version: v2.5.8 Monitoring Chart Version: 14.5.100 Situation Alerts in Monitoring v2 contain standard labels. But in some cases, users want to inject custom labels like Kubernetes cluster names to easily identify the environment or need the labels for further Alert routing. Resolution Use defaultRules.additionalRuleLabels in the Monitoring Apps's YAML spec to inject custom labels. To inject cluster name, open the Monitoring App in Apps& Marketplace from Cluster explorer. Under \"Edit as YAML\", add the custom label as below. defaultRules: additionalRuleLabels: cluster: \"My_Test_cluster\" Then click on \"Deploy\" or \"Upgrade\" if App is already installed. If your receiver is webhook, then the alerts will have the custom labels as shown in the below example alert. ... ... status\":\"firing\", \"labels\":{ \"alertname\":\"NodeClockNotSynchronising\", <b> &#34;cluster&#34;:&#34;My_Test_cluster&#34;</b>, <<<------ \"container\":\"node-exporter\", \"endpoint\":\"metrics\", \"instance\":\"192.168.110.157:9796\", \"job\":\"node-exporter\", \"namespace\":\"cattle-monitoring-system\", \"pod\":\"rancher-monitoring-prometheus-node-exporter-lg2g6\", \"prometheus\":\"cattle-monitoring-system/rancher-monitoring-prometheus\", \"service\":\"rancher-monitoring-prometheus-node-exporter\", ... ... Additional Information GitHub issue #3325 is opened to add additional labels via UI. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to add custom labels to Alerts in Monitoring v2"},{"location":"000020302/#how-to-add-custom-labels-to-alerts-in-monitoring-v2","text":"This document (000020302) is provided subject to the disclaimer at the end of this document.","title":"How to add custom labels to Alerts in Monitoring v2"},{"location":"000020302/#environment","text":"Rancher Version: v2.5.8 Monitoring Chart Version: 14.5.100","title":"Environment"},{"location":"000020302/#situation","text":"Alerts in Monitoring v2 contain standard labels. But in some cases, users want to inject custom labels like Kubernetes cluster names to easily identify the environment or need the labels for further Alert routing.","title":"Situation"},{"location":"000020302/#resolution","text":"Use defaultRules.additionalRuleLabels in the Monitoring Apps's YAML spec to inject custom labels. To inject cluster name, open the Monitoring App in Apps& Marketplace from Cluster explorer. Under \"Edit as YAML\", add the custom label as below. defaultRules: additionalRuleLabels: cluster: \"My_Test_cluster\" Then click on \"Deploy\" or \"Upgrade\" if App is already installed. If your receiver is webhook, then the alerts will have the custom labels as shown in the below example alert. ... ... status\":\"firing\", \"labels\":{ \"alertname\":\"NodeClockNotSynchronising\", <b> &#34;cluster&#34;:&#34;My_Test_cluster&#34;</b>, <<<------ \"container\":\"node-exporter\", \"endpoint\":\"metrics\", \"instance\":\"192.168.110.157:9796\", \"job\":\"node-exporter\", \"namespace\":\"cattle-monitoring-system\", \"pod\":\"rancher-monitoring-prometheus-node-exporter-lg2g6\", \"prometheus\":\"cattle-monitoring-system/rancher-monitoring-prometheus\", \"service\":\"rancher-monitoring-prometheus-node-exporter\", ... ...","title":"Resolution"},{"location":"000020302/#additional-information","text":"GitHub issue #3325 is opened to add additional labels via UI.","title":"Additional Information"},{"location":"000020302/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020306/","text":"What Desginated Name (DN) fields are used by Kubernetes when generating custom certificates from a provided CA Cert or PKI tool? This document (000020306) is provided subject to the disclaimer at the end of this document. Environment RKE Kubernetes cluster, where the user decides to generate custom certificates from an external, non-self-signed Root Cerficiate Authority (CA). Situation Upon issuing the following command, rke up a user might experience the error message below. INFO[0092] [authz] Creating rke-job-deployer ServiceAccount FATA[0119] Failed to apply the ServiceAccount needed for job execution: clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"kube-admin\" cannot create resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope Resolution While working with your own PKI toolsets, and providing a custom CA (Certificate Authority), check the Organization (O) fields for the generated certificates Common Names (CN) and their signing requests (CSR). The examples here are directly from Kubernetes the Hard Way , and outline a manual process for certificate generation. Where ${Instance} is mentioned, this is a variable for the name of the node that represents each kubelet, with every node having its own certificate. Kubernetes Component Certificates Common Name (CN)Organization (O)CA RootkubernetesKubernetesAdminadminsystem:mastersKubeletsystem:node:${Instance}system:nodesController Managersystem:kube-controller-managersystem:kube-controller-managerKube Proxysystem:kube-proxysystem:node-proxierSchedulersystem:kube-schedluersystem:kube-schedulerAPI ServerkubernetesKubernetesService Account Key Pairservice-accountsKubernetes Cause If the proper Organization fields are not set, Kubernetes cannot assign compatible permissions to the different underlying components. With self-signed certificates these fields are generated automatically. With custom certificates, it's important to verfiy these fields are correct, as they may have been set by external tooling or automation. Sometimes a Kubernetes administrator may leave all the fields of their certificates or CSRs blank or with predefined values, it can be easy to overlook the O field for a specific CN, an the proper kubernetes permissions will not get applied. Additional Information Kubernetes the Hard Way, Certificate Authority - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What Desginated Name (DN) fields are used by Kubernetes when generating custom certificates from a provided CA Cert or PKI tool?"},{"location":"000020306/#what-desginated-name-dn-fields-are-used-by-kubernetes-when-generating-custom-certificates-from-a-provided-ca-cert-or-pki-tool","text":"This document (000020306) is provided subject to the disclaimer at the end of this document.","title":"What Desginated Name (DN) fields are used by Kubernetes when generating custom certificates from a provided CA Cert or PKI tool?"},{"location":"000020306/#environment","text":"RKE Kubernetes cluster, where the user decides to generate custom certificates from an external, non-self-signed Root Cerficiate Authority (CA).","title":"Environment"},{"location":"000020306/#situation","text":"Upon issuing the following command, rke up a user might experience the error message below. INFO[0092] [authz] Creating rke-job-deployer ServiceAccount FATA[0119] Failed to apply the ServiceAccount needed for job execution: clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"kube-admin\" cannot create resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope","title":"Situation"},{"location":"000020306/#resolution","text":"While working with your own PKI toolsets, and providing a custom CA (Certificate Authority), check the Organization (O) fields for the generated certificates Common Names (CN) and their signing requests (CSR). The examples here are directly from Kubernetes the Hard Way , and outline a manual process for certificate generation. Where ${Instance} is mentioned, this is a variable for the name of the node that represents each kubelet, with every node having its own certificate. Kubernetes Component Certificates Common Name (CN)Organization (O)CA RootkubernetesKubernetesAdminadminsystem:mastersKubeletsystem:node:${Instance}system:nodesController Managersystem:kube-controller-managersystem:kube-controller-managerKube Proxysystem:kube-proxysystem:node-proxierSchedulersystem:kube-schedluersystem:kube-schedulerAPI ServerkubernetesKubernetesService Account Key Pairservice-accountsKubernetes","title":"Resolution"},{"location":"000020306/#cause","text":"If the proper Organization fields are not set, Kubernetes cannot assign compatible permissions to the different underlying components. With self-signed certificates these fields are generated automatically. With custom certificates, it's important to verfiy these fields are correct, as they may have been set by external tooling or automation. Sometimes a Kubernetes administrator may leave all the fields of their certificates or CSRs blank or with predefined values, it can be easy to overlook the O field for a specific CN, an the proper kubernetes permissions will not get applied.","title":"Cause"},{"location":"000020306/#additional-information","text":"Kubernetes the Hard Way, Certificate Authority - https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md","title":"Additional Information"},{"location":"000020306/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020328/","text":"containerd.io 1.4.4 bug advisory This document (000020328) is provided subject to the disclaimer at the end of this document. Environment Recently our team has been made aware of PLEG health issues caused by a recent containerd.io package version. This can manifest in a few ways, we usually see all Docker commands failing, but in recent cases, the Docker commands with a specific security flag fail and hang. We have isolated this to containerd.io package version 1.4.4-x, and has been logged in this GitHub issue . Situation ******Update as of 25MAY21 ******** Our team is now aware that this issue is resolved in rc95 or higher. Please note that the issue was resolved in rc94, but there was a CVE tied to that rc version. Our recommendation for anyone experiencing this issue to move to rc95 or higher to resolve the containerd issues as well as avoiding the known CVE found in rc94. Even if you are not experiencing issues tied to this advisory, we would recommend you move to rc95. ******End Update as of 25MAY21 ******** How do I know if I am impacted? Customers running RKE could be impacted by this. This is impacting customers running Docker 19.03 and 20.10 where containerd.io is using 1.4.4 -x. Nodes running containerd.io 1.4.4 may experience containers hanging on initialization after a certain number of containers with no-new-privileges are started. Often this has come as a result of upgrading Docker with Rancher 2.5.6. The symptoms include PLEG timeout errors in the Rancher UI, CoreDNS pods failing to start, and docker inspect commands to hang on certain containers. As the issue relates to the specific runc version (1.0.0-rc93) bundled with containerd.io, the following can be a basic test to identify if the node is running the affected runc build: runc --version | grep -q 1.0.0-rc93 && echo \"AFFECTED\" || echo \"NOT AFFECTED\" Resolution Is there a workaround? ******Update as of 25MAY21 ******** The below workaround should not be used any longer. With the release of rc95 (mentioned above) any customers experiencing this issue should upgrade to rc95 as the resolution is found there. ******End Update as of 25MAY21 ******** Yes, currently our team recommends that you take the following step: Downgrade or install the containerd.io package to a 1.4.3-x version. There is no need to modify privileges on CoreDNS pods, once downgraded to 1.4.3 you should pin that version to not auto-update. Please ensure your team is aware of CVE-2021-21334 in 1.4.3-x. As examples of downgrading the containerd.io package on affected nodes: Ubuntu : apt install containerd.io=1.4.3-1 EL : yum downgrade containerd.io-1.4.3-3.1.el7 As needed, drain and cordon the node, followed by restarting the Docker daemon. For the most accurate steps, we recommend you consult the documentation for your OS on downgrading and version pinning for the specific package manager and Linux distribution. For customers who have not upgraded their Rancher clusters to 2.5.6+, we recommend that you hold off on upgrading until this is resolved upstream. If you need to upgrade to Rancher 2.5.6+, you should be safe to upgrade to Rancher when using the above process to install and pin the containerd.io package to a 1.4.3-x version. In the meantime, if you have any questions, please reach out to your Customer Success Manager or Rancher Support via a Support Ticket. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"containerd.io 1.4.4 bug advisory"},{"location":"000020328/#containerdio-144-bug-advisory","text":"This document (000020328) is provided subject to the disclaimer at the end of this document.","title":"containerd.io 1.4.4 bug advisory"},{"location":"000020328/#environment","text":"Recently our team has been made aware of PLEG health issues caused by a recent containerd.io package version. This can manifest in a few ways, we usually see all Docker commands failing, but in recent cases, the Docker commands with a specific security flag fail and hang. We have isolated this to containerd.io package version 1.4.4-x, and has been logged in this GitHub issue .","title":"Environment"},{"location":"000020328/#situation","text":"","title":"Situation"},{"location":"000020328/#update-as-of-25may21","text":"Our team is now aware that this issue is resolved in rc95 or higher. Please note that the issue was resolved in rc94, but there was a CVE tied to that rc version. Our recommendation for anyone experiencing this issue to move to rc95 or higher to resolve the containerd issues as well as avoiding the known CVE found in rc94. Even if you are not experiencing issues tied to this advisory, we would recommend you move to rc95.","title":"******Update as of 25MAY21 ********"},{"location":"000020328/#end-update-as-of-25may21","text":"","title":"******End Update as of 25MAY21 ********"},{"location":"000020328/#how-do-i-know-if-i-am-impacted","text":"Customers running RKE could be impacted by this. This is impacting customers running Docker 19.03 and 20.10 where containerd.io is using 1.4.4 -x. Nodes running containerd.io 1.4.4 may experience containers hanging on initialization after a certain number of containers with no-new-privileges are started. Often this has come as a result of upgrading Docker with Rancher 2.5.6. The symptoms include PLEG timeout errors in the Rancher UI, CoreDNS pods failing to start, and docker inspect commands to hang on certain containers. As the issue relates to the specific runc version (1.0.0-rc93) bundled with containerd.io, the following can be a basic test to identify if the node is running the affected runc build: runc --version | grep -q 1.0.0-rc93 && echo \"AFFECTED\" || echo \"NOT AFFECTED\"","title":"How do I know if I am impacted?"},{"location":"000020328/#resolution","text":"","title":"Resolution"},{"location":"000020328/#is-there-a-workaround","text":"","title":"Is there a workaround?"},{"location":"000020328/#update-as-of-25may21_1","text":"The below workaround should not be used any longer. With the release of rc95 (mentioned above) any customers experiencing this issue should upgrade to rc95 as the resolution is found there.","title":"******Update as of 25MAY21 ********"},{"location":"000020328/#end-update-as-of-25may21_1","text":"Yes, currently our team recommends that you take the following step: Downgrade or install the containerd.io package to a 1.4.3-x version. There is no need to modify privileges on CoreDNS pods, once downgraded to 1.4.3 you should pin that version to not auto-update. Please ensure your team is aware of CVE-2021-21334 in 1.4.3-x. As examples of downgrading the containerd.io package on affected nodes: Ubuntu : apt install containerd.io=1.4.3-1 EL : yum downgrade containerd.io-1.4.3-3.1.el7 As needed, drain and cordon the node, followed by restarting the Docker daemon. For the most accurate steps, we recommend you consult the documentation for your OS on downgrading and version pinning for the specific package manager and Linux distribution. For customers who have not upgraded their Rancher clusters to 2.5.6+, we recommend that you hold off on upgrading until this is resolved upstream. If you need to upgrade to Rancher 2.5.6+, you should be safe to upgrade to Rancher when using the above process to install and pin the containerd.io package to a 1.4.3-x version. In the meantime, if you have any questions, please reach out to your Customer Success Manager or Rancher Support via a Support Ticket.","title":"******End Update as of 25MAY21 ********"},{"location":"000020328/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020350/","text":"Streaming server stopped unexpectedly: listen tcp x.x.x.x:0: bind: cannot assign requested address This document (000020350) is provided subject to the disclaimer at the end of this document. Environment Rancher v2.5.7 Kubernetes v1.20.4-rancher1 Situation Adding a new node to an existing cluster stuck in \u201cregistering\u201d. Kubelet pod is in Restarting state on the new node. $ docker ps -a |grep kubelet 66bd40b36e76 rancher/hyperkube:v1.20.4-rancher1 \"/opt/rke-tools/entr\u2026\" 7 minutes ago Restarting (255) 32 seconds ago kubelet The kubelet is failing with below error. $ docker logs kubelet \"2021-07-26T11:13:48.270162766Z F0726 11:13:48.270086 40730 docker_service.go:415] Streaming server stopped unexpectedly: listen tcp 27.0.0.1:0: bind: cannot assign requested address\" Resolution Update /etc/hosts file with the correct entry. 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 After updating the file, the kubelet will restart itself and pick up the modified /etc/host file to bind to the loopback IP. Cause File /etc/hosts in the new node had below incorrect entry. 27.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Streaming server stopped unexpectedly: listen tcp x.x.x.x:0: bind: cannot assign requested address"},{"location":"000020350/#streaming-server-stopped-unexpectedly-listen-tcp-xxxx0-bind-cannot-assign-requested-address","text":"This document (000020350) is provided subject to the disclaimer at the end of this document.","title":"Streaming server stopped unexpectedly: listen tcp x.x.x.x:0: bind: cannot assign requested address"},{"location":"000020350/#environment","text":"Rancher v2.5.7 Kubernetes v1.20.4-rancher1","title":"Environment"},{"location":"000020350/#situation","text":"Adding a new node to an existing cluster stuck in \u201cregistering\u201d. Kubelet pod is in Restarting state on the new node. $ docker ps -a |grep kubelet 66bd40b36e76 rancher/hyperkube:v1.20.4-rancher1 \"/opt/rke-tools/entr\u2026\" 7 minutes ago Restarting (255) 32 seconds ago kubelet The kubelet is failing with below error. $ docker logs kubelet \"2021-07-26T11:13:48.270162766Z F0726 11:13:48.270086 40730 docker_service.go:415] Streaming server stopped unexpectedly: listen tcp 27.0.0.1:0: bind: cannot assign requested address\"","title":"Situation"},{"location":"000020350/#resolution","text":"Update /etc/hosts file with the correct entry. 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 After updating the file, the kubelet will restart itself and pick up the modified /etc/host file to bind to the loopback IP.","title":"Resolution"},{"location":"000020350/#cause","text":"File /etc/hosts in the new node had below incorrect entry. 27.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4","title":"Cause"},{"location":"000020350/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020373/","text":"How to drain node from local node using docker command This document (000020373) is provided subject to the disclaimer at the end of this document. Environment RKE1 Rancher v2.4.x , v2.5.x Situation Need a way to cordon or drain node when OS patching is automated, and the automation doesn't have access to Rancher API. Resolution Integrate the below command in the automation to drain the node in the OS patching workflow. docker exec kubelet bash -c 'kubectl --kubeconfig <(kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\") drain $(hostname -s) --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s' Please note that the below flags need to be changed according to your requirements. --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s Cause Draining operation using kubeconfig file \" /etc/kubernetes/ssl/kubecfg-kube-node.yaml\" will result in errors like below since the \" system:node\" role is not authorized to access needed API groups. cannot delete daemonsets.apps \"nginx-ingress-controller\" is forbidden: User \"system:node\" cannot get resource \"daemonsets\" in API group \"apps\" in the namespace \"ingress-nginx\" But this kubeconfig file can access the config map \"full-cluster-state \" in namespace \" kube-system \" contains the kubeconfig file, which has the privilege to do the drain operation. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to drain node from local node using docker command"},{"location":"000020373/#how-to-drain-node-from-local-node-using-docker-command","text":"This document (000020373) is provided subject to the disclaimer at the end of this document.","title":"How to drain node from local node using docker command"},{"location":"000020373/#environment","text":"RKE1 Rancher v2.4.x , v2.5.x","title":"Environment"},{"location":"000020373/#situation","text":"Need a way to cordon or drain node when OS patching is automated, and the automation doesn't have access to Rancher API.","title":"Situation"},{"location":"000020373/#resolution","text":"Integrate the below command in the automation to drain the node in the OS patching workflow. docker exec kubelet bash -c 'kubectl --kubeconfig <(kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\") drain $(hostname -s) --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s' Please note that the below flags need to be changed according to your requirements. --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s","title":"Resolution"},{"location":"000020373/#cause","text":"Draining operation using kubeconfig file \" /etc/kubernetes/ssl/kubecfg-kube-node.yaml\" will result in errors like below since the \" system:node\" role is not authorized to access needed API groups. cannot delete daemonsets.apps \"nginx-ingress-controller\" is forbidden: User \"system:node\" cannot get resource \"daemonsets\" in API group \"apps\" in the namespace \"ingress-nginx\" But this kubeconfig file can access the config map \"full-cluster-state \" in namespace \" kube-system \" contains the kubeconfig file, which has the privilege to do the drain operation.","title":"Cause"},{"location":"000020373/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020415/","text":"Updating roles in Rancher UI fails with certificate error This document (000020415) is provided subject to the disclaimer at the end of this document. Environment Rancher v2.5.x Situation Editing roles in Rancher UI fails with the below error. Internal error occurred: failed calling webhook \"rancherauth.cattle.io\": Post \"https://rancher-webhook.cattle-system.svc:443/v1/webhook/validation?timeout=10s\": x509: certificate has expired or is not yet valid: current time 2021-10-25T07:43:50Z is after 2021-10-06T20:20:47Z Resolution Set kubectl context to Rancher management cluster. Take the backup of existing secret kubectl get secret -n cattle-system cattle-webhook-tls -o yaml > cattle-webhook-tls.yaml ``` Delete the secret that contains expired certificate kubectl delete secret -n cattle-system cattle-webhook-tls - Delete the rancher webhook Pod to regenerate the expired certificate. kubectl delete pod -n cattle-system -l app=rancher-webhook ``` Cause This issue is caused by the expired certificate of the rancher webhook. Additional Information The issue is tracked in GitHub issue 35068 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Updating roles in Rancher UI fails with certificate error"},{"location":"000020415/#updating-roles-in-rancher-ui-fails-with-certificate-error","text":"This document (000020415) is provided subject to the disclaimer at the end of this document.","title":"Updating roles in Rancher UI fails with certificate error"},{"location":"000020415/#environment","text":"Rancher v2.5.x","title":"Environment"},{"location":"000020415/#situation","text":"Editing roles in Rancher UI fails with the below error. Internal error occurred: failed calling webhook \"rancherauth.cattle.io\": Post \"https://rancher-webhook.cattle-system.svc:443/v1/webhook/validation?timeout=10s\": x509: certificate has expired or is not yet valid: current time 2021-10-25T07:43:50Z is after 2021-10-06T20:20:47Z","title":"Situation"},{"location":"000020415/#resolution","text":"Set kubectl context to Rancher management cluster. Take the backup of existing secret kubectl get secret -n cattle-system cattle-webhook-tls -o yaml > cattle-webhook-tls.yaml ``` Delete the secret that contains expired certificate kubectl delete secret -n cattle-system cattle-webhook-tls - Delete the rancher webhook Pod to regenerate the expired certificate. kubectl delete pod -n cattle-system -l app=rancher-webhook ```","title":"Resolution"},{"location":"000020415/#cause","text":"This issue is caused by the expired certificate of the rancher webhook.","title":"Cause"},{"location":"000020415/#additional-information","text":"The issue is tracked in GitHub issue 35068","title":"Additional Information"},{"location":"000020415/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020416/","text":"Downstream clusters flapping between available and unavailable state This document (000020416) is provided subject to the disclaimer at the end of this document. Environment Rancher version: v2.5.6 Management cluster K8S version: 1.21.5 Situation After upgrading the Kubernetes version of the Rancher management cluster, the downstream cluster status in the WebUI flaps between the available and unavailable states. Rancher Pod logs show errors like the below; Failed to connect to peer wss://x.x.x.x/v3/connect [local ID=y.y.y.y]: websocket: bad handshake Resolution Upgrade Rancher to v2.6.x A workaround until Rancher upgarde is to reduce the Rancher deployment replicas to one. Cause Rancher is storing the service account token from the initial Pod, and then trying to reuse that on subsequent requests even though that pod has been deleted. As of Kubernetes version v1.21, service account tokens are pod-specific, and are invalidated when the pod is deleted, which is why Rancher is unable to use it and thus unable to reach other Rancher replica instances via web-socket. Additional Information The issue is tracked in the GitHub issue 26082 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Downstream clusters flapping between available and unavailable state"},{"location":"000020416/#downstream-clusters-flapping-between-available-and-unavailable-state","text":"This document (000020416) is provided subject to the disclaimer at the end of this document.","title":"Downstream clusters flapping between available and unavailable state"},{"location":"000020416/#environment","text":"Rancher version: v2.5.6 Management cluster K8S version: 1.21.5","title":"Environment"},{"location":"000020416/#situation","text":"After upgrading the Kubernetes version of the Rancher management cluster, the downstream cluster status in the WebUI flaps between the available and unavailable states. Rancher Pod logs show errors like the below; Failed to connect to peer wss://x.x.x.x/v3/connect [local ID=y.y.y.y]: websocket: bad handshake","title":"Situation"},{"location":"000020416/#resolution","text":"Upgrade Rancher to v2.6.x A workaround until Rancher upgarde is to reduce the Rancher deployment replicas to one.","title":"Resolution"},{"location":"000020416/#cause","text":"Rancher is storing the service account token from the initial Pod, and then trying to reuse that on subsequent requests even though that pod has been deleted. As of Kubernetes version v1.21, service account tokens are pod-specific, and are invalidated when the pod is deleted, which is why Rancher is unable to use it and thus unable to reach other Rancher replica instances via web-socket.","title":"Cause"},{"location":"000020416/#additional-information","text":"The issue is tracked in the GitHub issue 26082","title":"Additional Information"},{"location":"000020416/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020430/","text":"[Rancher] What is the difference between \"stable\" and \"latest\" release tags? This document (000020430) is provided subject to the disclaimer at the end of this document. Resolution For the Rancher portfolio of products, we make available \u201clatest\u201d tagged releases for our community to test-drive a new release and provide us feedback. These \u201clatest\u201d tagged releases, whilst covered by Rancher Support Services, are not meant for production use cases. Customers are recommended to use the \u201cstable\u201d tagged releases for their own production use cases. Additional Information Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What is the difference between \"stable\" and \"latest\" release tags?"},{"location":"000020430/#rancher-what-is-the-difference-between-stable-and-latest-release-tags","text":"This document (000020430) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What is the difference between \"stable\" and \"latest\" release tags?"},{"location":"000020430/#resolution","text":"For the Rancher portfolio of products, we make available \u201clatest\u201d tagged releases for our community to test-drive a new release and provide us feedback. These \u201clatest\u201d tagged releases, whilst covered by Rancher Support Services, are not meant for production use cases. Customers are recommended to use the \u201cstable\u201d tagged releases for their own production use cases.","title":"Resolution"},{"location":"000020430/#additional-information","text":"Rancher Support FAQs","title":"Additional Information"},{"location":"000020430/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020431/","text":"[Rancher] What does 'Developer Support\" mean? This document (000020431) is provided subject to the disclaimer at the end of this document. Resolution Any mention of \"Developer Support\" is basically a reference to this: Rancher Support is part of Rancher Engineering organization and has access to Engineers who developed Rancher product features. These Engineers are able to assist Rancher Support should some deep troubleshooting investigation be needed. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What does 'Developer Support\" mean?"},{"location":"000020431/#rancher-what-does-developer-support-mean","text":"This document (000020431) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What does 'Developer Support\" mean?"},{"location":"000020431/#resolution","text":"Any mention of \"Developer Support\" is basically a reference to this: Rancher Support is part of Rancher Engineering organization and has access to Engineers who developed Rancher product features. These Engineers are able to assist Rancher Support should some deep troubleshooting investigation be needed.","title":"Resolution"},{"location":"000020431/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020432/","text":"Rancher Support FAQ This document (000020432) is provided subject to the disclaimer at the end of this document. Resolution Quick Links SUSE Rancher Product Lifecycle Information SUSE Rancher Product Support Matrices SUSE Rancher Support Advisories SUSE Rancher Hosted FAQ SUSE Customer Center (SCC) (Support Portal) SCC sign-in help Rancher 2.x Linux log collector script Rancher 2.x Windows log collector script Rancher 2.x Systems summary script Severity Levels, Designations, and Escalations Could you help us understand how to determine the severity level for my case, what the response times are? (SUSE Support blog post) How do I change the severity level on an open case? Could you illustrate the severity levels with subject lines of sample support cases? Product Releases and Support Phases What is the difference between \"stable\" and \"latest\" release tags? What does 'Developer Support\" mean? What is \"Full Support\" vs \"Limited Support\"? Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version? Request for Assistance Can Rancher Support validate our planned upgrade? We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events? How much in advance do we need to notify Rancher Support on upgrades? Can Rancher Support join me as I do my upgrade? We need help validating our deployment design and operational readiness, can Rancher Support help? Rancher and RancherOS Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on? Could you help us understand the development and support status of RancherOS for 2020 and beyond? Does Rancher support migration from single node installation to high availability installation? Does Rancher Support cover single node installations? Kubernetes How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version? I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue? Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes? As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster? Is Rancher Support only for RKE-provisioned clusters? Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases? Docker and OS What does Rancher support for Docker on Ubuntu cover? Will Rancher support us should our deployment be on Red Hat Atomic? What does Rancher support for Docker on RHEL cover? What does Rancher support for Docker on CentOS cover? Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL? What does Rancher support for Docker on Windows Server cover? What does Rancher support for Docker on Oracle Linux cover? Security How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance? How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities? Certified Integrations What are the certified integrations with persistent volume plugins covered by Rancher Support? What are the certified integrations with storage class provisioners covered by Rancher Support? What are the certified integrations with authentication providers covered by Rancher Support? Could you clarify what you generally mean by a \"certified integration\" to another software system or service? Included Open Source Software Components Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana? Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x? Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins? What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd? Will Rancher fix problems root-caused to be in nginx? Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher? Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico? Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal? Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio? We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers? Third-party Software Components Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software? Does Rancher include any 3rd party, commercial software components? Support Matrix We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA? We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then? I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then? Node Drivers and Infrastructure My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack? Does it matter what hardware my hosts are on? Are virtualized servers supported? I see node drivers tagged as \"Built-in\". What does that mean? I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean? Kubernetes Cloud Providers What are the Kubernetes cloud providers supported by Rancher? Requests for Enhancements (RFEs) I filed an RFE as a support case. What should I expect on how it will be followed up on? Support for all other Rancher software projects What about support for Harvester, Longhorn, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider? Does Rancher Support cover RKE (standalone CLI)? How about support for ancillary projects, such as an API client, from Rancher Labs? Customizations We have a few customizations with our on- premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations? Is there a way we can get our custom work be included into Rancher Support? Localization What language(s) is Rancher Support service offered in? Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support? Licensing and Usage Does my support subscription to Rancher include support for Longhorn? Does my support subscription to Rancher include support for RancherOS? Can we have a mix of unsupported and supported nodes at our choice/discretion? Can we request support for the management/upstream cluster and certain downstream clusters and not others? How does Rancher track our license usage? Is Rancher Support only for production environments? I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well? Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher Support FAQ"},{"location":"000020432/#rancher-support-faq","text":"This document (000020432) is provided subject to the disclaimer at the end of this document.","title":"Rancher Support FAQ"},{"location":"000020432/#resolution","text":"","title":"Resolution"},{"location":"000020432/#quick-links","text":"SUSE Rancher Product Lifecycle Information SUSE Rancher Product Support Matrices SUSE Rancher Support Advisories SUSE Rancher Hosted FAQ SUSE Customer Center (SCC) (Support Portal) SCC sign-in help Rancher 2.x Linux log collector script Rancher 2.x Windows log collector script Rancher 2.x Systems summary script","title":"Quick Links"},{"location":"000020432/#severity-levels-designations-and-escalations","text":"Could you help us understand how to determine the severity level for my case, what the response times are? (SUSE Support blog post) How do I change the severity level on an open case? Could you illustrate the severity levels with subject lines of sample support cases?","title":"Severity Levels, Designations, and Escalations"},{"location":"000020432/#product-releases-and-support-phases","text":"What is the difference between \"stable\" and \"latest\" release tags? What does 'Developer Support\" mean? What is \"Full Support\" vs \"Limited Support\"? Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version?","title":"Product Releases and Support Phases"},{"location":"000020432/#request-for-assistance","text":"Can Rancher Support validate our planned upgrade? We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events? How much in advance do we need to notify Rancher Support on upgrades? Can Rancher Support join me as I do my upgrade? We need help validating our deployment design and operational readiness, can Rancher Support help?","title":"Request for Assistance"},{"location":"000020432/#rancher-and-rancheros","text":"Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on? Could you help us understand the development and support status of RancherOS for 2020 and beyond? Does Rancher support migration from single node installation to high availability installation? Does Rancher Support cover single node installations?","title":"Rancher and RancherOS"},{"location":"000020432/#kubernetes","text":"How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version? I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue? Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes? As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster? Is Rancher Support only for RKE-provisioned clusters? Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?","title":"Kubernetes"},{"location":"000020432/#docker-and-os","text":"What does Rancher support for Docker on Ubuntu cover? Will Rancher support us should our deployment be on Red Hat Atomic? What does Rancher support for Docker on RHEL cover? What does Rancher support for Docker on CentOS cover? Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL? What does Rancher support for Docker on Windows Server cover? What does Rancher support for Docker on Oracle Linux cover?","title":"Docker and OS"},{"location":"000020432/#security","text":"How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance? How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?","title":"Security"},{"location":"000020432/#certified-integrations","text":"What are the certified integrations with persistent volume plugins covered by Rancher Support? What are the certified integrations with storage class provisioners covered by Rancher Support? What are the certified integrations with authentication providers covered by Rancher Support? Could you clarify what you generally mean by a \"certified integration\" to another software system or service?","title":"Certified Integrations"},{"location":"000020432/#included-open-source-software-components","text":"Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana? Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x? Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins? What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd? Will Rancher fix problems root-caused to be in nginx? Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher? Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico? Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal? Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio? We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?","title":"Included Open Source Software Components"},{"location":"000020432/#third-party-software-components","text":"Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software? Does Rancher include any 3rd party, commercial software components?","title":"Third-party Software Components"},{"location":"000020432/#support-matrix","text":"We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA? We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then? I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then?","title":"Support Matrix"},{"location":"000020432/#node-drivers-and-infrastructure","text":"My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack? Does it matter what hardware my hosts are on? Are virtualized servers supported? I see node drivers tagged as \"Built-in\". What does that mean? I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?","title":"Node Drivers and Infrastructure"},{"location":"000020432/#kubernetes-cloud-providers","text":"What are the Kubernetes cloud providers supported by Rancher?","title":"Kubernetes Cloud Providers"},{"location":"000020432/#requests-for-enhancements-rfes","text":"I filed an RFE as a support case. What should I expect on how it will be followed up on?","title":"Requests for Enhancements (RFEs)"},{"location":"000020432/#support-for-all-other-rancher-software-projects","text":"What about support for Harvester, Longhorn, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider? Does Rancher Support cover RKE (standalone CLI)? How about support for ancillary projects, such as an API client, from Rancher Labs?","title":"Support for all other Rancher software projects"},{"location":"000020432/#customizations","text":"We have a few customizations with our on- premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations? Is there a way we can get our custom work be included into Rancher Support?","title":"Customizations"},{"location":"000020432/#localization","text":"What language(s) is Rancher Support service offered in? Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support?","title":"Localization"},{"location":"000020432/#licensing-and-usage","text":"Does my support subscription to Rancher include support for Longhorn? Does my support subscription to Rancher include support for RancherOS? Can we have a mix of unsupported and supported nodes at our choice/discretion? Can we request support for the management/upstream cluster and certain downstream clusters and not others? How does Rancher track our license usage? Is Rancher Support only for production environments? I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well? Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue?","title":"Licensing and Usage"},{"location":"000020432/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020434/","text":"[Rancher] I filed an RFE as a support case. What should I expect on how it will be followed up on? This document (000020434) is provided subject to the disclaimer at the end of this document. Resolution For any RFEs received via a support case, Rancher Support will update the case with a GitHub link to the enhancement request. Rancher Support will try to understand your request first and then qualify the request with information on business impact, time sensitivity, and criticality. It is not uncommon at all for there to be follow-up questions from Rancher Support on your request. This information will be most helpful to advocate for the request with Rancher Product Management. Unless there is a pressing urgency that has been understood and acknowledged, it could take up to a few weeks to triage the RFE (from the product backlog) through our release planning and identify the earliest release vehicle the item could be considered for (or committed to). As there is progress and/or should there be specific questions from Engineering, Rancher Support will keep you updated via this case. As necessary, Rancher Support will also recommend and schedule/facilitate a direct web conference session for you with our product management to go over the RFE. For committed RFEs, Rancher Support will update you as the item gets closer to its general availability via a new Rancher release or keep you informed should there be any delays or proposed changes in the previously understood scope. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] I filed an RFE as a support case. What should I expect on how it will be followed up on?"},{"location":"000020434/#rancher-i-filed-an-rfe-as-a-support-case-what-should-i-expect-on-how-it-will-be-followed-up-on","text":"This document (000020434) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] I filed an RFE as a support case. What should I expect on how it will be followed up on?"},{"location":"000020434/#resolution","text":"For any RFEs received via a support case, Rancher Support will update the case with a GitHub link to the enhancement request. Rancher Support will try to understand your request first and then qualify the request with information on business impact, time sensitivity, and criticality. It is not uncommon at all for there to be follow-up questions from Rancher Support on your request. This information will be most helpful to advocate for the request with Rancher Product Management. Unless there is a pressing urgency that has been understood and acknowledged, it could take up to a few weeks to triage the RFE (from the product backlog) through our release planning and identify the earliest release vehicle the item could be considered for (or committed to). As there is progress and/or should there be specific questions from Engineering, Rancher Support will keep you updated via this case. As necessary, Rancher Support will also recommend and schedule/facilitate a direct web conference session for you with our product management to go over the RFE. For committed RFEs, Rancher Support will update you as the item gets closer to its general availability via a new Rancher release or keep you informed should there be any delays or proposed changes in the previously understood scope.","title":"Resolution"},{"location":"000020434/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020435/","text":"[Rancher] What is \"Full Support\" vs \"Limited Support\"? This document (000020435) is provided subject to the disclaimer at the end of this document. Resolution \"Full Support\" is the support phase when a product is between its GA (General Availability) and End of Maintenance (EOM) milestones. Support entails general troubleshooting of a specific issue to isolate potential causes. Issue resolution is pursued through one or more of the following: Applying configuration changes Upgrade recommendation to an existing newer version of the product Code-level maintenance in the form of product updates; typically, results in a maintenance release, which is a newer version of the product that was not existing at the time the issue was encountered \"Limited Support\" is the support phase when a product is between its EOM and End of Life (EOL) milestones. During this phase, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner in the form of: General troubleshooting of a specific issue to isolate potential causes Issue resolution is limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What is \"Full Support\" vs \"Limited Support\"?"},{"location":"000020435/#rancher-what-is-full-support-vs-limited-support","text":"This document (000020435) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What is \"Full Support\" vs \"Limited Support\"?"},{"location":"000020435/#resolution","text":"\"Full Support\" is the support phase when a product is between its GA (General Availability) and End of Maintenance (EOM) milestones. Support entails general troubleshooting of a specific issue to isolate potential causes. Issue resolution is pursued through one or more of the following: Applying configuration changes Upgrade recommendation to an existing newer version of the product Code-level maintenance in the form of product updates; typically, results in a maintenance release, which is a newer version of the product that was not existing at the time the issue was encountered \"Limited Support\" is the support phase when a product is between its EOM and End of Life (EOL) milestones. During this phase, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner in the form of: General troubleshooting of a specific issue to isolate potential causes Issue resolution is limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product","title":"Resolution"},{"location":"000020435/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020436/","text":"[Rancher] Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version? This document (000020436) is provided subject to the disclaimer at the end of this document. Resolution As our standard policy, any bug will only be fixed and delivered as a new maintenance release within the minor version of a Rancher product. Customer will be advised to upgrade to this new maintenance release that contains the fix. Table below illustrates this with an example: v2.1.3 Version that the Customer deployment is currently on. This is the product version that the bug has been found and acknowledged in, stemming from the support issue reported by Customer. v2.1.9 Version that is the latest maintenance release of Rancher v2.1 at the time the bug was found and acknowledged. v2.1.c Version \"c\" is where the bug is likely to get fixed. Here, \"c\" = 10 or greater. This is the soonest maintenance release of Rancher 2.1 that the Customer will need to upgrade to, to realize the bug fix. Should the bug fix be deemed as necessary for other minor versions of the same Rancher product, Rancher shall deliver the fix in an approach similar to the sequence in the above table. For a scenario where v2.0.14 and v2.2.2 are the latest maintenance releases of Rancher v2.0 and v2.2, and the same bug fix needs to be made available in these two minor versions, it is shall only be delivered in a release that is v2.0.15 or later and v2.2.3 or later, respectively. In extraordinary situations, as an exception to our standard policy and as feasible, Rancher can provide a patch fix on a specific Customer-requested version. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version?"},{"location":"000020436/#rancher-will-rancher-fix-the-specific-product-version-that-my-deployment-is-on-should-a-bug-be-reported-in-that-version","text":"This document (000020436) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version?"},{"location":"000020436/#resolution","text":"As our standard policy, any bug will only be fixed and delivered as a new maintenance release within the minor version of a Rancher product. Customer will be advised to upgrade to this new maintenance release that contains the fix. Table below illustrates this with an example: v2.1.3 Version that the Customer deployment is currently on. This is the product version that the bug has been found and acknowledged in, stemming from the support issue reported by Customer. v2.1.9 Version that is the latest maintenance release of Rancher v2.1 at the time the bug was found and acknowledged. v2.1.c Version \"c\" is where the bug is likely to get fixed. Here, \"c\" = 10 or greater. This is the soonest maintenance release of Rancher 2.1 that the Customer will need to upgrade to, to realize the bug fix. Should the bug fix be deemed as necessary for other minor versions of the same Rancher product, Rancher shall deliver the fix in an approach similar to the sequence in the above table. For a scenario where v2.0.14 and v2.2.2 are the latest maintenance releases of Rancher v2.0 and v2.2, and the same bug fix needs to be made available in these two minor versions, it is shall only be delivered in a release that is v2.0.15 or later and v2.2.3 or later, respectively. In extraordinary situations, as an exception to our standard policy and as feasible, Rancher can provide a patch fix on a specific Customer-requested version.","title":"Resolution"},{"location":"000020436/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020437/","text":"[Rancher] Can Rancher Support validate our planned upgrade? This document (000020437) is provided subject to the disclaimer at the end of this document. Resolution Rancher Support recommends Customers to notify them ahead of planned maintenance events for upgrade and migration. The notification can be made as a support case. At a minimum, this case should have information on: Current Rancher version Target Rancher version Reason for upgrade Upgrade date and maintenance time window But it would be best if Customer could provide the requested information in a document (please ask for the \" Upgrade Notification RFI Template \" in the support case). Based on the information provided, Rancher Support can provide any applicable advisories to the Customer for the planned event. Rancher Support may request Customer to run specific information gathering scripts. Data collected from these scripts will be used by Rancher Support to understand the Customer deployment and validate the upgrade path that is being considered. Additional Information Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Can Rancher Support validate our planned upgrade?"},{"location":"000020437/#rancher-can-rancher-support-validate-our-planned-upgrade","text":"This document (000020437) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Can Rancher Support validate our planned upgrade?"},{"location":"000020437/#resolution","text":"Rancher Support recommends Customers to notify them ahead of planned maintenance events for upgrade and migration. The notification can be made as a support case. At a minimum, this case should have information on: Current Rancher version Target Rancher version Reason for upgrade Upgrade date and maintenance time window But it would be best if Customer could provide the requested information in a document (please ask for the \" Upgrade Notification RFI Template \" in the support case). Based on the information provided, Rancher Support can provide any applicable advisories to the Customer for the planned event. Rancher Support may request Customer to run specific information gathering scripts. Data collected from these scripts will be used by Rancher Support to understand the Customer deployment and validate the upgrade path that is being considered.","title":"Resolution"},{"location":"000020437/#additional-information","text":"Rancher Support FAQs","title":"Additional Information"},{"location":"000020437/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020438/","text":"[Rancher] We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events? This document (000020438) is provided subject to the disclaimer at the end of this document. Resolution Rancher Support will not join the Customer team remotely for the duration of the event. However, they will remain on standby, on high alert, to respond per SLA should there be an issue. Rancher Support recommends Customer to notify in advance, via a support ticket, all necessary information about such events. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the event. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?"},{"location":"000020438/#rancher-we-run-a-burst-of-high-profile-events-eg-big-sale-that-run-as-k8s-workloads-managed-by-rancher-can-rancher-support-join-our-team-remotely-for-the-duration-of-these-events","text":"This document (000020438) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?"},{"location":"000020438/#resolution","text":"Rancher Support will not join the Customer team remotely for the duration of the event. However, they will remain on standby, on high alert, to respond per SLA should there be an issue. Rancher Support recommends Customer to notify in advance, via a support ticket, all necessary information about such events. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the event.","title":"Resolution"},{"location":"000020438/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020439/","text":"[Rancher] How much in advance do we need to notify Rancher Support on upgrades? This document (000020439) is provided subject to the disclaimer at the end of this document. Resolution On all regular, planned upgrades, Rancher Support requests Customer to notify as early as possible. Notifications that are one (1) calendar week in advance provide reasonable time for Rancher Support to follow up with any applicable advisories. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] How much in advance do we need to notify Rancher Support on upgrades?"},{"location":"000020439/#rancher-how-much-in-advance-do-we-need-to-notify-rancher-support-on-upgrades","text":"This document (000020439) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] How much in advance do we need to notify Rancher Support on upgrades?"},{"location":"000020439/#resolution","text":"On all regular, planned upgrades, Rancher Support requests Customer to notify as early as possible. Notifications that are one (1) calendar week in advance provide reasonable time for Rancher Support to follow up with any applicable advisories.","title":"Resolution"},{"location":"000020439/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020440/","text":"[Rancher] Can Rancher Support join me as I do my upgrade? This document (000020440) is provided subject to the disclaimer at the end of this document. Resolution Generally speaking, Rancher Support will not join a Customer for the purpose of performing upgrades of their environment. However, Customers are encouraged to notify Rancher Support of planned maintenance events for upgrade and migration. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the maintenance window. Additionally, such notifications provide for an opportunity to validate the planned upgrade proactively rather than react to issues that could have been avoided. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Can Rancher Support join me as I do my upgrade?"},{"location":"000020440/#rancher-can-rancher-support-join-me-as-i-do-my-upgrade","text":"This document (000020440) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Can Rancher Support join me as I do my upgrade?"},{"location":"000020440/#resolution","text":"Generally speaking, Rancher Support will not join a Customer for the purpose of performing upgrades of their environment. However, Customers are encouraged to notify Rancher Support of planned maintenance events for upgrade and migration. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the maintenance window. Additionally, such notifications provide for an opportunity to validate the planned upgrade proactively rather than react to issues that could have been avoided.","title":"Resolution"},{"location":"000020440/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020441/","text":"[Rancher] We need help validating our deployment design and operational readiness, can Rancher Support help? This document (000020441) is provided subject to the disclaimer at the end of this document. Resolution Validating deployment design and operational readiness is an activity for SUSE Rancher Premium Support and/or Consulting teams. Requests coming in as a support case for this activity shall be routed to the leadership team of Rancher Premium Support and Consulting, for follow-up with the Customer. Topics such as the following, but not limited to, will be handled in a similar manner: Best Practice Guidance Deployment Review Design Assistance Co-development Migration Assistance Performance Tuning Security Assessment Solution Consulting Topical Training Technology Recommendation Typically, these are topics that are not specific to incidents and issue troubleshooting. They are more requests with strategic drivers that usually require the help and guidance of SUSE Rancher Premium Support and/or Consulting teams. Additional Information Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] We need help validating our deployment design and operational readiness, can Rancher Support help?"},{"location":"000020441/#rancher-we-need-help-validating-our-deployment-design-and-operational-readiness-can-rancher-support-help","text":"This document (000020441) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] We need help validating our deployment design and operational readiness, can Rancher Support help?"},{"location":"000020441/#resolution","text":"Validating deployment design and operational readiness is an activity for SUSE Rancher Premium Support and/or Consulting teams. Requests coming in as a support case for this activity shall be routed to the leadership team of Rancher Premium Support and Consulting, for follow-up with the Customer. Topics such as the following, but not limited to, will be handled in a similar manner: Best Practice Guidance Deployment Review Design Assistance Co-development Migration Assistance Performance Tuning Security Assessment Solution Consulting Topical Training Technology Recommendation Typically, these are topics that are not specific to incidents and issue troubleshooting. They are more requests with strategic drivers that usually require the help and guidance of SUSE Rancher Premium Support and/or Consulting teams.","title":"Resolution"},{"location":"000020441/#additional-information","text":"Rancher Support FAQs","title":"Additional Information"},{"location":"000020441/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020442/","text":"[Rancher] Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on? This document (000020442) is provided subject to the disclaimer at the end of this document. Resolution Whilst technically possible, running other workloads or microservices in the same Kubernetes cluster that Rancher is installed on invalidates the Rancher Support SLA. So it is something that is recommended against. Any technical support that is offered for tickets stemming from this scenario shall only be on a best-effort basis. Note: Run Rancher on a Separate Cluster is also one of the top items called out in our docs page on Tips for Running Rancher . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?"},{"location":"000020442/#rancher-can-i-run-other-non-rancher-workloads-on-the-same-kubernetes-cluster-that-rancher-is-installed-on","text":"This document (000020442) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?"},{"location":"000020442/#resolution","text":"Whilst technically possible, running other workloads or microservices in the same Kubernetes cluster that Rancher is installed on invalidates the Rancher Support SLA. So it is something that is recommended against. Any technical support that is offered for tickets stemming from this scenario shall only be on a best-effort basis. Note: Run Rancher on a Separate Cluster is also one of the top items called out in our docs page on Tips for Running Rancher .","title":"Resolution"},{"location":"000020442/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020443/","text":"[Rancher] Could you help us understand the development and support status of RancherOS for 2020 and beyond? This document (000020443) is provided subject to the disclaimer at the end of this document. Resolution Development Status EDIT (NOV 2021): RancherOS 1.x is past its EOL milestone and no longer maintained. RancherOS 1.x is currently in a maintain-only-as-essential mode. That is to say, it is no longer being actively maintained at a code level other than addressing critical or security fixes. There are two significant reasons behind this product decision: 1. Docker. The current industry requirements for a container runtime is very much evolving. Container runtimes like containerd and CRIO are now being actively considered as the default choice. RancherOS 1.x, which was specifically designed around using Docker engine only, unfortunately does not lend itself, in its current design, to this new evolving requirement. 2. ISV Support. RancherOS was specifically designed as a minimalistic OS to support purpose-built containerized applications. It was not designed to be used as a general-purpose OS (such as CentOS or Ubuntu). As such, most ISVs have not certified their software to run on RancherOS, nor does RancherOS even contain the necessary components for many of these applications to run. Support Status RancherOS 1.x is no longer commercially supported. Please refer this FAQ: Does my support subscription to Rancher include support for RancherOS? Any assistance from Rancher Support on RancherOS topics, filed as support cases, is not SLA- bound. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Could you help us understand the development and support status of RancherOS for 2020 and beyond?"},{"location":"000020443/#rancher-could-you-help-us-understand-the-development-and-support-status-of-rancheros-for-2020-and-beyond","text":"This document (000020443) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Could you help us understand the development and support status of RancherOS for 2020 and beyond?"},{"location":"000020443/#resolution","text":"","title":"Resolution"},{"location":"000020443/#development-status","text":"","title":"Development Status"},{"location":"000020443/#edit-nov-2021-rancheros-1x-is-past-its-eol-milestone-and-no-longer-maintained","text":"RancherOS 1.x is currently in a maintain-only-as-essential mode. That is to say, it is no longer being actively maintained at a code level other than addressing critical or security fixes. There are two significant reasons behind this product decision: 1. Docker. The current industry requirements for a container runtime is very much evolving. Container runtimes like containerd and CRIO are now being actively considered as the default choice. RancherOS 1.x, which was specifically designed around using Docker engine only, unfortunately does not lend itself, in its current design, to this new evolving requirement. 2. ISV Support. RancherOS was specifically designed as a minimalistic OS to support purpose-built containerized applications. It was not designed to be used as a general-purpose OS (such as CentOS or Ubuntu). As such, most ISVs have not certified their software to run on RancherOS, nor does RancherOS even contain the necessary components for many of these applications to run.","title":"EDIT (NOV 2021): RancherOS 1.x is past its EOL milestone and no longer maintained."},{"location":"000020443/#support-status","text":"","title":"Support Status"},{"location":"000020443/#rancheros-1x-is-no-longer-commercially-supported","text":"Please refer this FAQ: Does my support subscription to Rancher include support for RancherOS? Any assistance from Rancher Support on RancherOS topics, filed as support cases, is not SLA- bound.","title":"RancherOS 1.x is no longer commercially supported."},{"location":"000020443/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020444/","text":"[Rancher] Does my support subscription to Rancher include support for RancherOS? This document (000020444) is provided subject to the disclaimer at the end of this document. Resolution No. RancherOS is an independent product. A separate commercial support subscription is needed for RancherOS support. But that is no longer available for purchase as RancherOS 1.x is past its End-of-Life (EOL) and End-of-Sale (EOS) milestone dates. Also, refer Could you help us understand the development and support status of RancherOS for 2020 and beyond? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does my support subscription to Rancher include support for RancherOS?"},{"location":"000020444/#rancher-does-my-support-subscription-to-rancher-include-support-for-rancheros","text":"This document (000020444) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does my support subscription to Rancher include support for RancherOS?"},{"location":"000020444/#resolution","text":"No. RancherOS is an independent product. A separate commercial support subscription is needed for RancherOS support. But that is no longer available for purchase as RancherOS 1.x is past its End-of-Life (EOL) and End-of-Sale (EOS) milestone dates. Also, refer Could you help us understand the development and support status of RancherOS for 2020 and beyond?","title":"Resolution"},{"location":"000020444/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020445/","text":"[Rancher] Does Rancher support migration from single node installation to high availability installation? This document (000020445) is provided subject to the disclaimer at the end of this document. Resolution There is currently no validated path that is officially covered by Rancher Support, for any Customer starting with a single node installation, and wishing to migrate to a high availability installation at a later time. Whilst there is a Rancher blog post that talks about one possible migration path, it is not covered by Rancher Support. Where scale and performance criteria are well understood to be critical, Customers are recommended to set up Rancher in a high availability configuration, right from the outset. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does Rancher support migration from single node installation to high availability installation?"},{"location":"000020445/#rancher-does-rancher-support-migration-from-single-node-installation-to-high-availability-installation","text":"This document (000020445) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does Rancher support migration from single node installation to high availability installation?"},{"location":"000020445/#resolution","text":"There is currently no validated path that is officially covered by Rancher Support, for any Customer starting with a single node installation, and wishing to migrate to a high availability installation at a later time. Whilst there is a Rancher blog post that talks about one possible migration path, it is not covered by Rancher Support. Where scale and performance criteria are well understood to be critical, Customers are recommended to set up Rancher in a high availability configuration, right from the outset.","title":"Resolution"},{"location":"000020445/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020446/","text":"[Rancher] Does Rancher Support cover single node installations? This document (000020446) is provided subject to the disclaimer at the end of this document. Resolution Rancher Support can cover single node installations only for support tickets that are NOT related to (a) scale and performance and (b) recovery of data and software (embedded etcd). Customer environments in need of scale and performance and meeting recovery criteria should be set up as high availability installations. Refer this Rancher docs page for single node installation versus high availability installation. Rancher recommends high-availability installs in production environments, where the Customer's user base requires 24\u20447 access to running applications. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does Rancher Support cover single node installations?"},{"location":"000020446/#rancher-does-rancher-support-cover-single-node-installations","text":"This document (000020446) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does Rancher Support cover single node installations?"},{"location":"000020446/#resolution","text":"Rancher Support can cover single node installations only for support tickets that are NOT related to (a) scale and performance and (b) recovery of data and software (embedded etcd). Customer environments in need of scale and performance and meeting recovery criteria should be set up as high availability installations. Refer this Rancher docs page for single node installation versus high availability installation. Rancher recommends high-availability installs in production environments, where the Customer's user base requires 24\u20447 access to running applications.","title":"Resolution"},{"location":"000020446/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020447/","text":"[Rancher] How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version? This document (000020447) is provided subject to the disclaimer at the end of this document. Resolution Kubernetes versions are expressed as x.y.z, where x is the major version, y is the minor version, and z is the patch version, following Semantic Versioning terminology. Where an upstream version x.y.z has been posted as supported in the support matrix, under Rancher Kubernetes, z is the highest patch version for that minor version (y) of Kubernetes, that has been tested and validated for the specific Rancher product version. In the case of Rancher v2.6.2 , the following are listed as the supported k8s upstream versions for Rancher Kubernetes: v1.21.5 v1.20.11 v1.19.15 v1.18.20 That is to say, the following are the k8s versions that are supported in Rancher v2.6.2: v1.21.x (v1.21.0-v1.21.5) v1.20.x (v1.20.0-v1.20.11) v1.19.x (v1.19.0-v1.19.15) v1.18.x (v1.18.0-v1.18.20) Note: As described in this Rancher docs page, the RKE metadata feature\u2014available as of v2.3.0\u2014allows users to provision clusters with new versions of Kubernetes as soon as they are released, without upgrading Rancher. For a specific version of Rancher, if a k8s patch version ( z+i ) that is higher than what is listed in the support matrix is available via a metadata refresh, then that patch version z+i is considered supported for that version of Rancher. Also, see Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?"},{"location":"000020447/#rancher-how-should-i-understand-the-kubernetes-patch-version-mentioned-in-the-support-matrix-under-rancher-kubernetes-for-a-specific-rancher-product-version","text":"This document (000020447) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?"},{"location":"000020447/#resolution","text":"Kubernetes versions are expressed as x.y.z, where x is the major version, y is the minor version, and z is the patch version, following Semantic Versioning terminology. Where an upstream version x.y.z has been posted as supported in the support matrix, under Rancher Kubernetes, z is the highest patch version for that minor version (y) of Kubernetes, that has been tested and validated for the specific Rancher product version. In the case of Rancher v2.6.2 , the following are listed as the supported k8s upstream versions for Rancher Kubernetes: v1.21.5 v1.20.11 v1.19.15 v1.18.20 That is to say, the following are the k8s versions that are supported in Rancher v2.6.2: v1.21.x (v1.21.0-v1.21.5) v1.20.x (v1.20.0-v1.20.11) v1.19.x (v1.19.0-v1.19.15) v1.18.x (v1.18.0-v1.18.20) Note: As described in this Rancher docs page, the RKE metadata feature\u2014available as of v2.3.0\u2014allows users to provision clusters with new versions of Kubernetes as soon as they are released, without upgrading Rancher. For a specific version of Rancher, if a k8s patch version ( z+i ) that is higher than what is listed in the support matrix is available via a metadata refresh, then that patch version z+i is considered supported for that version of Rancher. Also, see Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?","title":"Resolution"},{"location":"000020447/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020448/","text":"[Rancher] Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases? This document (000020448) is provided subject to the disclaimer at the end of this document. Resolution Rancher supports the most recent three minor releases of kubernetes that are still active. Additionally, Rancher may also support one (or more) recent upstream version to drop off its active maintenance status. As an example, below are the highest k8s minor versions supported by the respective Rancher 2.x product versions (as of January 2022): Rancher v2.4.18 => k8s 1.15, 1.16, 1.17, 1.18 Rancher v2.5.14 => k8s 1.17, 1.18, 1.19, 1.20 Rancher v2.6.3 => k8s 1.18, 1.19, 1.20, 1.21 Arithmetic progression, if any, in the sequence above is merely coincidental and should not be used to extrapolate the k8s versions that a future version of Rancher such as v2.7 would support. Also, for the most up-to-date information, please visit the All Supported Versions page. Generally speaking, the following should help understand the Rancher approach to supporting k8s versions: Rancher Labs strives to certify the latest GA release of k8s roughly in a month's timeframe from its availability. For example, k8s v1.16 became generally available in September 2019. The Rancher roadmap consideration would then be to certify and support k8s v1.16 in a release vehicle targeted for no later than October 2019. The ability to certify a new GA release of k8s, per above, could however be impacted by any unplanned- for CVEs that Rancher Labs needs to react to. This turned out to be the case for Rancher v2.3.1 that shipped on 16 Oct 2019. The focus of v2.3.1 shifted to addressing on priority a new k8s CVE (CVE-2019-11253) announced by upstream kubernetes. And, hence the support for k8s v1.16 got moved to the release vehicle after v2.3.1. And, when v1.16 is supported in that release, v1.13 shall be dropped in our support matrix from that version forward. This is to also keep up with the k8s version maintenance policy that you can see here: https://kubernetes.io/docs/setup/release/version-skew-policy/#supported-versions Specifically, \"The Kubernetes project maintains release branches for the most recent three minor releases. Minor releases occur approximately every 3 months, so each minor release branch is maintained for approximately 9 months.\" Also, see How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?"},{"location":"000020448/#rancher-could-you-help-us-understand-the-rancher-approach-to-supporting-specific-k8s-versions-and-how-rancher-keeps-up-with-new-k8s-releases","text":"This document (000020448) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?"},{"location":"000020448/#resolution","text":"Rancher supports the most recent three minor releases of kubernetes that are still active. Additionally, Rancher may also support one (or more) recent upstream version to drop off its active maintenance status. As an example, below are the highest k8s minor versions supported by the respective Rancher 2.x product versions (as of January 2022): Rancher v2.4.18 => k8s 1.15, 1.16, 1.17, 1.18 Rancher v2.5.14 => k8s 1.17, 1.18, 1.19, 1.20 Rancher v2.6.3 => k8s 1.18, 1.19, 1.20, 1.21 Arithmetic progression, if any, in the sequence above is merely coincidental and should not be used to extrapolate the k8s versions that a future version of Rancher such as v2.7 would support. Also, for the most up-to-date information, please visit the All Supported Versions page. Generally speaking, the following should help understand the Rancher approach to supporting k8s versions: Rancher Labs strives to certify the latest GA release of k8s roughly in a month's timeframe from its availability. For example, k8s v1.16 became generally available in September 2019. The Rancher roadmap consideration would then be to certify and support k8s v1.16 in a release vehicle targeted for no later than October 2019. The ability to certify a new GA release of k8s, per above, could however be impacted by any unplanned- for CVEs that Rancher Labs needs to react to. This turned out to be the case for Rancher v2.3.1 that shipped on 16 Oct 2019. The focus of v2.3.1 shifted to addressing on priority a new k8s CVE (CVE-2019-11253) announced by upstream kubernetes. And, hence the support for k8s v1.16 got moved to the release vehicle after v2.3.1. And, when v1.16 is supported in that release, v1.13 shall be dropped in our support matrix from that version forward. This is to also keep up with the k8s version maintenance policy that you can see here: https://kubernetes.io/docs/setup/release/version-skew-policy/#supported-versions Specifically, \"The Kubernetes project maintains release branches for the most recent three minor releases. Minor releases occur approximately every 3 months, so each minor release branch is maintained for approximately 9 months.\" Also, see How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?","title":"Resolution"},{"location":"000020448/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020449/","text":"[Rancher] I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue? This document (000020449) is provided subject to the disclaimer at the end of this document. Resolution Rancher Support is comprehensive for RKE-provisioned clusters. In imported clusters, Rancher Support applies only to the extent of troubleshooting and root-causing. For issues in (legacy) clusters under consideration for import into Rancher, Customer is advised to take it up with the party that is the provider of support for such clusters. Post-import, Rancher Support for such clusters is per response to this question, Is Rancher Support only for RKE-provisioned clusters? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?"},{"location":"000020449/#rancher-i-plan-to-import-my-legacy-k8s-cluster-into-rancher-and-having-issues-with-my-legacy-cluster-can-rancher-support-help-troubleshoot-the-issue","text":"This document (000020449) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?"},{"location":"000020449/#resolution","text":"Rancher Support is comprehensive for RKE-provisioned clusters. In imported clusters, Rancher Support applies only to the extent of troubleshooting and root-causing. For issues in (legacy) clusters under consideration for import into Rancher, Customer is advised to take it up with the party that is the provider of support for such clusters. Post-import, Rancher Support for such clusters is per response to this question, Is Rancher Support only for RKE-provisioned clusters?","title":"Resolution"},{"location":"000020449/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020450/","text":"[Rancher] Is Rancher Support only for RKE-provisioned clusters? This document (000020450) is provided subject to the disclaimer at the end of this document. Resolution Update: In November 2019 , k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here . With the general availability of k3s, Rancher Support extends to k3s clusters. Comprehensive Rancher Support, inclusive of Kubernetes and Docker, applies only to RKE-provisioned clusters for Rancher releases before 2.6.5. As of the 2.6.6 release of Rancher, k3s, RKE2 are fully supported as well What this means is the following: In the RKE-provisioned clusters, Rancher can provide patch fixes as needed at the levels of Kubernetes and Docker. For clusters that are brought under management of the Rancher control plane, as imported clusters, Rancher Support applies only to those Kubernetes versions published in the support matrix and to the extent of making sure Rancher control plane functionality works as published in Rancher docs. Issues root- caused to be inside these clusters will need to be taken up by Customer with the provider of support for these clusters. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Is Rancher Support only for RKE-provisioned clusters?"},{"location":"000020450/#rancher-is-rancher-support-only-for-rke-provisioned-clusters","text":"This document (000020450) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Is Rancher Support only for RKE-provisioned clusters?"},{"location":"000020450/#resolution","text":"Update: In November 2019 , k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here . With the general availability of k3s, Rancher Support extends to k3s clusters. Comprehensive Rancher Support, inclusive of Kubernetes and Docker, applies only to RKE-provisioned clusters for Rancher releases before 2.6.5. As of the 2.6.6 release of Rancher, k3s, RKE2 are fully supported as well What this means is the following: In the RKE-provisioned clusters, Rancher can provide patch fixes as needed at the levels of Kubernetes and Docker. For clusters that are brought under management of the Rancher control plane, as imported clusters, Rancher Support applies only to those Kubernetes versions published in the support matrix and to the extent of making sure Rancher control plane functionality works as published in Rancher docs. Issues root- caused to be inside these clusters will need to be taken up by Customer with the provider of support for these clusters.","title":"Resolution"},{"location":"000020450/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020451/","text":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes? This document (000020451) is provided subject to the disclaimer at the end of this document. Resolution Yes, should there be a critical need, Rancher can provide patch fixes to address issues root-caused in RKE-provisioned Kubernetes clusters. As a first option, Rancher will investigate if the fix is already available in a later version of Kubernetes. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later Kubernetes version that has the fix. Or validate and certify one of its existing versions to work with the later Kubernetes version that has the fix. Further, Rancher can submit a PR for the fix to Kubernetes for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Kubernetes. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?"},{"location":"000020451/#rancher-will-rancher-fix-the-issue-and-release-a-patch-if-the-problem-is-root-caused-in-kubernetes","text":"This document (000020451) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?"},{"location":"000020451/#resolution","text":"Yes, should there be a critical need, Rancher can provide patch fixes to address issues root-caused in RKE-provisioned Kubernetes clusters. As a first option, Rancher will investigate if the fix is already available in a later version of Kubernetes. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later Kubernetes version that has the fix. Or validate and certify one of its existing versions to work with the later Kubernetes version that has the fix. Further, Rancher can submit a PR for the fix to Kubernetes for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Kubernetes.","title":"Resolution"},{"location":"000020451/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020452/","text":"[Rancher] As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster? This document (000020452) is provided subject to the disclaimer at the end of this document. Resolution It is recommended to move to the k8s versions listed in the matrix for a Rancher version but not doing that should not break things. You could leave the cluster (for example, k8s v1.11) as is, for a move to say Rancher v2.2.6 from v2.2.2. It should be ok. Rancher Support will continue to help should there be a need. That said, should there be an issue on that cluster that requires a fix in k8s v1.11 we may not be able to do that and would at that point require an upgrade to a higher supported k8s version. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?"},{"location":"000020452/#rancher-as-part-of-a-rancher-upgrade-do-i-need-to-upgrade-my-k8s-cluster-to-a-version-listed-in-the-support-matrix-for-that-rancher-version-if-i-do-not-will-the-cluster-continue-to-work-fine-as-is-will-rancher-support-this-cluster","text":"This document (000020452) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?"},{"location":"000020452/#resolution","text":"It is recommended to move to the k8s versions listed in the matrix for a Rancher version but not doing that should not break things. You could leave the cluster (for example, k8s v1.11) as is, for a move to say Rancher v2.2.6 from v2.2.2. It should be ok. Rancher Support will continue to help should there be a need. That said, should there be an issue on that cluster that requires a fix in k8s v1.11 we may not be able to do that and would at that point require an upgrade to a higher supported k8s version.","title":"Resolution"},{"location":"000020452/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020453/","text":"[Rancher] What are the Kubernetes cloud providers supported by Rancher? This document (000020453) is provided subject to the disclaimer at the end of this document. Resolution Rancher Support is currently limited to Kubernetes cloud providers for Amazon and Azure. When adding a cluster, Amazon and Azure are the only two cloud providers that are currently surfaced up in the Rancher UX. For all other cloud providers, directly editing the yaml file via the custom option is the only way to pass configuration information. In this scenario, Rancher Support expects Customer to manage and troubleshoot the syntactic correctness of the yaml file, per guidance provided by Kubernetes. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What are the Kubernetes cloud providers supported by Rancher?"},{"location":"000020453/#rancher-what-are-the-kubernetes-cloud-providers-supported-by-rancher","text":"This document (000020453) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What are the Kubernetes cloud providers supported by Rancher?"},{"location":"000020453/#resolution","text":"Rancher Support is currently limited to Kubernetes cloud providers for Amazon and Azure. When adding a cluster, Amazon and Azure are the only two cloud providers that are currently surfaced up in the Rancher UX. For all other cloud providers, directly editing the yaml file via the custom option is the only way to pass configuration information. In this scenario, Rancher Support expects Customer to manage and troubleshoot the syntactic correctness of the yaml file, per guidance provided by Kubernetes.","title":"Resolution"},{"location":"000020453/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020454/","text":"[Rancher] What about support for Harvester, Longhorn, Rancher Desktop, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider? This document (000020454) is provided subject to the disclaimer at the end of this document. Resolution Note: In December 2021 , Harvester graduated from being just a community project led by SUSE Rancher to a product that is now commercially supported by SUSE Rancher with a subscription to an add-on plan. View release notes for Harvester v1.0.0 here . In June 2020 , Longhorn graduated from being just a community project led by Rancher Labs to a product that is now commercially supported by Rancher Labs with a subscription to an add-on plan. View release notes for Longhorn v1.0.0 here . In June 2020 , Terraform Provider for Rancher v2 graduated from a community project that was supported on a best-effort basis to being fully supported as part of an active subscription to a SUSE Rancher Support plan. Support for this project does not require any additional subscriptions. Visit the project repo here . In November 2019 , k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here . Projects Rancher Desktop, Kubewarden, Hypper, Epinio, and Opni are some of the new open-source software projects led by SUSE Rancher. These projects (and some legacy ones from Rancher Labs such as rio, k3os, and Submariner) are not yet available for commercial support from SUSE Rancher. For any bugs or questions, users are encouraged to post their issues here: Project GitHub Location Rancher Desktop https://github.com/rancher-sandbox/rancher-desktop/issues Kubewarden https://github.com/kubewarden Hypper https://github.com/rancher-sandbox/hypper/issues Epinio https://github.com/epinio/epinio/issues Opni https://github.com/rancher/opni/issues rio https://github.com/rancher/rio/issues k3os https://github.com/rancher/k3os/issues Submariner https://github.com/submariner-io/submariner/issues Additional Information Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What about support for Harvester, Longhorn, Rancher Desktop, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider?"},{"location":"000020454/#rancher-what-about-support-for-harvester-longhorn-rancher-desktop-rio-k3s-k3os-submariner-and-terraform-rancher2-provider","text":"This document (000020454) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What about support for Harvester, Longhorn, Rancher Desktop, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider?"},{"location":"000020454/#resolution","text":"Note: In December 2021 , Harvester graduated from being just a community project led by SUSE Rancher to a product that is now commercially supported by SUSE Rancher with a subscription to an add-on plan. View release notes for Harvester v1.0.0 here . In June 2020 , Longhorn graduated from being just a community project led by Rancher Labs to a product that is now commercially supported by Rancher Labs with a subscription to an add-on plan. View release notes for Longhorn v1.0.0 here . In June 2020 , Terraform Provider for Rancher v2 graduated from a community project that was supported on a best-effort basis to being fully supported as part of an active subscription to a SUSE Rancher Support plan. Support for this project does not require any additional subscriptions. Visit the project repo here . In November 2019 , k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here . Projects Rancher Desktop, Kubewarden, Hypper, Epinio, and Opni are some of the new open-source software projects led by SUSE Rancher. These projects (and some legacy ones from Rancher Labs such as rio, k3os, and Submariner) are not yet available for commercial support from SUSE Rancher. For any bugs or questions, users are encouraged to post their issues here: Project GitHub Location Rancher Desktop https://github.com/rancher-sandbox/rancher-desktop/issues Kubewarden https://github.com/kubewarden Hypper https://github.com/rancher-sandbox/hypper/issues Epinio https://github.com/epinio/epinio/issues Opni https://github.com/rancher/opni/issues rio https://github.com/rancher/rio/issues k3os https://github.com/rancher/k3os/issues Submariner https://github.com/submariner-io/submariner/issues","title":"Resolution"},{"location":"000020454/#additional-information","text":"Rancher Support FAQs","title":"Additional Information"},{"location":"000020454/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020456/","text":"[Rancher] How about support for ancillary projects, such as an API client, from Rancher Labs? This document (000020456) is provided subject to the disclaimer at the end of this document. Resolution Projects such as the Rancher Python Client are maintained by SUSE Rancher on a best effort basis only. Any Rancher user is welcome to use them. However, these projects are not covered by Rancher SLA. For any bugs or questions, users are encouraged to post their issues here: Project GitHub Location Rancher Client Python https://github.com/rancher/client-python/issues Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] How about support for ancillary projects, such as an API client, from Rancher Labs?"},{"location":"000020456/#rancher-how-about-support-for-ancillary-projects-such-as-an-api-client-from-rancher-labs","text":"This document (000020456) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] How about support for ancillary projects, such as an API client, from Rancher Labs?"},{"location":"000020456/#resolution","text":"Projects such as the Rancher Python Client are maintained by SUSE Rancher on a best effort basis only. Any Rancher user is welcome to use them. However, these projects are not covered by Rancher SLA. For any bugs or questions, users are encouraged to post their issues here: Project GitHub Location Rancher Client Python https://github.com/rancher/client-python/issues","title":"Resolution"},{"location":"000020456/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020457/","text":"[Rancher] Does Rancher Support cover RKE (standalone CLI)? This document (000020457) is provided subject to the disclaimer at the end of this document. Resolution Yes, support for RKE is implicit and covered under an active Rancher subscription to one of our support plans. There is currently no separate product SKU that offers commercial support separately and only for RKE. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does Rancher Support cover RKE (standalone CLI)?"},{"location":"000020457/#rancher-does-rancher-support-cover-rke-standalone-cli","text":"This document (000020457) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does Rancher Support cover RKE (standalone CLI)?"},{"location":"000020457/#resolution","text":"Yes, support for RKE is implicit and covered under an active Rancher subscription to one of our support plans. There is currently no separate product SKU that offers commercial support separately and only for RKE.","title":"Resolution"},{"location":"000020457/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020458/","text":"[Rancher] We have a few customizations with our on-premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations? This document (000020458) is provided subject to the disclaimer at the end of this document. Resolution Customizations are defined to include any changes to the original source code, including but not limited to changes to the User Interface, the addition or modification of adapters such as for authentication, VM or server provisioning, deploying the software (e.g., the management server) on an operating system or Docker versions that are not certified by SUSE Rancher, and altering the scripts and byte code included with the product. Customizations to this software may have unintended consequences and cause issues that are not present with the original, unmodified software. As a result, it is our policy that any bugs, defects, or other issues that are present in areas of the product that the Customer has altered must be reproduced by the Customer on an unmodified system prior to the submission of a support case or bug report. Additionally, the Customer is required to state all customizations present on the system when submitting a support case. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] We have a few customizations with our on-premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations?"},{"location":"000020458/#rancher-we-have-a-few-customizations-with-our-on-premise-deployment-for-example-we-run-a-forked-version-of-the-rke-openstack-driver-with-new-capabilities-will-rancher-support-cover-such-customizations","text":"This document (000020458) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] We have a few customizations with our on-premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations?"},{"location":"000020458/#resolution","text":"Customizations are defined to include any changes to the original source code, including but not limited to changes to the User Interface, the addition or modification of adapters such as for authentication, VM or server provisioning, deploying the software (e.g., the management server) on an operating system or Docker versions that are not certified by SUSE Rancher, and altering the scripts and byte code included with the product. Customizations to this software may have unintended consequences and cause issues that are not present with the original, unmodified software. As a result, it is our policy that any bugs, defects, or other issues that are present in areas of the product that the Customer has altered must be reproduced by the Customer on an unmodified system prior to the submission of a support case or bug report. Additionally, the Customer is required to state all customizations present on the system when submitting a support case.","title":"Resolution"},{"location":"000020458/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020459/","text":"[Rancher] Is there a way we can get our custom work be included into Rancher Support? This document (000020459) is provided subject to the disclaimer at the end of this document. Resolution By definition, something custom is needed because it solves a unique (or specific or snowflake) requirement for one particular team. A product is meant to solve for 1000\u2019s (and more) of teams. To take on support for custom work is not merely about Rancher Support services looking into tickets and issues reported in them. Besides the cost of the initial vetting and validation effort on the custom work, it has ongoing Engineering and QA impact. Rancher has to keep testing (as QA) the custom work across every next release to make sure that nothing is broken. Maintain knowledge of what the custom work is about and keep maintaining (as Engineering) that piece of code for its entire life cycle including potential improvements as needed. This is expensive, distracting, and misaligned with the objectives of any product team. Hence, custom work will not be considered for inclusion in product/support. And, any issues in such custom work are considered out of scope of Rancher Support. That said, here are scenarios and options that could be considered, if what started as custom work by one team is believed to be valuable for many teams and a customer is interested in exploring its productization (and support) in Rancher: Rancher is 100% open source. Users are welcome to make contributions / PRs on the public Rancher repos on GitHub. Contributions will be evaluated for inclusion in product, based on merit and alignment with the Rancher roadmap. Customers are welcome to optionally advocate for their contribution via their Rancher Customer Success Manager or Account Executive. To accelerate and secure commitment toward productizing a specific work or feature, Customer can explore the possibility of a Non-Recurring Engineering (NRE) engagement with Rancher on a commercial basis. In this scenario, Customer is recommended to request a conversation with Rancher Product Management, via their Rancher Customer Success Manager or Account Executive. Any NRE work shall be pursued by Rancher only if it is determined by Rancher Product Management as being viable and in alignment with the product roadmap and its strategic goals. Lastly, most Rancher customers that have such custom software have their own dev teams that support and maintain them across the versions of all vendor and open-source software, Rancher included, that they use for their overall solution. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Is there a way we can get our custom work be included into Rancher Support?"},{"location":"000020459/#rancher-is-there-a-way-we-can-get-our-custom-work-be-included-into-rancher-support","text":"This document (000020459) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Is there a way we can get our custom work be included into Rancher Support?"},{"location":"000020459/#resolution","text":"By definition, something custom is needed because it solves a unique (or specific or snowflake) requirement for one particular team. A product is meant to solve for 1000\u2019s (and more) of teams. To take on support for custom work is not merely about Rancher Support services looking into tickets and issues reported in them. Besides the cost of the initial vetting and validation effort on the custom work, it has ongoing Engineering and QA impact. Rancher has to keep testing (as QA) the custom work across every next release to make sure that nothing is broken. Maintain knowledge of what the custom work is about and keep maintaining (as Engineering) that piece of code for its entire life cycle including potential improvements as needed. This is expensive, distracting, and misaligned with the objectives of any product team. Hence, custom work will not be considered for inclusion in product/support. And, any issues in such custom work are considered out of scope of Rancher Support. That said, here are scenarios and options that could be considered, if what started as custom work by one team is believed to be valuable for many teams and a customer is interested in exploring its productization (and support) in Rancher: Rancher is 100% open source. Users are welcome to make contributions / PRs on the public Rancher repos on GitHub. Contributions will be evaluated for inclusion in product, based on merit and alignment with the Rancher roadmap. Customers are welcome to optionally advocate for their contribution via their Rancher Customer Success Manager or Account Executive. To accelerate and secure commitment toward productizing a specific work or feature, Customer can explore the possibility of a Non-Recurring Engineering (NRE) engagement with Rancher on a commercial basis. In this scenario, Customer is recommended to request a conversation with Rancher Product Management, via their Rancher Customer Success Manager or Account Executive. Any NRE work shall be pursued by Rancher only if it is determined by Rancher Product Management as being viable and in alignment with the product roadmap and its strategic goals. Lastly, most Rancher customers that have such custom software have their own dev teams that support and maintain them across the versions of all vendor and open-source software, Rancher included, that they use for their overall solution.","title":"Resolution"},{"location":"000020459/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020460/","text":"[Rancher] What language(s) is Rancher Support service offered in? This document (000020460) is provided subject to the disclaimer at the end of this document. Resolution English is the official language in which support is delivered to global customers, with the exception of China, where it is in Chinese. If it is deemed helpful and necessary, SUSE Rancher Support will engage colleagues, from our Premium Support, Consulting, and Customer Success teams, who are fluent in specific local languages for assistance on a case. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What language(s) is Rancher Support service offered in?"},{"location":"000020460/#rancher-what-languages-is-rancher-support-service-offered-in","text":"This document (000020460) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What language(s) is Rancher Support service offered in?"},{"location":"000020460/#resolution","text":"English is the official language in which support is delivered to global customers, with the exception of China, where it is in Chinese. If it is deemed helpful and necessary, SUSE Rancher Support will engage colleagues, from our Premium Support, Consulting, and Customer Success teams, who are fluent in specific local languages for assistance on a case.","title":"Resolution"},{"location":"000020460/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020461/","text":"[Rancher] Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support? This document (000020461) is provided subject to the disclaimer at the end of this document. Resolution No. English and Chinese (simplified+traditional) are the only languages currently covered by Rancher Support. Translations for all the other languages are maintained by Rancher Community users. Please visit https://translate.rancher.com/ if you would like to help translate Rancher into other languages. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support?"},{"location":"000020461/#rancher-besides-english-we-see-many-languages-listed-in-the-rancher-ui-dropdown-are-they-all-covered-by-rancher-support","text":"This document (000020461) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support?"},{"location":"000020461/#resolution","text":"No. English and Chinese (simplified+traditional) are the only languages currently covered by Rancher Support. Translations for all the other languages are maintained by Rancher Community users. Please visit https://translate.rancher.com/ if you would like to help translate Rancher into other languages.","title":"Resolution"},{"location":"000020461/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020462/","text":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana? This document (000020462) is provided subject to the disclaimer at the end of this document. Resolution Yes. Prometheus and Grafana are the components natively used and supported by Rancher v2.2+ for monitoring and dashboards. Any issues root-caused in one of these two projects, as an included component of Rancher v2.2+, will be supported fully, like how any Rancher product issue would be. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?"},{"location":"000020462/#rancher-will-rancher-fix-the-issue-and-release-a-patch-if-the-problem-is-root-caused-in-prometheus-and-grafana","text":"This document (000020462) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?"},{"location":"000020462/#resolution","text":"Yes. Prometheus and Grafana are the components natively used and supported by Rancher v2.2+ for monitoring and dashboards. Any issues root-caused in one of these two projects, as an included component of Rancher v2.2+, will be supported fully, like how any Rancher product issue would be.","title":"Resolution"},{"location":"000020462/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020463/","text":"[Rancher] Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x? This document (000020463) is provided subject to the disclaimer at the end of this document. Resolution Correct. To be more accurate, Rancher Support is only for the Prometheus/Grafana components that are natively embedded in Rancher 2.x for the functionality of monitoring and dashboards. In the exceptional scenario of any legacy support subscriptions that are limited to RKE-only (without Rancher 2.x), it does not include support of Prometheus/Grafana that are set up externally to monitor the RKE clusters. Refer this article for more details: Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x?"},{"location":"000020463/#rancher-is-support-for-prometheusgrafana-available-only-with-a-valid-support-subscription-for-rancher-2x","text":"This document (000020463) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x?"},{"location":"000020463/#resolution","text":"Correct. To be more accurate, Rancher Support is only for the Prometheus/Grafana components that are natively embedded in Rancher 2.x for the functionality of monitoring and dashboards. In the exceptional scenario of any legacy support subscriptions that are limited to RKE-only (without Rancher 2.x), it does not include support of Prometheus/Grafana that are set up externally to monitor the RKE clusters. Refer this article for more details: Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?","title":"Resolution"},{"location":"000020463/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020464/","text":"[Rancher] Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher? This document (000020464) is provided subject to the disclaimer at the end of this document. Resolution Support for Prometheus/Grafana is only for what is embedded in Rancher 2.x and natively used by it for the functionalities of monitoring and dashboards. Refer Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana? Where Prometheus/Grafana has been enabled from the chart in the \"Library\" catalog, refer Are the applications underlying the charts in the \"Library\" Catalog covered by Rancher Support? Prometheus/Grafana installed by all other means that did not originate from Rancher fall outside the scope of Rancher Support and are not covered by the terms of service of a Rancher Support subscription. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?"},{"location":"000020464/#rancher-does-my-rancher-support-cover-prometheusgrafana-deployments-that-did-not-originate-from-rancher","text":"This document (000020464) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?"},{"location":"000020464/#resolution","text":"Support for Prometheus/Grafana is only for what is embedded in Rancher 2.x and natively used by it for the functionalities of monitoring and dashboards. Refer Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana? Where Prometheus/Grafana has been enabled from the chart in the \"Library\" catalog, refer Are the applications underlying the charts in the \"Library\" Catalog covered by Rancher Support? Prometheus/Grafana installed by all other means that did not originate from Rancher fall outside the scope of Rancher Support and are not covered by the terms of service of a Rancher Support subscription.","title":"Resolution"},{"location":"000020464/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020465/","text":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins? This document (000020465) is provided subject to the disclaimer at the end of this document. Resolution Yes. Jenkins is the open-source component used and supported by Rancher 2.1+ as the native build engine for running pipelines. Any issues root-caused in Jenkins will be supported fully, like how any Rancher product issue would be. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?"},{"location":"000020465/#rancher-will-rancher-fix-the-issue-and-release-a-patch-if-the-problem-is-root-caused-in-jenkins","text":"This document (000020465) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?"},{"location":"000020465/#resolution","text":"Yes. Jenkins is the open-source component used and supported by Rancher 2.1+ as the native build engine for running pipelines. Any issues root-caused in Jenkins will be supported fully, like how any Rancher product issue would be.","title":"Resolution"},{"location":"000020465/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020466/","text":"[Rancher] What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd? This document (000020466) is provided subject to the disclaimer at the end of this document. Resolution Rancher supports its certified integrations with log aggregation systems such as Elasticsearch, Splunk, Kafka, Syslog, and Fluentd. What this means is the following: Rancher will help Customer troubleshoot the root cause for any issue related to one of these logging services. For issues root-caused to be in the integration to one of these services, Rancher will provide a fix to resolve such issues. For issues root-caused to be inside one of these logging services, Rancher will investigate if the fix is already available in a later version of the logging service. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later version that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the logging service, Rancher will advise Customer to contact the logging service vendor directly for issue resolution. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?"},{"location":"000020466/#rancher-what-is-the-nature-of-rancher-support-for-log-aggregation-services-such-as-elasticsearch-splunk-kafka-syslog-fluentd","text":"This document (000020466) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?"},{"location":"000020466/#resolution","text":"Rancher supports its certified integrations with log aggregation systems such as Elasticsearch, Splunk, Kafka, Syslog, and Fluentd. What this means is the following: Rancher will help Customer troubleshoot the root cause for any issue related to one of these logging services. For issues root-caused to be in the integration to one of these services, Rancher will provide a fix to resolve such issues. For issues root-caused to be inside one of these logging services, Rancher will investigate if the fix is already available in a later version of the logging service. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later version that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the logging service, Rancher will advise Customer to contact the logging service vendor directly for issue resolution.","title":"Resolution"},{"location":"000020466/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020467/","text":"[Rancher] Will Rancher fix problems root-caused to be in nginx? This document (000020467) is provided subject to the disclaimer at the end of this document. Resolution Rancher will do one of the following, to help Customer resolve the issue root-caused to be in nginx: Should the issue have been resolved in a later version of nginx, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of nginx. If the issue is unresolved in any acceptable nginx product versions, Rancher will advise Customer to contact nginx directly for issue resolution. Should a new version or patch be provided by nginx, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Note: Inclusion of nginx as a certified component in the Rancher support matrix , is based on Rancher's own testing and validation of nginx with its default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine tune the settings for scale and performance. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix problems root-caused to be in nginx?"},{"location":"000020467/#rancher-will-rancher-fix-problems-root-caused-to-be-in-nginx","text":"This document (000020467) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix problems root-caused to be in nginx?"},{"location":"000020467/#resolution","text":"Rancher will do one of the following, to help Customer resolve the issue root-caused to be in nginx: Should the issue have been resolved in a later version of nginx, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of nginx. If the issue is unresolved in any acceptable nginx product versions, Rancher will advise Customer to contact nginx directly for issue resolution. Should a new version or patch be provided by nginx, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Note: Inclusion of nginx as a certified component in the Rancher support matrix , is based on Rancher's own testing and validation of nginx with its default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine tune the settings for scale and performance.","title":"Resolution"},{"location":"000020467/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020468/","text":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico? This document (000020468) is provided subject to the disclaimer at the end of this document. Resolution No. However, Rancher will do one of the following, to help Customer resolve the issue root-caused to be in an add-on such as Weave, Cisco ACI, Cilium, and Calico: Should the issue have been resolved in a later version of the add-on component, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of the add-on. If the issue is unresolved in any acceptable versions of the add-on, Rancher will advise Customer to contact the add-on vendor directly for issue resolution. Should a new version or patch be provided by the add-on vendor, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Note: Any inclusion of such add-ons as a certified component in the Rancher support matrix , is based on Rancher's own testing and validation of these add-ons with their default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine-tune the settings for scale and performance. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?"},{"location":"000020468/#rancher-will-rancher-fix-the-issue-and-release-a-patch-if-the-problem-is-root-caused-in-one-of-the-rancher-2x-cni-plugin-add-ons-such-as-weave-cisco-aci-cilium-and-calico","text":"This document (000020468) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?"},{"location":"000020468/#resolution","text":"No. However, Rancher will do one of the following, to help Customer resolve the issue root-caused to be in an add-on such as Weave, Cisco ACI, Cilium, and Calico: Should the issue have been resolved in a later version of the add-on component, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of the add-on. If the issue is unresolved in any acceptable versions of the add-on, Rancher will advise Customer to contact the add-on vendor directly for issue resolution. Should a new version or patch be provided by the add-on vendor, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Note: Any inclusion of such add-ons as a certified component in the Rancher support matrix , is based on Rancher's own testing and validation of these add-ons with their default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine-tune the settings for scale and performance.","title":"Resolution"},{"location":"000020468/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020470/","text":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal? This document (000020470) is provided subject to the disclaimer at the end of this document. Resolution Yes. Any issues root-caused in one of these two projects will be supported fully, like how any Rancher product issue would be. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?"},{"location":"000020470/#rancher-will-rancher-fix-the-issue-and-release-a-patch-if-the-problem-is-root-caused-in-flannel-and-canal","text":"This document (000020470) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?"},{"location":"000020470/#resolution","text":"Yes. Any issues root-caused in one of these two projects will be supported fully, like how any Rancher product issue would be.","title":"Resolution"},{"location":"000020470/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020471/","text":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio? This document (000020471) is provided subject to the disclaimer at the end of this document. Resolution Yes. Istio is the open-source component used and supported by Rancher 2.3+ for service mesh functionality. Any issues root-caused in Istio will be supported fully, like how any Rancher product issue would be. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?"},{"location":"000020471/#rancher-will-rancher-fix-the-issue-and-release-a-patch-if-the-problem-is-root-caused-in-istio","text":"This document (000020471) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?"},{"location":"000020471/#resolution","text":"Yes. Istio is the open-source component used and supported by Rancher 2.3+ for service mesh functionality. Any issues root-caused in Istio will be supported fully, like how any Rancher product issue would be.","title":"Resolution"},{"location":"000020471/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020472/","text":"[Rancher] We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers? This document (000020472) is provided subject to the disclaimer at the end of this document. Resolution Fluentd is the open-source component used and supported by Rancher 2.x for the native log forwarder functionality. To the extent of this functionality, any issues root-caused in Fluentd will be supported fully, like how any Rancher product issue would be. In the context of Fluentd as a log aggregation system, Rancher Support is limited to ensuring its certified integration with Fluentd works as intended. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?"},{"location":"000020472/#rancher-we-came-across-a-mention-of-fluentd-as-a-software-rancher-provides-support-sla-on-could-you-clarify-what-that-covers","text":"This document (000020472) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?"},{"location":"000020472/#resolution","text":"Fluentd is the open-source component used and supported by Rancher 2.x for the native log forwarder functionality. To the extent of this functionality, any issues root-caused in Fluentd will be supported fully, like how any Rancher product issue would be. In the context of Fluentd as a log aggregation system, Rancher Support is limited to ensuring its certified integration with Fluentd works as intended.","title":"Resolution"},{"location":"000020472/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020473/","text":"[Rancher] Does Rancher include any 3rd party, commercial software components? This document (000020473) is provided subject to the disclaimer at the end of this document. Resolution No. There are no commercial software components included in Rancher. Rancher products are 100% open source and only include components that are also open source and with the right kind of open source license such as, but not limited to, Apache 2.0. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does Rancher include any 3rd party, commercial software components?"},{"location":"000020473/#rancher-does-rancher-include-any-3rd-party-commercial-software-components","text":"This document (000020473) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does Rancher include any 3rd party, commercial software components?"},{"location":"000020473/#resolution","text":"No. There are no commercial software components included in Rancher. Rancher products are 100% open source and only include components that are also open source and with the right kind of open source license such as, but not limited to, Apache 2.0.","title":"Resolution"},{"location":"000020473/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020474/","text":"[Rancher] Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software? This document (000020474) is provided subject to the disclaimer at the end of this document. Resolution Yes. Rancher products are 100% open source and free to use for anyone. There are no hidden product features that are unlocked by signing up for a Rancher Support subscription. Also, see Does Rancher include any 3rd party, commercial software components? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software?"},{"location":"000020474/#rancher-is-the-rancher-software-itself-completely-free-outside-of-the-choice-to-add-support-or-not-add-support-meaning-if-we-choose-no-support-we-can-still-run-the-same-full-features-software","text":"This document (000020474) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software?"},{"location":"000020474/#resolution","text":"Yes. Rancher products are 100% open source and free to use for anyone. There are no hidden product features that are unlocked by signing up for a Rancher Support subscription. Also, see Does Rancher include any 3rd party, commercial software components?","title":"Resolution"},{"location":"000020474/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020475/","text":"[Rancher] How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance? This document (000020475) is provided subject to the disclaimer at the end of this document. Resolution \"How are you going to do the support for production systems, particularly for financial institutions that have to comply with PCI-DSS standards and that will not allow external access to the systems?\" Not just for customer systems that need to comply with restrictions but the following is applicable to any customer system that is covered by Rancher Support: A Rancher Support Engineer will never work on an issue on any customer system directly and with unmonitored access. Any troubleshooting will be done via a combination of the following: Log collection Over a screen share session and with the customer on a jump box Should there be very high-security scenarios where troubleshooting via the above is still not possible, Rancher Support can still help troubleshoot issues but help can only be provided in a second-hand manner. That is to say, any response can only be provided to the extent of the sanitized information shared by the Customer, with Rancher Support, and in a back-and-forth request-response transaction that may not be very efficient. Where such scenarios are well known and access-based support is still sought, Customer is requested to inquire with their Rancher Account Executive or Customer Success Manager for other commercial models of engagement such as Premium Support or via SUSE RGS. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?"},{"location":"000020475/#rancher-how-would-rancher-support-troubleshoot-issues-on-systems-where-any-external-access-is-not-allowed-for-reasons-of-security-and-compliance","text":"This document (000020475) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?"},{"location":"000020475/#resolution","text":"\"How are you going to do the support for production systems, particularly for financial institutions that have to comply with PCI-DSS standards and that will not allow external access to the systems?\" Not just for customer systems that need to comply with restrictions but the following is applicable to any customer system that is covered by Rancher Support: A Rancher Support Engineer will never work on an issue on any customer system directly and with unmonitored access. Any troubleshooting will be done via a combination of the following: Log collection Over a screen share session and with the customer on a jump box Should there be very high-security scenarios where troubleshooting via the above is still not possible, Rancher Support can still help troubleshoot issues but help can only be provided in a second-hand manner. That is to say, any response can only be provided to the extent of the sanitized information shared by the Customer, with Rancher Support, and in a back-and-forth request-response transaction that may not be very efficient. Where such scenarios are well known and access-based support is still sought, Customer is requested to inquire with their Rancher Account Executive or Customer Success Manager for other commercial models of engagement such as Premium Support or via SUSE RGS.","title":"Resolution"},{"location":"000020475/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020476/","text":"[Rancher] How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities? This document (000020476) is provided subject to the disclaimer at the end of this document. Environment Rancher RKE RKE2 K3s Harvester Longhorn NeuVector Resolution For industry-reported vulnerabilities in Rancher, RKE, RKE2, K3s, Harvester, Longhorn, NeuVector and upstream vulnerabilities in Kubernetes, Docker, and containerd, SUSE Rancher strives to adhere to industry standards and best practices. Due to the nature of upstream dependencies inherent to open-source software, the final delivery of patch releases may vary in timeline. We will prioritize our efforts and coordinate with upstream organizations and third-party entities according to the following guidelines: Critical: Immediate engagement to remediate the issue in code, and/or coordinate with upstream and/or third-party entities to deliver the remediation in the shortest timeline available. This includes creating an emergency release patch version when an existing one is not readily available. High: Prioritized engagement to align the delivery of the remediation with our next available release cycle. Emergency releases should only be needed unless the timing is such that the next available security release cycle is not in a reasonable timeline. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?"},{"location":"000020476/#rancher-how-quickly-does-rancher-respond-to-resolve-industry-reported-vulnerabilities","text":"This document (000020476) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?"},{"location":"000020476/#environment","text":"Rancher RKE RKE2 K3s Harvester Longhorn NeuVector","title":"Environment"},{"location":"000020476/#resolution","text":"For industry-reported vulnerabilities in Rancher, RKE, RKE2, K3s, Harvester, Longhorn, NeuVector and upstream vulnerabilities in Kubernetes, Docker, and containerd, SUSE Rancher strives to adhere to industry standards and best practices. Due to the nature of upstream dependencies inherent to open-source software, the final delivery of patch releases may vary in timeline. We will prioritize our efforts and coordinate with upstream organizations and third-party entities according to the following guidelines: Critical: Immediate engagement to remediate the issue in code, and/or coordinate with upstream and/or third-party entities to deliver the remediation in the shortest timeline available. This includes creating an emergency release patch version when an existing one is not readily available. High: Prioritized engagement to align the delivery of the remediation with our next available release cycle. Emergency releases should only be needed unless the timing is such that the next available security release cycle is not in a reasonable timeline.","title":"Resolution"},{"location":"000020476/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020477/","text":"[Rancher] We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA? This document (000020477) is provided subject to the disclaimer at the end of this document. Resolution Rancher Support SLA is based on our QA validation and certification on default OS kernels for operating systems listed in the product support matrices . Antivirus software adds an unknown variable to the existing complexity of Kubernetes. Most of them have not yet kept up with newer technologies such as Kubernetes and have not reached a CNCF certified status. In environments where antivirus software had been enabled, Rancher Support has seen issues stemming from interfering actions from such software. As an example, there have been incidents where the antivirus software had pruned files in the Docker filesystem incorrectly, causing the Docker mounts to go corrupt. Issues resulting from third-party tools, such as antivirus and intrusion detection software, interfering with Docker or other necessary system calls are deemed resolved should disabling such tools restore functionality. Lastly, all certified configurations, as published in the product support matrices , are based on the default settings of individual components. Where a customer environment has deviated from certified configurations, Rancher Labs reserves the right to recommend the customer to revert to a certified configuration to resolve the reported issue. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA?"},{"location":"000020477/#rancher-we-need-to-run-antivirus-on-our-cluster-nodes-would-that-impact-the-terms-of-service-of-our-rancher-support-sla","text":"This document (000020477) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA?"},{"location":"000020477/#resolution","text":"Rancher Support SLA is based on our QA validation and certification on default OS kernels for operating systems listed in the product support matrices . Antivirus software adds an unknown variable to the existing complexity of Kubernetes. Most of them have not yet kept up with newer technologies such as Kubernetes and have not reached a CNCF certified status. In environments where antivirus software had been enabled, Rancher Support has seen issues stemming from interfering actions from such software. As an example, there have been incidents where the antivirus software had pruned files in the Docker filesystem incorrectly, causing the Docker mounts to go corrupt. Issues resulting from third-party tools, such as antivirus and intrusion detection software, interfering with Docker or other necessary system calls are deemed resolved should disabling such tools restore functionality. Lastly, all certified configurations, as published in the product support matrices , are based on the default settings of individual components. Where a customer environment has deviated from certified configurations, Rancher Labs reserves the right to recommend the customer to revert to a certified configuration to resolve the reported issue.","title":"Resolution"},{"location":"000020477/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020478/","text":"[Rancher] We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then? This document (000020478) is provided subject to the disclaimer at the end of this document. Resolution For open source components not listed in the Rancher support matrix page , support is limited to troubleshooting for root cause up to Rancher\u2019s drivers and interfaces to those components. Root causes that are identified to be beyond this limit will need to be pursued by Company with the maintainers and providers of commercial support for those components. For ensuring best support and clarity on supportability, Company is recommended to publish to Rancher a list of components that are critical to its deployment but not explicitly called out in the support matrix. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then?"},{"location":"000020478/#rancher-we-use-components-that-are-not-listed-in-the-rancher-support-matrix-would-rancher-support-be-not-valid-then","text":"This document (000020478) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then?"},{"location":"000020478/#resolution","text":"For open source components not listed in the Rancher support matrix page , support is limited to troubleshooting for root cause up to Rancher\u2019s drivers and interfaces to those components. Root causes that are identified to be beyond this limit will need to be pursued by Company with the maintainers and providers of commercial support for those components. For ensuring best support and clarity on supportability, Company is recommended to publish to Rancher a list of components that are critical to its deployment but not explicitly called out in the support matrix.","title":"Resolution"},{"location":"000020478/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020479/","text":"[Rancher] I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then? This document (000020479) is provided subject to the disclaimer at the end of this document. Resolution Certified configurations in the Rancher support matrix page are based on the default settings of individual components. Where Customer has deviated from certified configurations, Rancher Support reserves the right to recommend the Company to revert to a certified configuration to resolve the reported issue. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then?"},{"location":"000020479/#rancher-i-see-many-included-components-listed-in-the-support-matrix-i-plan-to-change-the-default-configurations-of-one-or-more-of-these-components-would-rancher-support-be-not-valid-then","text":"This document (000020479) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then?"},{"location":"000020479/#resolution","text":"Certified configurations in the Rancher support matrix page are based on the default settings of individual components. Where Customer has deviated from certified configurations, Rancher Support reserves the right to recommend the Company to revert to a certified configuration to resolve the reported issue.","title":"Resolution"},{"location":"000020479/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020480/","text":"[Rancher] What are the certified integrations with persistent volume plugins covered by Rancher Support? This document (000020480) is provided subject to the disclaimer at the end of this document. Resolution In Rancher v2.2+, Rancher Support applies to the following certified integrations: Amazon EBS Disk Azure Disk Azure Filesystem Google Persistent Disk Longhorn Local Node Disk Local Node Path NFS Share VMware vSphere Volume Note: For any other persistent volume plugins from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What are the certified integrations with persistent volume plugins covered by Rancher Support?"},{"location":"000020480/#rancher-what-are-the-certified-integrations-with-persistent-volume-plugins-covered-by-rancher-support","text":"This document (000020480) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What are the certified integrations with persistent volume plugins covered by Rancher Support?"},{"location":"000020480/#resolution","text":"In Rancher v2.2+, Rancher Support applies to the following certified integrations: Amazon EBS Disk Azure Disk Azure Filesystem Google Persistent Disk Longhorn Local Node Disk Local Node Path NFS Share VMware vSphere Volume Note: For any other persistent volume plugins from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.","title":"Resolution"},{"location":"000020480/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020481/","text":"[Rancher] What are the certified integrations with storage class provisioners covered by Rancher Support? This document (000020481) is provided subject to the disclaimer at the end of this document. Resolution In Rancher v2.2+, Rancher Support applies to the following certified integrations: Amazon EBS Disk Azure Disk Azure File Google Persistent Disk VMware vSphere Volume Longhorn Local Note: For any other storage class provisioners from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What are the certified integrations with storage class provisioners covered by Rancher Support?"},{"location":"000020481/#rancher-what-are-the-certified-integrations-with-storage-class-provisioners-covered-by-rancher-support","text":"This document (000020481) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What are the certified integrations with storage class provisioners covered by Rancher Support?"},{"location":"000020481/#resolution","text":"In Rancher v2.2+, Rancher Support applies to the following certified integrations: Amazon EBS Disk Azure Disk Azure File Google Persistent Disk VMware vSphere Volume Longhorn Local Note: For any other storage class provisioners from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.","title":"Resolution"},{"location":"000020481/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020482/","text":"[Rancher] What are the certified integrations with authentication providers covered by Rancher Support? This document (000020482) is provided subject to the disclaimer at the end of this document. Resolution In Rancher v2.2+, Rancher Support applies to the following certified integrations: Active Directory Azure AD GitHub Google (supported only from Rancher v2.3 or higher) PingIdentity (SAML) Keycloak (SAML) AD FS (SAML) Okta (SAML) FreeIPA (LDAP) OpenLDAP (LDAP) Rancher Support does not cover: Switching between these external authentication providers. Migration from authentication scheme of one provider to another that would typically require maintaining existing user settings/preferences, access levels and privileges, user and group authorizations. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What are the certified integrations with authentication providers covered by Rancher Support?"},{"location":"000020482/#rancher-what-are-the-certified-integrations-with-authentication-providers-covered-by-rancher-support","text":"This document (000020482) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What are the certified integrations with authentication providers covered by Rancher Support?"},{"location":"000020482/#resolution","text":"In Rancher v2.2+, Rancher Support applies to the following certified integrations: Active Directory Azure AD GitHub Google (supported only from Rancher v2.3 or higher) PingIdentity (SAML) Keycloak (SAML) AD FS (SAML) Okta (SAML) FreeIPA (LDAP) OpenLDAP (LDAP)","title":"Resolution"},{"location":"000020482/#rancher-support-does-not-cover","text":"Switching between these external authentication providers. Migration from authentication scheme of one provider to another that would typically require maintaining existing user settings/preferences, access levels and privileges, user and group authorizations.","title":"Rancher Support does not cover:"},{"location":"000020482/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020483/","text":"[Rancher] Could you clarify what you generally mean by a \"certified integration\" to another software system or service? This document (000020483) is provided subject to the disclaimer at the end of this document. Resolution Rancher supports its certified integrations with various software systems and services for functionality ranging from User Authentication to Infrastructure. What this means is the following: Rancher has validated the integration for one or more of its product versions to work as designed with such systems and services. Rancher Support will help Customer troubleshoot the root cause for any issue related to one of these integrations. For issues root-caused to be in the integration component to one of these systems or services, Rancher will provide a fix to resolve such issues. For issues root-caused to be inside one of these systems or services, Rancher will investigate if the fix is already available in a later version of the system or service. If it is, Rancher will provide a newer version of its own product that is validated and certified to work with the later version of the system that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the system or service, Rancher will advise Customer to directly contact the vendor providing support for the system or service for issue resolution. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Could you clarify what you generally mean by a \"certified integration\" to another software system or service?"},{"location":"000020483/#rancher-could-you-clarify-what-you-generally-mean-by-a-certified-integration-to-another-software-system-or-service","text":"This document (000020483) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Could you clarify what you generally mean by a \"certified integration\" to another software system or service?"},{"location":"000020483/#resolution","text":"Rancher supports its certified integrations with various software systems and services for functionality ranging from User Authentication to Infrastructure. What this means is the following: Rancher has validated the integration for one or more of its product versions to work as designed with such systems and services. Rancher Support will help Customer troubleshoot the root cause for any issue related to one of these integrations. For issues root-caused to be in the integration component to one of these systems or services, Rancher will provide a fix to resolve such issues. For issues root-caused to be inside one of these systems or services, Rancher will investigate if the fix is already available in a later version of the system or service. If it is, Rancher will provide a newer version of its own product that is validated and certified to work with the later version of the system that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the system or service, Rancher will advise Customer to directly contact the vendor providing support for the system or service for issue resolution.","title":"Resolution"},{"location":"000020483/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020484/","text":"[Rancher] What does Rancher support for Docker on Ubuntu cover? This document (000020484) is provided subject to the disclaimer at the end of this document. Situation Note: Only upstream Docker has been validated and certified for Rancher Support. Ubuntu's own distribution of Docker is not covered by the Rancher support matrix . Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Ubuntu, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Ubuntu and Rancher that has been certified for this later version of Docker. If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker. If a code fix is not possible, Rancher will advise Customer to contact Canonical directly for issue resolution. Should a new version or patch be provided by Canonical, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What does Rancher support for Docker on Ubuntu cover?"},{"location":"000020484/#rancher-what-does-rancher-support-for-docker-on-ubuntu-cover","text":"This document (000020484) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What does Rancher support for Docker on Ubuntu cover?"},{"location":"000020484/#situation","text":"Note: Only upstream Docker has been validated and certified for Rancher Support. Ubuntu's own distribution of Docker is not covered by the Rancher support matrix . Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Ubuntu, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Ubuntu and Rancher that has been certified for this later version of Docker. If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker. If a code fix is not possible, Rancher will advise Customer to contact Canonical directly for issue resolution. Should a new version or patch be provided by Canonical, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.","title":"Situation"},{"location":"000020484/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020485/","text":"[Rancher] Will Rancher support us should our deployment be on Red Hat Atomic? This document (000020485) is provided subject to the disclaimer at the end of this document. Resolution Rancher has not validated and certified Red Hat Atomic for inclusion in its Support Matrix as one of the supported operating systems. Following conditions shall apply to Customers procuring Rancher Support for running a Rancher deployment on Red Hat Atomic: Rancher Support is predicated on Customer running a Red Hat distribution of Docker that has been validated and certified for Rancher Support on a comparable, certified version of RHEL OS. Rancher Support will rely on Customer to reproduce the issue on a comparable, certified version of RHEL OS running a fully supported configuration. In the event of Customer encountering an issue whilst using Red Hat Atomic, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the Red Hat Atomic OS will need to be taken up by the Customer with Red Hat. Troubleshooting by Rancher Support shall be limited to running scripts and commands, log analysis, and screen share sessions with the Customer. It shall not extend to a setting up a Rancher deployment on Red Hat Atomic by Rancher Support to reproduce the issue. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher support us should our deployment be on Red Hat Atomic?"},{"location":"000020485/#rancher-will-rancher-support-us-should-our-deployment-be-on-red-hat-atomic","text":"This document (000020485) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher support us should our deployment be on Red Hat Atomic?"},{"location":"000020485/#resolution","text":"Rancher has not validated and certified Red Hat Atomic for inclusion in its Support Matrix as one of the supported operating systems. Following conditions shall apply to Customers procuring Rancher Support for running a Rancher deployment on Red Hat Atomic: Rancher Support is predicated on Customer running a Red Hat distribution of Docker that has been validated and certified for Rancher Support on a comparable, certified version of RHEL OS. Rancher Support will rely on Customer to reproduce the issue on a comparable, certified version of RHEL OS running a fully supported configuration. In the event of Customer encountering an issue whilst using Red Hat Atomic, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the Red Hat Atomic OS will need to be taken up by the Customer with Red Hat. Troubleshooting by Rancher Support shall be limited to running scripts and commands, log analysis, and screen share sessions with the Customer. It shall not extend to a setting up a Rancher deployment on Red Hat Atomic by Rancher Support to reproduce the issue.","title":"Resolution"},{"location":"000020485/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020486/","text":"[Rancher] What does Rancher support for Docker on RHEL cover? This document (000020486) is provided subject to the disclaimer at the end of this document. Resolution Note: Both, RHEL's own distribution of Docker and upstream Docker (Docker CE) on RHEL, have been validated and certified for Rancher Support. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with RHEL's own distribution of Docker, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of RHEL's own distribution of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version RHEL's own distribution of Docker. If no fix is available yet for the issue, Rancher will advise the Customer to contact Red Hat directly for issue resolution. Should a new version or patch be provided by Red Hat, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Where the root cause has been identified as an issue with upstream Docker (or Docker CE) on RHEL, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version of Docker. If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What does Rancher support for Docker on RHEL cover?"},{"location":"000020486/#rancher-what-does-rancher-support-for-docker-on-rhel-cover","text":"This document (000020486) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What does Rancher support for Docker on RHEL cover?"},{"location":"000020486/#resolution","text":"Note: Both, RHEL's own distribution of Docker and upstream Docker (Docker CE) on RHEL, have been validated and certified for Rancher Support. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with RHEL's own distribution of Docker, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of RHEL's own distribution of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version RHEL's own distribution of Docker. If no fix is available yet for the issue, Rancher will advise the Customer to contact Red Hat directly for issue resolution. Should a new version or patch be provided by Red Hat, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Where the root cause has been identified as an issue with upstream Docker (or Docker CE) on RHEL, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version of Docker. If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.","title":"Resolution"},{"location":"000020486/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020487/","text":"[Rancher] What does Rancher support for Docker on CentOS cover? This document (000020487) is provided subject to the disclaimer at the end of this document. Resolution Note: Only upstream Docker has been validated and certified for Rancher Support. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on CentOS, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of CentOS and Rancher that has been certified for this later version of Docker. If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What does Rancher support for Docker on CentOS cover?"},{"location":"000020487/#rancher-what-does-rancher-support-for-docker-on-centos-cover","text":"This document (000020487) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What does Rancher support for Docker on CentOS cover?"},{"location":"000020487/#resolution","text":"Note: Only upstream Docker has been validated and certified for Rancher Support. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on CentOS, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of CentOS and Rancher that has been certified for this later version of Docker. If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.","title":"Resolution"},{"location":"000020487/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020488/","text":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL? This document (000020488) is provided subject to the disclaimer at the end of this document. Resolution No. In this case, Customer needs to contact Red Hat. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL?"},{"location":"000020488/#rancher-will-rancher-fix-the-issue-and-release-a-patch-if-the-problem-is-root-caused-in-docker-on-rhel","text":"This document (000020488) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL?"},{"location":"000020488/#resolution","text":"No. In this case, Customer needs to contact Red Hat.","title":"Resolution"},{"location":"000020488/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020489/","text":"[Rancher] What does Rancher support for Docker on Windows Server cover? This document (000020489) is provided subject to the disclaimer at the end of this document. Resolution Note: Only Docker Enterprise (Docker EE) has been validated and certified for Rancher Support. Support for Windows Server is available only for Rancher v2.3 and higher. Windows clusters are supported for Kubernetes 1.15+ on Windows Server, versions 1809 and 1903. Windows clusters can only be created from new clusters and are supported only with the flannel network provider. You will not need to do any specific scheduling to ensure your Windows workloads are scheduled onto Windows nodes. When creating a Windows cluster, Rancher automatically adds taints to the required Linux nodes to prevent any Windows workloads to be scheduled. If you are trying to schedule Linux workloads into the cluster, you will need to add specific tolerations and node scheduling in order to have them deployed on the Linux nodes. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker Enterprise on Windows Server, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker Enterprise, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Windows Server and Rancher that have been certified for this later version of Docker Enterprise. If no fix is available yet for the issue, Rancher will advise the Customer to contact Microsoft directly for issue resolution. Should a new version or patch be provided by Microsoft Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What does Rancher support for Docker on Windows Server cover?"},{"location":"000020489/#rancher-what-does-rancher-support-for-docker-on-windows-server-cover","text":"This document (000020489) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What does Rancher support for Docker on Windows Server cover?"},{"location":"000020489/#resolution","text":"Note: Only Docker Enterprise (Docker EE) has been validated and certified for Rancher Support. Support for Windows Server is available only for Rancher v2.3 and higher. Windows clusters are supported for Kubernetes 1.15+ on Windows Server, versions 1809 and 1903. Windows clusters can only be created from new clusters and are supported only with the flannel network provider. You will not need to do any specific scheduling to ensure your Windows workloads are scheduled onto Windows nodes. When creating a Windows cluster, Rancher automatically adds taints to the required Linux nodes to prevent any Windows workloads to be scheduled. If you are trying to schedule Linux workloads into the cluster, you will need to add specific tolerations and node scheduling in order to have them deployed on the Linux nodes. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker Enterprise on Windows Server, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker Enterprise, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Windows Server and Rancher that have been certified for this later version of Docker Enterprise. If no fix is available yet for the issue, Rancher will advise the Customer to contact Microsoft directly for issue resolution. Should a new version or patch be provided by Microsoft Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.","title":"Resolution"},{"location":"000020489/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020490/","text":"[Rancher] What does Rancher support for Docker on Oracle Linux cover? This document (000020490) is provided subject to the disclaimer at the end of this document. Resolution Note: Only upstream Docker has been validated and certified for Rancher Support. Support for Oracle Linux is available only from Rancher v2.3.2. Some restrictive firewall rules will need to be turned off for a supported baseline configuration of Oracle Linux. Refer this docs page for more details. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Oracle Linux, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Oracle Linux and Rancher that has been certified for this later version of Docker. If no fix is available yet for the issue, however the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] What does Rancher support for Docker on Oracle Linux cover?"},{"location":"000020490/#rancher-what-does-rancher-support-for-docker-on-oracle-linux-cover","text":"This document (000020490) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] What does Rancher support for Docker on Oracle Linux cover?"},{"location":"000020490/#resolution","text":"Note: Only upstream Docker has been validated and certified for Rancher Support. Support for Oracle Linux is available only from Rancher v2.3.2. Some restrictive firewall rules will need to be turned off for a supported baseline configuration of Oracle Linux. Refer this docs page for more details. Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Oracle Linux, Rancher will do one of the following, to help Customer resolve the issue: Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Oracle Linux and Rancher that has been certified for this later version of Docker. If no fix is available yet for the issue, however the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.","title":"Resolution"},{"location":"000020490/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020491/","text":"[Rancher] My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack? This document (000020491) is provided subject to the disclaimer at the end of this document. Resolution OpenStack is not listed under supported node drivers in the Rancher support matrix . What this means is that currently the OpenStack node driver is not officially supported and because of this, there isn't a supported way to use Rancher as the cluster self-provisioning platform on an OpenStack-based infrastructure. This is not to say that clusters cannot be run in a Rancher-supported way on OpenStack-based infrastructure. To do that, the option of custom clusters will need to be used. In the event of a Customer encountering an issue in such custom clusters, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the OpenStack platform will need to be taken up by the Customer with the maintainer and provider of support for the OpenStack platform. Also, troubleshooting by Rancher Support is currently limited to log analysis and screen share sessions with the Customer and does not extend to an OpenStack setup by Rancher Support to recreate the issue. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack?"},{"location":"000020491/#rancher-my-clusters-run-on-openstack-i-do-not-see-openstack-listed-in-your-support-matrix-does-my-rancher-support-cover-clusters-that-are-run-on-openstack","text":"This document (000020491) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack?"},{"location":"000020491/#resolution","text":"OpenStack is not listed under supported node drivers in the Rancher support matrix . What this means is that currently the OpenStack node driver is not officially supported and because of this, there isn't a supported way to use Rancher as the cluster self-provisioning platform on an OpenStack-based infrastructure. This is not to say that clusters cannot be run in a Rancher-supported way on OpenStack-based infrastructure. To do that, the option of custom clusters will need to be used. In the event of a Customer encountering an issue in such custom clusters, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the OpenStack platform will need to be taken up by the Customer with the maintainer and provider of support for the OpenStack platform. Also, troubleshooting by Rancher Support is currently limited to log analysis and screen share sessions with the Customer and does not extend to an OpenStack setup by Rancher Support to recreate the issue.","title":"Resolution"},{"location":"000020491/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020492/","text":"[Rancher] Does it matter what hardware my hosts are on? Are virtualized servers supported? This document (000020492) is provided subject to the disclaimer at the end of this document. Resolution Rancher Support for Rancher 2.x covers running Rancher and Kubernetes clusters on supported OSes on a 64-bit x86 architecture host. Refer the support matrix page for supported OS by product version. Refer this docs page for installation requirements and this docs page for node requirements for user clusters. These host nodes could be bare-metal or a virtual server running on any Type-1 hypervisor or a cloud server on AWS, Azure, Digital Ocean, Google, and Linode. With the exception of KVM, hosts running on Type-2 hypervisors, such as VirtualBox, VMware Fusion or Parallels, are not in scope of Rancher Support. For use cases, where a Rancher Customer is SLA-bound to their downstream users, Rancher Support does not recommend running clusters and workloads on these hosts. Any assistance provided by Rancher Support in this scenario is not bound by the Rancher Support SLA. It shall be limited to being on a best effort basis only and not include troubleshooting issues related to the setup and configuration of the virtual infrastructure. Refer this page on wikipedia for what is a Type-1 and Type-2 hypervisor. Hosts on an ARM64 architecture are not covered by Rancher Support SLA. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does it matter what hardware my hosts are on? Are virtualized servers supported?"},{"location":"000020492/#rancher-does-it-matter-what-hardware-my-hosts-are-on-are-virtualized-servers-supported","text":"This document (000020492) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does it matter what hardware my hosts are on? Are virtualized servers supported?"},{"location":"000020492/#resolution","text":"Rancher Support for Rancher 2.x covers running Rancher and Kubernetes clusters on supported OSes on a 64-bit x86 architecture host. Refer the support matrix page for supported OS by product version. Refer this docs page for installation requirements and this docs page for node requirements for user clusters. These host nodes could be bare-metal or a virtual server running on any Type-1 hypervisor or a cloud server on AWS, Azure, Digital Ocean, Google, and Linode. With the exception of KVM, hosts running on Type-2 hypervisors, such as VirtualBox, VMware Fusion or Parallels, are not in scope of Rancher Support. For use cases, where a Rancher Customer is SLA-bound to their downstream users, Rancher Support does not recommend running clusters and workloads on these hosts. Any assistance provided by Rancher Support in this scenario is not bound by the Rancher Support SLA. It shall be limited to being on a best effort basis only and not include troubleshooting issues related to the setup and configuration of the virtual infrastructure. Refer this page on wikipedia for what is a Type-1 and Type-2 hypervisor. Hosts on an ARM64 architecture are not covered by Rancher Support SLA.","title":"Resolution"},{"location":"000020492/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020493/","text":"[Rancher] I see node drivers tagged as \"Built-in\". What does that mean? This document (000020493) is provided subject to the disclaimer at the end of this document. Resolution Built-in drivers are those that are included in a Rancher product distribution. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] I see node drivers tagged as \"Built-in\". What does that mean?"},{"location":"000020493/#rancher-i-see-node-drivers-tagged-as-built-in-what-does-that-mean","text":"This document (000020493) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] I see node drivers tagged as \"Built-in\". What does that mean?"},{"location":"000020493/#resolution","text":"Built-in drivers are those that are included in a Rancher product distribution.","title":"Resolution"},{"location":"000020493/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020494/","text":"[Rancher] I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean? This document (000020494) is provided subject to the disclaimer at the end of this document. Resolution Active drivers are those drivers that have been turned on. Only four Built-in node drivers are surfaced up in UI, as Active, in a default Rancher installation. It is only these five Built-in, Active node drivers that are validated and certified in any Rancher 2.x product version and consequently, published in the Rancher support matrix . These five Built-in, Active node drivers that are covered by Rancher Support are for: Digital Ocean AWS Azure vSphere - 6.5, 6.7, 7.0 update 2a Linode (supported only from Rancher v2.3 or higher) Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?"},{"location":"000020494/#rancher-i-see-some-node-drivers-tagged-as-active-when-i-install-rancher-what-does-this-mean","text":"This document (000020494) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?"},{"location":"000020494/#resolution","text":"Active drivers are those drivers that have been turned on. Only four Built-in node drivers are surfaced up in UI, as Active, in a default Rancher installation. It is only these five Built-in, Active node drivers that are validated and certified in any Rancher 2.x product version and consequently, published in the Rancher support matrix . These five Built-in, Active node drivers that are covered by Rancher Support are for: Digital Ocean AWS Azure vSphere - 6.5, 6.7, 7.0 update 2a Linode (supported only from Rancher v2.3 or higher)","title":"Resolution"},{"location":"000020494/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020495/","text":"[Rancher] Does my support subscription to Rancher include support for Longhorn? This document (000020495) is provided subject to the disclaimer at the end of this document. Resolution No, a subscription to Rancher does not include support for Longhorn. With the general availability of Longhorn in June 2020, a separate commercial subscription to an add-on plan is needed to receive SLA-based support for Longhorn. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Does my support subscription to Rancher include support for Longhorn?"},{"location":"000020495/#rancher-does-my-support-subscription-to-rancher-include-support-for-longhorn","text":"This document (000020495) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Does my support subscription to Rancher include support for Longhorn?"},{"location":"000020495/#resolution","text":"No, a subscription to Rancher does not include support for Longhorn. With the general availability of Longhorn in June 2020, a separate commercial subscription to an add-on plan is needed to receive SLA-based support for Longhorn.","title":"Resolution"},{"location":"000020495/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020496/","text":"[Rancher] Can we have a mix of unsupported and supported nodes at our choice/discretion? This document (000020496) is provided subject to the disclaimer at the end of this document. Resolution No. Within a supported Rancher environment, categorizing cluster nodes as \"supported'\" and \"unsupported\" at a team's own choice/discretion is not allowed. Also, see Can we request support for the management/upstream cluster and certain downstream clusters and not others? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Can we have a mix of unsupported and supported nodes at our choice/discretion?"},{"location":"000020496/#rancher-can-we-have-a-mix-of-unsupported-and-supported-nodes-at-our-choicediscretion","text":"This document (000020496) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Can we have a mix of unsupported and supported nodes at our choice/discretion?"},{"location":"000020496/#resolution","text":"No. Within a supported Rancher environment, categorizing cluster nodes as \"supported'\" and \"unsupported\" at a team's own choice/discretion is not allowed. Also, see Can we request support for the management/upstream cluster and certain downstream clusters and not others?","title":"Resolution"},{"location":"000020496/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020497/","text":"[Rancher] Can we request support for the management/upstream cluster and certain downstream clusters and not others? This document (000020497) is provided subject to the disclaimer at the end of this document. Resolution No. Mixing the management of \"supported'\" and \"unsupported\" clusters from a single Rancher server instance in this manner is not a supported configuration, so we would suggest that you run a separate Rancher server instance to manage clusters you do not want to be covered by Rancher Support. The other option you have here would be to take advantage of our blended Platinum and Standard plan, to cover less critical clusters with the Standard plan, and you can reach out to your Account Executive if you wish to look at the options with this. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Can we request support for the management/upstream cluster and certain downstream clusters and not others?"},{"location":"000020497/#rancher-can-we-request-support-for-the-managementupstream-cluster-and-certain-downstream-clusters-and-not-others","text":"This document (000020497) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Can we request support for the management/upstream cluster and certain downstream clusters and not others?"},{"location":"000020497/#resolution","text":"No. Mixing the management of \"supported'\" and \"unsupported\" clusters from a single Rancher server instance in this manner is not a supported configuration, so we would suggest that you run a separate Rancher server instance to manage clusters you do not want to be covered by Rancher Support. The other option you have here would be to take advantage of our blended Platinum and Standard plan, to cover less critical clusters with the Standard plan, and you can reach out to your Account Executive if you wish to look at the options with this.","title":"Resolution"},{"location":"000020497/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020498/","text":"[Rancher] How does Rancher track our license usage? This document (000020498) is provided subject to the disclaimer at the end of this document. Resolution Currently, Rancher relies on customers to report their usage and procure additional licenses if their usage has increased. From time to time, as part of support calls, a Rancher Support Engineer may request customers to run a simple command-line script on the Rancher Server that will generate high-level usage information: wget -O - https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sh - > rancher_stats.txt This is both to keep track of our customer's usage for license compliance as well as to offer advisories should customers be approaching any thresholds related to scale and performance. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] How does Rancher track our license usage?"},{"location":"000020498/#rancher-how-does-rancher-track-our-license-usage","text":"This document (000020498) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] How does Rancher track our license usage?"},{"location":"000020498/#resolution","text":"Currently, Rancher relies on customers to report their usage and procure additional licenses if their usage has increased. From time to time, as part of support calls, a Rancher Support Engineer may request customers to run a simple command-line script on the Rancher Server that will generate high-level usage information: wget -O - https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sh - > rancher_stats.txt This is both to keep track of our customer's usage for license compliance as well as to offer advisories should customers be approaching any thresholds related to scale and performance.","title":"Resolution"},{"location":"000020498/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020499/","text":"[Rancher] Is Rancher Support only for production environments? This document (000020499) is provided subject to the disclaimer at the end of this document. Resolution No. Rancher Support can be procured for any Rancher installation/environment for which the customer wishes to get SLA-based assistance. In addition to production environments, it is very common for orgs to procure support subscriptions to cover other environments, such as their Dev/Test, Staging, Demo, Perf, PreProd, and Production Sandbox, for which they have their own SLAs to meet with their downstream customer users. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Is Rancher Support only for production environments?"},{"location":"000020499/#rancher-is-rancher-support-only-for-production-environments","text":"This document (000020499) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Is Rancher Support only for production environments?"},{"location":"000020499/#resolution","text":"No. Rancher Support can be procured for any Rancher installation/environment for which the customer wishes to get SLA-based assistance. In addition to production environments, it is very common for orgs to procure support subscriptions to cover other environments, such as their Dev/Test, Staging, Demo, Perf, PreProd, and Production Sandbox, for which they have their own SLAs to meet with their downstream customer users.","title":"Resolution"},{"location":"000020499/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020500/","text":"[Rancher] I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well? This document (000020500) is provided subject to the disclaimer at the end of this document. Resolution No. In examples such as this one, the customer is required to procure license(s) for any additional Rancher Management Server installations for which support has been sought. In this example, whilst Rancher Support will help customers only on a best-effort basis, for their issues in Dev/Test and Staging, they will recommend a conversation for the customer with their Rancher Account Executive for procuring additional licenses at the earliest. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well?"},{"location":"000020500/#rancher-i-procured-a-support-subscription-to-1-rancher-management-server-and-a-20-node-starter-pack-for-my-production-environment-does-my-support-subscription-cover-issues-in-my-devtest-and-staging-environments-as-well","text":"This document (000020500) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well?"},{"location":"000020500/#resolution","text":"No. In examples such as this one, the customer is required to procure license(s) for any additional Rancher Management Server installations for which support has been sought. In this example, whilst Rancher Support will help customers only on a best-effort basis, for their issues in Dev/Test and Staging, they will recommend a conversation for the customer with their Rancher Account Executive for procuring additional licenses at the earliest.","title":"Resolution"},{"location":"000020500/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020501/","text":"[Rancher] Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue? This document (000020501) is provided subject to the disclaimer at the end of this document. Resolution We understand that this can happen, as you have your own customer adoption and growth. Yes, for sure, Rancher Support will help you on the specific issue. However, to avoid any future disruption, it will be required that the Rancher Support subscription is upgraded to the necessary quantity, at the earliest and within a reasonable period of time. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue?"},{"location":"000020501/#rancher-our-usage-has-exceeded-the-number-of-nodes-for-which-we-purchased-a-rancher-support-subscription-we-now-have-an-issue-can-we-continue-to-get-assistance-from-rancher-support-to-troubleshoot-and-resolve-the-issue","text":"This document (000020501) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue?"},{"location":"000020501/#resolution","text":"We understand that this can happen, as you have your own customer adoption and growth. Yes, for sure, Rancher Support will help you on the specific issue. However, to avoid any future disruption, it will be required that the Rancher Support subscription is upgraded to the necessary quantity, at the earliest and within a reasonable period of time.","title":"Resolution"},{"location":"000020501/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020502/","text":"[Rancher] Could you illustrate the severity levels with subject lines of sample support cases? This document (000020502) is provided subject to the disclaimer at the end of this document. Resolution Here are some sample subject lines by severity level: Severity Level Subject line of sample support cases Severity 1 Production cluster is down Apps in production cluster throwing '502 Bad Gateway' Error Enabled enhanced cluster monitoring and the cluster sporadically goes offline etcd not healthy in production rancher server Kubernetes ingress endpoint went unresponsive Severity 2 etcd restore failed for RKE-deployed cluster Microservices client encounters UnknownHostException in Production Network traffic routed over IPsec is super slow LDAP is intermittently failing to log in users in one of our environments DNS intermittent timeouts Severity 3 Webhook notifier not working as expected Need assistance with creating custom global roles to prevent cluster creation Problem adding new nodes to clusters Ingress timeout issue in Rancher-deployed cluster Bug: Unable to add AD groups to Rancher ACLs Severity 4 Ability to change UID and GID in docker containers on container creation. Feature Request: Show audit logs content in Rancher UI FYI: Scheduled upgrade XX/XX/XXXX XX:XXPM, request on-call assistance as needed How can I disable the creation of the Prometheus-Operator CRDs on cluster creation Understand the impact of changing IPs of worker nodes in our k8s cluster Additional Information Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Could you illustrate the severity levels with subject lines of sample support cases?"},{"location":"000020502/#rancher-could-you-illustrate-the-severity-levels-with-subject-lines-of-sample-support-cases","text":"This document (000020502) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Could you illustrate the severity levels with subject lines of sample support cases?"},{"location":"000020502/#resolution","text":"Here are some sample subject lines by severity level: Severity Level Subject line of sample support cases Severity 1 Production cluster is down Apps in production cluster throwing '502 Bad Gateway' Error Enabled enhanced cluster monitoring and the cluster sporadically goes offline etcd not healthy in production rancher server Kubernetes ingress endpoint went unresponsive Severity 2 etcd restore failed for RKE-deployed cluster Microservices client encounters UnknownHostException in Production Network traffic routed over IPsec is super slow LDAP is intermittently failing to log in users in one of our environments DNS intermittent timeouts Severity 3 Webhook notifier not working as expected Need assistance with creating custom global roles to prevent cluster creation Problem adding new nodes to clusters Ingress timeout issue in Rancher-deployed cluster Bug: Unable to add AD groups to Rancher ACLs Severity 4 Ability to change UID and GID in docker containers on container creation. Feature Request: Show audit logs content in Rancher UI FYI: Scheduled upgrade XX/XX/XXXX XX:XXPM, request on-call assistance as needed How can I disable the creation of the Prometheus-Operator CRDs on cluster creation Understand the impact of changing IPs of worker nodes in our k8s cluster","title":"Resolution"},{"location":"000020502/#additional-information","text":"Rancher Support FAQs","title":"Additional Information"},{"location":"000020502/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020526/","text":"Security vulnerability: log4j remote code execution aka log4shell CVE-2021-44228 This document (000020526) is provided subject to the disclaimer at the end of this document. Environment All products Situation A 0-day exploit in the log4j Java logging framework was found by Chen Zhaojun of Alibaba Cloud Security Team, which allowed remote attackers able to inject strings into log4j based Java logging to execute code by exploiting the default enabled JNDI bindings. This is possible without any preconditions, making it critical. Resolution SUSE considers log4j versions 2.0 and newer as affected, log4j 1.2.x does not have the same critical vulnerability and is not considered affected by this CVE. SUSE Linux Enterprise products do not ship log4j 2.x. SUSE Manager does not ship log4j 2.x. SUSE Enterprise Storage does not ship log4j 2.x. SUSE Openstack Cloud embeds log4j2 in the \"storm\" component, which will receive updates. SUSE NeuVector product does not ship log4j 2.x. SUSE Rancher is not affected by this vulnerability. The Helm chart for Istio 1.5, provided by Rancher and which is currently deprecated, includes Zipkin and is vulnerable to Log4j. Customers are advised to upgrade to the recent Istio version provided in Cluster Explorer, which does not uses Zipkin and is not affect to the vulnerability. Please refer to the upstream guidance from log4j on fixing and mitigation measures if you deploy your Java Application stacks. Status Security Alert Additional Information Additional information can be found here: https://suse.com/security/cve/CVE-2021-44228.html https://www.suse.com/c/suse-statement-on-log4j-log4shell-cve-2021-44228-vulnerability/ https://logging.apache.org/log4j/2.x/security.html Note in regards to SUSE Manager Server: The CVE-search will use meta-data within a patch to display the needed information. As there is no patch needed (as SUSE is not effected), the CVE-search for CVE-2021-44228 will return a \"not found\". Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Security vulnerability: log4j remote code execution aka log4shell CVE-2021-44228"},{"location":"000020526/#security-vulnerability-log4j-remote-code-execution-aka-log4shell-cve-2021-44228","text":"This document (000020526) is provided subject to the disclaimer at the end of this document.","title":"Security vulnerability: log4j remote code execution aka log4shell CVE-2021-44228"},{"location":"000020526/#environment","text":"All products","title":"Environment"},{"location":"000020526/#situation","text":"A 0-day exploit in the log4j Java logging framework was found by Chen Zhaojun of Alibaba Cloud Security Team, which allowed remote attackers able to inject strings into log4j based Java logging to execute code by exploiting the default enabled JNDI bindings. This is possible without any preconditions, making it critical.","title":"Situation"},{"location":"000020526/#resolution","text":"SUSE considers log4j versions 2.0 and newer as affected, log4j 1.2.x does not have the same critical vulnerability and is not considered affected by this CVE. SUSE Linux Enterprise products do not ship log4j 2.x. SUSE Manager does not ship log4j 2.x. SUSE Enterprise Storage does not ship log4j 2.x. SUSE Openstack Cloud embeds log4j2 in the \"storm\" component, which will receive updates. SUSE NeuVector product does not ship log4j 2.x. SUSE Rancher is not affected by this vulnerability. The Helm chart for Istio 1.5, provided by Rancher and which is currently deprecated, includes Zipkin and is vulnerable to Log4j. Customers are advised to upgrade to the recent Istio version provided in Cluster Explorer, which does not uses Zipkin and is not affect to the vulnerability. Please refer to the upstream guidance from log4j on fixing and mitigation measures if you deploy your Java Application stacks.","title":"Resolution"},{"location":"000020526/#status","text":"Security Alert","title":"Status"},{"location":"000020526/#additional-information","text":"Additional information can be found here: https://suse.com/security/cve/CVE-2021-44228.html https://www.suse.com/c/suse-statement-on-log4j-log4shell-cve-2021-44228-vulnerability/ https://logging.apache.org/log4j/2.x/security.html Note in regards to SUSE Manager Server: The CVE-search will use meta-data within a patch to display the needed information. As there is no patch needed (as SUSE is not effected), the CVE-search for CVE-2021-44228 will return a \"not found\".","title":"Additional Information"},{"location":"000020526/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020535/","text":"Security vulnerability: Trojan Source, invisible source code vulnerabilities. (CVE-2021-42574) This document (000020535) is provided subject to the disclaimer at the end of this document. Environment All products Situation CVE-2021-42574 ('Trojan Source') refers to vulnerabilities that can come about through the use of bi-directional unicode text in contexts where it is not properly displayed. Various source-code viewers and editors currently do not show content which is \"visually hidden by unicode\". These may include editors and pagers such as vi, emacs and less as well as the web interfaces of tools that display source code. The failure to display such things as bidirectional control characters can lead to a situation in which source code when compiled or interpreted behaves in ways that someone seeing the displayed text would not expect. This is not a compiler issue, but future compiler versions will also have options or features to display warnings in cases where such special unicode characters are used. Resolution Even where this does not affect SUSE products directly, SUSE is currently taking action to harden the supply chain for SUSE products in order to detect any such unicode sequences in code that could have harmful effects. Cause Unicode supports both left-to-right and right-to-left languages, and it makes use of invisible codepoints called \"bidirectional override\" to aid writing left-to-right words inside a right-to-left sentence. It is common to find these inside a sentence of another language to embed a word with a different text direction. Researchers discovered that these codepoints could be misused to manipulate how source code is displayed in some editors and code review tools, fooling a reviewer into approving code that behaves in unexpected ways (potentially maliciously). Status Security Alert Additional Information Additional information can be found at: - https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-42574 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Security vulnerability: Trojan Source, invisible source code vulnerabilities. (CVE-2021-42574)"},{"location":"000020535/#security-vulnerability-trojan-source-invisible-source-code-vulnerabilities-cve-2021-42574","text":"This document (000020535) is provided subject to the disclaimer at the end of this document.","title":"Security vulnerability: Trojan Source, invisible source code vulnerabilities. (CVE-2021-42574)"},{"location":"000020535/#environment","text":"All products","title":"Environment"},{"location":"000020535/#situation","text":"CVE-2021-42574 ('Trojan Source') refers to vulnerabilities that can come about through the use of bi-directional unicode text in contexts where it is not properly displayed. Various source-code viewers and editors currently do not show content which is \"visually hidden by unicode\". These may include editors and pagers such as vi, emacs and less as well as the web interfaces of tools that display source code. The failure to display such things as bidirectional control characters can lead to a situation in which source code when compiled or interpreted behaves in ways that someone seeing the displayed text would not expect. This is not a compiler issue, but future compiler versions will also have options or features to display warnings in cases where such special unicode characters are used.","title":"Situation"},{"location":"000020535/#resolution","text":"Even where this does not affect SUSE products directly, SUSE is currently taking action to harden the supply chain for SUSE products in order to detect any such unicode sequences in code that could have harmful effects.","title":"Resolution"},{"location":"000020535/#cause","text":"Unicode supports both left-to-right and right-to-left languages, and it makes use of invisible codepoints called \"bidirectional override\" to aid writing left-to-right words inside a right-to-left sentence. It is common to find these inside a sentence of another language to embed a word with a different text direction. Researchers discovered that these codepoints could be misused to manipulate how source code is displayed in some editors and code review tools, fooling a reviewer into approving code that behaves in unexpected ways (potentially maliciously).","title":"Cause"},{"location":"000020535/#status","text":"Security Alert","title":"Status"},{"location":"000020535/#additional-information","text":"Additional information can be found at: - https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-42574","title":"Additional Information"},{"location":"000020535/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020536/","text":"[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20 This document (000020536) is provided subject to the disclaimer at the end of this document. Resolution Note: This Rancher Labs operational advisory below was originally sent in December 2020. It has been published here to continue SUSE Rancher customer conversations around this topic via support cases and for sharing any relevant updates around it. For an update on this topic, please see [Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24 . Dear Rancher Customer, This is an operational advisory from Rancher Support related to the deprecation of dockershim in Kubernetes v1.20 As announced on the official Kubernetes blog, the dockershim, which enables the use of the Docker Daemon as the container runtime in Kubernetes, will be deprecated with the upcoming Kubernetes v1.20 release. What is dockershim? The dockershim is built into Kubernetes to provide a Container Runtime Interface (CRI) compliant layer between the kubelet and the Docker Daemon. The shim is necessary because Docker Daemon is not CRI-compliant. What does deprecation of dockershim in Kubernetes v1.20 mean? The dockershim will only be deprecated in Kubernetes v1.20, and will not yet be removed from the kubelet. As a result, no immediate action needs to be taken and Kubernetes clusters can continue to operate with the Docker Daemon container runtime in Kubernetes v1.20. The only change at this time will be a deprecation warning printed in the kubelet logs when running on Docker. What are Rancher's plans to ensure on-going container runtime support in future Kubernetes releases? We are working on our roadmap to ensure that all Rancher provisioned clusters will continue to operate on a CRI-compliant runtime. For existing RKE customers, users will continue to get Kubernetes updates until the shim is officially removed. The removal is currently targeted for late 2021 and will be supported with patches during the 12-month upstream maintenance window. Before the end of maintenance, we fully expect an upgrade path from RKE to RKE2. Looking forward, containerd is already the default runtime in both K3s and RKE2, so any removal of dockershim will have zero impact on future releases. As with RKE, organizations currently using K3s with the Docker runtime will continue to get Kubernetes updates until the shim is officially removed. The dockershim deprecation schedule is tracked by the upstream Kubernetes community in Kubernetes Enhancement Proposal (KEP) 1985, Rancher will continue to keep you updated with related news on the roadmap from our product management, as well as information related to migration off of Docker. Thanks, Rancher Support Team Additional Information [Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24 Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20"},{"location":"000020536/#rancher-operational-advisory-20201210-related-to-deprecation-of-dockershim-in-kubernetes-v120","text":"This document (000020536) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20"},{"location":"000020536/#resolution","text":"Note: This Rancher Labs operational advisory below was originally sent in December 2020. It has been published here to continue SUSE Rancher customer conversations around this topic via support cases and for sharing any relevant updates around it. For an update on this topic, please see [Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24 . Dear Rancher Customer, This is an operational advisory from Rancher Support related to the deprecation of dockershim in Kubernetes v1.20 As announced on the official Kubernetes blog, the dockershim, which enables the use of the Docker Daemon as the container runtime in Kubernetes, will be deprecated with the upcoming Kubernetes v1.20 release.","title":"Resolution"},{"location":"000020536/#what-is-dockershim","text":"The dockershim is built into Kubernetes to provide a Container Runtime Interface (CRI) compliant layer between the kubelet and the Docker Daemon. The shim is necessary because Docker Daemon is not CRI-compliant.","title":"What is dockershim?"},{"location":"000020536/#what-does-deprecation-of-dockershim-in-kubernetes-v120-mean","text":"The dockershim will only be deprecated in Kubernetes v1.20, and will not yet be removed from the kubelet. As a result, no immediate action needs to be taken and Kubernetes clusters can continue to operate with the Docker Daemon container runtime in Kubernetes v1.20. The only change at this time will be a deprecation warning printed in the kubelet logs when running on Docker.","title":"What does deprecation of dockershim in Kubernetes v1.20 mean?"},{"location":"000020536/#what-are-ranchers-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","text":"We are working on our roadmap to ensure that all Rancher provisioned clusters will continue to operate on a CRI-compliant runtime. For existing RKE customers, users will continue to get Kubernetes updates until the shim is officially removed. The removal is currently targeted for late 2021 and will be supported with patches during the 12-month upstream maintenance window. Before the end of maintenance, we fully expect an upgrade path from RKE to RKE2. Looking forward, containerd is already the default runtime in both K3s and RKE2, so any removal of dockershim will have zero impact on future releases. As with RKE, organizations currently using K3s with the Docker runtime will continue to get Kubernetes updates until the shim is officially removed. The dockershim deprecation schedule is tracked by the upstream Kubernetes community in Kubernetes Enhancement Proposal (KEP) 1985, Rancher will continue to keep you updated with related news on the roadmap from our product management, as well as information related to migration off of Docker. Thanks, Rancher Support Team","title":"What are Rancher's plans to ensure on-going container runtime support in future Kubernetes releases?"},{"location":"000020536/#additional-information","text":"[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24 Rancher Support FAQs","title":"Additional Information"},{"location":"000020536/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020538/","text":"[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24 This document (000020538) is provided subject to the disclaimer at the end of this document. Resolution Note: This is a follow-up to the Rancher Operational Advisory that was sent on this topic in December 2020 . In the latest announcement from the Kubernetes blog, it has been notified that dockershim removal has been planned in Kubernetes v1.24, slated for release around April 2022. What are SUSE's plans to ensure on-going container runtime support in future Kubernetes releases? Starting with Kubernetes v1.21, RKE added support for CRI plugin cri-dockerd, see here instructions to enable. All RKE clusters will need to leverage this CRI plugin before upgrading to Kubernetes v1.24. Future updates beyond Kubernetes v1.24 RKE will rely on the cri-dockerd shim. For more information on the dockershim removal schedule, you can check the upstream Kubernetes Enhancement Proposal (KEP) 2221 . K3s and RKE2 are not impacted by the removal of dockershim and use the CRI plugin containerd. We expect to deliver a migration path from RKE to RKE2 in the future. SUSE will continue to keep you updated with related news on the roadmap from our product management Additional Information [Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20 Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24"},{"location":"000020538/#rancher-operational-advisory-20220113-related-to-removal-of-dockershim-in-kubernetes-v124","text":"This document (000020538) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24"},{"location":"000020538/#resolution","text":"Note: This is a follow-up to the Rancher Operational Advisory that was sent on this topic in December 2020 . In the latest announcement from the Kubernetes blog, it has been notified that dockershim removal has been planned in Kubernetes v1.24, slated for release around April 2022.","title":"Resolution"},{"location":"000020538/#what-are-suses-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","text":"Starting with Kubernetes v1.21, RKE added support for CRI plugin cri-dockerd, see here instructions to enable. All RKE clusters will need to leverage this CRI plugin before upgrading to Kubernetes v1.24. Future updates beyond Kubernetes v1.24 RKE will rely on the cri-dockerd shim. For more information on the dockershim removal schedule, you can check the upstream Kubernetes Enhancement Proposal (KEP) 2221 . K3s and RKE2 are not impacted by the removal of dockershim and use the CRI plugin containerd. We expect to deliver a migration path from RKE to RKE2 in the future. SUSE will continue to keep you updated with related news on the roadmap from our product management","title":"What are SUSE's plans to ensure on-going container runtime support in future Kubernetes releases?"},{"location":"000020538/#additional-information","text":"[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20 Rancher Support FAQs","title":"Additional Information"},{"location":"000020538/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020543/","text":"[Rancher] Support Advisories This document (000020543) is provided subject to the disclaimer at the end of this document. Resolution [Rancher] Operational Advisory, 20220405: Rancher Kubernetes Distributions and Etcd 3.5 Updates [Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5 [Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24 [Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20 [Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits Additional Information Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Support Advisories"},{"location":"000020543/#rancher-support-advisories","text":"This document (000020543) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Support Advisories"},{"location":"000020543/#resolution","text":"[Rancher] Operational Advisory, 20220405: Rancher Kubernetes Distributions and Etcd 3.5 Updates [Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5 [Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24 [Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20 [Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits","title":"Resolution"},{"location":"000020543/#additional-information","text":"Rancher Support FAQs","title":"Additional Information"},{"location":"000020543/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020556/","text":"SUSE Rancher Hosted FAQ This document (000020556) is provided subject to the disclaimer at the end of this document. Resolution General What is SUSE Rancher Hosted? What do I need to provide to get started on SUSE Rancher Hosted? Do you have a whitepaper available for SUSE Rancher Hosted? Can I move from self-managed Rancher to SUSE Rancher Hosted? What if I no longer want SUSE Rancher Hosted to manage my downstream clusters? Is it possible to have alpha, beta, or release candidate (RC) versions on SUSE Rancher Hosted? Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers? Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage? Do I have access to the SUSE Rancher Hosted \"local\" cluster in the management UI? Where is SUSE Rancher Hosted hosted? Can I have more than one SUSE Rancher Hosted environment? What type of cluster is SUSE Rancher Hosted running on? Does SUSE Rancher Hosted provide downstream clusters? Can I move an existing cluster to SUSE Rancher Hosted? How is SUSE Rancher Hosted different than the open-source Rancher I can download for free? Maintenance & Operations Who creates user accounts in SUSE Rancher Hosted? How is my SUSE Rancher Hosted environment monitored? How often is maintenance performed on SUSE Rancher Hosted? Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted? Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters? Does SUSE Rancher Hosted offer a support SLA? How long are SUSE Rancher Hosted backups retained? Upgrades & Uptime What can be expected during a SUSE Rancher Hosted upgrade? How often is SUSE Rancher Hosted upgraded? How is uptime measured for my SUSE Rancher Hosted? Does SUSE Rancher Hosted offer an uptime SLA? Network, Security, & Logging What are the networking requirements for using SUSE Rancher Hosted? Can I use my own SSL/TLS certificates with SUSE Rancher Hosted? How to connect your SUSE Rancher Hosted network to your AWS transit gateway? How to make a VPN connection to your SUSE Rancher Hosted network? Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP-based directory service? Does SUSE Rancher Hosted support multi-factor authentication (MFA)? Are API audit logs enabled in SUSE Rancher Hosted? Can I get a copy of the Rancher API audit logs? What information is stored in SUSE Rancher Hosted and where is it stored? Do SUSE employees have a login account for my SUSE Rancher Hosted environment? Do SUSE employees have the credentials to my \u201cadmin\u201d account? Is SUSE Rancher Hosted data encrypted at rest? Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"SUSE Rancher Hosted FAQ"},{"location":"000020556/#suse-rancher-hosted-faq","text":"This document (000020556) is provided subject to the disclaimer at the end of this document.","title":"SUSE Rancher Hosted FAQ"},{"location":"000020556/#resolution","text":"","title":"Resolution"},{"location":"000020556/#general","text":"What is SUSE Rancher Hosted? What do I need to provide to get started on SUSE Rancher Hosted? Do you have a whitepaper available for SUSE Rancher Hosted? Can I move from self-managed Rancher to SUSE Rancher Hosted? What if I no longer want SUSE Rancher Hosted to manage my downstream clusters? Is it possible to have alpha, beta, or release candidate (RC) versions on SUSE Rancher Hosted? Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers? Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage? Do I have access to the SUSE Rancher Hosted \"local\" cluster in the management UI? Where is SUSE Rancher Hosted hosted? Can I have more than one SUSE Rancher Hosted environment? What type of cluster is SUSE Rancher Hosted running on? Does SUSE Rancher Hosted provide downstream clusters? Can I move an existing cluster to SUSE Rancher Hosted? How is SUSE Rancher Hosted different than the open-source Rancher I can download for free?","title":"General"},{"location":"000020556/#maintenance-operations","text":"Who creates user accounts in SUSE Rancher Hosted? How is my SUSE Rancher Hosted environment monitored? How often is maintenance performed on SUSE Rancher Hosted? Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted? Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters? Does SUSE Rancher Hosted offer a support SLA? How long are SUSE Rancher Hosted backups retained?","title":"Maintenance &amp; Operations"},{"location":"000020556/#upgrades-uptime","text":"What can be expected during a SUSE Rancher Hosted upgrade? How often is SUSE Rancher Hosted upgraded? How is uptime measured for my SUSE Rancher Hosted? Does SUSE Rancher Hosted offer an uptime SLA?","title":"Upgrades &amp; Uptime"},{"location":"000020556/#network-security-logging","text":"What are the networking requirements for using SUSE Rancher Hosted? Can I use my own SSL/TLS certificates with SUSE Rancher Hosted? How to connect your SUSE Rancher Hosted network to your AWS transit gateway? How to make a VPN connection to your SUSE Rancher Hosted network? Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP-based directory service? Does SUSE Rancher Hosted support multi-factor authentication (MFA)? Are API audit logs enabled in SUSE Rancher Hosted? Can I get a copy of the Rancher API audit logs? What information is stored in SUSE Rancher Hosted and where is it stored? Do SUSE employees have a login account for my SUSE Rancher Hosted environment? Do SUSE employees have the credentials to my \u201cadmin\u201d account? Is SUSE Rancher Hosted data encrypted at rest?","title":"Network, Security, &amp; Logging"},{"location":"000020556/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020557/","text":"Do you have a whitepaper available for SUSE Rancher Hosted? This document (000020557) is provided subject to the disclaimer at the end of this document. Resolution Yes, there is a SUSE Rancher Hosted architecture whitepaper that can be downloaded on SUSE's website. It can be found here - https://more.suse.com/fy21-global-web-landing-page-hosted-rancher-technical-guide Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Do you have a whitepaper available for SUSE Rancher Hosted?"},{"location":"000020557/#do-you-have-a-whitepaper-available-for-suse-rancher-hosted","text":"This document (000020557) is provided subject to the disclaimer at the end of this document.","title":"Do you have a whitepaper available for SUSE Rancher Hosted?"},{"location":"000020557/#resolution","text":"Yes, there is a SUSE Rancher Hosted architecture whitepaper that can be downloaded on SUSE's website. It can be found here - https://more.suse.com/fy21-global-web-landing-page-hosted-rancher-technical-guide","title":"Resolution"},{"location":"000020557/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020558/","text":"What can be expected during a SUSE Rancher Hosted upgrade? This document (000020558) is provided subject to the disclaimer at the end of this document. Resolution There is minimal impact or disruption during a SUSE Rancher Hosted upgrade. During the one-hour maintenance window you can expect a brief (usually 1-2 minutes) when your Rancher control plane UI/API is inaccessible. This does not impact the workloads on your managed downstream clusters, only your ability to make changes to these clusters. Immediately following the Rancher control plane upgrade, the cluster and node agents running in your managed downstream clusters will be upgraded and restarted. This also only takes a few minutes (unless you have very large clusters or very poor network speeds) and during this time Rancher will be unable to manage the cluster, but again the workloads running on the clusters should operate normally. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What can be expected during a SUSE Rancher Hosted upgrade?"},{"location":"000020558/#what-can-be-expected-during-a-suse-rancher-hosted-upgrade","text":"This document (000020558) is provided subject to the disclaimer at the end of this document.","title":"What can be expected during a SUSE Rancher Hosted upgrade?"},{"location":"000020558/#resolution","text":"There is minimal impact or disruption during a SUSE Rancher Hosted upgrade. During the one-hour maintenance window you can expect a brief (usually 1-2 minutes) when your Rancher control plane UI/API is inaccessible. This does not impact the workloads on your managed downstream clusters, only your ability to make changes to these clusters. Immediately following the Rancher control plane upgrade, the cluster and node agents running in your managed downstream clusters will be upgraded and restarted. This also only takes a few minutes (unless you have very large clusters or very poor network speeds) and during this time Rancher will be unable to manage the cluster, but again the workloads running on the clusters should operate normally.","title":"Resolution"},{"location":"000020558/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020559/","text":"What if I no longer want SUSE Rancher Hosted to manage my downstream clusters? This document (000020559) is provided subject to the disclaimer at the end of this document. Resolution If you decide you no longer want to continue using the SUSE Rancher Hosted service, we can provide a one-time backup of your Rancher control plane which you can use to restore into a self-managed Rancher management server. Information on the restore process can be found in our documentation . To request a backup file, you can submit a support case on our support portal . After restoring Rancher, you will need to reconfigure your downstream clusters to point to the new server URL of your self-managed Rancher management server. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"What if I no longer want SUSE Rancher Hosted to manage my downstream clusters?"},{"location":"000020559/#what-if-i-no-longer-want-suse-rancher-hosted-to-manage-my-downstream-clusters","text":"This document (000020559) is provided subject to the disclaimer at the end of this document.","title":"What if I no longer want SUSE Rancher Hosted to manage my downstream clusters?"},{"location":"000020559/#resolution","text":"If you decide you no longer want to continue using the SUSE Rancher Hosted service, we can provide a one-time backup of your Rancher control plane which you can use to restore into a self-managed Rancher management server. Information on the restore process can be found in our documentation . To request a backup file, you can submit a support case on our support portal . After restoring Rancher, you will need to reconfigure your downstream clusters to point to the new server URL of your self-managed Rancher management server.","title":"Resolution"},{"location":"000020559/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020561/","text":"How to connect your SUSE Rancher Hosted network to your AWS transit gateway? This document (000020561) is provided subject to the disclaimer at the end of this document. Resolution The following steps can be taken to connect your SUSE Rancher Hosted network to an AWS transit gateway running in your AWS account. Make sure you have provided the SUSE Rancher Hosted team with a CIDR that does not overlap with your existing infrastructure. If not, your SUSE Rancher Hosted environment may need to be redeployed with the new CIDR. The CIDR must be a /25 block or larger. Using a /24 is normally preferred. If you haven't already, create a transit gateway in your AWS account. See Create a transit gateway . In the AWS console, go to Resource Access Manager (RAM) service. In RAM, click the orange button in the top right corner labeled \"Create a resource share\". For the name, use something descriptive that includes both your company name and \"SUSE Rancher Hosted\". For example, \"Widget Corp transit gateway for SUSE Rancher Hosted\". For resource type, select Transit Gateways. Select the transit gateway you want to share. In Principals, check Allow external accounts and enter the AWS account number provided by the SUSE Rancher Hosted team. Click the orange \"Create resource share\" in the bottom right corner. Let the SUSE Rancher Hosted team know you have created the share. We will accept the share and make a request to attach the transit gateway to your SUSE Rancher Hosted VPC. Accept the request to attach your transit gateway to the SUSE Rancher Hosted VPC. To do this, go to the VPC service, click \"Transit Gateway Attachments\" in the navigation pane, select the transit gateway attachment, choose Actions -> Accept. Provide the SUSE Rancher Hosted team with a list of CIDRs you want to be routed through the transit gateway. See also Transit gateways and Transit gateway sharing considerations for more information. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to connect your SUSE Rancher Hosted network to your AWS transit gateway?"},{"location":"000020561/#how-to-connect-your-suse-rancher-hosted-network-to-your-aws-transit-gateway","text":"This document (000020561) is provided subject to the disclaimer at the end of this document.","title":"How to connect your SUSE Rancher Hosted network to your AWS transit gateway?"},{"location":"000020561/#resolution","text":"The following steps can be taken to connect your SUSE Rancher Hosted network to an AWS transit gateway running in your AWS account. Make sure you have provided the SUSE Rancher Hosted team with a CIDR that does not overlap with your existing infrastructure. If not, your SUSE Rancher Hosted environment may need to be redeployed with the new CIDR. The CIDR must be a /25 block or larger. Using a /24 is normally preferred. If you haven't already, create a transit gateway in your AWS account. See Create a transit gateway . In the AWS console, go to Resource Access Manager (RAM) service. In RAM, click the orange button in the top right corner labeled \"Create a resource share\". For the name, use something descriptive that includes both your company name and \"SUSE Rancher Hosted\". For example, \"Widget Corp transit gateway for SUSE Rancher Hosted\". For resource type, select Transit Gateways. Select the transit gateway you want to share. In Principals, check Allow external accounts and enter the AWS account number provided by the SUSE Rancher Hosted team. Click the orange \"Create resource share\" in the bottom right corner. Let the SUSE Rancher Hosted team know you have created the share. We will accept the share and make a request to attach the transit gateway to your SUSE Rancher Hosted VPC. Accept the request to attach your transit gateway to the SUSE Rancher Hosted VPC. To do this, go to the VPC service, click \"Transit Gateway Attachments\" in the navigation pane, select the transit gateway attachment, choose Actions -> Accept. Provide the SUSE Rancher Hosted team with a list of CIDRs you want to be routed through the transit gateway. See also Transit gateways and Transit gateway sharing considerations for more information.","title":"Resolution"},{"location":"000020561/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020562/","text":"How to make a VPN connection to your SUSE Rancher Hosted network? This document (000020562) is provided subject to the disclaimer at the end of this document. Resolution It's normally preferred to connect SUSE Rancher Hosted with your network using VPC peering through an AWS transit gateway. This is the most cost-effective, secure, and manageable solution. However, if this is not an option, a VPN connection can be established between your corporate network and SUSE Rancher Hosted through an IPSec VPN tunnel. The following steps are required to set this up: Provide the SUSE Rancher Hosted team with the following information about your VPN device: Public IP address for your VPN endpoint Routing option: a) static (no BGP support) or b) dynamic (BGP support) BGP ASN (only if dynamic routing) VPN device make and model used on-premise that we'll be connecting to. The SUSE Rancher Hosted team will configure the VPN connection and provide configuration information based on the VPN device Customer will configure their VPN device to connect to SUSE Rancher Hosted's network. See also AWS Site-to-Site VPN User Guide . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to make a VPN connection to your SUSE Rancher Hosted network?"},{"location":"000020562/#how-to-make-a-vpn-connection-to-your-suse-rancher-hosted-network","text":"This document (000020562) is provided subject to the disclaimer at the end of this document.","title":"How to make a VPN connection to your SUSE Rancher Hosted network?"},{"location":"000020562/#resolution","text":"It's normally preferred to connect SUSE Rancher Hosted with your network using VPC peering through an AWS transit gateway. This is the most cost-effective, secure, and manageable solution. However, if this is not an option, a VPN connection can be established between your corporate network and SUSE Rancher Hosted through an IPSec VPN tunnel. The following steps are required to set this up: Provide the SUSE Rancher Hosted team with the following information about your VPN device: Public IP address for your VPN endpoint Routing option: a) static (no BGP support) or b) dynamic (BGP support) BGP ASN (only if dynamic routing) VPN device make and model used on-premise that we'll be connecting to. The SUSE Rancher Hosted team will configure the VPN connection and provide configuration information based on the VPN device Customer will configure their VPN device to connect to SUSE Rancher Hosted's network. See also AWS Site-to-Site VPN User Guide .","title":"Resolution"},{"location":"000020562/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020563/","text":"Can I get a copy of the Rancher API audit logs? This document (000020563) is provided subject to the disclaimer at the end of this document. Resolution Yes, if you provide a log flow configuration to your logging solution, such as AWS CloudWatch, Elasticsearch, Splunk, etc. we can stream Rancher API audit logs to you. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Can I get a copy of the Rancher API audit logs?"},{"location":"000020563/#can-i-get-a-copy-of-the-rancher-api-audit-logs","text":"This document (000020563) is provided subject to the disclaimer at the end of this document.","title":"Can I get a copy of the Rancher API audit logs?"},{"location":"000020563/#resolution","text":"Yes, if you provide a log flow configuration to your logging solution, such as AWS CloudWatch, Elasticsearch, Splunk, etc. we can stream Rancher API audit logs to you.","title":"Resolution"},{"location":"000020563/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020592/","text":"Rancher on Windows RKE 1 to RKE 2 This document (000020592) is provided subject to the disclaimer at the end of this document. Environment Any customer running Rancher on Windows on RKE1 is impacted by this change. Rancher has been aware of a shifting trend in the cloud native ecosystem to move toward container runtimes with a smaller surface area than Docker Situation Rancher foresaw this trend and has built two Kubernetes distributions on the open source containerd container runtime, K3s and RKE 2. RKE 2 is built on K3s and is fully conformant Kubernetes distribution that focuses on security and compliance. The future of Windows containers on Rancher is only found on RKE 2 Resolution Customers currently running on RKE 1 will need to move to RKE 2 Because of the change in approach to provisioning Windows-specific clusters in RKE 1 to free-form mixed-OS clusters RKE 2, there is no direct migration path for Windows containers on RKE 1 to RKE 2. Rancher Labs recommends as part of customers testing workloads on the Windows containers on RKE 2 technical preview that they begin planning to refactor their Windows workloads on RKE2 using Fleet . Fleet is a GitOps solution from Rancher, now integrated directly into Rancher, with support for Windows containers. Windows containers on RKE 2 remains in technical preview as of Rancher 2.6.3, the current release of Rancher. Rancher 2.6.4 will be released in March bringing the Windows container experience on RKE 2 to general availability (GA). Windows containers on RKE 2 will match and then exceed the Windows containers on RKE 1 feature set, reaching even greater feature parity between Windows and Linux containers on Rancher. The Windows on Rancher team develops in the open , with full transparency into their development processes. Rancher Labs anticipates Windows containers on RKE 2 reaching general availability (GA) with official support alongside the Rancher Cluster Provisioning v2 in early March 2022. Additional Information Additional guidance and a guide for transitioning the most common workloads from RKE 1 to RKE 2 will be forthcoming. Customers seeking additional assistance in migrating between RKE 1 and RKE 2 should consult with SUSE Global Services . Customers with additional questions or concerns regarding this transition should contact the Windows on Rancher team: By filing issues in the Windows on Rancher GitHub By joining the Rancher Users Slack and posting in #windows Reaching out to their Customer Success Manager See our blog post here https://community.suse.com/posts/the-future-of-windows-containers-on-rancher?agree=true Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher on Windows RKE 1 to RKE 2"},{"location":"000020592/#rancher-on-windows-rke-1-to-rke-2","text":"This document (000020592) is provided subject to the disclaimer at the end of this document.","title":"Rancher on Windows RKE 1 to RKE 2"},{"location":"000020592/#environment","text":"Any customer running Rancher on Windows on RKE1 is impacted by this change. Rancher has been aware of a shifting trend in the cloud native ecosystem to move toward container runtimes with a smaller surface area than Docker","title":"Environment"},{"location":"000020592/#situation","text":"Rancher foresaw this trend and has built two Kubernetes distributions on the open source containerd container runtime, K3s and RKE 2. RKE 2 is built on K3s and is fully conformant Kubernetes distribution that focuses on security and compliance. The future of Windows containers on Rancher is only found on RKE 2","title":"Situation"},{"location":"000020592/#resolution","text":"Customers currently running on RKE 1 will need to move to RKE 2 Because of the change in approach to provisioning Windows-specific clusters in RKE 1 to free-form mixed-OS clusters RKE 2, there is no direct migration path for Windows containers on RKE 1 to RKE 2. Rancher Labs recommends as part of customers testing workloads on the Windows containers on RKE 2 technical preview that they begin planning to refactor their Windows workloads on RKE2 using Fleet . Fleet is a GitOps solution from Rancher, now integrated directly into Rancher, with support for Windows containers. Windows containers on RKE 2 remains in technical preview as of Rancher 2.6.3, the current release of Rancher. Rancher 2.6.4 will be released in March bringing the Windows container experience on RKE 2 to general availability (GA). Windows containers on RKE 2 will match and then exceed the Windows containers on RKE 1 feature set, reaching even greater feature parity between Windows and Linux containers on Rancher. The Windows on Rancher team develops in the open , with full transparency into their development processes. Rancher Labs anticipates Windows containers on RKE 2 reaching general availability (GA) with official support alongside the Rancher Cluster Provisioning v2 in early March 2022.","title":"Resolution"},{"location":"000020592/#additional-information","text":"Additional guidance and a guide for transitioning the most common workloads from RKE 1 to RKE 2 will be forthcoming. Customers seeking additional assistance in migrating between RKE 1 and RKE 2 should consult with SUSE Global Services . Customers with additional questions or concerns regarding this transition should contact the Windows on Rancher team: By filing issues in the Windows on Rancher GitHub By joining the Rancher Users Slack and posting in #windows Reaching out to their Customer Success Manager See our blog post here https://community.suse.com/posts/the-future-of-windows-containers-on-rancher?agree=true","title":"Additional Information"},{"location":"000020592/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020630/","text":"[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits This document (000020630) is provided subject to the disclaimer at the end of this document. Resolution Note : Below is the Rancher Customer email advisory sent in Nov 2020 on the topic of Docker Hub rate limits. Dear Rancher Customer, As announced by Docker Inc, the rate-limiting by Docker Hub is expected to progressively take effect beginning Nov 2. We have been engaged in direct conversations with many of you on the possible impact of this rate-limiting and steps toward managing that. This operational advisory is a summary of those conversations. Do the rate limits apply to Rancher images that we pull anonymously? No. To ensure customers pulling Rancher resources are not affected by this, Rancher Labs has partnered with Docker, Inc. so that pulls from the Rancher namespace on Docker Hub are exempt from these limits. If you run into any rate-limiting issues with images hosted in the Rancher namespace, please let us know. What about images that are outside the Rancher namespace that we pull anonymously? Yes. Rate limits do apply to the images that are outside of the Rancher namespace. What can we do about the limits on images outside the Rancher namespace? We can introduce you to the right contact at Docker Inc should you be interested in procuring an exemption for your org based on something like an IP range. This is to derisk being limited on image pulls outside of the Rancher namespace. What other practical options can we pursue? Other options to mitigate this issue are: Moving from anonymous pulls to authenticated pulls on Docker Hub Copying resources to a private registry The viability of these options depends on the specific context of your environment. Are there any plans to host Rancher images elsewhere to help alleviate the potential issues caused by this? We are looking into alternates to Docker Hub. There have been recent offerings announced by AWS and GitHub . We are exploring them as well as other options and should have an update on this from our product management in the near future. Thanks, Rancher Support Team Additional Information Rancher Support Advisories Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Operational Advisory, 20201105: Related to Docker Hub rate limits"},{"location":"000020630/#rancher-operational-advisory-20201105-related-to-docker-hub-rate-limits","text":"This document (000020630) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits"},{"location":"000020630/#resolution","text":"Note : Below is the Rancher Customer email advisory sent in Nov 2020 on the topic of Docker Hub rate limits. Dear Rancher Customer, As announced by Docker Inc, the rate-limiting by Docker Hub is expected to progressively take effect beginning Nov 2. We have been engaged in direct conversations with many of you on the possible impact of this rate-limiting and steps toward managing that. This operational advisory is a summary of those conversations.","title":"Resolution"},{"location":"000020630/#do-the-rate-limits-apply-to-rancher-images-that-we-pull-anonymously","text":"No. To ensure customers pulling Rancher resources are not affected by this, Rancher Labs has partnered with Docker, Inc. so that pulls from the Rancher namespace on Docker Hub are exempt from these limits. If you run into any rate-limiting issues with images hosted in the Rancher namespace, please let us know.","title":"Do the rate limits apply to Rancher images that we pull anonymously?"},{"location":"000020630/#what-about-images-that-are-outside-the-rancher-namespace-that-we-pull-anonymously","text":"Yes. Rate limits do apply to the images that are outside of the Rancher namespace.","title":"What about images that are outside the Rancher namespace that we pull anonymously?"},{"location":"000020630/#what-can-we-do-about-the-limits-on-images-outside-the-rancher-namespace","text":"We can introduce you to the right contact at Docker Inc should you be interested in procuring an exemption for your org based on something like an IP range. This is to derisk being limited on image pulls outside of the Rancher namespace.","title":"What can we do about the limits on images outside the Rancher namespace?"},{"location":"000020630/#what-other-practical-options-can-we-pursue","text":"Other options to mitigate this issue are: Moving from anonymous pulls to authenticated pulls on Docker Hub Copying resources to a private registry The viability of these options depends on the specific context of your environment.","title":"What other practical options can we pursue?"},{"location":"000020630/#are-there-any-plans-to-host-rancher-images-elsewhere-to-help-alleviate-the-potential-issues-caused-by-this","text":"We are looking into alternates to Docker Hub. There have been recent offerings announced by AWS and GitHub . We are exploring them as well as other options and should have an update on this from our product management in the near future. Thanks, Rancher Support Team","title":"Are there any plans to host Rancher images elsewhere to help alleviate the potential issues caused by this?"},{"location":"000020630/#additional-information","text":"Rancher Support Advisories Rancher Support FAQs","title":"Additional Information"},{"location":"000020630/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020631/","text":"[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5 This document (000020631) is provided subject to the disclaimer at the end of this document. Resolution Note: This Rancher Customer advisory below was originally sent by email to subscribed customer users on March 30, 2022. Dear SUSE Rancher user, We have been sharing important SUSE Rancher product lifecycle dates on our Product Support Lifecycle page. To help you plan for any necessary upgrades for your deployments, we would like to bring to your attention information on some Rancher Manager product versions that are approaching (or have reached) their End of Maintenance (EOM) and End of Life (EOL) milestones. EOM Dates Rancher Version EOM Date v2.4.x July 30, 2021 v2.5.x January 5, 2022 EOL Dates Rancher Version EOL Date v2.4.x March 31, 2022 v2.5.x October 5, 2022 What does the above mean? After a product release reaches its EOM date, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner until it reaches EOL, in the form of: General troubleshooting of a specific issue to isolate potential causes Upgrade recommendation to an existing newer version of product Issue resolution limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product Once a product release reaches its EOL date, a Rancher user may continue to use the product within the terms of the product licensing agreement. However, Support Plan SLAs from SUSE Rancher do not apply to product versions that are past their EOL dates. Please review in detail the following resources to understand changes and prepare for your upgrade. Rancher Support Matrix Rancher 2.5.0 release notes Rancher 2.6.0 release notes Rancher Support Upgrade checklist Rancher Support FAQs In addition, please note that with the upcoming release of Kubernetes 1.24, scheduled for April 19th, dockershim removal from Kubernetes will be final. Please see the Kubernetes blog and our Rancher Support Advisory for more information. If you have any questions on this advisory or would like assistance validating your upgrade path, simply contact your Customer Success Manager or open a new support case via the SCC portal referencing this advisory. Thanks, SUSE Rancher Support Team Additional Information Rancher Support Advisories Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5"},{"location":"000020631/#rancher-product-lifecycle-advisory-20220330-2022-eomeol-dates-for-rancher-24-and-25","text":"This document (000020631) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5"},{"location":"000020631/#resolution","text":"Note: This Rancher Customer advisory below was originally sent by email to subscribed customer users on March 30, 2022. Dear SUSE Rancher user, We have been sharing important SUSE Rancher product lifecycle dates on our Product Support Lifecycle page. To help you plan for any necessary upgrades for your deployments, we would like to bring to your attention information on some Rancher Manager product versions that are approaching (or have reached) their End of Maintenance (EOM) and End of Life (EOL) milestones. EOM Dates Rancher Version EOM Date v2.4.x July 30, 2021 v2.5.x January 5, 2022 EOL Dates Rancher Version EOL Date v2.4.x March 31, 2022 v2.5.x October 5, 2022","title":"Resolution"},{"location":"000020631/#what-does-the-above-mean","text":"After a product release reaches its EOM date, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner until it reaches EOL, in the form of: General troubleshooting of a specific issue to isolate potential causes Upgrade recommendation to an existing newer version of product Issue resolution limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product Once a product release reaches its EOL date, a Rancher user may continue to use the product within the terms of the product licensing agreement. However, Support Plan SLAs from SUSE Rancher do not apply to product versions that are past their EOL dates. Please review in detail the following resources to understand changes and prepare for your upgrade. Rancher Support Matrix Rancher 2.5.0 release notes Rancher 2.6.0 release notes Rancher Support Upgrade checklist Rancher Support FAQs In addition, please note that with the upcoming release of Kubernetes 1.24, scheduled for April 19th, dockershim removal from Kubernetes will be final. Please see the Kubernetes blog and our Rancher Support Advisory for more information. If you have any questions on this advisory or would like assistance validating your upgrade path, simply contact your Customer Success Manager or open a new support case via the SCC portal referencing this advisory. Thanks, SUSE Rancher Support Team","title":"What does the above mean?"},{"location":"000020631/#additional-information","text":"Rancher Support Advisories Rancher Support FAQs","title":"Additional Information"},{"location":"000020631/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020632/","text":"[Rancher] Operational Advisory, 20220405: Rancher Kubernetes Distributions and Etcd 3.5 Updates This document (000020632) is provided subject to the disclaimer at the end of this document. Environment The etcd maintainers have recommended against the use of etcd 3.5.0-3.5.2 for new production workloads, due to a recently discovered bug that may cause data loss when etcd is killed under high load. Their published advisory2 provides recommendations of how to avoid triggering the issue, but as of today, there is no official fix/resolution to the existing issue4 . Situation Who does this affect? Users running Rancher 2.6.4+ who have deployed Rancher on a single node as a Docker installation . Reminder : This installation method is not recommended for any production environment and is only recommended for development/sandbox testing. If you are running Rancher on a managed Kubernetes cluster, then you will have to refer to your Kubernetes service provider to determine if you are affected by this advisory. Users running Kubernetes 1.22+ and 1.23+ of any of the Rancher Kubernetes Distributions (RKE, RKE2, K3s) and using etcd as your datastore. The default datastore for RKE and RKE2 is etcd. This applies to standalone Kubernetes clusters as well as any downstream clusters provisioned by Rancher. Resolution What should you do? Stop deploying into production any new Kubernetes clusters using Rancher Kubernetes distributions versions 1.22/1.23 until a proper fix is provided by the etcd maintainers and included into the affected distribution. Update your etcd configuration to enable the experimental-initial-corrupt-checkoption. This flag will be turned on by default in etcd v3.6, but does not by itself fix the problem; it can only detect and repair the issue if it does occur. Note : each distribution has its own recommendation on how to enable this option; see below for more details. - Avoid terminating etcd unexpectedly (using kill \u20139, etc) - For RKE1 clusters, avoid stopping/killing the etcd containers adhoc without properly cordoning/draining nodes and taking backups - For RKE2 clusters, - Avoid sending SIGKILL to the etcd or rke2 process. - Avoid using the killall script (rke2-killall.sh) to stop RKE2 on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services. - For k3s clusters, - Avoid sending SIGKILL to the k3s process. - Avoid using the killall script (k3s-killall.sh) to stop K3s on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services. - Ensure nodes are not under significant memory pressure that may cause the Linux kernel to terminate the etcd process. Ensure that nodes are not terminated unexpectedly. Avoid force-terminating VMs, unexpected power loss, etc. How do I enable the recommended flag in etccd? For Users provisioning RKE/k3s/RKE2 clusters through Rancher Provisioned RKE Clusters If you are running 1.22 or 1.23, upgrade to the following respective versions to enable the recommended experimental-initial-corrupt-check flag in etcd. RKE 1.22 - v1.22.7-rancher1-2 RKE 1.23 (Experimental) - v1.23.4-rancher1-2 Provisioned K3s/RKE2 Clusters (Tech Preview) Provisioned k3s/RKE2 clusters are still in tech preview, so we do not recommend running production workloads on these clusters. If you have provisioned clusters, you can enable the recommended experimental-initial-corrupt-check flag by editing the cluster as YAML. If you have an imported k3s/RKE2 cluster, review the standalone Kubernetes distribution section. From the \u201cCluster Management\u201d page, click the vertical three-dots on the right-hand side for the cluster you want to edit. From the menu, select \u201cEdit YAML\u201d. Edit the spec.rkeConfig.machineGlobalConfig.etcd-arg section of the YAML to add in an etcd argument. Note: Your YAML may be slightly different from the example below. Example: spec: cloudCredentialSecretName: cattle-global-data:cc-xxxxx kubernetesVersion: v1.22.7+rke2r2 localClusterAuthEndpoint: {} rkeConfig: chartValues: rke2-calico: {} etcd: snapshotRetention: 5 snapshotScheduleCron: 0 */5 * * * machineGlobalConfig: cni: calico etcd-arg: [\"experimental-initial-corrupt-check=true\"] Click \u201cSave\u201d at the bottom. Rancher will update the configuration and restart the necessary services. For Users running standalone Kubernetes distributions RKE Clusters As of RKE v1.3.8, the default version of Kubernetes was set to 1.22.x. In order to not use the default Kubernetes version, please set the kubernetes_version to other available versions in your cluster.yml file for any new deployments through RKE. If you already have an existing RKE1 cluster using an affected version, you can set experimental-initial-corrupt-check: true in extra_args for etcd. RKE v1.3.9 was released where the default version of Kubernetes is still set to 1.22.x, but the default version (1.22-rancher1-2) has the recommended flag enabled by default. RKE2/k3s Clusters The flag only needs to be added if you are using HA with embedded etcd. Single-server clusters with sqlite, or clusters using HA with an external SQL datastore are not affected. The flag only needs to be enabled on servers, as agents do not run etcd. Customization of etcd was introduced with 1.22.4 and 1.23.0, so if you are running a lower version of 1.22.x, then you will need to upgrade to at least 1.22.4 in order to customize the etcd configuration. RKE2 Clusters Create or edit the config file at /etc/rancher/rke2/config.yaml. Add the following line to the end of the file: etcd-arg: [\"experimental-initial-corrupt-check=true\"] Save the config file and then run systemctl restart rke2-server to apply the change. K3S Clusters Create or edit the config file at /etc/rancher/k3s/config.yaml. Add the following line to the end of the file: etcd-arg: [\"experimental-initial-corrupt-check=true\"] Save the config file and then run systemctl restart k3s to apply the change. Additional Information Rancher Support Advisories Rancher Support FAQs Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"\\[Rancher\\] Operational Advisory, 20220405: Rancher Kubernetes Distributions and Etcd 3.5 Updates"},{"location":"000020632/#rancher-operational-advisory-20220405-rancher-kubernetes-distributions-and-etcd-35-updates","text":"This document (000020632) is provided subject to the disclaimer at the end of this document.","title":"[Rancher] Operational Advisory, 20220405: Rancher Kubernetes Distributions and Etcd 3.5 Updates"},{"location":"000020632/#environment","text":"The etcd maintainers have recommended against the use of etcd 3.5.0-3.5.2 for new production workloads, due to a recently discovered bug that may cause data loss when etcd is killed under high load. Their published advisory2 provides recommendations of how to avoid triggering the issue, but as of today, there is no official fix/resolution to the existing issue4 .","title":"Environment"},{"location":"000020632/#situation","text":"Who does this affect? Users running Rancher 2.6.4+ who have deployed Rancher on a single node as a Docker installation . Reminder : This installation method is not recommended for any production environment and is only recommended for development/sandbox testing. If you are running Rancher on a managed Kubernetes cluster, then you will have to refer to your Kubernetes service provider to determine if you are affected by this advisory. Users running Kubernetes 1.22+ and 1.23+ of any of the Rancher Kubernetes Distributions (RKE, RKE2, K3s) and using etcd as your datastore. The default datastore for RKE and RKE2 is etcd. This applies to standalone Kubernetes clusters as well as any downstream clusters provisioned by Rancher.","title":"Situation"},{"location":"000020632/#resolution","text":"What should you do? Stop deploying into production any new Kubernetes clusters using Rancher Kubernetes distributions versions 1.22/1.23 until a proper fix is provided by the etcd maintainers and included into the affected distribution. Update your etcd configuration to enable the experimental-initial-corrupt-checkoption. This flag will be turned on by default in etcd v3.6, but does not by itself fix the problem; it can only detect and repair the issue if it does occur. Note : each distribution has its own recommendation on how to enable this option; see below for more details. - Avoid terminating etcd unexpectedly (using kill \u20139, etc) - For RKE1 clusters, avoid stopping/killing the etcd containers adhoc without properly cordoning/draining nodes and taking backups - For RKE2 clusters, - Avoid sending SIGKILL to the etcd or rke2 process. - Avoid using the killall script (rke2-killall.sh) to stop RKE2 on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services. - For k3s clusters, - Avoid sending SIGKILL to the k3s process. - Avoid using the killall script (k3s-killall.sh) to stop K3s on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services. - Ensure nodes are not under significant memory pressure that may cause the Linux kernel to terminate the etcd process. Ensure that nodes are not terminated unexpectedly. Avoid force-terminating VMs, unexpected power loss, etc. How do I enable the recommended flag in etccd? For Users provisioning RKE/k3s/RKE2 clusters through Rancher Provisioned RKE Clusters If you are running 1.22 or 1.23, upgrade to the following respective versions to enable the recommended experimental-initial-corrupt-check flag in etcd. RKE 1.22 - v1.22.7-rancher1-2 RKE 1.23 (Experimental) - v1.23.4-rancher1-2 Provisioned K3s/RKE2 Clusters (Tech Preview) Provisioned k3s/RKE2 clusters are still in tech preview, so we do not recommend running production workloads on these clusters. If you have provisioned clusters, you can enable the recommended experimental-initial-corrupt-check flag by editing the cluster as YAML. If you have an imported k3s/RKE2 cluster, review the standalone Kubernetes distribution section. From the \u201cCluster Management\u201d page, click the vertical three-dots on the right-hand side for the cluster you want to edit. From the menu, select \u201cEdit YAML\u201d. Edit the spec.rkeConfig.machineGlobalConfig.etcd-arg section of the YAML to add in an etcd argument. Note: Your YAML may be slightly different from the example below. Example: spec: cloudCredentialSecretName: cattle-global-data:cc-xxxxx kubernetesVersion: v1.22.7+rke2r2 localClusterAuthEndpoint: {} rkeConfig: chartValues: rke2-calico: {} etcd: snapshotRetention: 5 snapshotScheduleCron: 0 */5 * * * machineGlobalConfig: cni: calico etcd-arg: [\"experimental-initial-corrupt-check=true\"] Click \u201cSave\u201d at the bottom. Rancher will update the configuration and restart the necessary services. For Users running standalone Kubernetes distributions RKE Clusters As of RKE v1.3.8, the default version of Kubernetes was set to 1.22.x. In order to not use the default Kubernetes version, please set the kubernetes_version to other available versions in your cluster.yml file for any new deployments through RKE. If you already have an existing RKE1 cluster using an affected version, you can set experimental-initial-corrupt-check: true in extra_args for etcd. RKE v1.3.9 was released where the default version of Kubernetes is still set to 1.22.x, but the default version (1.22-rancher1-2) has the recommended flag enabled by default. RKE2/k3s Clusters The flag only needs to be added if you are using HA with embedded etcd. Single-server clusters with sqlite, or clusters using HA with an external SQL datastore are not affected. The flag only needs to be enabled on servers, as agents do not run etcd. Customization of etcd was introduced with 1.22.4 and 1.23.0, so if you are running a lower version of 1.22.x, then you will need to upgrade to at least 1.22.4 in order to customize the etcd configuration. RKE2 Clusters Create or edit the config file at /etc/rancher/rke2/config.yaml. Add the following line to the end of the file: etcd-arg: [\"experimental-initial-corrupt-check=true\"] Save the config file and then run systemctl restart rke2-server to apply the change. K3S Clusters Create or edit the config file at /etc/rancher/k3s/config.yaml. Add the following line to the end of the file: etcd-arg: [\"experimental-initial-corrupt-check=true\"] Save the config file and then run systemctl restart k3s to apply the change.","title":"Resolution"},{"location":"000020632/#additional-information","text":"Rancher Support Advisories Rancher Support FAQs","title":"Additional Information"},{"location":"000020632/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020657/","text":"Exclude cattle-system namespace from Dynatrace monitoring This document (000020657) is provided subject to the disclaimer at the end of this document. Environment The cattle-cluster-agent pod is stuck in a CrashLoopBackOff, logging the following messages: usage: agent <absolute path of static Go application> [Go application arguments] Situation We have observed messages from downstream clusters where the cattle-cluster-agent on any Rancher version can contain no logging outputs, with messages only relating to environment variables. Running kubectl exec to access a shell on the pod and running /usr/bin/run.sh also produces the error. Resolution It is recommend to exclude the cattle-system namespace from being monitored by Dynatrace. The below documentation is specific for Dynatrace, however other solutions could have similar options. Option 3: https://www.dynatrace.com/support/help/setup-and-configuration/setup-on-container-platforms/kubernetes/get-started-with-kubernetes-monitoring/dto-config-options-k8s#annotate Cause Solutions like Dynatrace can potentially inject sidecar containers alongside cattle-cluster-agent/cattle-node-agent containers, causing the agent to exit unpredictably.. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Exclude cattle-system namespace from Dynatrace monitoring"},{"location":"000020657/#exclude-cattle-system-namespace-from-dynatrace-monitoring","text":"This document (000020657) is provided subject to the disclaimer at the end of this document.","title":"Exclude cattle-system namespace from Dynatrace monitoring"},{"location":"000020657/#environment","text":"The cattle-cluster-agent pod is stuck in a CrashLoopBackOff, logging the following messages: usage: agent <absolute path of static Go application> [Go application arguments]","title":"Environment"},{"location":"000020657/#situation","text":"We have observed messages from downstream clusters where the cattle-cluster-agent on any Rancher version can contain no logging outputs, with messages only relating to environment variables. Running kubectl exec to access a shell on the pod and running /usr/bin/run.sh also produces the error.","title":"Situation"},{"location":"000020657/#resolution","text":"It is recommend to exclude the cattle-system namespace from being monitored by Dynatrace. The below documentation is specific for Dynatrace, however other solutions could have similar options. Option 3: https://www.dynatrace.com/support/help/setup-and-configuration/setup-on-container-platforms/kubernetes/get-started-with-kubernetes-monitoring/dto-config-options-k8s#annotate","title":"Resolution"},{"location":"000020657/#cause","text":"Solutions like Dynatrace can potentially inject sidecar containers alongside cattle-cluster-agent/cattle-node-agent containers, causing the agent to exit unpredictably..","title":"Cause"},{"location":"000020657/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020682/","text":"Azure AD API Removal This document (000020682) is provided subject to the disclaimer at the end of this document. Situation Summary of Changes Microsoft is ending support of the existing AzureAD Graph API before 2023. Accordingly, Rancher has updated our AzureAD auth provider to use the new Microsoft Graph API to access users and groups in Active Directory. Details of Old vs New Old ADAL is the authentication library we use to get access tokens to the deprecated Azure AD Graph API. New MSAL is the new authentication library we will instead use to get access tokens to the new Microsoft Graph API. Actions Required of Users New users of v2.6.x and v2.7.x will use the new Microsoft Graph API when they register Rancher with Azure AD. There will be no need for a transition. Existing users who have Azure AD as the auth provider will see an informational notification/banner that will urge them to upgrade Rancher's auth provider before the end of 2022. Beforehand, their app in Azure will need to have the necessary permissions for Rancher to be able to work with Users and Groups in AD. To upgrade, the UI will have a button to instruct the backend to use the new authentication/authorization flow without requiring Rancher admins to reconfigure the existing auth provider. AD admins must add the necessary Microsoft Graph permissions to their apps. Specifically, User.Read.All and Group.Read.All - both must be Application (not Delegated) permissions. Support Considerations or Gotchas When you choose to upgrade the existing Azure AD auth provider configuration in Rancher, please keep in mind that all users' access tokens to the deprecated Azure AD Graph API will be deleted, since Rancher won't need them anymore because it won't be communicating with it. Instead, Rancher will store in a secret only one access token to the new Microsoft Graph API - that of the service principal associated with the App registration in Azure AD. This token is refreshed once an hour (not in the background, but when its use triggers a refresh). Additional migration instructions can be found at these links: For Rancher 2.6.x For Rancher 2.7.x Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Azure AD API Removal"},{"location":"000020682/#azure-ad-api-removal","text":"This document (000020682) is provided subject to the disclaimer at the end of this document.","title":"Azure AD API Removal"},{"location":"000020682/#situation","text":"","title":"Situation"},{"location":"000020682/#summary-of-changes","text":"Microsoft is ending support of the existing AzureAD Graph API before 2023. Accordingly, Rancher has updated our AzureAD auth provider to use the new Microsoft Graph API to access users and groups in Active Directory.","title":"Summary of Changes"},{"location":"000020682/#details-of-old-vs-new","text":"Old ADAL is the authentication library we use to get access tokens to the deprecated Azure AD Graph API. New MSAL is the new authentication library we will instead use to get access tokens to the new Microsoft Graph API.","title":"Details of Old vs New"},{"location":"000020682/#actions-required-of-users","text":"New users of v2.6.x and v2.7.x will use the new Microsoft Graph API when they register Rancher with Azure AD. There will be no need for a transition. Existing users who have Azure AD as the auth provider will see an informational notification/banner that will urge them to upgrade Rancher's auth provider before the end of 2022. Beforehand, their app in Azure will need to have the necessary permissions for Rancher to be able to work with Users and Groups in AD. To upgrade, the UI will have a button to instruct the backend to use the new authentication/authorization flow without requiring Rancher admins to reconfigure the existing auth provider. AD admins must add the necessary Microsoft Graph permissions to their apps. Specifically, User.Read.All and Group.Read.All - both must be Application (not Delegated) permissions.","title":"Actions Required of Users"},{"location":"000020682/#support-considerations-or-gotchas","text":"When you choose to upgrade the existing Azure AD auth provider configuration in Rancher, please keep in mind that all users' access tokens to the deprecated Azure AD Graph API will be deleted, since Rancher won't need them anymore because it won't be communicating with it. Instead, Rancher will store in a secret only one access token to the new Microsoft Graph API - that of the service principal associated with the App registration in Azure AD. This token is refreshed once an hour (not in the background, but when its use triggers a refresh). Additional migration instructions can be found at these links: For Rancher 2.6.x For Rancher 2.7.x","title":"Support Considerations or Gotchas"},{"location":"000020682/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020683/","text":"vSphere 6.7 EOL This document (000020683) is provided subject to the disclaimer at the end of this document. Situation Summary of Changes VMWare vSphere 6.7 will enter end of life (EoL) status on October 15, 2022. The original date was November 15, 2021, however, VMWare opted to extend their end of general support (EoGS) date based on customer requests at the time. As far as Rancher is concerned, vSphere 6.7 will reach EoL in the Rancher Support Matrix on the stated VMWare vSphere 6.7 EoGS date of October 15, 2022. VMWare Blog Announcing vSphere 6.7 EoGS https://blogs.vmware.com/vsphere/2020/06/announcing-extension-of-vsphere-6-7-general-support-period.html VMWare Product Lifecycle Matrix https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/support/product-lifecycle-matrix.pdf Details of Old vs New vSphere 7, released in early 2020, introduces native support and integrations for Kubernetes. This includes new functionality utilized in the Rancher vSphere CSI and CPI charts. Actions Required of Users End-users must upgrade to vSphere 7.0 by October 15, 2022, to stay in compliance with the Rancher Support Matrix. Support Considerations or Gotchas vSphere 7.0 has been supported since Rancher 2.4.9 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-4-9/ , 2.5.2 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-5-2/ , and v2.6.0 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-6-0/ . Customers have had ample time to migrate from vSphere 6.7 to 7.0. There will not be any extensions of the Rancher EoL date for vSphere 6.7. The original VMWare vSphere EoTG (End of Technical Guidance) date of November 15, 2023 still applies for vSphere 6.7. Rancher Support should expect customers to potentially remain on vSphere 6.7 beyond the end of general support (EoGS) period. However, once the EoGS date is reached, Rancher and its products will no longer be validated on 6.7 and customers should expect a best effort from Support and Engineering for issues involving vSphere 6.7 environments Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"vSphere 6.7 EOL"},{"location":"000020683/#vsphere-67-eol","text":"This document (000020683) is provided subject to the disclaimer at the end of this document.","title":"vSphere 6.7 EOL"},{"location":"000020683/#situation","text":"","title":"Situation"},{"location":"000020683/#summary-of-changes","text":"VMWare vSphere 6.7 will enter end of life (EoL) status on October 15, 2022. The original date was November 15, 2021, however, VMWare opted to extend their end of general support (EoGS) date based on customer requests at the time. As far as Rancher is concerned, vSphere 6.7 will reach EoL in the Rancher Support Matrix on the stated VMWare vSphere 6.7 EoGS date of October 15, 2022. VMWare Blog Announcing vSphere 6.7 EoGS https://blogs.vmware.com/vsphere/2020/06/announcing-extension-of-vsphere-6-7-general-support-period.html VMWare Product Lifecycle Matrix https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/support/product-lifecycle-matrix.pdf","title":"Summary of Changes"},{"location":"000020683/#details-of-old-vs-new","text":"vSphere 7, released in early 2020, introduces native support and integrations for Kubernetes. This includes new functionality utilized in the Rancher vSphere CSI and CPI charts.","title":"Details of Old vs New"},{"location":"000020683/#actions-required-of-users","text":"End-users must upgrade to vSphere 7.0 by October 15, 2022, to stay in compliance with the Rancher Support Matrix.","title":"Actions Required of Users"},{"location":"000020683/#support-considerations-or-gotchas","text":"vSphere 7.0 has been supported since Rancher 2.4.9 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-4-9/ , 2.5.2 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-5-2/ , and v2.6.0 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-6-0/ . Customers have had ample time to migrate from vSphere 6.7 to 7.0. There will not be any extensions of the Rancher EoL date for vSphere 6.7. The original VMWare vSphere EoTG (End of Technical Guidance) date of November 15, 2023 still applies for vSphere 6.7. Rancher Support should expect customers to potentially remain on vSphere 6.7 beyond the end of general support (EoGS) period. However, once the EoGS date is reached, Rancher and its products will no longer be validated on 6.7 and customers should expect a best effort from Support and Engineering for issues involving vSphere 6.7 environments","title":"Support Considerations or Gotchas"},{"location":"000020683/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020684/","text":"Windows RKE1 EOL This document (000020684) is provided subject to the disclaimer at the end of this document. Situation Summary of Changes RKE1 Windows will move to EOL status on September 1, 2022, due to the deprecation of Docker EE support by Microsoft. Docker EE is the only supported container runtime for Windows nodes in RKE1. The impact on customers and internal development/QA efforts after September 1, 2022, is summarized below: - Rancher users should expect that they are unable to provision new RKE1 Windows clusters. - Rancher users should expect to have no available upgrade path for existing RKE1 Windows clusters. - Customers who are unable to move to RKE2 Windows would need to purchase a support contract for Mirantis Container Runtime, previously known as Docker Engine - Enterprise. - Rancher Support is unable to provide full-stack support for workloads deployed on the Mirantis Container Runtime. - We expect cloud providers to completely remove images that contain Docker EE for Windows Server. - Microsoft has indicated to us in an unofficial capacity that the current and only installation method for Docker EE on Windows Server, which is through the PSGallery, will be removed as part of the EOL of Docker EE. Details of Old vs New RKE2 Windows is the only path forward for Windows support in Rancher. RKE1 Windows Clusters: Each Linux node in an RKE1 Windows cluster, regardless of the role assigned to it, will have have a default taint that prevents workloads to be scheduled on it unless the workload has a toleration configured. This is a major design feature for RKE1 Windows clusters which were designed to only run Windows workloads. RKE2 Hybrid Clusters: Based on feedback and requests for hybrid workload support, RKE2 Windows was designed to support both Linux and Windows workloads by default. RKE2 scheduling relies on node selectors. This is a marked change from RKE1 as taints and tolerations were not incorporated into RKE2. Node selectors were a critical part of RKE1 Windows clusters, which makes for an easy migration of your workloads. Actions Required of Users Moving forward, Rancher customers will need to upgrade to Rancher v2.6.5+ (to have GA of RKE2 Windows provisioning available) and migrate their container workloads to run on RKE2 Hybrid clusters, which are built on the containerd runtime. These steps will be required for them to stay in compliance with the Rancher Support Matrix. Support Considerations or Gotchas It should be assumed that all existing functionality required for creating new RKE1 Windows clusters or upgrading existing RKE1 Windows clusters will cease functioning on this date. Rancher will be unable to publish any future versions of RKE1 with Windows Support for Docker EE. RKE2 is the only option for Windows customers who wish to stay in a supported configuration. Windows Server 2019 LTSC and Windows Server 2022 LTSC are the only supported versions of Windows for RKE2. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Windows RKE1 EOL"},{"location":"000020684/#windows-rke1-eol","text":"This document (000020684) is provided subject to the disclaimer at the end of this document.","title":"Windows RKE1 EOL"},{"location":"000020684/#situation","text":"","title":"Situation"},{"location":"000020684/#summary-of-changes","text":"RKE1 Windows will move to EOL status on September 1, 2022, due to the deprecation of Docker EE support by Microsoft. Docker EE is the only supported container runtime for Windows nodes in RKE1. The impact on customers and internal development/QA efforts after September 1, 2022, is summarized below: - Rancher users should expect that they are unable to provision new RKE1 Windows clusters. - Rancher users should expect to have no available upgrade path for existing RKE1 Windows clusters. - Customers who are unable to move to RKE2 Windows would need to purchase a support contract for Mirantis Container Runtime, previously known as Docker Engine - Enterprise. - Rancher Support is unable to provide full-stack support for workloads deployed on the Mirantis Container Runtime. - We expect cloud providers to completely remove images that contain Docker EE for Windows Server. - Microsoft has indicated to us in an unofficial capacity that the current and only installation method for Docker EE on Windows Server, which is through the PSGallery, will be removed as part of the EOL of Docker EE.","title":"Summary of Changes"},{"location":"000020684/#details-of-old-vs-new","text":"RKE2 Windows is the only path forward for Windows support in Rancher. RKE1 Windows Clusters: Each Linux node in an RKE1 Windows cluster, regardless of the role assigned to it, will have have a default taint that prevents workloads to be scheduled on it unless the workload has a toleration configured. This is a major design feature for RKE1 Windows clusters which were designed to only run Windows workloads. RKE2 Hybrid Clusters: Based on feedback and requests for hybrid workload support, RKE2 Windows was designed to support both Linux and Windows workloads by default. RKE2 scheduling relies on node selectors. This is a marked change from RKE1 as taints and tolerations were not incorporated into RKE2. Node selectors were a critical part of RKE1 Windows clusters, which makes for an easy migration of your workloads.","title":"Details of Old vs New"},{"location":"000020684/#actions-required-of-users","text":"Moving forward, Rancher customers will need to upgrade to Rancher v2.6.5+ (to have GA of RKE2 Windows provisioning available) and migrate their container workloads to run on RKE2 Hybrid clusters, which are built on the containerd runtime. These steps will be required for them to stay in compliance with the Rancher Support Matrix.","title":"Actions Required of Users"},{"location":"000020684/#support-considerations-or-gotchas","text":"It should be assumed that all existing functionality required for creating new RKE1 Windows clusters or upgrading existing RKE1 Windows clusters will cease functioning on this date. Rancher will be unable to publish any future versions of RKE1 with Windows Support for Docker EE. RKE2 is the only option for Windows customers who wish to stay in a supported configuration. Windows Server 2019 LTSC and Windows Server 2022 LTSC are the only supported versions of Windows for RKE2.","title":"Support Considerations or Gotchas"},{"location":"000020684/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020699/","text":"How to recreate rancher-webhook-tls secret if incorrectly deleted This document (000020699) is provided subject to the disclaimer at the end of this document. Environment Rancher 2.5.8 or higher, incorrectly deleted rancher-webhook-tls secret instead of cattle-webhook-tls secret Situation The rancher-webhook-tls is expired on the local rancher cluster. After following the documentation to renew the certificate, the rancher-webhook pods cannot start. https://rancher.com/docs/rancher/v2.6/en/troubleshooting/expired-webhook-certificates Resolution Trigger recreation of the rancher-webhook-tls secret: 1. Remove rancher.cattle.io validating and mutating webhooks, as well as the webhook-service: kubectl delete mutatingwebhookconfigurations rancher.cattle.io kubectl delete validatingwebhookconfigurations rancher.cattle.io kubectl -n cattle-system delete service webhook-service 2. Navigate to Apps & Marketplace in the local cluster Explorer, Installed Apps, and perform an 'upgrade' of rancher-webhook to trigger the recreation of deleted resources and a new rancher-webhook-tls certificate secret. Cause Unintentionally deletion of _rancher-webhook-tls_secretinstead of_cattle-webhook-tls_secret Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to recreate rancher-webhook-tls secret if incorrectly deleted"},{"location":"000020699/#how-to-recreate-rancher-webhook-tls-secret-if-incorrectly-deleted","text":"This document (000020699) is provided subject to the disclaimer at the end of this document.","title":"How to recreate rancher-webhook-tls secret if incorrectly deleted"},{"location":"000020699/#environment","text":"Rancher 2.5.8 or higher, incorrectly deleted rancher-webhook-tls secret instead of cattle-webhook-tls secret","title":"Environment"},{"location":"000020699/#situation","text":"The rancher-webhook-tls is expired on the local rancher cluster. After following the documentation to renew the certificate, the rancher-webhook pods cannot start. https://rancher.com/docs/rancher/v2.6/en/troubleshooting/expired-webhook-certificates","title":"Situation"},{"location":"000020699/#resolution","text":"Trigger recreation of the rancher-webhook-tls secret: 1. Remove rancher.cattle.io validating and mutating webhooks, as well as the webhook-service: kubectl delete mutatingwebhookconfigurations rancher.cattle.io kubectl delete validatingwebhookconfigurations rancher.cattle.io kubectl -n cattle-system delete service webhook-service 2. Navigate to Apps & Marketplace in the local cluster Explorer, Installed Apps, and perform an 'upgrade' of rancher-webhook to trigger the recreation of deleted resources and a new rancher-webhook-tls certificate secret.","title":"Resolution"},{"location":"000020699/#cause","text":"Unintentionally deletion of _rancher-webhook-tls_secretinstead of_cattle-webhook-tls_secret","title":"Cause"},{"location":"000020699/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020703/","text":"Switching encryption method to at-rest data encryption This document (000020703) is provided subject to the disclaimer at the end of this document. Situation I already have encryption enabled per the hardening guide but I'm having trouble switching to the new at-rest data encryption method documented here . What needs to be done? Resolution If you already have encryption enabled, use the following steps: Disable the current Encryption 1. Edit /opt/kubernetes/encryption.yaml 2. Move the - identity: {} line above the - aescbc: line 3. Update cluster.yaml and set secrets_encryption_config to false 4. Save cluster.yaml and wait for the cluster to finish decrypting Cleanup and enable the at-rest data encryption 1. Update cluster.yaml: - Remove extra_binds: - \"/opt/kubernetes:/opt/kubernetes\" - Set secrets_encryption_config to true 2. Wait for the cluster to finish encrypting Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Switching encryption method to at-rest data encryption"},{"location":"000020703/#switching-encryption-method-to-at-rest-data-encryption","text":"This document (000020703) is provided subject to the disclaimer at the end of this document.","title":"Switching encryption method to at-rest data encryption"},{"location":"000020703/#situation","text":"I already have encryption enabled per the hardening guide but I'm having trouble switching to the new at-rest data encryption method documented here . What needs to be done?","title":"Situation"},{"location":"000020703/#resolution","text":"","title":"Resolution"},{"location":"000020703/#if-you-already-have-encryption-enabled-use-the-following-steps","text":"","title":"If you already have encryption enabled, use the following steps:"},{"location":"000020703/#disable-the-current-encryption","text":"1. Edit /opt/kubernetes/encryption.yaml 2. Move the - identity: {} line above the - aescbc: line 3. Update cluster.yaml and set secrets_encryption_config to false 4. Save cluster.yaml and wait for the cluster to finish decrypting","title":"Disable the current Encryption"},{"location":"000020703/#cleanup-and-enable-the-at-rest-data-encryption","text":"1. Update cluster.yaml: - Remove extra_binds: - \"/opt/kubernetes:/opt/kubernetes\" - Set secrets_encryption_config to true 2. Wait for the cluster to finish encrypting","title":"Cleanup and enable the at-rest data encryption"},{"location":"000020703/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020710/","text":"After Rancher 2.6.x upgrade, HTTP 403 Errors in Rancher UI This document (000020710) is provided subject to the disclaimer at the end of this document. Environment Several features of Rancher UI don't work and return HTTP 403 for some users after Rancher upgrade from 2.6.x : - Shell execution - Yaml editing Situation For some users, several Rancher features are not working and returning HTTP 403 (Forbidden) Rancher Trace log: User-system-serviceaccount-cattle-impersonation-system-cattle-impersonation-u-vnds56pccy-cannot-impersonate-resource-users-in-API-group-at-the-cluster-scope-due-to-missing-clusterrolebinding Resolution 1. Check RBAC Clusterroles and Clusterrolebindings of the affected user ## Clusterroles of the user $ kubectl get clusterrole | grep u-b3l74guter ## Clusterrolebindings of the user $ kubectl get clusterrolebinding | grep u-b3l74guter 2. From the previous output, the expected Clusterrole cattle-impersonation-u-xxxxxxxx is present, but the Clusterrolebinding is absent. 3. Delete the cattle-impersonation-user-xxxx Clusterrole of the user $ kubectl delete clusterrole cattle-impersonation-u-b3l74guter 4. Trigger the recreation of the Clusterrole and Clusterrolebinding by browsing to a Rancher feature. e.g: open a Monitoring link in the cluster This action triggered the recreation of the Clusterrole and Clusterrolebinding Additional Information https://github.com/rancher/rancher/issues/33912 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"After Rancher 2.6.x upgrade, HTTP 403 Errors in Rancher UI"},{"location":"000020710/#after-rancher-26x-upgrade-http-403-errors-in-rancher-ui","text":"This document (000020710) is provided subject to the disclaimer at the end of this document.","title":"After Rancher 2.6.x upgrade, HTTP 403 Errors in Rancher UI"},{"location":"000020710/#environment","text":"Several features of Rancher UI don't work and return HTTP 403 for some users after Rancher upgrade from 2.6.x : - Shell execution - Yaml editing","title":"Environment"},{"location":"000020710/#situation","text":"For some users, several Rancher features are not working and returning HTTP 403 (Forbidden) Rancher Trace log: User-system-serviceaccount-cattle-impersonation-system-cattle-impersonation-u-vnds56pccy-cannot-impersonate-resource-users-in-API-group-at-the-cluster-scope-due-to-missing-clusterrolebinding","title":"Situation"},{"location":"000020710/#resolution","text":"1. Check RBAC Clusterroles and Clusterrolebindings of the affected user ## Clusterroles of the user $ kubectl get clusterrole | grep u-b3l74guter ## Clusterrolebindings of the user $ kubectl get clusterrolebinding | grep u-b3l74guter 2. From the previous output, the expected Clusterrole cattle-impersonation-u-xxxxxxxx is present, but the Clusterrolebinding is absent. 3. Delete the cattle-impersonation-user-xxxx Clusterrole of the user $ kubectl delete clusterrole cattle-impersonation-u-b3l74guter 4. Trigger the recreation of the Clusterrole and Clusterrolebinding by browsing to a Rancher feature. e.g: open a Monitoring link in the cluster This action triggered the recreation of the Clusterrole and Clusterrolebinding","title":"Resolution"},{"location":"000020710/#additional-information","text":"https://github.com/rancher/rancher/issues/33912","title":"Additional Information"},{"location":"000020710/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020712/","text":"Is it possible use mTLS for rancher agent connectivity? This document (000020712) is provided subject to the disclaimer at the end of this document. Resolution It is not possible to configure mTLS authentication for the rancher-agent connectivity; however, the connection to Rancher is secured via TLS and the agents use a token to authenticate themselves to Rancher. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Is it possible use mTLS for rancher agent connectivity?"},{"location":"000020712/#is-it-possible-use-mtls-for-rancher-agent-connectivity","text":"This document (000020712) is provided subject to the disclaimer at the end of this document.","title":"Is it possible use mTLS for rancher agent connectivity?"},{"location":"000020712/#resolution","text":"It is not possible to configure mTLS authentication for the rancher-agent connectivity; however, the connection to Rancher is secured via TLS and the agents use a token to authenticate themselves to Rancher.","title":"Resolution"},{"location":"000020712/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020713/","text":"Firewalld block rancher cluster dns: Weave CNI does not work with Firewalld on RHEL 8 based OSs This document (000020713) is provided subject to the disclaimer at the end of this document. Environment Issue: Firewalld service block Rancher cluster DNS on Rhel8. Steps to reproduce: 1. Install and setup RKE on RHEL 8 2. CoreDNS is deployed as part of the RKE setup 3. Start firewalld service on RHEL8 nodes. After starting firewalld service, k8s pod logs return connection error: ent-041273.voicelab.local. A: read udp 172.21.0.19:58953->1.10.64.26:53: i/o timeout -------------- Situation The Internal Kubernetes DNS server (coredns) is blocked. Once firewalld is stopped, the Kubernetes DNS works well. Firewalld block these ports that are required: - 2379-2380/tcp - 4789/udp - 5000/tcp - 6443/tcp - 6783/tcp - 6783-6784/udp - 9100/tcp - 10250/tcp - 10257/tcp - 10259/tcp Resolution Stop firewalld on RHEL8 nodes, it is a requirement as described in Rancher requirements: https://rancher.com/docs/rancher/v2.6/en/installation/requirements/#operating-systems-and-container-runtime-requirements Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Firewalld block rancher cluster dns: Weave CNI does not work with Firewalld on RHEL 8 based OSs"},{"location":"000020713/#firewalld-block-rancher-cluster-dns-weave-cni-does-not-work-with-firewalld-on-rhel-8-based-oss","text":"This document (000020713) is provided subject to the disclaimer at the end of this document.","title":"Firewalld block rancher cluster dns: Weave CNI does not work with Firewalld on RHEL 8 based OSs"},{"location":"000020713/#environment","text":"Issue: Firewalld service block Rancher cluster DNS on Rhel8. Steps to reproduce: 1. Install and setup RKE on RHEL 8 2. CoreDNS is deployed as part of the RKE setup 3. Start firewalld service on RHEL8 nodes. After starting firewalld service, k8s pod logs return connection error: ent-041273.voicelab.local. A: read udp 172.21.0.19:58953->1.10.64.26:53: i/o timeout --------------","title":"Environment"},{"location":"000020713/#situation","text":"The Internal Kubernetes DNS server (coredns) is blocked. Once firewalld is stopped, the Kubernetes DNS works well. Firewalld block these ports that are required: - 2379-2380/tcp - 4789/udp - 5000/tcp - 6443/tcp - 6783/tcp - 6783-6784/udp - 9100/tcp - 10250/tcp - 10257/tcp - 10259/tcp","title":"Situation"},{"location":"000020713/#resolution","text":"Stop firewalld on RHEL8 nodes, it is a requirement as described in Rancher requirements: https://rancher.com/docs/rancher/v2.6/en/installation/requirements/#operating-systems-and-container-runtime-requirements","title":"Resolution"},{"location":"000020713/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020721/","text":"How to test Rancher RC/Alpha versions This document (000020721) is provided subject to the disclaimer at the end of this document. Environment Rancher RC versions Situation By default, Helm only returns the final releases. This article provides details on how to install Rancher RC versions. Resolution First, you need to add the Rancher 'latest' helm repository. helm repo add rancher-latest <strong>https://releases.rancher.com/server-charts/latest</strong> Then you can list the current repositories on your configuration machine with $ helm repo ls NAME URL coredns https://coredns.github.io/helm rancher-charts https://charts.rancher.io rancher-stable https://releases.rancher.com/server-charts/stable rancher-latest https://releases.rancher.com/server-charts/latest You can list the final releases with $ helm search repo \"rancher-latest\" --versions NAME CHART VERSION APP VERSION DESCRIPTION rancher-latest/rancher 2.6.6 v2.6.6 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.5 v2.6.5 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.4 v2.6.4 Install Rancher Server to manage Kubernetes clu... [...] rancher-latest/rancher 2.0.4 v2.0.4 Install Rancher Server to manage Kubernetes clu... To list the RC versions, you can use the --devel argument. helm search repo \"rancher-latest\" --versions --devel NAME CHART VERSION APP VERSION DESCRIPTION rancher-latest/rancher 2.6.7-rc7 v2.6.7-rc7 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.7-rc6 v2.6.7-rc6 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.7-rc5 v2.6.7-rc5 Install Rancher Server to manage Kubernetes clu... [...] rancher-latest/rancher 2.0.4 v2.0.4 Install Rancher Server to manage Kubernetes clu... The Rancher installation command line becomes helm install rancher <rancher-latest-repo> --devel --version 2.6.7-rc1 --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set replicas=3 Additional Information https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to test Rancher RC/Alpha versions"},{"location":"000020721/#how-to-test-rancher-rcalpha-versions","text":"This document (000020721) is provided subject to the disclaimer at the end of this document.","title":"How to test Rancher RC/Alpha versions"},{"location":"000020721/#environment","text":"Rancher RC versions","title":"Environment"},{"location":"000020721/#situation","text":"By default, Helm only returns the final releases. This article provides details on how to install Rancher RC versions.","title":"Situation"},{"location":"000020721/#resolution","text":"First, you need to add the Rancher 'latest' helm repository. helm repo add rancher-latest <strong>https://releases.rancher.com/server-charts/latest</strong> Then you can list the current repositories on your configuration machine with $ helm repo ls NAME URL coredns https://coredns.github.io/helm rancher-charts https://charts.rancher.io rancher-stable https://releases.rancher.com/server-charts/stable rancher-latest https://releases.rancher.com/server-charts/latest You can list the final releases with $ helm search repo \"rancher-latest\" --versions NAME CHART VERSION APP VERSION DESCRIPTION rancher-latest/rancher 2.6.6 v2.6.6 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.5 v2.6.5 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.4 v2.6.4 Install Rancher Server to manage Kubernetes clu... [...] rancher-latest/rancher 2.0.4 v2.0.4 Install Rancher Server to manage Kubernetes clu... To list the RC versions, you can use the --devel argument. helm search repo \"rancher-latest\" --versions --devel NAME CHART VERSION APP VERSION DESCRIPTION rancher-latest/rancher 2.6.7-rc7 v2.6.7-rc7 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.7-rc6 v2.6.7-rc6 Install Rancher Server to manage Kubernetes clu... rancher-latest/rancher 2.6.7-rc5 v2.6.7-rc5 Install Rancher Server to manage Kubernetes clu... [...] rancher-latest/rancher 2.0.4 v2.0.4 Install Rancher Server to manage Kubernetes clu... The Rancher installation command line becomes helm install rancher <rancher-latest-repo> --devel --version 2.6.7-rc1 --namespace cattle-system \\ --set hostname=rancher.my.org \\ --set replicas=3","title":"Resolution"},{"location":"000020721/#additional-information","text":"https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart","title":"Additional Information"},{"location":"000020721/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020727/","text":"Rancher upgrade FAQ This document (000020727) is provided subject to the disclaimer at the end of this document. Situation As we near the end of maintenance, end of life, and end of support for Rancher 2.5, we felt it pertinent to provide a one-stop FAQ page as customers begin to plan their upgrades. All upgrades should go from the latest to the latest. For instance, if you are on 2.5.3, you should go to 2.5.latest and then to 2.6.latest. The stop on 2.5.latest should be about a week to ensure you can catch any issues before moving forward. After the upgrade to 2.5.latest, you should then upgrade the underlying Kubernetes version to the latest supported version of the Rancher release. And again, the new Kubernetes version should be tested and verified for around one week. From there, progress to the next Rancher upgrade, test, and then Kubernetes. Many customers ask why we recommend one week. The 1-week recommendation is because this is often enough time for your clusters and app to be thoroughly tested and any issues flagged. We have seen customers who do less of a testing phase and only find issues when their cluster is being fully used and under normal \"strain.\" Here are some useful links for upgrades \u2014 see link (1) below for our team's general best practices around upgrade paths. During the course of your upgrade, see link (2) for how you and your team can bump the severity of this case if there is an impacting event during your upgrade. Doing so will notify our on-call engineer, who will engage as quickly as possible. Please review link (3) to verify that the Rancher and Kubernetes versions remain inline. It is best to ensure that any testing is done in a lower environment first so that you and your team can be more aware of any issues that the version jumps could have on your environment and applications. Lastly, for our team to better understand the updated environment, would you please run our system support script (4). This script will give our team a better understanding of the upgraded environment and the ability to call out items that may be out of our best practices. (1) https://www.suse.com/support/kb/doc/?id=000020061 (2) https://www.suse.com/support/kb/doc/?id=000020296 (3) https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/ (4) https://www.suse.com/support/kb/doc/?id=000020192 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Rancher upgrade FAQ"},{"location":"000020727/#rancher-upgrade-faq","text":"This document (000020727) is provided subject to the disclaimer at the end of this document.","title":"Rancher upgrade FAQ"},{"location":"000020727/#situation","text":"As we near the end of maintenance, end of life, and end of support for Rancher 2.5, we felt it pertinent to provide a one-stop FAQ page as customers begin to plan their upgrades. All upgrades should go from the latest to the latest. For instance, if you are on 2.5.3, you should go to 2.5.latest and then to 2.6.latest. The stop on 2.5.latest should be about a week to ensure you can catch any issues before moving forward. After the upgrade to 2.5.latest, you should then upgrade the underlying Kubernetes version to the latest supported version of the Rancher release. And again, the new Kubernetes version should be tested and verified for around one week. From there, progress to the next Rancher upgrade, test, and then Kubernetes. Many customers ask why we recommend one week. The 1-week recommendation is because this is often enough time for your clusters and app to be thoroughly tested and any issues flagged. We have seen customers who do less of a testing phase and only find issues when their cluster is being fully used and under normal \"strain.\" Here are some useful links for upgrades \u2014 see link (1) below for our team's general best practices around upgrade paths. During the course of your upgrade, see link (2) for how you and your team can bump the severity of this case if there is an impacting event during your upgrade. Doing so will notify our on-call engineer, who will engage as quickly as possible. Please review link (3) to verify that the Rancher and Kubernetes versions remain inline. It is best to ensure that any testing is done in a lower environment first so that you and your team can be more aware of any issues that the version jumps could have on your environment and applications. Lastly, for our team to better understand the updated environment, would you please run our system support script (4). This script will give our team a better understanding of the upgraded environment and the ability to call out items that may be out of our best practices. (1) https://www.suse.com/support/kb/doc/?id=000020061 (2) https://www.suse.com/support/kb/doc/?id=000020296 (3) https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/ (4) https://www.suse.com/support/kb/doc/?id=000020192","title":"Situation"},{"location":"000020727/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020728/","text":"Collecte de journaux Linux Rancher v2.x This document (000020728) is provided subject to the disclaimer at the end of this document. Situation Collecte de journaux Linux Rancher v2.x Les journaux peuvent \u00eatre collect\u00e9s \u00e0 partir d'un n\u0153ud Linux dans un cluster Rancher v2.x \u00e0 l'aide du script de collecte de journaux Rancher v2.x . Important: Ce script ne peut \u00eatre utilis\u00e9 que pour collecter des journaux \u00e0 partir de clusters provisionn\u00e9s par l'interface de ligne de commande Rancher Kubernetes Engine (RKE) , de clusters K3s , de clusters personnalis\u00e9s provisionn\u00e9s par Rancher et de clusters provisionn\u00e9s par Rancher \u00e0 l'aide d'un pilote de n\u0153ud . Ce script n'est pas adapt\u00e9 \u00e0 la collecte de journaux \u00e0 partir de clusters de fournisseurs Kubernetes h\u00e9berg\u00e9s . Le script doit \u00eatre t\u00e9l\u00e9charg\u00e9 et ex\u00e9cut\u00e9 directement sur l'h\u00f4te en utilisant l'utilisateur root ou en utilisant sudo, comme suit: wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s Par d\u00e9faut, la sortie sera \u00e9crite dans /tmpdans un fichier tar gzipp\u00e9 nomm\u00e9 - .tar.gz Choix Les indicateurs disponibles pouvant \u00eatre transmis au script se trouvent dans le script de collecteur de journaux Rancher v2.x README Disclaimer Cette base de connaissances de support technique fournit un outil pr\u00e9cieux aux clients SUSE et autres parties int\u00e9ress\u00e9es par nos produits et solutions pour obtenir des informations, des id\u00e9es et apprendre r\u00e9ciproquement. Les documents sont fournis \u00e0 des fins d'information, personnelles ou non commerciales au sein de votre organisation et sont pr\u00e9sent\u00e9s \u00abEN L'\u00c9TAT \u00bb SANS GARANTIE D'AUCUNE SORTE.","title":"Collecte de journaux Linux Rancher v2.x"},{"location":"000020728/#collecte-de-journaux-linux-rancher-v2x","text":"This document (000020728) is provided subject to the disclaimer at the end of this document.","title":"Collecte de journaux Linux Rancher v2.x"},{"location":"000020728/#situation","text":"","title":"Situation"},{"location":"000020728/#collecte-de-journaux-linux-rancher-v2x_1","text":"Les journaux peuvent \u00eatre collect\u00e9s \u00e0 partir d'un n\u0153ud Linux dans un cluster Rancher v2.x \u00e0 l'aide du script de collecte de journaux Rancher v2.x . Important: Ce script ne peut \u00eatre utilis\u00e9 que pour collecter des journaux \u00e0 partir de clusters provisionn\u00e9s par l'interface de ligne de commande Rancher Kubernetes Engine (RKE) , de clusters K3s , de clusters personnalis\u00e9s provisionn\u00e9s par Rancher et de clusters provisionn\u00e9s par Rancher \u00e0 l'aide d'un pilote de n\u0153ud . Ce script n'est pas adapt\u00e9 \u00e0 la collecte de journaux \u00e0 partir de clusters de fournisseurs Kubernetes h\u00e9berg\u00e9s . Le script doit \u00eatre t\u00e9l\u00e9charg\u00e9 et ex\u00e9cut\u00e9 directement sur l'h\u00f4te en utilisant l'utilisateur root ou en utilisant sudo, comme suit: wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s Par d\u00e9faut, la sortie sera \u00e9crite dans /tmpdans un fichier tar gzipp\u00e9 nomm\u00e9 - .tar.gz","title":"Collecte de journaux Linux Rancher v2.x"},{"location":"000020728/#choix","text":"Les indicateurs disponibles pouvant \u00eatre transmis au script se trouvent dans le script de collecteur de journaux Rancher v2.x README","title":"Choix"},{"location":"000020728/#disclaimer","text":"Cette base de connaissances de support technique fournit un outil pr\u00e9cieux aux clients SUSE et autres parties int\u00e9ress\u00e9es par nos produits et solutions pour obtenir des informations, des id\u00e9es et apprendre r\u00e9ciproquement. Les documents sont fournis \u00e0 des fins d'information, personnelles ou non commerciales au sein de votre organisation et sont pr\u00e9sent\u00e9s \u00abEN L'\u00c9TAT \u00bb SANS GARANTIE D'AUCUNE SORTE.","title":"Disclaimer"},{"location":"000020731/","text":"Tuning for nodes with a high number of CPUs allocated This document (000020731) is provided subject to the disclaimer at the end of this document. Environment An RKE cluster built by Rancher, or the RKE CLI Situation Some components in a Kubernetes cluster apply a linear scaling mechanism, often based on the number of CPU cores allocated. For nodes that have a high number CPU cores allocated the defaults can create a steep scaling curve and can introduce issues. Two components provided with RKE that scale in this way are kube-proxy and ingress-nginx. However, additional workloads (like nginx) may be deployed to the cluster and also need consideration. Adjusting the scaling for these components can avoid these issues. Resolution kube-proxy As explained in the Kubernetes GitHub issue here , the default scaling of the conntrack-max setting allocates 32K of memory per CPU core. This can manifest in the below events in OS logs: kernel: nf_conntrack: falling back to vmalloc. This static default can present issues with contiguous memory being allocated for the conntrack table, or reach unnecessary levels of space allocated. When observed frequently, this has been associated with network instability. As a starting point, the suggestion is to halve this amount for a cluster with affected nodes, this can be done by editing the cluster as YAML , or the cluster.yml file when using the RKE CLI. kubeproxy: extra_args: conntrack-max-per-core: '16384' ingress-nginx A common configuration of nginx is to set the worker_processes to auto. When set, nginx will scale the worker_processes to the number of CPU cores on the node. This can result in high numbers of PIDs and consume open files with the threads consumed (number of cores * 32 (default thread_pool size)). \\* http://nginx.org/en/docs/ngx_core_module.html#worker_processes \\* http://nginx.org/en/docs/ngx_core_module.html#thread_pool This can be adjusted by editing the cluster as YAML , or the cluster.yml file when using the RKE CLI. An example of 8 worker_processes is used below. For a nodes that may process a high amount of ingress traffic, you may wish to use a higher number. ingress: provider: nginx options: worker-processes: \"8\" Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Tuning for nodes with a high number of CPUs allocated"},{"location":"000020731/#tuning-for-nodes-with-a-high-number-of-cpus-allocated","text":"This document (000020731) is provided subject to the disclaimer at the end of this document.","title":"Tuning for nodes with a high number of CPUs allocated"},{"location":"000020731/#environment","text":"An RKE cluster built by Rancher, or the RKE CLI","title":"Environment"},{"location":"000020731/#situation","text":"Some components in a Kubernetes cluster apply a linear scaling mechanism, often based on the number of CPU cores allocated. For nodes that have a high number CPU cores allocated the defaults can create a steep scaling curve and can introduce issues. Two components provided with RKE that scale in this way are kube-proxy and ingress-nginx. However, additional workloads (like nginx) may be deployed to the cluster and also need consideration. Adjusting the scaling for these components can avoid these issues.","title":"Situation"},{"location":"000020731/#resolution","text":"","title":"Resolution"},{"location":"000020731/#kube-proxy","text":"As explained in the Kubernetes GitHub issue here , the default scaling of the conntrack-max setting allocates 32K of memory per CPU core. This can manifest in the below events in OS logs: kernel: nf_conntrack: falling back to vmalloc. This static default can present issues with contiguous memory being allocated for the conntrack table, or reach unnecessary levels of space allocated. When observed frequently, this has been associated with network instability. As a starting point, the suggestion is to halve this amount for a cluster with affected nodes, this can be done by editing the cluster as YAML , or the cluster.yml file when using the RKE CLI. kubeproxy: extra_args: conntrack-max-per-core: '16384'","title":"kube-proxy"},{"location":"000020731/#ingress-nginx","text":"A common configuration of nginx is to set the worker_processes to auto. When set, nginx will scale the worker_processes to the number of CPU cores on the node. This can result in high numbers of PIDs and consume open files with the threads consumed (number of cores * 32 (default thread_pool size)). \\* http://nginx.org/en/docs/ngx_core_module.html#worker_processes \\* http://nginx.org/en/docs/ngx_core_module.html#thread_pool This can be adjusted by editing the cluster as YAML , or the cluster.yml file when using the RKE CLI. An example of 8 worker_processes is used below. For a nodes that may process a high amount of ingress traffic, you may wish to use a higher number. ingress: provider: nginx options: worker-processes: \"8\"","title":"ingress-nginx"},{"location":"000020731/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020733/","text":"Reduce Memory and CPU footprint of Prometheus Monitoring Operator This document (000020733) is provided subject to the disclaimer at the end of this document. Environment rancher-monitoring: 100.1.3+up19.0.3 Situation On a fresh install of rancher-monitoring, the Prometheus monitoring operator consumes high CPU and Memory resources without any custom Prometheus CRDs configured. You would notice the following error messages on the Prometheus Operator Pod very frequently. $ kubectl logs -n cattle-monitoring-system rancher-monitoring-operator-784c69bc54-dvndg -f level=info ts=2022-07-26T09:34:55.46606005Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus level=info ts=2022-07-26T09:34:55.612269913Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager level=info ts=2022-07-26T09:34:55.694485011Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager level=info ts=2022-07-26T09:34:55.92009322Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus level=info ts=2022-07-26T09:34:59.042606472Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager level=info ts=2022-07-26T09:34:59.043983987Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus Resolution Add SecretListWatchSelector to reduce memory and CPU footprint. prometheusOperator: secretFieldSelector: \"type!=helm.sh/release.v1\" Status Top Issue Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Reduce Memory and CPU footprint of Prometheus Monitoring Operator"},{"location":"000020733/#reduce-memory-and-cpu-footprint-of-prometheus-monitoring-operator","text":"This document (000020733) is provided subject to the disclaimer at the end of this document.","title":"Reduce Memory and CPU footprint of Prometheus Monitoring Operator"},{"location":"000020733/#environment","text":"rancher-monitoring: 100.1.3+up19.0.3","title":"Environment"},{"location":"000020733/#situation","text":"On a fresh install of rancher-monitoring, the Prometheus monitoring operator consumes high CPU and Memory resources without any custom Prometheus CRDs configured. You would notice the following error messages on the Prometheus Operator Pod very frequently. $ kubectl logs -n cattle-monitoring-system rancher-monitoring-operator-784c69bc54-dvndg -f level=info ts=2022-07-26T09:34:55.46606005Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus level=info ts=2022-07-26T09:34:55.612269913Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager level=info ts=2022-07-26T09:34:55.694485011Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager level=info ts=2022-07-26T09:34:55.92009322Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus level=info ts=2022-07-26T09:34:59.042606472Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager level=info ts=2022-07-26T09:34:59.043983987Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus","title":"Situation"},{"location":"000020733/#resolution","text":"Add SecretListWatchSelector to reduce memory and CPU footprint. prometheusOperator: secretFieldSelector: \"type!=helm.sh/release.v1\"","title":"Resolution"},{"location":"000020733/#status","text":"Top Issue","title":"Status"},{"location":"000020733/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020737/","text":"How to set up Alertmanager configs in Monitoring V2 in Rancher This document (000020737) is provided subject to the disclaimer at the end of this document. Environment Before we get started: Admin permissions are needed Gather the required information for the alerting you'd like to set up Monitoring will need to be installed Please note that I will set up Slack alerts in this example To set up Slack, add and configure the Incoming webhooks app within your Slack environment Copy the Webhook URL and paste it into a blank notepad Situation Resolution Create an Opaque Secret in the cattle-monitoring-system namespace Click on Projects/Namespaces Select cattle-monitoring-system Then click on Secrets Then select Create Choose Opaque Secret Specify a key name (Insert name here for the secret) Then paste the Slack Webhook URL under Value Verify that the secret has been created successfully Next, select the Monitoring tab on the left side Select Alerting Then create a new AlertManagerConfig in the same namespace Add a name for this configuration Then select Create Once added, we will need to Edit Config From there, we can add a new Receiver (This is where we can specify which type of notifications to receive) For Slack, select Add Slack Under Secret with Slack Webhook URL , select the Secret name Then under the Key drop-down, we'll see the new, generated webhook secret key Next, specify the channel that the notifications will be sent to (Optional) specify a proxy if applicable Then select Create After creation, a Slack receiver will be shown, pointing to the webhook URL secret Next, edit the AlertManagerConfig again and point the base route of the config to the slack receiver Under the drop-down, our receiver will show up Select the Receiver Specify Groupings and Matchers and set different intervals here For faster testing, change the default interval times to quicker times like 5 seconds, 10 seconds, 1 minute, etc. Then click Save Under status, now we see the resulting config in the Alertmanager section in the Rancher UI Then, if everything has been set up correctly, a notification should be sent to the desired Slack channel Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to set up Alertmanager configs in Monitoring V2 in Rancher"},{"location":"000020737/#how-to-set-up-alertmanager-configs-in-monitoring-v2-in-rancher","text":"This document (000020737) is provided subject to the disclaimer at the end of this document.","title":"How to set up Alertmanager configs in Monitoring V2 in Rancher"},{"location":"000020737/#environment","text":"Before we get started: Admin permissions are needed Gather the required information for the alerting you'd like to set up Monitoring will need to be installed Please note that I will set up Slack alerts in this example To set up Slack, add and configure the Incoming webhooks app within your Slack environment Copy the Webhook URL and paste it into a blank notepad","title":"Environment"},{"location":"000020737/#situation","text":"","title":"Situation"},{"location":"000020737/#resolution","text":"Create an Opaque Secret in the cattle-monitoring-system namespace Click on Projects/Namespaces Select cattle-monitoring-system Then click on Secrets Then select Create Choose Opaque Secret Specify a key name (Insert name here for the secret) Then paste the Slack Webhook URL under Value Verify that the secret has been created successfully Next, select the Monitoring tab on the left side Select Alerting Then create a new AlertManagerConfig in the same namespace Add a name for this configuration Then select Create Once added, we will need to Edit Config From there, we can add a new Receiver (This is where we can specify which type of notifications to receive) For Slack, select Add Slack Under Secret with Slack Webhook URL , select the Secret name Then under the Key drop-down, we'll see the new, generated webhook secret key Next, specify the channel that the notifications will be sent to (Optional) specify a proxy if applicable Then select Create After creation, a Slack receiver will be shown, pointing to the webhook URL secret Next, edit the AlertManagerConfig again and point the base route of the config to the slack receiver Under the drop-down, our receiver will show up Select the Receiver Specify Groupings and Matchers and set different intervals here For faster testing, change the default interval times to quicker times like 5 seconds, 10 seconds, 1 minute, etc. Then click Save Under status, now we see the resulting config in the Alertmanager section in the Rancher UI Then, if everything has been set up correctly, a notification should be sent to the desired Slack channel","title":"Resolution"},{"location":"000020737/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020747/","text":"Prometheus Exporter not able to scrape filesystem metrics from nodes when SELinux is enabled. This document (000020747) is provided subject to the disclaimer at the end of this document. Environment Any Linux distribution with SELinux enabled would have this issue. Situation Using Prometheus Graphs, when users are trying to query filesystem metrics from nodes, they would not see any metrics due to SELinux being enabled. SELinux policies reject the query due to insufficient privileges. PromQL Query: node_filesystem_* Error on the Prometheus exporter pod: level=error ts=2022-07-21T16:13:08.502Z caller=collector.go:169 msg=\"collector failed\" name=filesystem duration_seconds=0.000837846 err=\"open /host/proc/1/mounts: permission denied\" Resolution This is not an issue with the Prometheus node exporter from the rancher monitoring chart but rather an issue from the SELinux side. There are three ways to verify that the SELinux team needs to be involved in fixing this issue. 1.) Disable SELinux, and the Prometheus node exporter should be able to start querying the node-level metrics. (NOT RECOMMENDED) 2.) If SELinux cannot be disabled, you can modify the helm chart and update the Prometheus-node-exporter section with the below seLinuxOption called spc_t , which gives container super-privileged access. (NOT RECOMMENDED) securityContext: seLinuxOptions: type: spc_t 3.) If super-privileged access cannot be provided to the container, ask the SELinux team to create necessary policies for processes to access the files that node-exporter pods are looking for. (RECOMMENDED APPROACH) Cause We notice this issue with SELinux enabled cluster nodes. Status Top Issue Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Prometheus Exporter not able to scrape filesystem metrics from nodes when SELinux is enabled."},{"location":"000020747/#prometheus-exporter-not-able-to-scrape-filesystem-metrics-from-nodes-when-selinux-is-enabled","text":"This document (000020747) is provided subject to the disclaimer at the end of this document.","title":"Prometheus Exporter not able to scrape filesystem metrics from nodes when SELinux is enabled."},{"location":"000020747/#environment","text":"Any Linux distribution with SELinux enabled would have this issue.","title":"Environment"},{"location":"000020747/#situation","text":"Using Prometheus Graphs, when users are trying to query filesystem metrics from nodes, they would not see any metrics due to SELinux being enabled. SELinux policies reject the query due to insufficient privileges. PromQL Query: node_filesystem_* Error on the Prometheus exporter pod: level=error ts=2022-07-21T16:13:08.502Z caller=collector.go:169 msg=\"collector failed\" name=filesystem duration_seconds=0.000837846 err=\"open /host/proc/1/mounts: permission denied\"","title":"Situation"},{"location":"000020747/#resolution","text":"This is not an issue with the Prometheus node exporter from the rancher monitoring chart but rather an issue from the SELinux side. There are three ways to verify that the SELinux team needs to be involved in fixing this issue. 1.) Disable SELinux, and the Prometheus node exporter should be able to start querying the node-level metrics. (NOT RECOMMENDED) 2.) If SELinux cannot be disabled, you can modify the helm chart and update the Prometheus-node-exporter section with the below seLinuxOption called spc_t , which gives container super-privileged access. (NOT RECOMMENDED) securityContext: seLinuxOptions: type: spc_t 3.) If super-privileged access cannot be provided to the container, ask the SELinux team to create necessary policies for processes to access the files that node-exporter pods are looking for. (RECOMMENDED APPROACH)","title":"Resolution"},{"location":"000020747/#cause","text":"We notice this issue with SELinux enabled cluster nodes.","title":"Cause"},{"location":"000020747/#status","text":"Top Issue","title":"Status"},{"location":"000020747/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020750/","text":"Customize Helm Chart values for RKE2 default addons This document (000020750) is provided subject to the disclaimer at the end of this document. Environment Rancher 2.6.x RKE2 cluster Situation By default, RKE2 installs multiple addons , including CoreDNS, Local-Storage, Nginx-Ingress, etc.: kubectl -n kube-system get addons Many of these addons are deployed from a Helm chart and represented within the cluster via a HelmChart custom resource : kubectl -n kube-system get helmchart These built-in addons deployed from a Helm chart can be customized with the use of a HelmChartConfig custom resource : kubectl -n kube-system get helmchartconfig This is where you can use the Helm chart values to change an addon's default installation. Resolution To edit the values of a Helm chart, you must find the currently installed version. To do this navigate to the RKE2 GitHub repository releases page to find the Packaged Component Versions (https://github.com/rancher/rke2/releases/) for the specific RKE release. For example, RKE2 1.23.10+rke2r1 uses ingress-nginx 4.1.0 (https://github.com/rancher/rke2/releases/tag/v1.23.10+rke2r1). Checking the values for this version of the ingress-nginx chart within the ingress-nginx GitHub repository you can determine the possible values (https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.1.0/charts/ingress-nginx/values.yaml). Thus, to add tolerations to the ingress-nginx controller, you can use the below manifest as an example. All of the values from the chart can be customised via this schema. --- apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-ingress-nginx namespace: kube-system spec: valuesContent: |- controller: tolerations: - key: \"key\" operator: \"Exists\" effect: \"NoSchedule\" After creating the HelmChartConfig manifest, you need to apply it via Rancher. To do so: 1. Navigate to Cluster Management . 2. On the selected cluster, click Edit Config . 3. Click on the Add-On Config tab and enter the manifest at the bottom in Additional Manifest . Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Customize Helm Chart values for RKE2 default addons"},{"location":"000020750/#customize-helm-chart-values-for-rke2-default-addons","text":"This document (000020750) is provided subject to the disclaimer at the end of this document.","title":"Customize Helm Chart values for RKE2 default addons"},{"location":"000020750/#environment","text":"Rancher 2.6.x RKE2 cluster","title":"Environment"},{"location":"000020750/#situation","text":"By default, RKE2 installs multiple addons , including CoreDNS, Local-Storage, Nginx-Ingress, etc.: kubectl -n kube-system get addons Many of these addons are deployed from a Helm chart and represented within the cluster via a HelmChart custom resource : kubectl -n kube-system get helmchart These built-in addons deployed from a Helm chart can be customized with the use of a HelmChartConfig custom resource : kubectl -n kube-system get helmchartconfig This is where you can use the Helm chart values to change an addon's default installation.","title":"Situation"},{"location":"000020750/#resolution","text":"To edit the values of a Helm chart, you must find the currently installed version. To do this navigate to the RKE2 GitHub repository releases page to find the Packaged Component Versions (https://github.com/rancher/rke2/releases/) for the specific RKE release. For example, RKE2 1.23.10+rke2r1 uses ingress-nginx 4.1.0 (https://github.com/rancher/rke2/releases/tag/v1.23.10+rke2r1). Checking the values for this version of the ingress-nginx chart within the ingress-nginx GitHub repository you can determine the possible values (https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.1.0/charts/ingress-nginx/values.yaml). Thus, to add tolerations to the ingress-nginx controller, you can use the below manifest as an example. All of the values from the chart can be customised via this schema. --- apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-ingress-nginx namespace: kube-system spec: valuesContent: |- controller: tolerations: - key: \"key\" operator: \"Exists\" effect: \"NoSchedule\" After creating the HelmChartConfig manifest, you need to apply it via Rancher. To do so: 1. Navigate to Cluster Management . 2. On the selected cluster, click Edit Config . 3. Click on the Add-On Config tab and enter the manifest at the bottom in Additional Manifest .","title":"Resolution"},{"location":"000020750/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020767/","text":"Downstream Cluster not Available with Websockets failing This document (000020767) is provided subject to the disclaimer at the end of this document. Environment Rancher 2.6.x AKS 1.22+ Situation After upgrading AKS to version 1.22+ users may experience a situation where the Downstream clusters show as unavailable on Rancher. Testing the Websocket using these instructions will show the following error: Bad Request {\"baseType\":\"error\",\"code\":\"ServerError\",\"message\":\"websocket: the client is not using the websocket protocol: 'upgrade' token not found in 'Connection' header\",\"status\":400,\"type\":\"error\"} Resolution Update the Kubernetes Ingress NGINX with the tag --set controller.watchIngressWithoutClass=true: helm upgrade --install \\ ingress-nginx ingress-nginx/ingress-nginx \\ --namespace ingress-nginx \\ --set controller.service.type=LoadBalancer \\ --version 4.0.18 \\ --create-namespace \\ --set controller.watchIngressWithoutClass=true Alternatively, on Rancher 2.6.7 onward, you can add the class name on the helm install/upgrade steps : --set ingress.ingressClassName=nginx Cause Kubernetes version 1.22 deprecated versions of the Ingress APIs in favor of the stable networking.k8s.io/v1 API. That leads to this scenario, where we update the controller.watchIngressWithoutClass tag. Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Downstream Cluster not Available with Websockets failing"},{"location":"000020767/#downstream-cluster-not-available-with-websockets-failing","text":"This document (000020767) is provided subject to the disclaimer at the end of this document.","title":"Downstream Cluster not Available with Websockets failing"},{"location":"000020767/#environment","text":"Rancher 2.6.x AKS 1.22+","title":"Environment"},{"location":"000020767/#situation","text":"After upgrading AKS to version 1.22+ users may experience a situation where the Downstream clusters show as unavailable on Rancher. Testing the Websocket using these instructions will show the following error: Bad Request {\"baseType\":\"error\",\"code\":\"ServerError\",\"message\":\"websocket: the client is not using the websocket protocol: 'upgrade' token not found in 'Connection' header\",\"status\":400,\"type\":\"error\"}","title":"Situation"},{"location":"000020767/#resolution","text":"Update the Kubernetes Ingress NGINX with the tag --set controller.watchIngressWithoutClass=true: helm upgrade --install \\ ingress-nginx ingress-nginx/ingress-nginx \\ --namespace ingress-nginx \\ --set controller.service.type=LoadBalancer \\ --version 4.0.18 \\ --create-namespace \\ --set controller.watchIngressWithoutClass=true Alternatively, on Rancher 2.6.7 onward, you can add the class name on the helm install/upgrade steps : --set ingress.ingressClassName=nginx","title":"Resolution"},{"location":"000020767/#cause","text":"Kubernetes version 1.22 deprecated versions of the Ingress APIs in favor of the stable networking.k8s.io/v1 API. That leads to this scenario, where we update the controller.watchIngressWithoutClass tag.","title":"Cause"},{"location":"000020767/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020771/","text":"How to create seperate ETCD and Controlplane nodes in RKE2 This document (000020771) is provided subject to the disclaimer at the end of this document. Environment RKE2 1.21.2 and higher Situation As part of a HA set-up, it may be required to run RKE2 with the ETCD database split from the Control plane nodes. Resolution 1. On the desired ETCD node create `/etc/rancher/rke2/config.yaml` with the following contents: disable-apiserver: true disable-controller-manager: true disable-kube-proxy: false disable-scheduler: true 2. On the etcd node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\" \" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` 3. On the controlplane node create `/etc/rancher/rke2/config.yaml` with the following contents: server: https://<ip of the etcd node>:9345 token: <token string from /var/lib/rancher/rke2/server/node-token on the etcd node> disable-etcd: true disable-kube-proxy: false etcd-expose-metrics: false 4. On the controlplane node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\" \" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` 5. Add agent nodes (https://docs.rke2.io/install/ha/#5-optional-join-agent-nodes). Additional Information This is only an example to show this configuration in a working state. In a prod environment, you should configure a fixed registration address per the documentation at https://docs.rke2.io/install/ha/#1-configure-the-fixed-registration-address Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to create seperate ETCD and Controlplane nodes in RKE2"},{"location":"000020771/#how-to-create-seperate-etcd-and-controlplane-nodes-in-rke2","text":"This document (000020771) is provided subject to the disclaimer at the end of this document.","title":"How to create seperate ETCD and Controlplane nodes in RKE2"},{"location":"000020771/#environment","text":"RKE2 1.21.2 and higher","title":"Environment"},{"location":"000020771/#situation","text":"As part of a HA set-up, it may be required to run RKE2 with the ETCD database split from the Control plane nodes.","title":"Situation"},{"location":"000020771/#resolution","text":"1. On the desired ETCD node create `/etc/rancher/rke2/config.yaml` with the following contents: disable-apiserver: true disable-controller-manager: true disable-kube-proxy: false disable-scheduler: true 2. On the etcd node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\" \" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` 3. On the controlplane node create `/etc/rancher/rke2/config.yaml` with the following contents: server: https://<ip of the etcd node>:9345 token: <token string from /var/lib/rancher/rke2/server/node-token on the etcd node> disable-etcd: true disable-kube-proxy: false etcd-expose-metrics: false 4. On the controlplane node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\" \" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` 5. Add agent nodes (https://docs.rke2.io/install/ha/#5-optional-join-agent-nodes).","title":"Resolution"},{"location":"000020771/#additional-information","text":"This is only an example to show this configuration in a working state. In a prod environment, you should configure a fixed registration address per the documentation at https://docs.rke2.io/install/ha/#1-configure-the-fixed-registration-address","title":"Additional Information"},{"location":"000020771/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020788/","text":"How to clean the orphaned cluster objects from the deleted cluster namespaces. This document (000020788) is provided subject to the disclaimer at the end of this document. Environment Rancher 2.6.x Situation In some cases there may be orphaned cluster objects left behind after the in-proper deletion of a downstream cluster in Rancher. These orphaned objects could introduce a condition that causes the leader Rancher pod to enter a CrashLoop state. Examples of errors from the Rancher pod logs. [ERROR] failed to call leader func: namespaces \"<strong>c-xxxxx</strong>\" not found fatal error: concurrent map read and map write [ERROR] error syncing \u2018<strong>c-xxxx/p-xxxx</strong>\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing [ERROR] error syncing \u2018<strong>c-xxxxx/p-xxxxx</strong>\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing [ERROR] error syncing \u2018<strong>c-xxxxx/p-xxxxx</strong>\u2019: handler cluster-registration-token: clusters.management.cattle.io \"<strong>c-xxxxx</strong>\" not found, requeuing Resolution Find the objects under the deleted cluster namespaces and manually delete each objects. Make sure there are no such orphaned objects or namespaces left in the local cluster. 1. Set a kubeconfig for the Rancher (local) management cluster to be used with the following steps 2. Verify the Active downstream clusters kubectl get clusters.management.cattle.io -o custom-columns=\"ID:.metadata.name,NAME:.spec.displayName,K8S_VERSION:.status.version.gitVersion,CREATED:.metadata.creationTimestamp,DELETED:.metadata.deletionTimestamp,LAST_READY:.status.conditions[?(@.type == 'Ready')].lastUpdateTime,READY:.status.conditions[?(@.type == 'Ready')].status\" --sort-by=.metadata.creationTimestamp 3. Cross verify with the Rancher pod logs to get the deleted downstream cluster namespace and collect the details. Compare with the active list of clusters versus the cluster namespaces. kubectl logs -n cattle-system -l app=rancher -c rancher kubectl get ns -A |grep \"c-\" 4. If there is a cluster that is stuck deleting, this may not complete. In this case, the finalizer object can be removed from the cluster.management.cattle.io object. Please note the c-xxxxx needs to be replaced with the cluster ID that is stuck deleting. kubectl patch clusters.management.cattle.io <strong>&lt;c-xxxxx&gt;</strong> -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge 5. If there is a namespace for a cluster that no longer exists, get the orphaned object details under the deleted cluster namespace. kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <strong>&lt;c-xxxxx&gt;</strong> 6. Do the cleanup of orphaned objects. Create the cluster namespace which is deleted, ignore if the cluster namespace is present kubectl create ns <strong>&lt;c-xxxxx&gt;</strong> Check the objects detected (in step 5) if desired, each object should have a deletion timestamp if a finalizer is preventing the object from being deleted. kubectl -n <strong>&lt;c-xxxxx&gt;</strong> get <resource type> <name of object> -o yaml Remove the finalizer to unblock the deletion of the objects. The command needs to be run for each object. kubectl -n <strong>&lt;c-xxxxx&gt;</strong> patch <strong>&lt;resource type&gt; &lt;name of object&gt;</strong> -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge Make sure there are no objects left in the namespace. kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <strong>&lt;c-xxxxx&gt;</strong> Finally, delete the namespace. kubectl delete ns <strong>&lt;c-xxxxx&gt;</strong> Cause It is important to delete downstream clusters in a process to allow Rancher to delete clusters and clean nodes that are in an Active state. Downstream cluster deletion is ideally performed from the Rancher UI / API, where nodes are available and able to be gracefully removed. For example, where possible do not terminate nodes in the infrastructure before the deletion is completed. Status Top Issue Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to clean the orphaned cluster objects from the deleted cluster namespaces."},{"location":"000020788/#how-to-clean-the-orphaned-cluster-objects-from-the-deleted-cluster-namespaces","text":"This document (000020788) is provided subject to the disclaimer at the end of this document.","title":"How to clean the orphaned cluster objects from the deleted cluster namespaces."},{"location":"000020788/#environment","text":"Rancher 2.6.x","title":"Environment"},{"location":"000020788/#situation","text":"In some cases there may be orphaned cluster objects left behind after the in-proper deletion of a downstream cluster in Rancher. These orphaned objects could introduce a condition that causes the leader Rancher pod to enter a CrashLoop state. Examples of errors from the Rancher pod logs. [ERROR] failed to call leader func: namespaces \"<strong>c-xxxxx</strong>\" not found fatal error: concurrent map read and map write [ERROR] error syncing \u2018<strong>c-xxxx/p-xxxx</strong>\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing [ERROR] error syncing \u2018<strong>c-xxxxx/p-xxxxx</strong>\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing [ERROR] error syncing \u2018<strong>c-xxxxx/p-xxxxx</strong>\u2019: handler cluster-registration-token: clusters.management.cattle.io \"<strong>c-xxxxx</strong>\" not found, requeuing","title":"Situation"},{"location":"000020788/#resolution","text":"Find the objects under the deleted cluster namespaces and manually delete each objects. Make sure there are no such orphaned objects or namespaces left in the local cluster. 1. Set a kubeconfig for the Rancher (local) management cluster to be used with the following steps 2. Verify the Active downstream clusters kubectl get clusters.management.cattle.io -o custom-columns=\"ID:.metadata.name,NAME:.spec.displayName,K8S_VERSION:.status.version.gitVersion,CREATED:.metadata.creationTimestamp,DELETED:.metadata.deletionTimestamp,LAST_READY:.status.conditions[?(@.type == 'Ready')].lastUpdateTime,READY:.status.conditions[?(@.type == 'Ready')].status\" --sort-by=.metadata.creationTimestamp 3. Cross verify with the Rancher pod logs to get the deleted downstream cluster namespace and collect the details. Compare with the active list of clusters versus the cluster namespaces. kubectl logs -n cattle-system -l app=rancher -c rancher kubectl get ns -A |grep \"c-\" 4. If there is a cluster that is stuck deleting, this may not complete. In this case, the finalizer object can be removed from the cluster.management.cattle.io object. Please note the c-xxxxx needs to be replaced with the cluster ID that is stuck deleting. kubectl patch clusters.management.cattle.io <strong>&lt;c-xxxxx&gt;</strong> -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge 5. If there is a namespace for a cluster that no longer exists, get the orphaned object details under the deleted cluster namespace. kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <strong>&lt;c-xxxxx&gt;</strong> 6. Do the cleanup of orphaned objects. Create the cluster namespace which is deleted, ignore if the cluster namespace is present kubectl create ns <strong>&lt;c-xxxxx&gt;</strong> Check the objects detected (in step 5) if desired, each object should have a deletion timestamp if a finalizer is preventing the object from being deleted. kubectl -n <strong>&lt;c-xxxxx&gt;</strong> get <resource type> <name of object> -o yaml Remove the finalizer to unblock the deletion of the objects. The command needs to be run for each object. kubectl -n <strong>&lt;c-xxxxx&gt;</strong> patch <strong>&lt;resource type&gt; &lt;name of object&gt;</strong> -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge Make sure there are no objects left in the namespace. kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <strong>&lt;c-xxxxx&gt;</strong> Finally, delete the namespace. kubectl delete ns <strong>&lt;c-xxxxx&gt;</strong>","title":"Resolution"},{"location":"000020788/#cause","text":"It is important to delete downstream clusters in a process to allow Rancher to delete clusters and clean nodes that are in an Active state. Downstream cluster deletion is ideally performed from the Rancher UI / API, where nodes are available and able to be gracefully removed. For example, where possible do not terminate nodes in the infrastructure before the deletion is completed.","title":"Cause"},{"location":"000020788/#status","text":"Top Issue","title":"Status"},{"location":"000020788/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020792/","text":"How to make a simple terraform API request This document (000020792) is provided subject to the disclaimer at the end of this document. Environment Latest version of Rancher 2.6.X Rancher Terraform Provider 1.24.1+ Latest Version of Terraform 1.3.1+ User with a token created via the API and proper permissions to the local Rancher cluster. Local directory for terraform plan files ( name.tf ) and a local terraform.tfstate file. Situation Sometimes it is necessary to create a basic skeleton for beginning a task, like using the Rancher2 Terraform Provider to speak with the Rancher API. This represents a starting point to choose a simple read-only task like \"query the cluster information for the local Rancher cluster\". Resolution Terraform commands are very easy, below are the main options one may typically use. terraform init -- download needed files like the rancher2 terraform provider terraform plan -- compare the environment to the state file, plan is like a diff of any changes to be made terraform apply -- apply the planned changes terraform refresh -- update the state to match remote systems terraform output -- show output values from the main.tf plan terraform destroy -- clean up anything created by terraform terraform fmt -- spacing is important in HCL, terraform's language, use this command to format all spacing in the current working directory To get started, create a directory to hold all of the files. Terraform will examine the local file or files, and then populate a local terraform.tfstate data file which represents the most recent refresh of the information from the Rancher API. The files below can be separate or all together in a main.tf file. Separating plan files into individual pieces can make managing a larger project easier. Terraform will take actions required using variables supplied by the user or admin, or computed during the \"apply\" operation. As a typical rule of thumb for any provider, \"data\" sources are read operations while \"resource\" operations are write/create/change. Upon running \"terraform apply\" with the main.tf file below, terraform will contact the Rancher API, authenticate, request the cluster_info for the local Rancher cluster with ID \"local\" and store it into the terraform statefile, as well as output to the screen. The comments explain a potential name for each file, the only requirement that it ends in \"tf\". ### tfvars.tf or environment.tf # these outline the url speaking to, and the authorization token variable \"api_url\" { description = \"rancher api url\" default = \"https://urlto.rancher-fqdn.com/v3\" } variable \"token_key\" { description = \"api key to use for tf\" default = \"token-nameid:jwt-long-hash-string\" } ### providers.tf # use the variables from the earlier section to define the provider provider \"rancher2\" { api_url = var.api_url token_key = var.token_key insecure = true } ### versions.tf # tell terraform what versions of providers and terraform itself, to expect terraform { required_providers { rancher2 = { source = \"rancher/rancher2\" version = \">= 1.24.1\" } } required_version = \">= 1.3.1\" } ### main.tf ## hard-coded example, read cluster info for local ## export with 'terraform output cluster_info' data \"rancher2_cluster\" \"local\" { name = \"local\" } output \"cluster_info\" { value = data.rancher2_cluster.local } Status Top Issue Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to make a simple terraform API request"},{"location":"000020792/#how-to-make-a-simple-terraform-api-request","text":"This document (000020792) is provided subject to the disclaimer at the end of this document.","title":"How to make a simple terraform API request"},{"location":"000020792/#environment","text":"Latest version of Rancher 2.6.X Rancher Terraform Provider 1.24.1+ Latest Version of Terraform 1.3.1+ User with a token created via the API and proper permissions to the local Rancher cluster. Local directory for terraform plan files ( name.tf ) and a local terraform.tfstate file.","title":"Environment"},{"location":"000020792/#situation","text":"Sometimes it is necessary to create a basic skeleton for beginning a task, like using the Rancher2 Terraform Provider to speak with the Rancher API. This represents a starting point to choose a simple read-only task like \"query the cluster information for the local Rancher cluster\".","title":"Situation"},{"location":"000020792/#resolution","text":"Terraform commands are very easy, below are the main options one may typically use. terraform init -- download needed files like the rancher2 terraform provider terraform plan -- compare the environment to the state file, plan is like a diff of any changes to be made terraform apply -- apply the planned changes terraform refresh -- update the state to match remote systems terraform output -- show output values from the main.tf plan terraform destroy -- clean up anything created by terraform terraform fmt -- spacing is important in HCL, terraform's language, use this command to format all spacing in the current working directory To get started, create a directory to hold all of the files. Terraform will examine the local file or files, and then populate a local terraform.tfstate data file which represents the most recent refresh of the information from the Rancher API. The files below can be separate or all together in a main.tf file. Separating plan files into individual pieces can make managing a larger project easier. Terraform will take actions required using variables supplied by the user or admin, or computed during the \"apply\" operation. As a typical rule of thumb for any provider, \"data\" sources are read operations while \"resource\" operations are write/create/change. Upon running \"terraform apply\" with the main.tf file below, terraform will contact the Rancher API, authenticate, request the cluster_info for the local Rancher cluster with ID \"local\" and store it into the terraform statefile, as well as output to the screen. The comments explain a potential name for each file, the only requirement that it ends in \"tf\". ### tfvars.tf or environment.tf # these outline the url speaking to, and the authorization token variable \"api_url\" { description = \"rancher api url\" default = \"https://urlto.rancher-fqdn.com/v3\" } variable \"token_key\" { description = \"api key to use for tf\" default = \"token-nameid:jwt-long-hash-string\" } ### providers.tf # use the variables from the earlier section to define the provider provider \"rancher2\" { api_url = var.api_url token_key = var.token_key insecure = true } ### versions.tf # tell terraform what versions of providers and terraform itself, to expect terraform { required_providers { rancher2 = { source = \"rancher/rancher2\" version = \">= 1.24.1\" } } required_version = \">= 1.3.1\" } ### main.tf ## hard-coded example, read cluster info for local ## export with 'terraform output cluster_info' data \"rancher2_cluster\" \"local\" { name = \"local\" } output \"cluster_info\" { value = data.rancher2_cluster.local }","title":"Resolution"},{"location":"000020792/#status","text":"Top Issue","title":"Status"},{"location":"000020792/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"},{"location":"000020831/","text":"How to troubleshoot Overlay Network Connectivity issues This document (000020831) is provided subject to the disclaimer at the end of this document. Situation pod-to-pod communication not happening Resolution Pod-to-Pod communication should depend on multiple factors. Mainly network communication should be allowed in between the nodes. The following checkpoints help us trace the problem's root cause. Check ports for their overlay are open between nodes (if they have multiple subnets/VLANs/DCs); testing from just one node to nodes in the other network should be good enough, for e.g., `nc -uvz 8472` (if they\u2019re using canal, change the port as needed).[https://rancher.com/docs/rancher/v2.6/en/installation/requirements/ports/#commonly-used-ports] Check the DNS from a test pod with suitable tools (not busybox, it has nslookup issues), `rancherlabs/swiss-army-knife` is good for this. `dig @ `, do this for all coredns pod IPs. -Use the same test pod to test their upstream nameservers (all 3, over a few retries), `dig @ ` [ https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/dns ] Note: In an air-gap environment, Swiss-army-knife is not available. You can try a specific busy box image with network tools like busybox image v1.28. Run the overlay test mentioned in the Rancher documentation to test pod-to-pod communication. Overlay network test steps test the pod to pod connectivity between the nodes :https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/networking#check-if-overlay-network-is-functioning-correctly. [Note: This overlay test performs the pod-to-pod communication using ICMP protocol, which means you will still see networking issues because TCP communication might be blocked even though the test passes. So you have to test with good network tools like NC and iperf.] Check the Infra VMS knowns issues and overlay network ports are allowed at the switch level. e.g., In case of Vmware vSphere version 6.7u2. Change the VXLAN port to 8472 (when NSX is not used) or 4789 (when NSX is used) Disable the VXLAN hardware offload feature on the VMXNET3 NIC (which recent Linux driver version enable by default. [https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer PR 2766401 , https://github.com/projectcalico/calico/issues/4727 ] Additional Information Reference Artiles& Links: https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer PR 2766401 https://github.com/projectcalico/calico/issues/4727 Disclaimer This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"How to troubleshoot Overlay Network Connectivity issues"},{"location":"000020831/#how-to-troubleshoot-overlay-network-connectivity-issues","text":"This document (000020831) is provided subject to the disclaimer at the end of this document.","title":"How to troubleshoot Overlay Network Connectivity issues"},{"location":"000020831/#situation","text":"pod-to-pod communication not happening","title":"Situation"},{"location":"000020831/#resolution","text":"Pod-to-Pod communication should depend on multiple factors. Mainly network communication should be allowed in between the nodes. The following checkpoints help us trace the problem's root cause. Check ports for their overlay are open between nodes (if they have multiple subnets/VLANs/DCs); testing from just one node to nodes in the other network should be good enough, for e.g., `nc -uvz 8472` (if they\u2019re using canal, change the port as needed).[https://rancher.com/docs/rancher/v2.6/en/installation/requirements/ports/#commonly-used-ports] Check the DNS from a test pod with suitable tools (not busybox, it has nslookup issues), `rancherlabs/swiss-army-knife` is good for this. `dig @ `, do this for all coredns pod IPs. -Use the same test pod to test their upstream nameservers (all 3, over a few retries), `dig @ ` [ https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/dns ] Note: In an air-gap environment, Swiss-army-knife is not available. You can try a specific busy box image with network tools like busybox image v1.28. Run the overlay test mentioned in the Rancher documentation to test pod-to-pod communication. Overlay network test steps test the pod to pod connectivity between the nodes :https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/networking#check-if-overlay-network-is-functioning-correctly. [Note: This overlay test performs the pod-to-pod communication using ICMP protocol, which means you will still see networking issues because TCP communication might be blocked even though the test passes. So you have to test with good network tools like NC and iperf.] Check the Infra VMS knowns issues and overlay network ports are allowed at the switch level. e.g., In case of Vmware vSphere version 6.7u2. Change the VXLAN port to 8472 (when NSX is not used) or 4789 (when NSX is used) Disable the VXLAN hardware offload feature on the VMXNET3 NIC (which recent Linux driver version enable by default. [https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer PR 2766401 , https://github.com/projectcalico/calico/issues/4727 ]","title":"Resolution"},{"location":"000020831/#additional-information","text":"Reference Artiles& Links: https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer PR 2766401 https://github.com/projectcalico/calico/issues/4727","title":"Additional Information"},{"location":"000020831/#disclaimer","text":"This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.","title":"Disclaimer"}]}