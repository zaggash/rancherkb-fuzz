{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rancher KB Alt website","text":"<p>This website provide another look and feel and a better search to the SUSE Rancher KB articles.</p>"},{"location":"kbs/000019957/","title":"Cluster controlplane error RHEL7 CentOS7 nodes","text":"<p>This document (000019957) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000019957/#environment","title":"Environment","text":""},{"location":"kbs/000019957/#situation","title":"Situation","text":"<p>Docker version 1.13.1.203 or newer stopped working on downstream cluster RHEL 7.9 nodes,</p> <p>Docker gets stalled and unresponsive, after setting the newer Kubernetes version in Rancher UI on the downstream cluster for upgrade of Kubernetes to\u00a0 v1.17.4 from v1.16.8 on RHEL 7.9</p> <p>Error message in Rancher UI:</p> <p>\"Cannot proceed with the upgrade of controlplain since 1 host(s) cannot reached prior to upgrade\"</p> <p>Rancher v2.4.11 or newer</p> <p>Node RHEL 7.9</p> <p>Docker version 1.13.1.203</p> <p>Kubernetes v1.16.8</p>"},{"location":"kbs/000019957/#resolution","title":"Resolution","text":"<p>- Downgrade docker to Version 1.13.1.161 (from Version 1.13.1.204 or 1.13.1.203).</p> <p>- Remove all files from /var/lib/docker/containers</p> <p>- Rejoining the nodes with the command line from Rancher UI</p> <p>The node will join the cluster and upgrade to the cluster set Kubernetes \u00a0v1.17.4</p> <p>on all nodes, one by one.</p>"},{"location":"kbs/000019957/#cause","title":"Cause","text":"<p>Docker not working</p>"},{"location":"kbs/000019957/#additional-information","title":"Additional Information","text":"<p>responsive</p>"},{"location":"kbs/000019957/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000019959/","title":"How can I configure NetworkManager to ignore Kubernetes Interfaces?","text":"<p>This document (000019959) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000019959/#situation","title":"Situation","text":""},{"location":"kbs/000019959/#question","title":"Question","text":"<p>How can I configure NetworkManager to ignore Kubernetes interfaces managed by a CNI like Calico?</p>"},{"location":"kbs/000019959/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster with nodes using NetworkManager</li> </ul>"},{"location":"kbs/000019959/#answer","title":"Answer","text":"<p>NetworkManager is a tool in some operating systems distributions to make the management of network interfaces easier. Kubernetes will typically take the provisioned interfaces and create other virtual interfaces via the Container Network Interface (CNI). While NetworkManager may make the operation and configuration of the default interfaces easier, it can interfere with the Kubernetes management, and create problems when troubleshooting.</p> <p>Because many modern Linux and other Unix systems have NetworkManager enabled by default, there is a configuration available to have it avoid or ignore the Kubernetes interfaces.</p> <p>This example assumes the Calico CNI is used.</p> <ol> <li>Create the following file, if it does not already exist.</li> </ol> <pre><code>/etc/NetworkManager/conf.d/calico.conf\n</code></pre> <ol> <li>Give it the following content:</li> </ol> <pre><code>[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico\n</code></pre> <ol> <li>Restart the NetworkManager service, if necessary.</li> </ol> <pre><code>sudo systemctl restart NetworkManager\n</code></pre> <p>Other options are available, for example, completely disabling NetworkManager, but this may not be viable in all use-cases. If other CNI interfaces are required, create another file or add to the list above, for example <code>flannel*</code>.</p> <p>Add the configuration to any automation or images used in node provisioning wherever NetworkManager may be used.</p>"},{"location":"kbs/000019959/#further-reading","title":"Further Reading","text":"<ul> <li>Calico - Configure NetworkManager</li> <li>RKE2 - NetworkManager</li> </ul>"},{"location":"kbs/000019959/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020001/","title":"Can I configure tolerations for Alertnmanager with monitoring v1?","text":"<p>This document (000020001) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020001/#situation","title":"Situation","text":""},{"location":"kbs/000020001/#question","title":"Question","text":"<p>Is it possible to configure tolerations or selectors for the Alertmanager container deployed by Rancher monitoring v1 so that the Alertmanager pod will deploy to a node of choice?</p>"},{"location":"kbs/000020001/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Monitoring v1</li> </ul>"},{"location":"kbs/000020001/#answer","title":"Answer","text":"<p>It is not possible to configure selectors or tolerations for AlertManager in Monitoring v1 as it is deployed independently of the built-in monitoring functionality.</p> <p>Due to the limitations in Monitoring v1 and the maturation of the upstream prometheus-operator chart, Monitoring v2 now uses the upstream prometheus-operator helm chart. This means that you can now configure selectors, tolerations, and more options that were not available in v1. We advise upgrading/migrating to v2 if you wish to make use of this functionality.</p>"},{"location":"kbs/000020001/#further-reading","title":"Further Reading","text":"<ul> <li>Alertmanager does not honor node selector gh#24524</li> <li>prometheus-operator Helm chart</li> </ul>"},{"location":"kbs/000020001/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020002/","title":"Pods return to the Initializing status even though containers in the pod are running","text":"<p>This document (000020002) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020002/#situation","title":"Situation","text":""},{"location":"kbs/000020002/#issue","title":"Issue","text":"<p>After successfully starting pods return to the 'Initializing status'. When reviewed an init container is failing to start, even though other containers are running. If one of the running containers fails it will not be restarted due to the failed init container. This can be seen in the following example: </p> <p>Init container is failing to start, even though other containers are running: </p>"},{"location":"kbs/000020002/#workaround","title":"Workaround","text":""},{"location":"kbs/000020002/#isssue-background","title":"Isssue background","text":"<p>This happens when the init container is removed from the host matchine. Because init containers have run to complettion, and are terminated, this can happen when a <code>docker system prune</code> or <code>docker container prune</code> is run on the node. When the kubelet sees that the init container in no longer exists it will try and rerun it. Depending on the init container operation this may fail on a pod that is already running (e.g linkerd).</p>"},{"location":"kbs/000020002/#avoiding-the-issue","title":"Avoiding the issue","text":"<p>Where possible, manually pruning images on Kubernetes nodes should be avoided. The kubelet has a built in image cleanup mechanism to remove unused containers and images. Where it's not possible to avoid manual clean up, init containers that are stopped should not be removed. A list of init container IDs can be generated with the following command:</p> <pre><code>kubectl get pods --all-namespaces -o jsonpath='{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' | cut -d/ -f3\n</code></pre> <p>The below script can be used to generate a list of containers to clean on a remote node, e.g.</p> <pre><code>NODE_TO_CLEAN=&lt;node_ip&gt;\nUSER=&lt;user&gt;\n\nINIT_CONTAINERS=$(kubectl get pods --all-namespaces -o jsonpath='{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' | cut -d/ -f3)\nTERMED_PODS=$(ssh -o LogLevel=QUIET -t ${USER}@${NODE_TO_CLEAN} sudo docker ps -qa --filter status=exited --no-trunc | sed -e 's/\\r//g')\n\nCONTAINERS_TO_REMOVE=$(comm -23 &lt;(echo $TERMED_PODS | sort) &lt;(echo $INIT_CONTAINERS | sort) )\nPASS_CONTAINERS=$(typeset -p CONTAINERS_TO_REMOVE)\n\nssh -o LogLevel=QUIET -t ${USER}@${NODE_TO_CLEAN} bash &lt;&lt;EOF\n    $PASS_CONTAINERS\n    sudo docker rm $(echo \"\\${CONTAINERS_TO_REMOVE}\") &amp;&amp; sudo docker image prune -af\nEOF\n</code></pre>"},{"location":"kbs/000020002/#resolution","title":"Resolution","text":"<p>There is currently no long term resolution.</p>"},{"location":"kbs/000020002/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020003/","title":"Overlay connectivity broken after a node reboot until flannel is restarted","text":"<p>This document (000020003) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020003/#situation","title":"Situation","text":""},{"location":"kbs/000020003/#issue","title":"Issue","text":"<p>After rebooting a Kubernetes node, you may notice that pod to pod network connectivity(via the overlay network) does not function correctly until you restart the canal workload on that node in Kubernetes.</p>"},{"location":"kbs/000020003/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes cluster running canal or flannel as the CNI</li> <li>Linux nodes running Systemd v242 or higher</li> </ul>"},{"location":"kbs/000020003/#root-cause","title":"Root Cause","text":"<p>This is caused by a race condition between flannel and systemd-networkd that is being tracked in this upstream issue.</p> <p>This doesn't appear to affect Ubuntu 20.04, due to it's use of netplan to manage networking configuration.</p>"},{"location":"kbs/000020003/#workaround","title":"Workaround","text":"<p>Either restart canal on the node ( <code>kubectl delete pod -n kube-system canal-XXXX</code>) as needed or change the MACAddressPolicy for the flannel interfaces on your nodes to none:</p> <pre><code>cat&lt;&lt;'EOF'&gt;/etc/systemd/network/10-flannel.link\n[Match]\nOriginalName=flannel*\n\n[Link]\nMACAddressPolicy=none\nEOF\n</code></pre>"},{"location":"kbs/000020003/#resolution","title":"Resolution","text":"<p>At present there is no resolution and this bug is still open upstream.</p>"},{"location":"kbs/000020003/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020004/","title":"I have the alert 'Node Disk is Running Full in 24 hours', what does this mean?","text":"<p>This document (000020004) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020004/#situation","title":"Situation","text":""},{"location":"kbs/000020004/#issue","title":"Issue","text":"<p>When using Rancher Alerting you may see the Alert 'Node Disk is running full in 24 Hours', however you may not understand what this means and what the impact of this is.</p>"},{"location":"kbs/000020004/#background","title":"Background","text":"<p>The alert is an early warning for potentially more serious issues with the disk space on a node becoming full.</p> <p>It uses the following equation:</p> <p><code>predict_linear(node_filesystem_free_bytes{mountpoint!~\"^/etc/(?:resolv.conf|hosts|hostname)$\"}[6h], 3600 * 24) &lt; 0</code></p> <p>This equation measures the rate at which the disk is losing available space and based on the trend from the last 6 hours will warn you that, at that rate, in 24 hours the disk will have no available space at all.</p> <p>It is important to make sure that you investigate the affected node for any potential issues with excessive logging, under-provisioned disk space or perhaps something else that could cause such a situation.</p>"},{"location":"kbs/000020004/#impact","title":"Impact","text":"<p>If the Node does run out of disk space, this will cause a 'Disk Pressure' event on the node.</p> <p>When a Node experiences Disk Pressure it will evict all running containers and then become unschedulable until the requisite disk space is made.</p> <p>This is a serious situation, especially if the cause of the failure is a rogue container over-logging.</p> <p>If an over-logging or failing workload is forced to reschedule on another node they may all end up becoming unschedulable as the issue will follow the workload when it is rescheduled.</p> <p>In a typical Kubernetes installation Disk Pressure is caused by available space being less than 10%.</p>"},{"location":"kbs/000020004/#investigative-steps","title":"Investigative Steps","text":"<p>The cause of this issue can vary heavily across different environments, but as the alert is node specific you should start there.</p> <p>The first place to investigate is a <code>df -h</code>, this will show you the percentage of disk space that is filled on your node, you may be able to identify immediately a place where disk space is no longer available.</p> <p>This is the fastest way to assess more urgent issues and once you've identified the disk that may be reporting as nearly full you can immediately take precautions, such as clearing out old log files or increasing the size of a disk.</p> <p>As you use Rancher Monitoring you can also look at Node-specific statistics graphed over time and make an assessment on when the issue began, compared to running workloads and other node logs.</p> <p>During operations seeing an increase in either logging or a gradual increase in storage used is often expected.</p> <p>For example, if the alert is sounding because a specific workload encountered issues, logged more but then recovered and cleaned up the logs; you may no longer have an issue on your hands but could consider reducing down the logging of the workload to prevent further alerts.</p> <p>The 'Node Disk is running full in 24 Hours' is a preemptive alert that will always require investigation and understanding to ensure the best operational health of your Nodes and Clusters.</p>"},{"location":"kbs/000020004/#short-term-solutions","title":"Short-Term Solutions","text":"<p>If you can identify the reason for this alert, the solution should hopefully be more straight-forward. It may be you need to delete some old files on a Node, or you may need to reduce logging; for example, if debug logging is running.</p> <p>If you do hit a Disk Pressure event and you need to recover you need to access the node directly and reduce the amount of space taken manually, when requisite space is made you should either restart the Node or Docker on the Node to mark it as schedulable in Kubernetes again.</p>"},{"location":"kbs/000020004/#long-term-solutions","title":"Long-Term Solutions","text":"<p>There are different solutions that will mitigate this alert:</p> <ol> <li>Having larger disks</li> </ol> <p>While Rancher does not have specific requirements for disk space on a Node, it would be recommended to at least have 30GB or more to better mitigate this alerts occurrence.</p> <ol> <li>Exporting container logs</li> </ol> <p>Rancher provides a logging deployment you can configure to export your container logs, for example, to an in-house Elasticsearch Cluster.</p> <p>While these logs will be buffered locally they will then be exported remotely, thereby reducing the amount of accumulated logs over time.</p> <ol> <li>Running regular cleanups on System Logs</li> </ol> <p>While out-of-scope of Rancher, a large of amount of system logs can contribute to this alert sounding, it is encouraged to manage logging at an OS Level with either logrotate or by exporting logs.</p>"},{"location":"kbs/000020004/#further-reading","title":"Further reading","text":"<p>Kubernetes Out of Resource Handling Documentation: https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/</p>"},{"location":"kbs/000020004/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020005/","title":"Bind on Port 80 Fails Due to Permissions in NGINX Ingress","text":"<p>This document (000020005) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020005/#situation","title":"Situation","text":""},{"location":"kbs/000020005/#issue","title":"Issue","text":"<p>Sometimes an admin may see an error in the logs like the one below:</p> <pre><code>nginx: [emerg] bind() to 0.0.0.0:80 failed (13: Permission denied)\n</code></pre>"},{"location":"kbs/000020005/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020005/#workaround","title":"Workaround","text":"<p>Remove the ingress container image from the node(s), using the following commands:</p> <pre><code># Find the name of the nginx-ingress-controller pod\nNGINX_INGRESS_CONTROLLER=\"$(kubectl get pods -n ingress-nginx | awk '/nginx-ingress-controller/ { print $1 }')\"\n\n# Remove nginx-ingress-controller pod and then clean up the container image\ndocker rm -f \"${NGINX_INGRESS_CONTROLLER}\" &amp;&amp; \\\ndocker system prune -af\n</code></pre>"},{"location":"kbs/000020005/#resolution","title":"Resolution","text":"<p>According to kubernetes/ingress-nginx GitHub Issue #3858, this is caused by a capabilities failure on one of the layers of the nginx-ingress-controller image, due to the xattrs not being copied correctly.</p>"},{"location":"kbs/000020005/#further-reading","title":"Further reading","text":"<ul> <li>OCI Image Format Specification: Image Layer Filesystem Changeset</li> <li>xattr(7) - Linux manual page</li> </ul>"},{"location":"kbs/000020005/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020006/","title":"Controlplane/Etcd nodes are stuck at provisioning for a new cluster","text":"<p>This document (000020006) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020006/#situation","title":"Situation","text":""},{"location":"kbs/000020006/#issue","title":"Issue","text":"<p>When creating a new cluster, the controlplane and etcd nodes are stuck with a Provisioning status</p>"},{"location":"kbs/000020006/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher version &gt;= v2.5.6</li> <li>A cluster created in Rancher using either Existing nodes or with RKE and new nodes in an infrastructure provider</li> <li>No nodes have been added with the worker role</li> </ul>"},{"location":"kbs/000020006/#resolution","title":"Resolution","text":"<p>As of Rancher v2.5.6, new RKE clusters require at least one node with the <code>worker</code> or <code>all</code> roles to begin provisioning. The reason for this is that an RKE cluster requires a worker node to host the Rancher Cluster Agent, DNS, Metrics Service, and the Ingress Controller. Without these workloads a cluster will not provision correctly.</p>"},{"location":"kbs/000020006/#further-reading","title":"Further reading","text":"<ul> <li>Rancher v2.5 provisioned Kubernetes clusters, without a worker role node, display \"Cluster health check failed: cluster agent is not ready\" error</li> </ul>"},{"location":"kbs/000020006/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020007/","title":"Rancher Kubernetes Engine\uff08RKE\uff09\u306eCLI\u307e\u305f\u306fRancher v2.x\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305fKubernetes\u30af\u30e9\u30b9\u30bf\u3067CoreDNS ConfigMap\u3092\u66f4\u65b0\u3059\u308b\u65b9\u6cd5","text":"<p>This document (000020007) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020007/#situation","title":"Situation","text":""},{"location":"kbs/000020007/#_1","title":"\u80cc\u666f","text":"<p>\u4f8b\u3048\u3070\u3001 query logging \u3084update the resolver policy\u3092\u6709\u52b9\u306b\u3059\u308b\u305f\u3081\u306b\u3001kube-system Namespace\u306ecoredns ConfigMap\u3067\u5b9a\u7fa9\u3055\u308c\u305fCoreDNS\u306eCorefile\u69cb\u6210\u3092\u66f4\u65b0\u3057\u305f\u3044\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001\u3053\u306eConfigMap\u3092\u66f4\u65b0\u3057\u3001Rancher Kubernetes Engine (RKE) CLI\u307e\u305f\u306fRancher v2.x\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u3067\u5909\u66f4\u3092\u6c38\u7d9a\u5316\u3059\u308b\u65b9\u6cd5\u3092\u8a73\u7d30\u306b\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020007/#_2","title":"\u4e8b\u524d\u6e96\u5099","text":"<ul> <li>CoreDNS dns addon \u3092\u4f7f\u7528\u3057\u3066\u3001Rancher Kubernetes Engine\uff08RKE\uff09CLI\u307e\u305f\u306fRancher v2.x\u306b\u3088\u3063\u3066\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305fKubernetes\u30af\u30e9\u30b9\u30bf\u3002</li> <li>global admin\u307e\u305f\u306fcluster owner\u306e\u30e6\u30fc\u30b6\u30fc\u3068\u3057\u305fkubeconfig\u306b\u3088\u308b\u30af\u30e9\u30b9\u30bf\u30fc\u3078\u306ekubectl\u30a2\u30af\u30bb\u30b9\u3002</li> </ul>"},{"location":"kbs/000020007/#steps","title":"Steps","text":"<ol> <li>\u6b21\u306e <code>kubectl</code> \u30b3\u30de\u30f3\u30c9\u3067\u3001\u73fe\u5728\u306e coredns ConfigMap \u5b9a\u7fa9\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</li> </ol> <pre><code>kubectl -n kube-system get configmap coredns -o go-template={{.data.Corefile}}\n</code></pre> <p>\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5185\u5bb9\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u3002</p> <pre><code>.:53 {\n       errors\n       health\n       ready\n       kubernetes cluster.local in-addr.arpa ip6.arpa {\n         pods insecure\n         fallthrough in-addr.arpa ip6.arpa\n       }\n       prometheus :9153\n       forward . \"/etc/resolv.conf\" {\n         policy random\n       }\n       cache 30\n       loop\n       reload\n       loadbalance\n}\n</code></pre> <ol> <li>\u30af\u30e9\u30b9\u30bf\u69cb\u6210\u306eYAML\u3092\u7de8\u96c6\u3057\u3066\u3001\u5fc5\u8981\u306a\u5909\u66f4\u3092\u52a0\u3048\u305fcoredns ConfigMap\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 RKE \u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5834\u5408\u306f\u3001\u3053\u306e\u5185\u5bb9\u3092 <code>cluster.yml</code> \u30d5\u30a1\u30a4\u30eb\u306b\u8ffd\u52a0\u3057\u307e\u3059\u3002 Rancher\u3067\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5834\u5408\u3001Rancher UI\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u30d3\u30e5\u30fc\u306b\u79fb\u52d5\u3057\u3001\u30af\u30e9\u30b9\u30bf\u30fc\u30d3\u30e5\u30fc\u306e\u7de8\u96c6\u3092\u958b\u304d\u3001 <code>Edit as YAML</code> \u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002</li> </ol> <p>\u4ee5\u4e0b\u306e\u5185\u5bb9\u3067\u30a2\u30c9\u30aa\u30f3\u3092\u4f5c\u6210\u3057\u3001Corefile\u306e\u5b9a\u7fa9\u3092\u624b\u98061\u3067\u53d6\u5f97\u3057\u305f\u65e2\u5b58\u306e\u69cb\u6210\u306b\u7f6e\u304d\u63db\u3048\u307e\u3059\u3002\u305d\u306e\u5f8c\u3001\u5fc5\u8981\u306a\u5909\u66f4\u3092\u52a0\u3048\u307e\u3059\u3002\u3053\u306e\u4f8b\u3067\u306f\u3001resolver policy\u3092\u65e2\u5b58\u306erandom\u304b\u3089sequental\u306b\u66f4\u65b0\u3057\u307e\u3059\u3002</p> <p><code>yaml addons: |- --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . \"/etc/resolv.conf\" { policy sequential } cache 30 loop reload loadbalance }</code></p> <ol> <li>\u65b0\u3057\u3044\u69cb\u6210\u3067\u30af\u30e9\u30b9\u30bf\u30fc\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002 RKE\u306e\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5834\u5408\u3001 <code>rke up --cluster.yml</code> \u3092\u5b9f\u884c\u3057\u307e\u3059( <code>rke up</code> \u3092\u5b9f\u884c\u3059\u308b\u969b\u306b\u306f\u3001\u4f5c\u696d\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306bcluster.rkestate\u30d5\u30a1\u30a4\u30eb\u304c\u5b58\u5728\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044)\u3002 Rancher\u3067\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5834\u5408\u306f\u3001Rancher UI\u306e <code>Edit as YAML</code> \u30d3\u30e5\u30fc\u3067 <code>Save</code> \u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002</li> </ol>"},{"location":"kbs/000020007/#_3","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>How to update CoreDNS's resolver policy</li> </ul>"},{"location":"kbs/000020007/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020008/","title":"How to send logs to Amazon Web Services (AWS) CloudWatch with the new logging services available on Rancher v2.5.x","text":"<p>This document (000020008) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020008/#situation","title":"Situation","text":""},{"location":"kbs/000020008/#task","title":"Task","text":"<p>New logging service introduced in Rancher v2.5.x allows users to send logs to Amazon Web Services (AWS) Cloudwatch. This article details how to send logs to AWS CloudWatch with the new logging services available on Rancher v2.5.x, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.5.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020008/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with Logging in Rancher 2.5 enabled</li> <li>Rancher v2.5.x</li> <li>AWS IAM policy with at least the following permissions, the policy is attached to either an IAM user with credentials, or an EC2 instance profile which is attached to the nodes in the cluster</li> </ul> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": [\n\"logs:PutLogEvents\",\n\"logs:CreateLogGroup\",\n\"logs:PutRetentionPolicy\",\n\"logs:CreateLogStream\",\n\"logs:DescribeLogGroups\",\n\"logs:DescribeLogStreams\"\n],\n\"Effect\": \"Allow\",\n\"Resource\": \"*\"\n}\n]\n}\n</code></pre>"},{"location":"kbs/000020008/#steps","title":"Steps","text":"<ol> <li>Ensure the Rancher v2.5 logging is enabled on the cluster; visit and follow the Logging section in the Rancher docs if it is not already enabled.</li> <li>Optional Create a secret containing the AWS Access key ID and Secret access key in <code>cattle-logging-system</code> namespace:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\ndata:\nid: &lt;AWS Access key ID&gt;\nsecret: &lt;AWS Secret access key&gt;\nkind: Secret\nmetadata:\nname: aws\nnamespace: cattle-logging-system\ntype: Opaque\nEOF\n</code></pre> <pre><code>\\&gt; Note, this step is not required if using an EC2 instance profile\n</code></pre> <ol> <li>Create the ClusterOutput and ClusterFlow to forward the logs to the CloudWatch.</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterOutput\nmetadata:\nname: cloudwatch\nnamespace: cattle-logging-system\nspec:\ncloudwatch:\nauto_create_stream: true  #Set to false to disable automatically create Log Stream under the Log Group\naws_key_id:\nvalueFrom:\nsecretKeyRef:\nkey: id\nname: aws\naws_sec_key:\nvalueFrom:\nsecretKeyRef:\nkey: secret\nname: aws\nbuffer:\ntimekey: 30s\ntimekey_use_utc: true\ntimekey_wait: 30s\nlog_group_name: &lt;LOG GROUP NAME ON THE CLOUDWATCH&gt;\nlog_stream_name: &lt;LOG STREAM NAME UNDER THE LOG GROUP&gt;\nregion: &lt;AWS REGION&gt;\n---\napiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterFlow\nmetadata:\nname: logging\nnamespace: cattle-logging-system\nspec:\nglobalOutputRefs:\n- cloudwatch\nEOF\n</code></pre> <p>&gt; Note, the <code>aws_key_id</code> and <code>aws_sec_key</code> should be removed if using an EC2 instance profile 4. The logs will start sending to the CloudWatch once the ClusterOuput and ClusterFlow are created. You may visit the docs in the below to explore all the available configurations for the Rancher v2.5 logging.</p>"},{"location":"kbs/000020008/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Logging Documentation</li> <li>Full Configuration of CloudWatch output plugin for Fluentd</li> </ul>"},{"location":"kbs/000020008/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020009/","title":"How to enable HA for Alertmanager in Rancher lanched Monitoring","text":"<p>This document (000020009) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020009/#situation","title":"Situation","text":""},{"location":"kbs/000020009/#task","title":"Task","text":"<p>To increase the replica count of the alertmanager pod and enable HA so that if the node with the <code>Alertmanager</code> pod dies, alerts will still function.</p> <p>Increasing the replica count in the chart is the only change required as alerts are sent to ALL the alertmanager pods. They communicate to each other which alerts have already been dispatched, so they are not duplicated.</p>"},{"location":"kbs/000020009/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.5.x</li> <li>Monitoring deployed via Rancher Apps &amp; Marketplace</li> </ul>"},{"location":"kbs/000020009/#resolution","title":"Resolution","text":"<p>You can enable HA alertmanager by increasing the replica count in the chart.</p> <ol> <li>Go to Monitoring Chart in the Apps Marketplace</li> </ol> <p> 2. Under Chart Options, click on the Edit as YAML button</p> <p> 3. In the alertmanager block, modify replicas to your preference. Here it is set to 2</p> <p> 4. Click on Upgrade button to apply changes</p> <p></p>"},{"location":"kbs/000020009/#further-reading","title":"Further reading","text":"<ul> <li>Alerting chart user-guide</li> </ul>"},{"location":"kbs/000020009/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020010/","title":"Rancher v2.2.x - v2.4.x\u3067\u30af\u30e9\u30b9\u30bf\u3068\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306eMonitoring\u6a5f\u80fd\u306e\u4f7f\u7528\u72b6\u6cc1\u5206\u6790\u30ec\u30dd\u30fc\u30c8\u306e\u5916\u90e8\u9001\u4fe1\u3092\u7121\u52b9\u306b\u3059\u308b\u65b9\u6cd5","text":"<p>This document (000020010) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020010/#situation","title":"Situation","text":""},{"location":"kbs/000020010/#_1","title":"\u30bf\u30b9\u30af","text":"<p>\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u3001Grafana\u306f\u4f7f\u7528\u72b6\u6cc1\u306e\u5206\u6790\u3092stats.grafana.org\u306b\u9001\u4fe1\u3057\u307e\u3059\u3002\u30a8\u30a2\u30ae\u30e3\u30c3\u30d7\u307e\u305f\u306f\u30d7\u30ed\u30ad\u30b7\u7d4c\u7531\u306e\u74b0\u5883\u3067\u306f\u3001\u6b21\u306e\u4f8b\u306e\u3088\u3046\u306b\u3001\u30af\u30e9\u30b9\u30bf\u3068\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76e3\u8996\u306eGrafana pod\u30ed\u30b0\u306b\u63a5\u7d9a\u30bf\u30a4\u30e0\u30a2\u30a6\u30c8\u3084\u30d7\u30ed\u30ad\u30b7\u8a3c\u660e\u66f8\u306e\u30a8\u30e9\u30fc\u304c\u591a\u6570\u767a\u751f\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002</p> <pre><code>lvl=eror msg=\"Failed to send usage stats\" logger=metrics err=\"Post https://stats.grafana.org/grafana-usage-report: x509: certificate signed by unknown authority\"\n</code></pre> <p>\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001Rancher v2.2.2.x - v2.4.x \u306e\u30af\u30e9\u30b9\u30bf\u304a\u3088\u3073\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u76e3\u8996\u306e Grafana \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u3001\u3053\u306e\u4f7f\u7528\u72b6\u6cc1\u5206\u6790\u30ec\u30dd\u30fc\u30c8\u3092\u7121\u52b9\u306b\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020010/#_2","title":"\u4e8b\u524d\u6e96\u5099","text":"<ul> <li>Rancher v2.2.x - v2.4.x</li> <li>\u30af\u30e9\u30b9\u30bf\u307e\u305f\u306f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306emonitoring\u6a5f\u80fd\u304c\u6709\u52b9(enabled)</li> </ul>"},{"location":"kbs/000020010/#_3","title":"\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3","text":"<ol> <li>Monitoring\u6a5f\u80fd\u304c\u6709\u52b9\u306b\u3057\u305f\u30af\u30e9\u30b9\u30bf\u307e\u305f\u306f\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30da\u30fc\u30b8\u306b\u79fb\u52d5\u3057\u307e\u3059\u3002</li> <li>\u4e0a\u90e8\u30e1\u30cb\u30e5\u30fc\u30d0\u30fc\u306e\u300c\u30c4\u30fc\u30eb\u300d\u2192\u300cMonitoring\u300d\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002</li> <li>\u30da\u30fc\u30b8\u4e0b\u90e8\u306e\u300c\u8a73\u7d30\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u8868\u793a\u300d\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3068\u3001\u300c\u30a2\u30f3\u30b5\u30fc\u300d\u6b04\u304c\u8868\u793a\u3055\u308c\u3001\u300c\u30a2\u30f3\u30b5\u30fc\u3092\u8ffd\u52a0\u300d\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u4ee5\u4e0b\u306e2\u3064\u306e\u5909\u6570\u3068\u5024\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002</li> </ol> <pre><code>grafana.extraVars[0].name=GF_ANALYTICS_REPORTING_ENABLED\ngrafana.extraVars[0].value='false'\n</code></pre> <ol> <li> <p>\u300c\u4fdd\u5b58\u300d\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002</p> </li> <li> <p>\u3053\u306e\u5909\u66f4\u306fGrafana Pod\u306e\u30ed\u30b0\u304b\u3089\u78ba\u8a8d\u3067\u304d\u307e\u3059\u3002\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u6642\u306e\u30ed\u30b0\u306e\u5148\u982d\u4ed8\u8fd1\u306b\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30ed\u30b0\u304c\u8868\u793a\u3055\u308c\u308b\u306f\u305a\u3067\u3059\u3002</p> </li> </ol> <pre><code>lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_ANALYTICS_REPORTING_ENABLED='false'\"\n</code></pre>"},{"location":"kbs/000020010/#_4","title":"\u53c2\u8003\u8cc7\u6599","text":"<ul> <li>Rancher Cluster Monitoring documentation</li> <li>Grafana analytics configuration documentation</li> </ul>"},{"location":"kbs/000020010/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020011/","title":"How to check all ingresses from the NGINX ingress controller to all backend pods","text":"<p>This document (000020011) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020011/#situation","title":"Situation","text":""},{"location":"kbs/000020011/#task","title":"Task","text":"<p>This script is designed to walk through all the ingresses in a cluster and test that it can curl the backend pods from the NGINX pods. This is mainly done to verify the overlay network is working along with checking the overall configuration.</p>"},{"location":"kbs/000020011/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>kubectl access to the cluster</li> </ul>"},{"location":"kbs/000020011/#run-script","title":"Run script","text":"<pre><code>curl https://raw.githubusercontent.com/rancherlabs/support-tools/master/NGINX-to-pods-check/check.sh | bash\n</code></pre>"},{"location":"kbs/000020011/#example-output","title":"Example output","text":""},{"location":"kbs/000020011/#broken-pod","title":"Broken pod","text":"<pre><code>bash ./check.sh -F Table\n####################################################\nPod: webserver-bad-85cf9ccdf8-8v4mh\nPodIP: 10.42.0.252\nPort: 80\nEndpoint: ingress-1d8af467b8b7c9682fda18c8d5053db7\nIngress: test-bad\nIngress Pod: nginx-ingress-controller-b2s2d\nNode: a1ubphylbp01\nStatus: Fail!\n####################################################\n</code></pre> <pre><code>bash ./check.sh -F Inline\nChecking Pod webserver-bad-8v4mh PodIP 10.42.0.252 on Port 80 in endpoint ingress-bad for ingress test-bad from nginx-ingress-controller-b2s2d on node a1ubphylbp01 NOK\n</code></pre>"},{"location":"kbs/000020011/#working-pod","title":"Working pod","text":"<pre><code>bash ./check.sh -F Table\n####################################################\nPod: webserver-bad-85cf9ccdf8-8v4mh\nPodIP: 10.42.0.252\nPort: 80\nEndpoint: ingress-1d8af467b8b7c9682fda18c8d5053db7\nIngress: test-bad\nIngress Pod: nginx-ingress-controller-b2s2d\nNode: a1ubphylbp01\nStatus: Pass!\n####################################################\n</code></pre> <pre><code>bash ./check.sh -F Inline\nChecking Pod webserver-good-65644cffd4-gbpkj PodIP 10.42.0.251 on Port 80 in endpoint ingress-good for ingress test-good from nginx-ingress-controller-b2s2d on node a1ubphylbp01 OK\n</code></pre>"},{"location":"kbs/000020011/#testing","title":"Testing","text":"<p>The following commands will deploy two workloads and ingresses. One that is working with a web server that is responding on port 80. And the other will have the webserver disabled, so it will fail to connect.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/rancherlabs/support-tools/master/NGINX-to-pods-check/example-deployment.yml\n</code></pre>"},{"location":"kbs/000020011/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020012/","title":"How to add a Grafana Dashboard for Logging v2","text":"<p>This document (000020012) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020012/#situation","title":"Situation","text":""},{"location":"kbs/000020012/#task","title":"Task","text":"<p>Vistualising Logging statistics can be useful for troubleshooting and capacity planning, this article demonstrates how to add a Logging Dashboard to Grafana with persistence throughout pod restarts.</p> <p>The article focusses on a Logging Dashboard, however these same steps can be adapted to suit other Dashboards.</p>"},{"location":"kbs/000020012/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.5.x with Monitoring and Logging v2 apps installed</li> </ul>"},{"location":"kbs/000020012/#steps","title":"Steps","text":"<ul> <li>Upgrade the Logging v2 app to enable the service monitor by clicking 'Chart Options' and 'Edit as YAML', below is an example showing the <code>monitoring.serviceMonitorsection.enabled</code> field set to <code>true</code>:</li> </ul> <pre><code>monitoring:\nserviceMonitor:\nenabled: true\n</code></pre> <p>Once complete, the target ( <code>cattle-logging-system/rancher-logging</code>) should show up in the Prometheus Targets list with an <code>UP</code> state - this can take a few minutes</p> <p>Add a logging dashboard to Grafana by adding the JSON to a ConfigMap for persistence. In this example, the Logging Dashboard is used</p> <ul> <li> <p>Click the 'Download JSON' button for the Dashboard</p> </li> <li> <p>Replace the data source variable with <code>Prometheus</code> in the downloaded file</p> </li> </ul> <p>For example:</p> <pre><code>sed 's/${DS_PROMETHEUS}/Prometheus/' logging-dashboard_rev4.json &gt; logging.json\n</code></pre> <ul> <li>Create and label the ConfigMap using the updated <code>logging.json</code> file</li> </ul> <pre><code>kubectl create configmap --from-file logging.json logging -n cattle-dashboards\nkubectl label configmap  -n cattle-dashboards logging grafana_dashboard=1\n</code></pre> <ul> <li>Visit the Grafana UI, the Dashboard should now be available</li> </ul>"},{"location":"kbs/000020012/#further-reading","title":"Further reading","text":"<ul> <li>Monitoring in Rancher v2.5</li> <li>Logging in Rancher v2.5</li> </ul>"},{"location":"kbs/000020012/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020013/","title":"Configuring a buffer with total_limit_size in Logging v2","text":"<p>This document (000020013) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020013/#situation","title":"Situation","text":""},{"location":"kbs/000020013/#task","title":"Task","text":"<p>Some users run into an issue where fluentd buffers grow indefinetely and fill up the disk on nodes. This documentation explains how to limit this size in V2 Logging.</p>"},{"location":"kbs/000020013/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher 2.5.x and above</li> <li>Logging App installed with Cluster Explorer</li> </ul>"},{"location":"kbs/000020013/#resolution","title":"Resolution","text":"<p>When you're ready to configure your Output (the location that the logs are sent to) you can get there with the following steps after you have Logging installed:</p> <ol> <li>Cluster Explorer</li> <li>Dropdown in the top left</li> <li>Logging</li> <li>ClusterOutput/Output</li> <li>Create</li> </ol> <p>From here, you can edit the Output as YAML to access the buffer configurations. Using ElasticSearch as an example you can fill out the form easily, but then Edit as YAML:</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterOutput\nmetadata:\n    name: \"elasticsearch-output\"\n    namespace: \"cattle-logging-system\"\n  elasticsearch:\n    host: 1.2.3.4\n    index_name: some-index\n    port: 9200\n    scheme: http\n    buffer:\n      type: file\n      total_limit_size: 2GB\n</code></pre> <p>As far as the buffer size is concerned, the <code>total_limit_size</code> is the important parameter to change. 2GB is a good starting point and is unlikely to need to be changed, but you can adjust this based on your needs.</p>"},{"location":"kbs/000020013/#further-reading","title":"Further reading","text":"<p>As stated in the pre-reqs, this is only possible on the V2 Logging App which is available in Rancher 2.5. This uses the Banzai Cloud logging operator, which allows us to use CRDs to configure it.</p> <ul> <li>V2 Logging Documentation</li> <li>Banzai Cloud Operator Documentation</li> </ul>"},{"location":"kbs/000020013/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020014/","title":"Troubleshooting high ingress request times","text":"<p>This document (000020014) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020014/#situation","title":"Situation","text":"<p>This article aims to provide steps to gather and analyse data to help troubleshoot ingress performance issues.</p>"},{"location":"kbs/000020014/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster built by Rancher v2.x or Rancher Kubernetes Engine (RKE)</li> <li>Use of the Nginx Ingress Controller</li> </ul>"},{"location":"kbs/000020014/#steps","title":"Steps","text":""},{"location":"kbs/000020014/#review-request-times","title":"Review request times","text":"<p>To narrow down on which requests are taking the longest, analyzing the ingress-nginx logs is very helpful.</p> <p>Retrieve requests with a high (&gt;2s) <code>upstream_response_time</code>. This log field represents the time taken for the response from the upstream target - the pod endpoint in the service.</p> <pre><code>kubectl logs -n ingress-nginx -l app=ingress-nginx -f --tail=2000 | awk '/- -/ &amp;&amp; $(NF-2)&gt;2.0'\n</code></pre> <p>The same can be done for <code>request_time</code>, this represents the time taken to complete the entire request, including the above <code>upstream_response_time</code>.</p> <pre><code>kubectl logs -n ingress-nginx -l app=ingress-nginx -f --tail=2000 | awk '/- -/ &amp;&amp; $(NF-7)&gt;2.0'\n</code></pre> <p>Please adjust the time to suit, where <code>&gt;2.0</code> will filter for any times greater than 2.0 seconds.</p> <p>Comparing the diference in timings between <code>request_time</code> and <code>upstream_response_time</code> can help to understand the issue further:</p> <ul> <li>Locate any potential upstream targets (pods), or nodes these may be running on, that are frequently associated with a higher <code>upstream_response_time</code></li> <li>If all upstream targets in a particular ingress/service are experiencing higher response times:</li> </ul> <p>- What dependencies does the application have? For example, external APIs, databases, other services, etc - Investigate the application logs - Simulate the same requests directly to pods to bypass ingress-nginx, are they also slow?</p> <ul> <li>If the <code>upstream_response_time</code> is much lower than <code>request_time</code>, the time is being spent elsewhere, check any tuning, performance or resource issues on the nodes</li> </ul> <p>Note: The <code>request_time</code> metric is also used to create the ingress controller graphs when Cluster Monitoring is enabled.</p>"},{"location":"kbs/000020014/#review-request-details","title":"Review request details","text":"<p>Along with the output in the previous step, it is also useful to analyse the request details, such as the request itself, source/destination IP address, response code, user agent, and the unique name for the ingress for common patterns.</p> <p>You may need to review these with the related application teams. For example, a request to retrieve a large amount of data, or perform a complex query may genuinely take a long time, these can potentially be ignored.</p> <p>Some requests may be opening a websocket, and in the scenario that the service scales up/down regularly, a small number of upstream targets could have a long-running connection creating an unfair distribution to occur on these targets.</p> <p>It's also worthwhile to consider the time when the issue occurs, the number of pods in the service, performance metrics, and requests/limits in place. For example, do the requests occur during a peak load time? Is HPA configured to scale the deployment? Is monitoring data available to identify trends and correlate with the logs?</p>"},{"location":"kbs/000020014/#check-ingress-nginx-logs","title":"Check ingress-nginx logs","text":"<p>With the focus previously on requests themselves, it is also useful to exclude the access logs and ensure there are no fundamental issues with ingress-nginx.</p> <p>The following command should exclude all access.log output, retrieving output from the ingress controller and the nginx error.log only.</p> <pre><code>kubectl logs -n ingress-nginx -l app=ingress-nginx -f --tail=100 | awk '!/- -/'\n</code></pre> <p>Please adjust the <code>--tail</code> flag as needed, this example retrieves the last 100 lines from each ingress-nginx pod.</p>"},{"location":"kbs/000020014/#real-time-view-of-all-requests","title":"Real-time view of all requests","text":"<p>Another option to get a broader overview is using a tool like goaccess. After installing the package, the below can be used to feed ingress-nginx logs to goaccess to get a real-time view of the logs.</p> <pre><code>kubectl logs -f -n ingress-nginx -l app=ingress-nginx --tail=2000 | goaccess --log-format=\"%h - - [%d:%t] \\\"%m %r %H\\\" %s %b \\\"%R\\\" \\\"%u\\\" %^ %T [%v]\" --time-format '%H:%M:%S %z' --date-format \"%d/%b/%Y\"\n</code></pre> <p>Please adjust the history of logs with the <code>--tail</code> flag.</p>"},{"location":"kbs/000020014/#measure-requests-to-ingress-nginx","title":"Measure requests to ingress-nginx","text":"<p>If you have isolated all areas so far, it might be worthwhile to focus on the Load Balancer or network devices that provide client access to ingress-nginx.</p> <p>The following articles contain curl commands to perform SNI-compliant requests and measure statistics, these requests could also be compared from the ingress-nignx logs (as above) to understand what portion of the time was spend with ingress-nginx handling the request.</p> <p>You may also be able to obtain metrics from your Load Balancer or infrastructure to troubleshoot this further.</p> <ul> <li>How to troubleshoot HTTP request performance with curl statistics</li> <li>How to troubleshoot SNI enabled endpoints with curl and openssl</li> </ul>"},{"location":"kbs/000020014/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020015/","title":"Cluster wide vs Global registries","text":"<p>This document (000020015) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020015/#situation","title":"Situation","text":""},{"location":"kbs/000020015/#often-asked-question","title":"Often Asked Question","text":"<p>What is the difference between a global registry versus a cluster-wide registry? Where would I use one over the other?</p>"},{"location":"kbs/000020015/#heres-the-skinny","title":"Here's the skinny","text":""},{"location":"kbs/000020015/#global-private-registry","title":"Global Private Registry","text":"<p>Global-level private registries allow for administrators to store or proxy images through a centralized image repository. Global registries allow for air-gapped setups to pull images needed for cluster provisioning and end-user workloads without specifying the private registry server. This registry is used as the default pull location in place of DockerHub. The global private registry does not support image repositories requiring authentication. Use cluster-level registries if you need to authenticate against your image repository during cluster provisioning. Use registries within the cluster if you need to authenticate against your image repository for end-user workloads. Docs to Global private registry configuration</p>"},{"location":"kbs/000020015/#cluster-provisioning-private-registry","title":"Cluster Provisioning Private Registry","text":"<p>Cluster-level private registries allow administrators to use a registry server with RKE-based clusters to provision system components required to run Kubernetes. List of system images here. Administrators can pass in credentials if the registry server requires them. Outside of the RKE system-images and RKE add-ons, the Rancher agent image used in cluster provisioning will use the cluster-level registry for custom clusters. Eventually, node drivers will pull the Rancher agent image from the cluster-level registry as well. Track progress on that feature here, give it a thumbs-up and watch it if you're interested in its progress..</p> <p>Note: There are still specific images not covered by the cluster-level private registry that are part of the cluster provisioning process. Your cluster will need access to either DockerHub or have these images in your global registry: Busybox, shell, and pause.</p>"},{"location":"kbs/000020015/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020016/","title":"kube-apiserver \"socket: too many open files\" error messages","text":"<p>This document (000020016) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020016/#situation","title":"Situation","text":""},{"location":"kbs/000020016/#issue","title":"Issue","text":"<p>During normal operation of a Kubernetes cluster, you may experience intermittent stability issues and the kube-apiserver logs may contain messages of the following format:</p> <ul> <li><code>clientconn.go:1208] grpc: addrConn.createTransport failed to connect to {https://x.x.x.x:2379  &lt;nil&gt; 0 &lt;nil&gt;}. Err :connection error: desc = \"transport: Error while dialing dial tcp x.x.x.x:2379: socket: too many open files\". Reconnecting...</code></li> <li><code>clientconn.go:1208] grpc: addrConn.createTransport failed to connect to {https://x.x.x.x:2379  &lt;nil&gt; 0 &lt;nil&gt;}. Err :connection error: desc = \"transport: authentication handshake failed: context canceled\". Reconnecting...</code></li> <li><code>clientconn.go:1208] grpc: addrConn.createTransport failed to connect to {https://x.x.x.x:2379  &lt;nil&gt; 0 &lt;nil&gt;}. Err :connection error: desc = \"transport: authentication handshake failed: context deadline exceeded\". Reconnecting...</code></li> </ul>"},{"location":"kbs/000020016/#root-cause","title":"Root Cause","text":"<p>These symptoms can be caused by the kube-apiserver being blocked by configuration that limits the number of files a process can have open. This limit could also affect other components and OS services.</p> <p>This is typically a result of restrictive ulimits, or a high number of open connections.</p> <p>Below is a non-exhaustive list of places where the number of open files ulimit can be set for a Docker container.</p>"},{"location":"kbs/000020016/#system-ulimits-etcsecuritylimitsconf","title":"System ulimits (/etc/security/limits.conf):","text":"<p>This file defines the persisted configuration for the system-wide ulimits, such as file size limits, and how much memory can be used by the different components of the process, including the stack, data and text segments.</p> <p>The limit of interest is the <code>nofile</code> limit, which defines the number of files a process can have open at any given time. This can be set per user, or for all users( <code>*</code>) and there are two limits to define:</p> <ul> <li>Soft limit - These limits are ones that the user can move up or down within the range permitted by any pre-existing hard limits. A user can modify the soft limit by running the command <code>ulimit -n X</code> where X is the desired new value.</li> <li>Hard limit - These limits are set by the superuser and enforced by the Kernel. Users cannot exceed this.</li> </ul> <p>The <code>nofile</code> hard limit for the current user can be seen by running <code>ulimit -Hn</code> and the soft limit can be seen by running <code>ulimit -Sn</code>.</p> <p>More info on limits.conf can be found here.</p>"},{"location":"kbs/000020016/#systemd-configuration","title":"Systemd configuration","text":"<p>By design, systemd will ignore ulimits set via <code>/etc/security/limits.conf</code>, and instead apply its own limits. These can be configured per-service or system-wide.</p> <p>The system-wide systemd nofile limit is defined in <code>/etc/systemd/system.conf</code> as <code>DefaultLimitNOFILE=X:Y</code>. Where X is the soft limit and Y is the hard limit.</p> <p>It is possible to set <code>nofile</code> for a specific service, either by defining <code>LimitNOFILE</code> within the service file itself or creating an override file. For example, defining it directly within the docker systemd service file (/lib/systemd/system/docker.service):</p> <pre><code>[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nBindsTo=containerd.service\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=docker.socket\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\n\nLimitNOFILE=infinity\n</code></pre> <p>Or creating a systemd override file (/etc/systemd/system/docker.d/override.conf):</p> <pre><code>[Service]\nLimitNOFILE=infinity\n</code></pre> <p>Note: The <code>docker.d</code> directory name may be slightly different between Linux distributions. It is usually recommended to create an override, as this will persist through system updates.</p> <p>Note: On older versions of systemd, <code>LimitNOFILE=infinity</code> results in a limit of <code>65535</code>. This is fixed as part of this commit which was merged in systemd v234. More info is available here.</p>"},{"location":"kbs/000020016/#docker-daemon-configuration","title":"Docker daemon configuration","text":"<p>It is possible to configure Docker to enforce its own open file limits on specific containers through the command line flags <code>--default-ulimit nofile=X:Y</code>.</p> <p>This can be applied to all containers by specifying the limit within the <code>/etc/docker/daemon.json</code> configuration file:</p> <pre><code>{\n\"default-ulimits\": {\n\"nofile\": {\n\"Name\": \"nofile\",\n\"Hard\": 64000,\n\"Soft\": 64000\n}\n</code></pre>"},{"location":"kbs/000020016/#resolution","title":"Resolution","text":"<p>If you have any non-default configuration that is applying nofile restrictions on either docker, or containers, revert these to the default configuration, or increase the limits and re-test.</p>"},{"location":"kbs/000020016/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020017/","title":"How to prevent NetworkManager from interfering with Kubernetes cluster networking interfaces","text":"<p>This document (000020017) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020017/#situation","title":"Situation","text":""},{"location":"kbs/000020017/#issue","title":"Issue","text":"<p>NetworkManager can interfere with Kubernetes cluster networking, by attempting to manage interfaces created on cluster nodes by the CNI (Container Network Interface) plugin. This article details how to prevent NetworkManager from managing CNI interfaces, in clusters running with the Canal, Calico, Flannel or Weave CNI plugins.</p>"},{"location":"kbs/000020017/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Linux node with NetworkManager running</li> </ul>"},{"location":"kbs/000020017/#resolution","title":"Resolution","text":"<ol> <li>Create the file <code>/etc/NetworkManager/conf.d/cni.conf</code> on each of the Linux nodes in the cluster, with the content below, depending upon the CNI plugin used:</li> </ol> <p>Canal:</p> <pre><code>[keyfile]\nunmanaged-devices=interface-name:flannel.1;interface-name:cali*\n</code></pre> <p>Calico:</p> <pre><code>[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico\n</code></pre> <p>Flannel:</p> <pre><code>[keyfile]\nunmanaged-devices=interface-name:flannel.1;interface-name:veth*;interface-name:cni0\n</code></pre> <p>Weave:</p> <pre><code>[keyfile]\nunmanaged-devices=interface-name:weave;interface-name:datapath;interface-name:veth*;interface-name:vxlan-*\n</code></pre> <ol> <li>Restart the NetworkManager service:</li> </ol> <pre><code>systemctl restart NetworkManager\n</code></pre>"},{"location":"kbs/000020017/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020018/","title":"How to retrieve a kubeconfig from RKE v0.2.x+ or Rancher v2.2.x+","text":"<p>This document (000020018) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020018/#situation","title":"Situation","text":""},{"location":"kbs/000020018/#task","title":"Task","text":"<p>During a Rancher outage or other disaster event, you may lose access to a downstream cluster via Rancher and be unable to manage your applications. This process allows to bypass Rancher and connects directly to the downstream cluster.</p>"},{"location":"kbs/000020018/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.2.x or newer</li> <li>RKE v0.2.x or newer</li> <li>SSH access to one of the controlplane nodes</li> </ul>"},{"location":"kbs/000020018/#resolution","title":"Resolution","text":""},{"location":"kbs/000020018/#ssh-access","title":"SSH access","text":"<ul> <li>You should SSH into one of the controlplane nodes in the cluster</li> <li> <p>You'll need root/sudo or access to the docker cli</p> </li> <li> <p>Download and install kubectl</p> </li> </ul> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nmv kubectl /usr/local/bin/   ## This step is optional can be stepped if you don't have root/sudo permissions\n</code></pre>"},{"location":"kbs/000020018/#oneliner-rke-and-rancher-custom-cluster","title":"Oneliner (RKE and Rancher custom cluster)","text":"<p>This option requires kubectl and jq to be installed on the server.</p> <pre><code>kubectl --kubeconfig $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\" &gt; kubeconfig_admin.yaml\nkubectl --kubeconfig kubeconfig_admin.yaml get nodes\n</code></pre>"},{"location":"kbs/000020018/#docker-run-commands-rancher-custom-cluster","title":"Docker run commands (Rancher custom cluster)","text":"<p>This option does not require kubectl or jq on the server because this uses the <code>rancher/rancher-agent</code> image to retrieve the kubeconfig.</p> <ul> <li>Get kubeconfig</li> </ul> <pre><code>docker run --rm --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro --entrypoint bash $(docker inspect $(docker images -q --filter=label=org.label-schema.vcs-url=https://github.com/rancher/hyperkube.git) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\"' &gt; kubeconfig_admin.yaml\n</code></pre> <ul> <li>Run <code>kubectl get nodes</code></li> </ul> <pre><code>docker run --rm --net=host -v $PWD/kubeconfig_admin.yaml:/root/.kube/config:z --entrypoint bash $(docker inspect $(docker images -q --filter=label=org.label-schema.vcs-url=https://github.com/rancher/hyperkube.git) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl get nodes'\n</code></pre>"},{"location":"kbs/000020018/#script","title":"Script","text":"<p>Run <code>https://raw.githubusercontent.com/rancherlabs/support-tools/master/how-to-retrieve-kubeconfig-from-custom-cluster/rke-node-kubeconfig.sh</code> and follow the instructions given.</p>"},{"location":"kbs/000020018/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020019/","title":"How to regenerate Service Account tokens in Kubernetes","text":"<p>This document (000020019) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020019/#situation","title":"Situation","text":""},{"location":"kbs/000020019/#task","title":"Task","text":"<p>At times Service Account tokens could be rendered invalid. This could be due to restoring etcd backups or regenerating CA certificates within your cluster.</p> <p>This problem manifests in clients unable to authenticate against the Kubernetes API, for both pods within the cluster, or system components like the controller-manager or kube-proxy. A telling sign would be errors in these clients of the format <code>connect: connection refused</code> or within the Kubernetes API Server logs, showing a large number of clients failing TLS authentication.</p>"},{"location":"kbs/000020019/#steps","title":"Steps","text":"<p>With an admin kubeconfig sourced for the cluster facing issues, run the command below, to generate the list of kubectl commands required to delete all Service Account token secrets. After running the provided kubectl commands from the output, you will need to recreate pods, e.g. by deleting them, in order to regenerate the Service Account token.</p> <pre><code>kubectl get secret --all-namespaces | awk '{ if ($3 == \"kubernetes.io/service-account-token\") print \"kubectl -n\", $1 \" delete secret\", $2 }'\n</code></pre>"},{"location":"kbs/000020019/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020020/","title":"How to improve DNS resolution performance inside a kubernetes cluster?","text":"<p>This document (000020020) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020020/#situation","title":"Situation","text":""},{"location":"kbs/000020020/#task","title":"Task","text":"<p>This article gives a quick overview of DNS resolution inside a Kubernetes cluster. It also explains how performance can be improved.</p>"},{"location":"kbs/000020020/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A working Kubernetes cluster with CoreDNS or kube-dns installed.</li> </ul>"},{"location":"kbs/000020020/#overview","title":"Overview","text":"<p>When a workload is deployed on a Kubernetes cluster, the nameserver (DNS server) is set to the Service IP address of CoreDNS, along with the search options.</p> <p>Example:</p> <pre><code># cat /etc/resolv.conf\nnameserver 10.43.0.10\nsearch cattle-system.svc.cluster.local svc.cluster.local cluster.local members.linode.com\noptions ndots:5\n</code></pre> <ul> <li>The first search option is generally <code>&lt;namespace-where-workload-is-deployed&gt;.svc.&lt;cluster domain&gt;</code></li> <li>Second option is <code>svc.&lt;cluster domain&gt;</code></li> <li>Third one being just <code>&lt;cluster domain&gt;</code></li> <li>The rest of the options are copied over from what's configured on the node. (options from <code>/etc/resolv.conf</code> on the node)</li> </ul> <p>The above options are helpful in the following ways: - When a workload tries to connect to a service in the same namespace, it can simply reference it using the destination service name. Ex: hello-world-service - Similarly, when it tries to connect to a service in a different namespace, it's enough to append the namespace name after the service name. Example: database-service.finance-app</p> <p>The above conveniences come at a cost, though! Suppose the workload tries to resolve a service name outside of the cluster. In that case, the DNS client first exhausts appending all the options to this name before finally resolving to the correct IP, this is known as search path expansion.</p> <p>Say the workload needs to resolve <code>login.mycompany.com</code>. The DNS client sends out a query for <code>login.mycompany.com.&lt;namespace-where-workload-is-deployed&gt;.svc.&lt;cluster domain&gt;</code>. The DNS server responds correctly with a nonexistent domain message (NXDOMAIN) as it's an invalid domain.</p> <p>Next, the DNS client tries to resolve <code>login.mycompany.com.svc.&lt;cluster domain&gt;</code>, followed by <code>login.mycompany.com.&lt;cluster domain&gt;</code>.</p> <p>As you can see, there are a lot of lookups, on top of this there is an <code>A</code> (IPv4) query, and a quad A <code>AAAA</code> (IPv6) query sent out for each of the above.</p> <p>All these unnecessary queries add to the network traffic and increase the load on the CoreDNS service.</p>"},{"location":"kbs/000020020/#resolution","title":"Resolution","text":"<p>To avoid DNS client cycling through each of the options specified in <code>/etc/resolv.conf</code>, add a trailing dot <code>.</code> to the name that's being resolved outside of the cluster, this ensures the DNS client performs an absolute lookup. Example: <code>login.mycompany.com.</code></p> <p>Similarly, when possible, use the full service name ( <code>&lt;destination-service-name&gt;.&lt;destination-namespace&gt;.svc.&lt;cluster domain&gt;</code>). As a further recommendation, some clusters can benefit from deploying NodeLocal DNS. This is particularly useful in a production cluster or a cluster with a high frequency of DNS queries.</p> <p>There are a number of ways that NodeLocal DNS improves on the reliability and scalability of CoreDNS, please see the documentation for details.</p>"},{"location":"kbs/000020020/#further-reading","title":"Further reading","text":"<ul> <li>https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</li> <li>https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/</li> </ul>"},{"location":"kbs/000020020/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020021/","title":"How to configure expiry (TTL) on kubeconfig tokens in Rancher v2.4.6+","text":"<p>This document (000020021) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020021/#situation","title":"Situation","text":""},{"location":"kbs/000020021/#task","title":"Task","text":"<p>In Rancher v2.4.6 and higher, it is possible to configure an expiry (TTL) on Rancher-generated kubeconfig tokens for Rancher managed Kubernetes clusters. This article details how to configure kubeconfig token expiry as a Rancher administrator and how users can authenticate via <code>kubectl</code> when this is configured.</p>"},{"location":"kbs/000020021/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, from v2.4.6 and higher</li> <li>The <code>kubectl</code> binary and <code>Rancher CLI</code> installed locally</li> </ul>"},{"location":"kbs/000020021/#resolution","title":"Resolution","text":""},{"location":"kbs/000020021/#disable-automatic-kubeconfig-token-generation-and-configure-ttl","title":"Disable automatic kubeconfig token generation and configure TTL","text":"<p>As a Rancher global admin, disable automatic kubeconfig token generation and configure the expiry time (TTL) for kubeconfig tokens, per the steps in the Rancher documentation here.</p>"},{"location":"kbs/000020021/#authenticating-via-the-rancher-cli-with-kubectl","title":"Authenticating via the Rancher CLI with kubectl","text":"<p>Once the kubeconfig TTL has been configured by an admin, users will need to download the Rancher CLI in order to authenticate against Rancher when using Rancher-generated kubeconfig files to connect to Rancher-managed clusters.</p> <ol> <li>Download the required Rancher CLI binary per the Rancher documentation.</li> <li>Ensure the <code>rancher</code> CLI binary is executable and in your PATH.</li> <li>Download a copy of the kubeconfig file for a cluster from the Rancher UI and add it to the default ~/.kube/config file or source it with <code>KUBECONFIG=/path/to/file</code>.</li> <li>Execute <code>kubectl get nodes</code> and observe you will be prompted for your Rancher username and password. If you are using an authentication provider you will also be prompted to select this versus local authentication. You can prevent this prompt by adding the <code>--auth-provider=&lt;provider&gt;</code> argument in the kubeconfig file, per the following example:</li> </ol> <pre><code>     args:\n    - token\n    - --auth-provider=openLdapProvider\n    - --server=rancher.example.com\n</code></pre> <ol> <li>After providing the username and password, the kubeconfig token will be generated and valid for the TTL ( <code>kubeconfig-token-ttl-minutes</code>) configured in Rancher.</li> <li>You can verify the configured expiry time of the kubeconfig token within the Rancher UI, under <code>API &amp; Keys</code>.</li> <li>Once the token expires, you will be prompted to log in again upon executing <code>kubectl</code> commands against the cluster, per step 4.</li> </ol> <p>N.B. By default the generated kubeconfig token is cached within the directory <code>.cache</code> in the working directory from which you invoke <code>kubectl</code>, when you are prompted to log in. As a result executing <code>kubectl</code> from a different directory, will re-prompt for authentication and generate a fresh token cache under <code>.cache</code>. In Rancher CLI v2.4.10+ you can set the token cache location with the environment variable <code>RANCHER_CONFIG_DIR</code>, e.g. <code>export RANCHER_CONFIG_DIR=~/.rancher</code> to avoid being prompted for authentication when you change the working directory.</p>"},{"location":"kbs/000020021/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020022/","title":"How to enable debug logging when using Terraform?","text":"<p>This document (000020022) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020022/#situation","title":"Situation","text":""},{"location":"kbs/000020022/#task","title":"Task","text":"<p>If you encounter an issue with the Rancher2 Terraform Provider it may be helpful to capture debug output from Terraform, so that you can provide this to Rancher Support. This article details how to enable debug output and set the log location for Terraform commands such as <code>terraform apply</code>.</p>"},{"location":"kbs/000020022/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Terraform</li> </ul>"},{"location":"kbs/000020022/#resolution","title":"Resolution","text":"<p>You can set the Terraform log level and location via the <code>TF_LOG</code> and <code>TF_LOG_PATH</code> environment variables.</p>"},{"location":"kbs/000020022/#set-log-level-with-tf_log","title":"Set log level with <code>TF_LOG</code>","text":"<p>The environment variable <code>TF_LOG</code> defines the log level. Valid log levels are (in order of decreasing verbosity): <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Set the log level in your environment with the appropriate command (substituting your preferred log level):</p> <p>Bash: <code>export TF_LOG=\"DEBUG\"</code></p> <p>PowerShell: <code>$env:TF_LOG=\"DEBUG\"</code></p>"},{"location":"kbs/000020022/#redirect-terraform-logs-with-tf_log","title":"Redirect Terraform logs with <code>TF_LOG</code>","text":"<p>The environment variable <code>TF_LOG_PATH</code> specifies the file in which Terraform will write logs. If <code>TF_LOG_PATH</code> is not set, output is sent to standard output and error in the terminal. If the environment variable is set, Terraform will append logs from each run to the specified file.</p> <p>Set the Terraform log location in your environment with the appropriate command (substituting the path to your preferred file):</p> <p>Bash: <code>export TF_LOG_PATH=\"tmp/terraform.log\"</code></p> <p>PowerShell: <code>$env:TF_LOG_PATH=\"C:\\tmp\\terraform.log\"</code></p> <p>To set them permanently, you can add these environment variables to your <code>.profile</code>, <code>.bashrc</code>, PowerShell profile (if it exists, the path is stored in <code>$profile</code> environment variable) file, or the appropriate profile for your chosen shell.</p> <p>N.B. As this will append the log with the Terraform output every time you run a Terraform command, you may wish to configure log rotation for the chosen log file if this is enabled permanently. Alternatively, disable logging to file once you have finished troubleshooting.</p>"},{"location":"kbs/000020022/#further-reading","title":"Further reading","text":"<ul> <li>Debugging Terraform (official) documentation</li> </ul>"},{"location":"kbs/000020022/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020023/","title":"What do I need to provide to get started on SUSE Rancher Hosted?","text":"<p>This document (000020023) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020023/#situation","title":"Situation","text":""},{"location":"kbs/000020023/#resolution","title":"Resolution","text":"<p>To get started on SUSE Rancher Hosted, please provide the following information:</p> <ol> <li>The <code>rancher.cloud</code> vanity URL to be used for Web and API access. For example, mycompany.rancher.cloud. This cannot be changed, so please choose carefully.</li> <li>The region you would like your SUSE Rancher Hosted environment to reside in. Current options are US West, US East, Canada Central, EU West (Dublin or Paris), EU Central, EU North, AP Southeast (Sydney or Singapore), and South Africa. Typically you'll want the region closest to where your downstream/managed clusters will reside.</li> <li>If you have special networking requirements, such as a site-to-site VPN or VPC peering between SUSE Rancher Hosted and your infrastructure, you can specify a RFC-1918 CIDR to be used in SUSE Rancher Hosted. The CIDR must be at least /25 or larger network. You will want to choose a network block that does not overlap with your infrastructure.</li> <li>We perform regular maintenance on Hosted Rancher. Select one of two maintenance windows: Tuesday 00:00 - 01:00 UTC or Thursday 17:00 - 18:00 UTC. Note that windows are UTC-based and the times may shift by an hour twice a year if you reside in a region that observes Daylight Savings Time.</li> </ol> <p>To request your Hosted Rancher environment, you can open a support ticket on our support portal.</p>"},{"location":"kbs/000020023/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020024/","title":"What are the networking requirements for using SUSE Rancher Hosted?","text":"<p>This document (000020024) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020024/#resolution","title":"Resolution","text":"<p>The network requirements for SUSE Rancher Hosted are going to depend on your use cases. The following is a list of common uses cases and what is required:</p> <ul> <li>To access the Rancher UI or API, you must have outbound TCP/443 connectivity to SUSE Rancher Hosted.</li> <li>All downstream/managed clusters require outbound TCP/443 connectivity to SUSE Rancher Hosted.</li> <li>If you are creating a node driver based cluster, SUSE Rancher Hosted will need TCP/22 (SSH) connectivity to each node for the initial provisioning. Additionally, SUSE Rancher Hosted will need connectivity to the orchestration API, for example, the vSphere API.</li> <li>For authentication provider integration , SUSE Rancher Hosted will require connectivity to the authentication provider's endpoint. Generally, no networking setup is needed if using a SaaS authentication provider such as Azure Active Directory, Okta, or GitHub.</li> </ul> <p>More detailed documentation for networking requirements can be found in Rancher's port requirements docs.</p>"},{"location":"kbs/000020024/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020025/","title":"Can I use my own SSL/TLS certificates with SUSE Rancher Hosted?","text":"<p>This document (000020025) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020025/#resolution","title":"Resolution","text":"<p>No, currently we use a rancher.cloud vanity hostname for SUSE Rancher Hosted and fully manage the SSL/TLS certificate. The certificate is automatically created when your environment is provisioned and automatically renewed annually.</p>"},{"location":"kbs/000020025/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020026/","title":"What is the difference between Project and Cluster Monitoring in Rancher v2.2 - v2.4?","text":"<p>This document (000020026) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020026/#situation","title":"Situation","text":""},{"location":"kbs/000020026/#question","title":"Question","text":"<p>In Rancher v2.2 - v2.4 monitoring can be enabled at both the cluster and project level, this article explains the differences between the two.</p>"},{"location":"kbs/000020026/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.2 - v2.4 instance</li> </ul>"},{"location":"kbs/000020026/#answer","title":"Answer","text":""},{"location":"kbs/000020026/#cluster-monitoring","title":"Cluster monitoring","text":"<p>Per the documentation on monitoring in Rancher v2.2 - v2.4, \"cluster monitoring allows you to view the health of your Kubernetes cluster. Prometheus collects metrics from the cluster components below, which you can view in graphs and charts\". This includes metrics for control plane components, the etcd database, and cluster nodes.</p> <p>With the metrics provided by cluster monitoring you can then configure cluster alerts, for example for high node CPU or memory usage, or etcd member unavailability, etc.</p>"},{"location":"kbs/000020026/#project-monitoring","title":"Project monitoring","text":"<p>Project monitoring can only be enabled with cluster monitoring enabled first.</p> <p>Per the documentation, \"project monitoring allows you to view the state of pods running in a given project. Prometheus collects metrics from the project\u2019s deployed HTTP and TCP/UDP workloads\". This enables you to configure scraping of custom metrics endpoints on workloads you deploy into the cluster.</p> <p>With project monitoring you can then configure project alerts based on the custom metrics scraped, or workloads metrics, such as a pod's memory usage exceeding its quota.</p>"},{"location":"kbs/000020026/#further-reading","title":"Further Reading","text":"<ul> <li>Documentation on monitoring in Rancher v2.2 - v2.4</li> <li>Documentation on monitoring in Rancher v2.5</li> </ul>"},{"location":"kbs/000020026/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020027/","title":"Why longhorn volume still shows more space than the actual size after deleting the content?","text":"<p>This document (000020027) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020027/#situation","title":"Situation","text":""},{"location":"kbs/000020027/#question","title":"Question","text":"<p>Why does Longhorn report higher <code>Actual Size</code> for a volume than the usage reported by the filesystem, even after deleting content?</p>"},{"location":"kbs/000020027/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster with Longhorn installed</li> <li>An existing workload using a Persistent Volume (PV) of the Longhorn StorageClass (SC)</li> </ul>"},{"location":"kbs/000020027/#answer","title":"Answer","text":"<p>Whilst Longhorn storage is thin-provisioned, a volume cannot shrink in size after content is removed.</p> <p>Per the Longhorn Documentation: \"This happens because Longhorn operates on the block level, not the filesystem level, so Longhorn doesn\u2019t know if the content has been removed by a user or not. That information is mostly kept at the filesystem level.\"</p> <p>You can demonstrate this behaviour, by attaching a 10GB Longhorn volume to a Pod, creating 6GB of files on the volume and then deleting 4GB, as follows:</p> <ol> <li>Increase the disk usage of the volume:</li> </ol> <pre><code>dd if=/dev/zero of=4gbfile bs=4G count=1\ndd if=/dev/zero of=2gbfile bs=2G count=1\n</code></pre> <ol> <li>Remove the <code>4gbfile</code> from the mounted volume:</li> </ol> <pre><code>rm 4gbfile\n</code></pre> <ol> <li> <p>Check the usage reported by the filesystem, by running <code>du -sh</code> on the mounted volume, which will show 2GB.</p> </li> <li> <p>Navigate to the Longhorn UI and check the <code>Actual Size</code>, reported under <code>Volume Details</code> in the Volume view, which will show 6GB of usage still.</p> </li> </ol>"},{"location":"kbs/000020027/#further-reading","title":"Further Reading","text":"<ul> <li>The Longhorn Documentation</li> </ul>"},{"location":"kbs/000020027/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020028/","title":"Does Rancher v2.x support two-factor (2FA) or multi-factor authentication (MFA)?","text":"<p>This document (000020028) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020028/#situation","title":"Situation","text":""},{"location":"kbs/000020028/#question","title":"Question","text":"<p>Does Rancher v2.x support two-factor (2FA) or multi-factor authentication (MFA)?</p>"},{"location":"kbs/000020028/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> </ul>"},{"location":"kbs/000020028/#answer","title":"Answer","text":"<p>The built-in Rancher local authentication provider does not have 2FA or MFA capabilities; however, it is possible to secure Rancher with 2FA/MFA, by using one of the Rancher-supported authentication providers with this capability, such as GitHub, Keycloak, or Okta.</p>"},{"location":"kbs/000020028/#further-reading","title":"Further Reading","text":"<ul> <li>Rancher documentation on authentication</li> <li>Supported authentication integration details</li> </ul>"},{"location":"kbs/000020028/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020029/","title":"Missing container metrics in Rancher monitoring for AKS clusters in Rancher v2.2 - v2.4","text":"<p>This document (000020029) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020029/#situation","title":"Situation","text":""},{"location":"kbs/000020029/#issue","title":"Issue","text":"<p>There is a bug affecting Rancher monitoring v1 in Rancher v2.2 - v2.4, in which some metrics for Rancher-managed AKS clusters are missing. When viewing Grafana dashboards for affected clusters, some graphs will display a <code>Not enough data for graph</code> or <code>No Data</code> message.</p>"},{"location":"kbs/000020029/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.2 - v2.4 instance</li> <li>A Rancher-managed AKS cluster with monitoring enabled</li> </ul>"},{"location":"kbs/000020029/#root-cause","title":"Root cause","text":"<p>This is due to the Kubelet scrape port not being properly set for Prometheus, as the cluster is not being detected as an AKS instance.</p>"},{"location":"kbs/000020029/#resolution","title":"Resolution","text":"<p>Simply adding the <code>exporter-kubelets.https: true</code> answer in the advanced section of the monitoring configuration page is all that is needed.</p> <p>Navigate to the cluster view in the Rancher UI for the relevant cluster and click <code>Tools</code> &gt; <code>Monitoring</code>. Click <code>Show advanced options</code> in the bottom-right corner, and add <code>exporter-kubelets.https: true</code> under <code>Answers</code>, per the screenshot below. Once you hit <code>Save</code> container metrics should start being populated in Prometheus and will be visible in the Grafana dashboards.</p> <p></p> <p>It is worth noting that this is considered a \"hidden answer\" which means that it will not remain visible under <code>Answers</code> on the monitoring configuration page. As a result, you will have to re-add this answer every time there is a monitoring update or configuration change.</p>"},{"location":"kbs/000020029/#further-reading","title":"Further reading","text":"<ul> <li>This issue is tracked in GitHub issue #27550.</li> </ul>"},{"location":"kbs/000020029/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020030/","title":"`Get secret: secrets \"webhook-receiver\" not found` errors observed in Rancher v2.4 and v2.5 server logs","text":"<p>This document (000020030) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020030/#situation","title":"Situation","text":""},{"location":"kbs/000020030/#issue","title":"Issue","text":"<p>Sometimes an admin may see an error in the Rancher server logs like the one below:</p> <pre><code>[project-alert-group-controller] failed with : Update Webhook Receiver Config: Get secret: secrets \"webhook-receiver\" not found\n</code></pre>"},{"location":"kbs/000020030/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.4.x instance prior to v2.4.11, or a Rancher v2.5.x instance prior to v2.5.4</li> <li>Rancher Monitoring v1 enabled and an Alert Group configured with a non-webhook notifier</li> </ul>"},{"location":"kbs/000020030/#root-cause","title":"Root cause","text":"<p>The alert controller in Monitoring v1 would log an error when a secret containing a webhook-notifier configuration could not be found, even if no webhook based notifiers have been configured on alerts. This log message is harmless and can be ignored.</p>"},{"location":"kbs/000020030/#resolution","title":"Resolution","text":"<p>This erroneous log message has been fixed in Rancher v2.4.x, at v2.4.11 and above, and in Rancher v2.5.x, at v2.5.4 and above. Following a Rancher upgrade, the error should no longer show in the Rancher server logs.</p>"},{"location":"kbs/000020030/#further-reading","title":"Further Reading","text":"<ul> <li>Github Issue #28954 tracking this behaviour in Rancher v2.5.x</li> <li>Github Issue #30088 tracking this behaviour in Rancher v.4.x</li> </ul>"},{"location":"kbs/000020030/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020031/","title":"How to shutdown a Kubernetes cluster (Rancher Kubernetes Engine (RKE) CLI provisioned or Rancher v2.x Custom clusters)","text":"<p>This document (000020031) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020031/#situation","title":"Situation","text":""},{"location":"kbs/000020031/#task","title":"Task","text":"<p>This article provides instructions for safely shutting down a Kubernetes cluster provisioned via the Rancher Kubernetes Engine (RKE) CLI or a Rancher v2.x provisioned Custom Cluster.</p>"},{"location":"kbs/000020031/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster launched with the RKE CLI or from Rancher 2.x as a Custom Cluster</li> </ul>"},{"location":"kbs/000020031/#background","title":"Background","text":"<p>If you have a need to shut down the infrastructure running a Kubernetes cluster (datacenter maintenance, migration, etc.) this guide will provide steps in the proper order to ensure a safe cluster shutdown. This guide has command examples for RKE-deployed clusters but the order of operations and the process is similar for most Kubernetes distributions.</p> <p>Please ensure you complete an etcd backup before continuing this process. A guide regarding the backup and restore process can be found here.</p>"},{"location":"kbs/000020031/#solution","title":"Solution","text":"<p>N.B. If you have nodes that share worker, control plane, or etcd roles, postpone the <code>docker stop</code> and shutdown operations until worker or control plane containers have been stopped.</p>"},{"location":"kbs/000020031/#draining-nodes","title":"Draining nodes.","text":""},{"location":"kbs/000020031/#for-all-nodes-prior-to-stopping-the-containers-run","title":"For all nodes, prior to stopping the containers, run:","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"kbs/000020031/#to-identify-the-desired-node-then-run","title":"To identify the desired node, then run:","text":"<pre><code>kubectl drain &lt;node name&gt;\n</code></pre>"},{"location":"kbs/000020031/#this-will-safely-evict-any-pods-and-you-can-proceed-with-the-following-steps-to-a-shutdown","title":"This will safely evict any pods, and you can proceed with the following steps to a shutdown.","text":""},{"location":"kbs/000020031/#shutting-down-the-workers-nodes","title":"Shutting down the workers nodes","text":"<p>For each worker node:</p> <ol> <li>ssh into the worker node</li> <li>stop kubelet and kube-proxy by running <code>sudo docker stop kubelet kube-proxy</code></li> <li>stop docker by running <code>sudo service docker stop</code> or <code>sudo systemctl stop docker</code></li> <li>shutdown the system <code>sudo shutdown now</code></li> </ol>"},{"location":"kbs/000020031/#shutting-down-the-control-plane-nodes","title":"Shutting down the control plane nodes","text":"<p>For each control plane node:</p> <ol> <li>ssh into the control plane node</li> <li>stop kubelet and kube-proxy by running <code>sudo docker stop kubelet kube-proxy</code></li> <li>stop kube-scheduler and kube-controller-manager by running <code>sudo docker stop kube-scheduler kube-controller-manager</code></li> <li>stop kube-apiserver by running <code>sudo docker stop kube-apiserver</code></li> <li>stop docker by running <code>sudo service docker stop</code> or <code>sudo systemctl stop docker</code></li> <li>shutdown the system <code>sudo shutdown now</code></li> </ol>"},{"location":"kbs/000020031/#shutting-down-the-etcd-nodes","title":"Shutting down the etcd nodes","text":"<p>For each etcd node:</p> <ol> <li>ssh into the etcd node</li> <li>stop kubelet and kube-proxy by running <code>sudo docker stop kubelet kube-proxy</code></li> <li>stop etcd by running <code>sudo docker stop etcd</code></li> <li>stop docker by running <code>sudo service docker stop</code> or <code>sudo systemctl stop docker</code></li> <li>shutdown the system <code>sudo shutdown now</code></li> </ol>"},{"location":"kbs/000020031/#shutting-down-storage","title":"Shutting down storage","text":"<p>Shut down any persistent storage devices that you might have in your datacenter (such as NAS storage devices) if applicable. It iss important that you do this after shutting everything else down to prevent data loss/corruption for containers requiring persistency.</p> <p>N.B. If you are running a cluster that was not deployed through RKE then the order of the process is still the same, however the commands may vary. For instance, some distributions run kubelet and other control plane items as a service on the node rather than in docker. Check documentation for the specific Kubernetes distribution for information as to how to stop these services.</p>"},{"location":"kbs/000020031/#starting-a-kubernetes-cluster-up-after-shutdown","title":"Starting a Kubernetes cluster up after shutdown","text":"<p>Kubernetes is good about recovering from a cluster shutdown and requires little intervention, though there is a specific order in which things should be powered back on to minimize errors.</p> <ol> <li>Power on any storage devices if applicable.</li> </ol> <p>Check with your storage vendor on how to properly power on you storage devices and verify that they are ready.</p> <ol> <li>For each etcd node:</li> <li>Power on the system/start the instance.</li> <li>Log into the system via ssh.</li> <li>Ensure docker has started <code>sudo service docker status</code> or <code>sudo systemctl status docker</code></li> <li>Ensure etcd and kubelet\u2019s status shows Up in Docker <code>sudo docker ps</code></li> <li>For each control plane node:</li> <li>Power on the system/start the instance.</li> <li>Log into the system via ssh.</li> <li>Ensure docker has started <code>sudo service docker status</code> or <code>sudo systemctl status docker</code></li> <li>Ensure kube-apiserver, kube-scheduler, kube-controller-manager, and kubelet\u2019s status shows Up in Docker <code>sudo docker ps</code></li> <li>For each worker node:</li> <li>Power on the system/start the instance.</li> <li>Log into the system via ssh.</li> <li>Ensure docker has started <code>sudo service docker status</code> or <code>sudo systemctl status docker</code></li> <li>Ensure kubelet\u2019s status shows Up in Docker <code>sudo docker ps</code></li> <li>Log into the Rancher UI (or use kubectl) and check your various projects to ensure workloads have started as expected. This may take a few minutes depending on the number of workloads and your server capacity.</li> </ol>"},{"location":"kbs/000020031/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020032/","title":"How to block specific user agents from connecting through the nginx ingress controller","text":"<p>This document (000020032) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020032/#situation","title":"Situation","text":""},{"location":"kbs/000020032/#task","title":"Task","text":"<p>At times it's necessary to block specific user agents from connecting to workloads within your cluster. Whether it's bad actors or for compliance reasons, we'll go through how to get it done with Rancher/RKE created clusters.</p>"},{"location":"kbs/000020032/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster with Nginx for its ingress controller.</li> </ul>"},{"location":"kbs/000020032/#steps","title":"Steps","text":""},{"location":"kbs/000020032/#1-identify-the-user-agents-that-will-be-blocked","title":"1. Identify the user agents that will be blocked.","text":"<p>There are multiple ways to surface user agents needing to be blocked, the most practical being your nginx ingress controllers' logs. In the logs one would see something similar to the following entries:</p> <pre><code>172.16.10.101 - - [02/Jan/2021:21:51:22 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:84.0) Gecko/20100101 Firefox/84.0\" 367 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.000 200 1da439122bd7d7014f6627f32e4cefc3\n172.16.10.101 - - [02/Jan/2021:21:51:22 +0000] \"GET /favicon.ico HTTP/1.1\" 499 0 \"http://test.default.54.202.152.214.xip.io/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:84.0) Gecko/20100101 Firefox/84.0\" 341 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 0 0.000 - 92a1e851206da86fbec0610d346e2ddd\n172.16.10.101 - - [02/Jan/2021:21:51:26 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"curl/7.64.1\" 98 0.000 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.000 200 4c51046660b05cf2703dbedfae2272aa\n172.16.10.101 - - [02/Jan/2021:21:51:29 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"Wget/1.20.3 (darwin19.0.0)\" 164 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.000 200 5334e799b3268dab31d74a5d2239702b\n</code></pre> <p>We can see three unique user agents here; <code>curl</code>, <code>Wget</code>, and <code>Mozilla/5.0</code>.</p>"},{"location":"kbs/000020032/#2-modify-the-clusteryaml-to-include-the-nginx-option-to-block-user-agents","title":"2. Modify the cluster.yaml to include the Nginx option to block user agents","text":"<p>In the example configuration, we'll block connections from curl and Mozilla using regular expressions. It's essential to separate the list of agents we're looking to restrict with commas.</p> <pre><code>  ingress:\n    options:\n      block-user-agents: '~*curl.*,~*Mozilla.*'\n    provider: nginx\n</code></pre>"},{"location":"kbs/000020032/#3-test-the-configuration","title":"3. Test the configuration","text":"<p>Navigating to the workload or service behind the ingress now blocks the agents blacklisted.</p> <pre><code>172.16.10.101 - - [02/Jan/2021:22:23:00 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"Wget/1.20.3 (darwin19.0.0)\" 164 0.000 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.004 200 df2b5c3fca1ba33683e8ee6d2708b214\n172.16.10.101 - - [02/Jan/2021:22:23:05 +0000] \"GET / HTTP/1.1\" 403 153 \"-\" \"curl/7.64.1\" 98 0.000 [] [] - - - - 1902bca1f3622bf5374f966358b10463\n172.16.10.101 - - [02/Jan/2021:22:31:56 +0000] \"GET / HTTP/1.1\" 403 153 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:84.0) Gecko/20100101 Firefox/84.0\" 478 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 0 0.004 304 9e0df037eaea1afedc5d3c93229dca80\n</code></pre>"},{"location":"kbs/000020032/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020033/","title":"How to remove and replace an unresponsive control plane / etcd node in the local Rancher server cluster, provisioned by the Rancher Kubernetes Engine (RKE) CLI","text":"<p>This document (000020033) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020033/#situation","title":"Situation","text":""},{"location":"kbs/000020033/#task","title":"Task","text":"<p>This article details how to remove and replace an unresponsive control plane / etcd node from a local Rancher server cluster, provisioned via the Rancher Kubernetes Engine (RKE) CLI.</p>"},{"location":"kbs/000020033/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI provisioned cluster</li> <li>A Highly Available control plane / etcd configuration, with an odd number of mixed role control plane / etcd nodes, commonly 3 or 5</li> <li>The cluster is quorate, i.e. with 3 control plane / etcd nodes only a single node is unresponsive, or with 5 control plane / etcd nodes upto two nodes are unresponsive</li> <li>The cluster configuration file (e.g. <code>cluster.yml</code>) and .rkestate file (e.g. <code>cluster.rkestate</code>)</li> <li>The RKE binary and SSH access to the nodes</li> </ul>"},{"location":"kbs/000020033/#resolution","title":"Resolution","text":"<p>This operation is relatively simple, and uses the example <code>cluster.yaml</code> below for demonstration purposes.</p> <p>N.B. Be sure to use your <code>cluster.yaml</code> and matching <code>cluster.rkestate</code> for the relevant cluster.</p> <p>In this demonstration example, the node that is failing has the address <code>1.2.3.3</code>:</p> <pre><code>nodes:\n- address: 1.2.3.1\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n- address: 1.2.3.2\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n- address: 1.2.3.3\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n[...] # rest of cluster.yaml except control plane / etcd nodes restracted\n</code></pre>"},{"location":"kbs/000020033/#step-1-validate-the-cluster-is-quorate-and-confirm-the-unresponsive-node","title":"Step 1. Validate the cluster is quorate and confirm the unresponsive node","text":"<p>On the control plane / etcd nodes perform the following command, per the Rancher Troubleshooting Documentation to determine etcd endpoint health:</p> <pre><code>docker exec -e ETCDCTL_ENDPOINTS=$(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\") etcd etcdctl endpoint health\n</code></pre> <p>On the unresponsive node the command may fail to execute, on the healthy nodes you should see output of the following format indicating the health status of each node:</p> <pre><code>{\"level\":\"warn\",\"ts\":\"2020-12-31T12:11:41.840Z\",\"caller\":\"clientv3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"endpoint://client-c65a15b4-9646-4c71-914d-f3c892c04c2f/1.2.3.3:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: all SubConns are in TransientFailure, latest connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 1.2.3.3:2379: connect: connection refused\\\"\"}\nhttps://1.2.3.1:2379 is healthy: successfully committed proposal: took = 13.442336ms\nhttps://1.2.3.2:2379 is healthy: successfully committed proposal: took = 18.227226ms\nhttps://1.2.3.3:2379 is unhealthy: failed to commit proposal: context deadline exceeded\n</code></pre>"},{"location":"kbs/000020033/#step-2-remove-the-unresponsive-node","title":"Step 2. Remove the unresponsive node","text":"<p>Having confirmed which node is unresponsive in the cluster, remove this from the nodes block in the cluster configuration file ( <code>cluster.yaml</code>), per the example of <code>1.2.3.3</code> removed below:</p> <pre><code>nodes:\n- address: 1.2.3.1\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n- address: 1.2.3.2\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n\n[...] # rest of cluster.yaml except control plane / etcd nodes restracted\n</code></pre> <p>After updating the <code>cluster.yaml</code> file, execute an <code>rke up</code> run to remove the node:</p> <pre><code>rke up --config cluster.yaml\n</code></pre> <p>The above action will remove the problematic and unresponsive control plane / etcd node.</p>"},{"location":"kbs/000020033/#step-3-clean-and-add-the-removed-node-back-to-the-cluster","title":"Step 3. Clean and add the removed node back to the cluster","text":"<p>Once the <code>rke up</code> invocation has run through without any errors, and you can see the node removed from the Rancher UI or <code>kubectl get nodes</code> output, it is safe to move onto adding the node back in.</p> <p>First clean the removed node ( <code>1.2.3.3</code>) in our example, using the Extended Rancher 2 Cleanup script.</p> <p>After cleaning the node, add this back into the cluster configuration ( <code>cluster.yaml</code>) file:</p> <pre><code>nodes:\n- address: 1.2.3.1\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n- address: 1.2.3.2\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n- address: 1.2.3.3\nuser: ubuntu\nrole:\n- controlplane\n- etcd\n[...] # rest of cluster.yaml except control plane / etcd nodes restracted\n</code></pre> <p>And run the rke up command again:</p> <pre><code>rke up --config cluster.yaml\n</code></pre>"},{"location":"kbs/000020033/#step-4-validate-final-cluster-state","title":"Step 4. Validate final cluster state","text":"<p>Once the <code>rke up</code> command has completed, without errors, you can now verify the node is visible and ready via <code>kubectl get nodes</code> and the Rancher UI.</p> <p>The etcd endpoint health commands on the control plane / etcd nodes should also show each endpoint as healthy, per the following example output:</p> <pre><code>https://1.2.3.1:2379 is healthy: successfully committed proposal: took = 13.442336ms\nhttps://1.2.3.2:2379 is healthy: successfully committed proposal: took = 18.227226ms\nhttps://1.2.3.3:2379 is healthy: successfully committed proposal: took = 22.065616ms\n</code></pre>"},{"location":"kbs/000020033/#further-reading","title":"Further reading","text":"<ul> <li>Extended Rancher 2 Cleanup script</li> <li>RKE Documentation on cluster state files (.rkestate)</li> </ul>"},{"location":"kbs/000020033/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020034/","title":"Preventing LoadBalancer service traffic from flowing through control plane and etcd nodes in a Kubernetes cluster with the AWS Cloud Provider","text":"<p>This document (000020034) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020034/#situation","title":"Situation","text":""},{"location":"kbs/000020034/#task","title":"Task","text":"<p>This article details how to prevent LoadBalancer type service traffic from flowing through control plane and etcd nodes, in a cluster configured with the AWS Cloud Provider.</p>"},{"location":"kbs/000020034/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster, provisioned on EC2 instances</li> <li>Separate worker nodes from control plane and etcd nodes</li> <li>The AWS Cloud Provider configured</li> </ul>"},{"location":"kbs/000020034/#making-the-changes","title":"Making the changes","text":"<p>Nodes of a Kubernetes cluster created by Rancher/RKE, that use AWS as the cloud provider, automatically get added to service load balancers (ELB). The behavior results in both controlplane and etcd nodes routing end-user application traffic, breaking the role separations model. To prevent this, label the control plane and etcd nodes with the label <code>node-role.kubernetes.io/master</code> and the cloud-controller will not automatically add them to the service load balancers.</p>"},{"location":"kbs/000020034/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020035/","title":"How to enable IPVS proxy mode for kube-proxy","text":"<p>This document (000020035) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020035/#situation","title":"Situation","text":""},{"location":"kbs/000020035/#task","title":"Task","text":"<p>The default proxy mode for kube-proxy in Kubernetes and clusters is iptables, this is also the case for clusters created with Rancher 2.x and the Rancher Kubernetes Engine (RKE) CLI.</p> <p>This article aims to provide all the needed steps and configuration to deploy or update a cluster to use IPVS proxy mode.</p> <p>Please note, IPVS provides load balancing functionality, with this in mind it does not cover all of the traffic handling maintained by kube-proxy. Some scenarios will still utilise iptables, such as services that require NAT, like NodePort and LoadBalancer services.</p>"},{"location":"kbs/000020035/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed using Rancher v2.3.3 or greater</li> </ul> <p>Or</p> <ul> <li>A cluster managed using Rancher Kubernetes Engine (RKE) CLI v1.1.0 or greater</li> </ul>"},{"location":"kbs/000020035/#resolution","title":"Resolution","text":"<p>The <code>--proxy-mode</code> flag for kube-proxy is used to override the default iptables mode, using the below steps for Rancher or RKE the <code>--proxy-mode</code> flag can be provided to enable IPVS.</p> <p>Note: Enabling IPVS is best done when creating a cluster, the process to update an existing cluster does include some follow-up steps at the end of this article, please ensure to read these beforehand, and complete these when migrating to IPVS on an existing cluster.</p>"},{"location":"kbs/000020035/#rancher-v2x","title":"Rancher v2.x","text":"<p>Log into the Rancher UI:</p> <ul> <li>From the Global view click on the cluster</li> <li>Click the Edit Cluster button, and Edit as YAML</li> <li>Locate or create the <code>services.kubeproxy</code> field under <code>rancher_kubernetes_engine_config</code></li> </ul> <p>Add <code>extra_args</code> under <code>kubeproxy</code> to apply the IPVS changes to the kube-proxy component when it is started as a container on all nodes.</p> <p>This example uses the <code>lc</code> (least connection) load balancing algorithm, <code>rr</code> (round-robin) is the default.</p> <pre><code>    kubeproxy:\nextra_args:\nipvs-scheduler: lc\nproxy-mode: ipvs\n</code></pre> <ul> <li>Click Save, the above changes will be applied to the cluster</li> </ul>"},{"location":"kbs/000020035/#rancher-kubernetes-engine-rke-cli","title":"Rancher Kubernetes Engine (RKE) CLI","text":"<p>Edit the cluster.yaml configuration file for your cluster:</p> <ul> <li>Locate or create the <code>services.kubeproxy</code> field</li> </ul> <p>Add <code>extra_args</code> under <code>kubeproxy</code> to apply the IPVS changes to the kube-proxy component when it is started as a container on all nodes.</p> <p>This example uses the <code>lc</code> (least connection) load balancing algorithm, <code>rr</code> (round-robin) is the default.</p> <pre><code>    kubeproxy:\nextra_args:\nipvs-scheduler: lc\nproxy-mode: ipvs\n</code></pre> <ul> <li>Use the <code>rke up</code> command to apply the changes to the cluster</li> </ul>"},{"location":"kbs/000020035/#migrating-to-ipvs-on-an-existing-cluster","title":"Migrating to IPVS on an existing cluster","text":"<p>In recent Kubernetes versions when a proxy-mode is changed the managed iptables rules are not cleaned. To avoid inconsistency and unpredictable outcomes it is recommended to restart nodes that are in an existing cluster to ensure all service connectivity is accurate.</p> <p>If using using an immutable approach in your environment, replacing each node is also an option instead of restarting.</p> <p>Once the cluster has applied the above arguments to kube-proxy successfully and returned to the Active state, plan to drain, restart and/or replace each node during a maintenance period.</p> <p>This can be done on one node initially, and performed on one or more nodes at a time once tested.</p>"},{"location":"kbs/000020035/#further-reading","title":"Further reading","text":"<ul> <li>IPVS proxy mode</li> <li>Comparing kube-proxy modes: iptables or IPVS</li> <li>IPVS-Based In-Cluster Load Balancing Deep Dive</li> </ul>"},{"location":"kbs/000020035/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020036/","title":"How to check the default CoreDNS configmap of a Rancher Kubernetes Engine (RKE) Kubernetes version","text":"<p>This document (000020036) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020036/#situation","title":"Situation","text":""},{"location":"kbs/000020036/#task","title":"Task","text":"<p>You might have modified the default configmap for CoreDNS using Rancher Kubernetes Engine's (RKE) cluster configuration YAML ( <code>cluster.yml</code>). In this case, you may want to know the default configmap prior to upgrading Kubernetes. This verification step will help you to add all of the default/mandatory parameters to the modified configmap in RKE's <code>cluster.yml</code>, upon upgrade.</p>"},{"location":"kbs/000020036/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>python2</li> </ul>"},{"location":"kbs/000020036/#resolution","title":"Resolution","text":"<p>Download the <code>kontainer-metadata</code> according to the Rancher version you are running.</p> <p>Rancher 2.4.x:</p> <pre><code>curl -O https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.4/data/data.json\n</code></pre> <p>Rancher v2.5.x:</p> <pre><code>curl -O https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/dev-v2.5/data/data.json\n</code></pre> <p>Get the available template list:</p> <pre><code>python2 -c \"import sys, json; d=json.load(sys.stdin)['K8sVersionedTemplates']['coreDNS']; print json.dumps(d,indent=4)\" &lt;data.json\n</code></pre> <p>Output:</p> <pre><code>{\n\"&gt;=1.8.0-rancher0 &lt;1.16.0-alpha\": \"coredns-v1.8\",\n\"&gt;=1.17.0-alpha\": \"coredns-v1.17\",\n\"&gt;=1.16.0-alpha &lt;1.17.0-alpha\": \"coredns-v1.16\"\n}\n</code></pre> <p>Translation of one of the entry from the list is as follows:</p> <pre><code>\"&gt;=1.8.0-rancher0 &lt;1.16.0-alpha\": \"coredns-v1.8\",\n</code></pre> <p>If Kubenetes version is greater than or equal to <code>1.8.0-rancher0</code> and less than <code>1.16.0-alpha</code> ,then the CoreDNS key we have to use in next step is <code>coredns-v1.8</code></p> <pre><code>python2 -c \"import sys, json; print json.load(sys.stdin)['K8sVersionedTemplates']['templateKeys']['coredns-v1.8']\" &lt;data.json\n</code></pre> <p>The configmap will be printed on the screen along with other YAML template specs associated with CoreDNS.</p> <p>Sample output:</p> <pre><code>[...]\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: coredns\nnamespace: kube-system\nlabels:\naddonmanager.kubernetes.io/mode: EnsureExists\ndata:\nCorefile: |\n.:53 {\nerrors\nhealth\nkubernetes {{.ClusterDomain}} {{ if .ReverseCIDRs }}{{ .ReverseCIDRs }}{{ else }}{{ \"in-addr.arpa ip6.arpa\" }}{{ end }} {\npods insecure\nupstream\nfallthrough in-addr.arpa ip6.arpa\nttl 30\n}\nprometheus :9153\n{{- if .UpstreamNameservers }}\nforward . {{range $i, $v := .UpstreamNameservers}}{{if $i}} {{end}}{{.}}{{end}}\n{{- else }}\nforward . \"/etc/resolv.conf\"\n{{- end }}\ncache 30\nloop\nreload\nloadbalance\n}\n---\n[...]\n</code></pre>"},{"location":"kbs/000020036/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020037/","title":"Best practices for deploying Rancher v2.x and Rancher provisioned Kubernetes clusters in China","text":"<p>This document (000020037) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020037/#situation","title":"Situation","text":""},{"location":"kbs/000020037/#task","title":"Task","text":"<p>Users might experience a slow or unstable connection to GitHub and Docker Hub from China. As a result, Rancher Docker images and catalogs should be configured to use the Alibaba Cloud Docker image registry and Gitee git repositories, when deploying Rancher and downstream clusters within China. This article details how to configure these in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned clusters.</p>"},{"location":"kbs/000020037/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, being deployed in China, either a single Docker container or a Highly Available (HA) installation in Kubernetes</li> </ul>"},{"location":"kbs/000020037/#configure-the-alibaba-cloud-docker-image-repository-for-installation-of-rancher-v2x","title":"Configure the Alibaba Cloud Docker image repository for installation of Rancher v2.x","text":""},{"location":"kbs/000020037/#a-single-docker-container-installation-of-rancher","title":"A single Docker container installation of Rancher","text":"<p>Specify the repository URL in the <code>docker run</code> command, e.g.:</p> <pre><code>docker run -itd -p 80:80 -p 443:443 \\\n    --restart=unless-stopped \\\n    -e CATTLE_AGENT_IMAGE=\"registry.cn-hangzhou.aliyuncs.com/rancher/rancher-agent:v2.4.2\" \\\n    registry.cn-hangzhou.aliyuncs.com/rancher/rancher:v2.4.2\n</code></pre>"},{"location":"kbs/000020037/#a-highly-available-ha-installation-of-rancher-in-a-kubernetes-cluster","title":"A Highly Available (HA) installation of Rancher in a Kubernetes cluster","text":"<p>Specify private_registries in the RKE configuration file (cluster.yaml), e.g.:</p> <pre><code>nodes:\n  - address: x.x.x.x\n    internal_address: y.y.y.y\n    user: ubuntu\n    role: [controlplane,worker,etcd]\nprivate_registries:\n     - url: registry.cn-hangzhou.aliyuncs.com\n       is_default: true\n</code></pre>"},{"location":"kbs/000020037/#helm-v2-installation-only","title":"Helm v2 installation only","text":"<p>Specify the tiller-image for tiller initialisation. e.g.:</p> <pre><code>helm init --service-account tiller --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:&lt;tag&gt;\n</code></pre> <p>The list of tiller image tags are available here.</p>"},{"location":"kbs/000020037/#downstream-clusters","title":"Downstream clusters","text":"<p>Configure the system-default-registry for Rancher, setting the value to <code>registry.cn-hangzhou.aliyuncs.com</code>, per the following example:</p> <p></p>"},{"location":"kbs/000020037/#configure-the-gitte-git-repository-for-rancher-app-catalogs","title":"Configure the Gitte git repository for Rancher App catalogs","text":"<p>Having installed Rancher, replace the default catalog URLs with the equivalent Gitee URLs, per the following table:</p> Rancher Catalog URL Rancher GitHub URL Rancher Gitee URL https://git.rancher.io/helm3-charts https://github.com/rancher/helm3-charts https://gitee.com/rancher/helm3-charts https://git.rancher.io/charts https://github.com/rancher/charts https://gitee.com/rancher/charts https://git.rancher.io/system-charts https://github.com/rancher/system-charts https://gitee.com/rancher/system-charts"},{"location":"kbs/000020037/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020038/","title":"How to enable Envoy access logging in Rancher v2.3 and v2.4 deployed Istio","text":"<p>This document (000020038) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020038/#situation","title":"Situation","text":""},{"location":"kbs/000020038/#task","title":"Task","text":"<p>This article details how to enable Envoy's access logging, for Rancher deployed Istio, in Rancher v2.3 and v2.4</p>"},{"location":"kbs/000020038/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster managed by Rancher v2.3 or v2.4, with Istio enabled</li> </ul>"},{"location":"kbs/000020038/#resolution","title":"Resolution","text":"<p>Access logging can be enabled for Envoy, in Rancher deployed Istio, by setting the <code>global.proxy.accessLogFile</code> path and <code>global.proxy.accessLogEncoding</code> type via Custom Answers on the Istio configuration.</p> <p>Setting the <code>accessLogFile</code> path to <code>/dev/stdout</code> will route the Envoy access logs to the <code>istio-sidecar</code> container logs, exposing them via <code>kubectl logs</code> or any log forwarding endpoint you have configured in the cluster.</p> <p>The log format, specified in <code>accessLogEncoding</code>, can be set to JSON or TEXT.</p> <p>To enable access logging, perform the following steps:</p> <ol> <li>Navigate to the cluster view in the Rancher UI for the desired cluster and select <code>Tools</code> &gt; <code>Istio</code>.</li> <li>Under the <code>Custom Answers</code> section, enter the following two value pairs and click <code>Save</code> or <code>Enable</code> (the option will depend on whether you have Istio enabled in the cluster already):</li> </ol> <pre><code>global.proxy.accessLogFile=/dev/stdout\nglobal.proxxy.accessLogEncoding=JSON\n</code></pre> <ol> <li>After enabling access logging, you can test the configuration with the Istio <code>sleep</code> and <code>httpbin</code> sample applications, per the Istio documentation.</li> </ol>"},{"location":"kbs/000020038/#further-reading","title":"Further reading","text":"<ul> <li>Istio \"Getting Envoy's Access Logs\" Documentation</li> <li>Envoy Access Logging Documentation</li> </ul>"},{"location":"kbs/000020038/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020039/","title":"How to collect a stacktrace from the Rancher server process in Rancher v2.x","text":"<p>This document (000020039) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020039/#situation","title":"Situation","text":""},{"location":"kbs/000020039/#task","title":"Task","text":"<p>During troubleshooting it may be useful to collect a stacktrace from a running Rancher instance, and this article details the steps for creating one.</p>"},{"location":"kbs/000020039/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, either a single Docker container or a Highly Available (HA) installation in Kubernetes</li> </ul>"},{"location":"kbs/000020039/#steps","title":"Steps","text":""},{"location":"kbs/000020039/#a-single-docker-container-installation-of-rancher","title":"A single Docker container installation of Rancher","text":"<ol> <li> <p>Open two shell sessions on the server running the single container installation of Rancher.</p> </li> <li> <p>In one of the shell sessions, run <code>docker ps</code>, to locate and note the ID or name of the Rancher server container.</p> </li> <li> <p>In the same shell, start streaming the Docker container logs to file with the command below, replacing <code>&lt;RANCHER_CONTAINER_ID_OR_NAME&gt;</code> with the Rancher container ID or name identified in step 2.</p> </li> </ol> <pre><code>docker logs -f &lt;RANCHER_CONTAINER_ID_OR_NAME&gt; &gt; rancher.log 2&gt;&amp;1\n</code></pre> <ol> <li>In the other shell, send a SIGABRT signal to the rancher process in the container, to trigger a stacktrace, with the command below:</li> </ol> <pre><code>docker exec &lt;RANCHER_CONTAINER_ID_OR_NAME&gt; bash -c 'kill -SIGABRT $(pgrep -x rancher)'\n</code></pre> <ol> <li>When the stacktrace generation is complete the Rancher container will restart and the <code>docker logs</code> command in step 3. will exit. You can then provide the <code>rancher.log</code> file, containing the trace, to Rancher Support.</li> </ol>"},{"location":"kbs/000020039/#a-highly-available-ha-installation-of-rancher-in-a-kubernetes-cluster","title":"A Highly Available (HA) installation of Rancher in a Kubernetes cluster","text":"<ol> <li> <p>Open two shell sessions and source an admin kubeconfig for the Rancher server cluster in both.</p> </li> <li> <p>In one of the shell sessions, determine the name (e.g. <code>rancher-7b9f4764f5-rs2jx</code>) of the Rancher leader pod per the steps in the article here.</p> </li> <li> <p>In the same shell, start streaming the Rancher leader pod's logs to file, replacing with the pod name identified in step 2.:</p> </li> </ol> <pre><code>kubectl logs -n cattle-system --tail=-1 -f &lt;LEADER_POD_NAME&gt; &gt; rancher.log\n</code></pre> <ol> <li>In the other shell, send a SIGABRT signal to the rancher process in the leader pod, with the command below, to trigger a stacktrace, replacing with the pod name identified in step 2.:</li> </ol> <pre><code>kubectl -n cattle-system exec &lt;LEADER_POD_NAME&gt; -- bash -c 'kill -SIGABRT  $(pgrep -x rancher)'\n</code></pre> <ol> <li>When the stacktrace generation is complete the Rancher leader pod will restart and the <code>kubectl logs</code> command in step 3. will exit. You can then provide the <code>rancher.log</code> file, containing the trace, to Rancher Support.</li> </ol>"},{"location":"kbs/000020039/#validating-the-success-of-the-stacktrace","title":"Validating the success of the stacktrace","text":"<p>If you want to validate the stacktrace was successfully generated, you can confirm the presence of the <code>SIGABRT</code> signal and <code>goroutine</code> traces in the <code>rancher.log</code> file as below:</p> <pre><code>SIGABRT: abort\nPC=0x461781 m=0 sigcode=0\n\ngoroutine 0 [idle]:\nruntime.futex(0x7370308, 0x80, 0x0, 0x0, 0xc000000000, 0x7ffdd633d8f0, 0x435863, 0xc000446848, 0x7ffdd633d910, 0x40b04f, ...)\n    /usr/local/go/src/runtime/sys_linux_amd64.s:535 +0x21\nruntime.futexsleep(0x7370308, 0x7ffd00000000, 0xffffffffffffffff)\n    /usr/local/go/src/runtime/os_linux.go:44 +0x46\nruntime.notesleep(0x7370308)\n    /usr/local/go/src/runtime/lock_futex.go:151 +0x9f\nruntime.stoplockedm()\n    /usr/local/go/src/runtime/proc.go:2068 +0x88\nruntime.schedule()\n    /usr/local/go/src/runtime/proc.go:2469 +0x485\nruntime.park_m(0xc000be4780)\n    /usr/local/go/src/runtime/proc.go:2610 +0x9d\nruntime.mcall(0x0)\n    /usr/local/go/src/runtime/asm_amd64.s:318 +0x5b\n</code></pre> <p>This should end with a section similar to the following after the goroutine traces:</p> <pre><code>rax    0xca\nrbx    0x73701c0\nrcx    0x461783\nrdx    0x0\nrdi    0x7370308\nrsi    0x80\nrbp    0x7ffdd633d8d8\nrsp    0x7ffdd633d890\nr8     0x0\nr9     0x0\nr10    0x0\nr11    0x286\nr12    0x0\nr13    0x1\nr14    0xc000dde7e0\nr15    0x0\nrip    0x461781\nrflags 0x286\ncs     0x33\nfs     0x0\ngs     0x0\n</code></pre>"},{"location":"kbs/000020039/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Documentation on Troubleshooting the Rancher Server Kubernetes Cluster</li> </ul>"},{"location":"kbs/000020039/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020040/","title":"Can I move from self-managed Rancher to SUSE Rancher Hosted?","text":"<p>This document (000020040) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020040/#resolution","title":"Resolution","text":"<p>Yes, we can do a one time migration of your entire self-hosted Rancher into the SUSE Rancher Hosted service. Requirements for migration are:</p> <ol> <li>You must be running the latest stable version of Rancher (currently v2.7.1) for at least one week.</li> <li>Rancher cluster must be running Kubernetes v1.23 or higher for at least one week.</li> <li>All managed/downstream clusters must be running Kubernetes v1.23 or higher for at least one week.</li> <li>Have a kubeconfig file for all your managed (downstream) Kubernetes clusters. The kubeconfig file should point directly to the cluster\u2019s kube-apiserver endpoint and not to Rancher. For RKE clusters, see Authorized Cluster Endpoint for setup.</li> <li>Authentication integration must be with a public endpoint that SUSE Rancher Hosted can reach.</li> <li>Global and cluster catalogs must have a public endpoint that SUSE Rancher Hosted can reach.</li> <li>Clusters provisioned using the vSphere node driver cannot be managed unless the vSphere API endpoint is made available to SUSE Rancher Hosted.</li> <li>All downstream/managed clusters must have outbound Internet access over port 443, or at least outbound access to SUSE Rancher Hosted.</li> <li>You must provide the output of Helm Chart values used to install Rancher: <code>helm get values -n cattle-system rancher &gt; rancher-values.yaml</code></li> <li>You must provide a copy of the system summary report for your Rancher environment. See this article for instructions.</li> </ol> <p>To get started on migrating to SUSE Rancher Hosted, open a support case on our support portal.</p>"},{"location":"kbs/000020040/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020041/","title":"How can I validate network policies within a Kubernetes cluster?","text":"<p>This document (000020041) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020041/#situation","title":"Situation","text":""},{"location":"kbs/000020041/#question","title":"Question","text":"<p>Traffic flowing inside a kubernetes cluster is non-isolated by default so that each pod can communicate with every other pod. In some environments there is a need to ensure the proper isolation or restriction of each application in differing namespaces. Network Policies handle this microservice network segmentation and meet this need by defining the control to other entities' IP addresses (on OSI Layer 3) or network ports (on OSI Layer 4).</p> <p>Access for Pod communication with other entities are identified with Network Policies and defined by other pods, the relevant namespaces, and IP CIDR Blocks. Pods and Namespaces are specified using a selector, like <code>app=example</code> .</p> <p>Once the Network Policies are defined, how can a Kubernetes administrator test and validate them?</p>"},{"location":"kbs/000020041/#answer","title":"Answer","text":"<p>Illuminatio is an open-source project written in Python3 by Inovex. It can run standalone, and is also available as a docker container. It creates test-cases for both the network policies and their inverse rules, generates an illuminatio-runner daemonset, tests all the cases against the defined network policies, and reports back on success or failure for each rule and inverted-rule. Illuminatio can use the current kubectl config for cluster access while working in the shell session, or designate the config file with the optional --kubeconfig flag.</p>"},{"location":"kbs/000020041/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster v1.15.2+</li> <li>kubectl access to the cluster</li> <li>Network Plugin (CNI) which supports Network Policies</li> </ul>"},{"location":"kbs/000020041/#basic-usage","title":"Basic Usage","text":"<p>Assuming the Kubernetes admin has some network policies to test, the tool is very easy to use. It has three verbs to choose from, \"clean\", \"generate\" and, \"run\". The generate verb will only generate the tests, while clean removes them and run performs the test. Most users will want to use <code>illuminatio clean run</code> to start fresh, run the generated tests and report on their success. The results are also written to a configmap.</p> <p>The following are some common examples of Network Policies, and how Illuminatio can assist with validation. Examples are taken from this network policies recipes github repo, and applied to a kubernetes cluster, in the default namespace. Validation is performed with Illuminatio instead of a temporary pod.</p>"},{"location":"kbs/000020041/#deny-traffic-to-an-application","title":"Deny Traffic to an application","text":"<p>Save this file as <code>web-deny-all.yaml</code> and then apply the network policy with <code>kubectl -f web-deny-all.yaml</code>. Notice it is deploying to the default namespace. Prepare the pod for this example with a selector of app=web.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\nname: web-deny-all\nspec:\npodSelector:\nmatchLabels:\napp: web\ningress: []\n</code></pre> <pre><code>kubectl run --generator=run-pod/v1 web --image=nginx --labels app=web --expose --port 80\n</code></pre> <p>Show the current network policies, then run cases for all of them. Illuminatio will deploy a deamonset and run all the test cases, any passing tests show \"success\" in the last column of the report. Note: success indicates the test was successful, even if testing a connection denial.</p> <pre><code>$ kubectl get netpol\nNAME           POD-SELECTOR   AGE\nweb-deny-all   app=web        13m\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nGenerated 1 cases in 0.0616 seconds\n\nFROM             TO               PORT\ndefault:app=web  default:app=web  -*\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 1 tests in 7.1175 seconds\nFROM             TO               PORT  RESULT\ndefault:app=web  default:app=web  -*    success\n</code></pre>"},{"location":"kbs/000020041/#limit-traffic-to-an-application","title":"Limit Traffic to an application","text":"<p>Allow app=bookstore pods to communicate with only other app=bookstore pods.</p> <p><code>kubectl run --generator=run-pod/v1 apiserver --image=nginx --labels app=bookstore,role=api --expose --port 80</code></p> <p>Save the following as api-allow.yaml and issue kubectl apply -f api-allow.yaml.</p> <p>Network policies are accumulative.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\nname: api-allow\nspec:\npodSelector:\nmatchLabels:\napp: bookstore\nrole: api\ningress:\n- from:\n- podSelector:\nmatchLabels:\napp: bookstore\n</code></pre> <pre><code>$ kubectl apply -f api-allow.yaml\nnetworkpolicy.networking.k8s.io/api-allow created\n\n$ kubectl get netpol\nNAME           POD-SELECTOR             AGE\napi-allow      app=bookstore,role=api   5s\nweb-deny-all   app=web                  18m\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nGenerated 5 cases in 0.0594 seconds\n\nFROM                                                             TO                              PORT\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*\ndefault:app=web                                                  default:app=web                 -*\ndefault:app=bookstore                                            default:app=bookstore,role=api  *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 5 tests in 13.2368 seconds\nFROM                                                             TO                              PORT  RESULT\n\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*    success\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*    success\ndefault:app=web                                                  default:app=web                 -*    success\ndefault:app=bookstore                                            default:app=bookstore,role=api  *     success\n</code></pre>"},{"location":"kbs/000020041/#allow-whitelisted-traffic-for-appweb","title":"Allow whitelisted traffic for app=web","text":"<p>This policy will whitelist the app=web pods from the first example, with a new web-allow-all.yaml file. This Network Policy also voids the first example, by allowing all traffic. Because the traffic connections are allowed, Illuminatio recognizes this and avoids generating the negative (inverted) test cases.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\nname: web-allow-all\nnamespace: default\nspec:\npodSelector:\nmatchLabels:\napp: web\ningress:\n- {}\n</code></pre> <pre><code>$ kubectl apply -f web-allow-all.yaml\nnetworkpolicy.networking.k8s.io/web-allow-all created\n\n$ kubectl get netpol\nNAME            POD-SELECTOR             AGE\napi-allow       app=bookstore,role=api   20m\nweb-allow-all   app=web                  32s\nweb-deny-all    app=web                  39m\n</code></pre> <pre><code>$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nNot generating negative tests for host ClusterHost(namespace=default, podLabels={'app': 'web'})as all connecti\nons to it are allowed\nGenerated 5 cases in 0.0551 seconds\n\nFROM                                                             TO                              PORT\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*\ndefault:app=bookstore                                            default:app=bookstore,role=api  *\n*:*                                                              default:app=web                 *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 5 tests in 13.4065 seconds\nFROM                                                             TO                              PORT  RESULT\n\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*    success\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*    success\ndefault:app=bookstore                                            default:app=bookstore,role=api  *     success\n*:*                                                              default:app=web                 *     success\n</code></pre>"},{"location":"kbs/000020041/#limit-access-to-a-namespace","title":"Limit access to a Namespace","text":"<p>This policy will deny all traffic from other namespaces, limiting to just the current namespace. In other words, the <code>secondary</code> namespace allows connections internally, denying any from the <code>default</code> namespace in previous examples. Note how Illuimnatio tests all network policies, cluster-wide in all namespaces.</p> <pre><code>kubectl create namespace secondary\n\nkubectl run --generator=run-pod/v1 web --namespace secondary --image=nginx \\\n    --labels=app=web --expose --port 80\n</code></pre> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\nnamespace: secondary\nname: deny-from-other-namespaces\nspec:\npodSelector:\nmatchLabels:\ningress:\n- from:\n- podSelector: {}\n</code></pre> <pre><code>$ kubectl apply -f deny-from-other-namespaces.yaml\nnetworkpolicy \"deny-from-other-namespaces\" created\"\n\n$ kubectl get netpol -n secondary\nNAME                         POD-SELECTOR   AGE\ndeny-from-other-namespaces   &lt;none&gt;         7s\n\n$ kubectl get netpol -n default\nNAME            POD-SELECTOR             AGE\napi-allow       app=bookstore,role=api   44m\nweb-allow-all   app=web                  24m\nweb-deny-all    app=web                  63m\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nNot generating negative tests for host ClusterHost(namespace=default, podLabels={'app': 'web'})as all connecti\nons to it are allowed\nGenerated 7 cases in 0.0621 seconds\n\nFROM                                                             TO                              PORT\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-secondary:*                                 secondary:*                     -*\ndefault:app=bookstore                                            default:app=bookstore,role=api  *\n*:*                                                              default:app=web                 *\nsecondary:*                                                      secondary:*                     *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 7 tests in 13.2361 seconds\nFROM                                                             TO                              PORT  RESULT\n\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*    success\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-secondary:*                                 secondary:*                     -*    success\ndefault:app=bookstore                                            default:app=bookstore,role=api  *     success\n*:*                                                              default:app=web                 *     success\nsecondary:*                                                      secondary:*                     *     success\n</code></pre>"},{"location":"kbs/000020041/#allow-all-traffic-from-a-certain-namespace","title":"Allow All Traffic from a certain Namespace","text":"<p>In this example, there are two namespaces, <code>dev</code> with purpose=testing and <code>prod</code> with purpose=production. The <code>default</code> namespace should allow connections from <code>production</code> but not <code>dev</code>. This is convenient for establishing policies along namespace boundaries. All previous network policies have been removed for this scenario.</p> <pre><code>kubectl run --generator=run-pod/v1 web --image=nginx \\\n    --labels=app=web --expose --port 80\n\nkubectl create namespace dev\nkubectl label namespace/dev purpose=testing\n\nkubectl create namespace prod\nkubectl label namespace/prod purpose=production\n</code></pre> <p>The contents of the web-allow-prod.yaml file.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\nname: web-allow-prod\nspec:\npodSelector:\nmatchLabels:\napp: web\ningress:\n- from:\n- namespaceSelector:\nmatchLabels:\npurpose: production\n</code></pre> <pre><code>$ kubectl apply -f web-allow-prod.yaml\nnetworkpolicy \"web-allow-prod\" created\n\n$ kubectl get netpol -A\nNAMESPACE   NAME             POD-SELECTOR   AGE\ndefault     web-allow-prod   app=web        44s\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nGenerated 2 cases in 0.0645 seconds\n\nFROM                                       TO               PORT\nilluminatio-inverted-purpose=production:*  default:app=web  -*\npurpose=production:*                       default:app=web  *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 2 tests in 7.1767 seconds\nFROM                                       TO               PORT  RESULT\nilluminatio-inverted-purpose=production:*  default:app=web  -*    success\npurpose=production:*                       default:app=web  *     success\n</code></pre> <p>To view the results of the test programmatically, check the configmap for the illuminatio namespace, before performing another \"clean\" operation.</p> <pre><code>$ kubectl get cm -n illuminatio\nNAME                               DATA   AGE\nilluminatio-cases-cfgmap           1      45s\nilluminatio-runner-s87rw-results   2      40s\nilluminatio-runner-z52gb-results   2      41s\n\n$ kubectl get cm -n illuminatio illuminatio-runner-s87rw-results -o yaml\napiVersion: v1\ndata:\n  results: |\n    illuminatio-inverted-purposeproduction:illuminatio-dummy-nqtc7:\n      10.43.168.221:\n        '-80':\n          nmap-state: filtered\n          string: 'Test 10.43.168.221:-80 succeeded\n\n            Couldn''t reach 10.43.168.221 on port 80. Expected target to not be reachable'\n          success: true\n    prod:illuminatio-dummy-tc5v9:\n      10.43.168.221:\n        '80':\n          nmap-state: open\n          string: 'Test 10.43.168.221:80 succeeded\n\n            Could reach 10.43.168.221 on port 80. Expected target to be reachable'\n          success: true\n  runtimes: |\n    overall: error\n    tests:\n      illuminatio-inverted-purposeproduction:illuminatio-dummy-nqtc7:\n        10.43.168.221: 2.1185858249664307\n      prod:illuminatio-dummy-tc5v9:\n        10.43.168.221: 0.2853882312774658\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2020-11-23T21:47:53Z\"\n  labels:\n    illuminatio-cleanup: always\n  managedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      f:data:\n        .: {}\n        f:results: {}\n        f:runtimes: {}\n      f:metadata:\n        f:labels:\n          .: {}\n          f:illuminatio-cleanup: {}\n    manager: Swagger-Codegen\n    operation: Update\n    time: \"2020-11-23T21:47:53Z\"\n  name: illuminatio-runner-s87rw-results\n  namespace: illuminatio\n  resourceVersion: \"906614\"\n  selfLink: /api/v1/namespaces/illuminatio/configmaps/illuminatio-runner-s87rw-results\n  uid: 2c2f7434-d1ee-49c0-b77d-c11b7848f4da\n</code></pre>"},{"location":"kbs/000020041/#further-reading-and-other-useful-links","title":"Further Reading and Other Useful Links","text":"<ul> <li>Securing Kubernetes Cluster Networking</li> <li>Example Network Policy Recipes, ahmetb/kubernetes-network-policy-recipes</li> <li>Inovex Illuminatio, Kubernetes Network Policy Validator</li> <li>Inovex/Illuminatio GitHub Project Page</li> </ul>"},{"location":"kbs/000020041/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020042/","title":"Rancher v2.x provisioned vSphere cluster nodes stuck in provisioning, with \"Waiting for SSH to be available\", as a result of pre-existing cloud-init configuration in VM Template","text":"<p>This document (000020042) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020042/#situation","title":"Situation","text":""},{"location":"kbs/000020042/#issue","title":"Issue","text":"<p>Upon launching a vSphere Node Driver cluster in Rancher v2.x, nodes within the cluster are stuck in provisioning, with the message <code>Waiting for SSH to be available</code>. Logging into the nodes via SSH and checking the auth log directly reveals failed SSH connection attempts for a missing <code>docker</code> user.</p>"},{"location":"kbs/000020042/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned vSphere cluster, using the vSphere Node Driver.</li> </ul>"},{"location":"kbs/000020042/#root-cause","title":"Root cause","text":"<p>When provisioning a vSphere Node Driver cluster Rancher v2.x uses cloud-init to generate an ssh-keypair for the user <code>docker</code> and copy this into the Virtual Machine on initial boot.</p> <p>In some Linux distributions, including Ubuntu Server 18.04, the standard OS installation process generates a cloud-init configuration. Installation of the OS is performed during the intitial setup of the VM Templates, prior to cluster provisioning via Rancher, and this existing cloud-init configuration within the Template can intefere with Rancher's ability to insert its own cloud-init.</p>"},{"location":"kbs/000020042/#resolution","title":"Resolution","text":"<p>Convert the Template back to a VM and run:</p> <pre><code>sudo cloud-init clean\n</code></pre> <p>This command will clean the Template of any existing cloud-inits, once complete you can convert the VM back to a template to try again.</p>"},{"location":"kbs/000020042/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020043/","title":"Rancher v2.5 provisioned Kubernetes clusters, without a worker role node, display \"Cluster health check failed: cluster agent is not ready\" error","text":"<p>This document (000020043) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020043/#situation","title":"Situation","text":""},{"location":"kbs/000020043/#issue","title":"Issue","text":"<p>When provisioning or updating a Rancher-provisioned Kubernetes cluster in Rancher v2.5.x, such that the cluster does not have a node with the worker role, the cluster will enter an <code>Error</code> status, displaying the message <code>Cluster health check failed: cluster agent is not ready</code>. By comparison, in Rancher v2.4.x, the cluster status would show <code>Active</code> in this scenario.</p>"},{"location":"kbs/000020043/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.5.x instance</li> <li>Rancher <code>server-url</code> configured to a resolvable hostname, e.g. rancher.example.com, and not an IP address</li> <li>A Rancher provisioned Kubernetes cluster, either a Custom cluster or with nodes in an infrastructure provider using a Node Driver</li> </ul>"},{"location":"kbs/000020043/#root-cause","title":"Root cause","text":"<p>Rancher v2.5.x implements an additional cluster health check to ensure that the Pod for the <code>cluster-agent</code> Deployment in the <code>cattle-system</code> namespace of the downstream cluster is ready and successfully connected to the Rancher server. The <code>cluster-agent</code> Pod will use cluster DNS to resolve the Rancher server hostname. As a result, in the instance that there is no node with the worker role, CoreDNS Pods will be unable to schedule and the <code>cluster-agent</code> will thus be unable to resolve the Rancher hostname, causing this check to fail.</p>"},{"location":"kbs/000020043/#resolution","title":"Resolution","text":"<p>Provision a node in the cluster with the worker role, to ensure that CoreDNS Pods can be successfully scheduled.</p>"},{"location":"kbs/000020043/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020044/","title":"Rancher log forwarding to an Elasticsearch endpoint stops functioning as a result of connection reload behaviour in Rancher v2.3, prior to v2.3.8, and v2.4, prior to v2.4.4","text":"<p>This document (000020044) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020044/#situation","title":"Situation","text":""},{"location":"kbs/000020044/#issue","title":"Issue","text":"<p>In Rancher v2.0 - v2.2, v2.3 prior to v2.3.8, and v2.4 prior to v2.4.4, a previously functioning log forwarding configuration to an Elasticsearch instance could stop successfully forwarding logs, without any configuration change and whilst the Elasticsearch endpoint was still available. The logs of the <code>rancher-logging-fluentd</code> Pod(s) in the <code>cattle-logging</code> Namespace of the affected cluster, reveal log messages of the following format:</p> <pre><code>failed to flush the buffer, retry_time=0, next_retry_seconds=2019-07-24 07:07:31 +0000, chunk=58e67cdcd7d1406de13fe55a26fe6cad, error_class=Fluent::Plugin::ElasticSearchOutput::RecoverableRequestFailure error=\"could not push logs to ElasticSearch cluster ({:host=&gt;elasticsearch.example.com, :port=&gt;443, :scheme=&gt;\\\"https\\\"}): connect_write timeout reached\"\n</code></pre> <p>or</p> <pre><code>failed to flush the buffer. retry_time=10 next_retry_seconds=2019-07-24 07:07:31 +0000 chunk=\"58e67cdcd7d1406de13fe55a26fe6cad\" error_class=Elasticsearch::Transport::Transport::Error error=\"Cannot get new connection from pool.\"\n</code></pre>"},{"location":"kbs/000020044/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, running Rancher v2.0 - v2.2, v2.3 prior to v2.3.8, or v2.4 prior to v2.4.4</li> <li>A Rancher managed cluster with Rancher log forwarding configured to an Elasticsearch endpoint</li> </ul>"},{"location":"kbs/000020044/#root-cause","title":"Root cause","text":"<p>By default the <code>fluent-plugin-elasticsearch</code> fluentd plugin will attempt to reload the host list from elasticsearch after 10000 requests. This behaviour is a result of default functionality in the elasticsearch-ruby gem, as documented in the plugin's FAQ. This reload behaviour is not compatible with all elasticsearch environments, and failure of the reload results in the plugin failing to forward further log events.</p>"},{"location":"kbs/000020044/#resolution","title":"Resolution","text":"<p>In Rancher v2.3, from v2.3.8, and Rancher v2.4, from v2.4.4, the Rancher log forwarding configuration for Elasticsearch endpoints was updated to include the option <code>reload_connections false</code>. This disables the default connection reload behaviour, preventing occurrences of this issue.</p>"},{"location":"kbs/000020044/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020045/","title":"How To Fix Hairpin Connectivity with IPVS enabled","text":"<p>This document (000020045) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020045/#situation","title":"Situation","text":""},{"location":"kbs/000020045/#issue","title":"Issue","text":"<p>Many users enable IPVS for kube-proxy to help alleviate bottlenecks associated with iptables. An issue arises on Kubernetes 1.15 and below where the masquerade iptables rule doesn't get applied and therefore hairpin connectivity stops working.</p> <p>You can determine if this isn't working by connecting to a pod, from itself via its service. The connection should time out. It's worth noting that if the node is never rebooted after enabling IPVS the masquerade rule will remain, but it will not be restored after reboot.</p>"},{"location":"kbs/000020045/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes 1.15 and below</li> <li>IPVS enabled for kube-proxy</li> </ul>"},{"location":"kbs/000020045/#workaround","title":"Workaround","text":"<p>The workaround is to apply the <code>masquerade-all=true</code> flag to kube-proxy to force it to apply the masquerade iptables rule.</p>"},{"location":"kbs/000020045/#resolution","title":"Resolution","text":"<p>Edit the cluster yaml and change <code>services.kubeproxy.extra_args</code> to reflect the following and hit save:</p> <pre><code>  kubeproxy:\nextra_args:\nproxy-mode: ipvs\nmasquerade-all: true\n</code></pre> <p>Once this is done, hairpin connectivity should be restored.</p>"},{"location":"kbs/000020045/#further-reading","title":"Further reading","text":"<ul> <li>Kubernetes blog deep-dive on kube-proxy IPVS mode</li> </ul>"},{"location":"kbs/000020045/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020046/","title":"Resolving \"Conflict. The container name is already in use\" errors when updating a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020046) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020046/#situation","title":"Situation","text":""},{"location":"kbs/000020046/#issue","title":"Issue","text":"<p>When invoking <code>rke up</code>, to update a Rancher Kubernetes Engine (RKE) CLI provisioned Kubernetes cluster, or using Rancher to update a Rancher v2.x provisioned cluster, you encounter a <code>Conflict. The container name is already in use</code> error, of the following format:</p> <p>Format of error from RKE CLI, for RKE provisioned clusters</p> <pre><code>FATA[0219] [file-deploy] Failed to deploy file [/etc/kubernetes/audit-policy.yaml] on node [172.27.6.22]: Failed to create [file-deployer] container on host [172.27.6.22]: Failed to create Docker container [file-deployer] on host [172.27.6.22]: Error response from daemon: Conflict. The container name \"/file-deployer\" is already in use by container \"66b777d981aa0b0a9d6bc73e381e0f2bc8fc33ec00926aa0db51347607f8fcf8\". You have to remove (or rename) that container to be able to reuse that name.\n</code></pre> <p>Format of error in Rancher UI, for Rancher v2.x provisioned clusters</p> <pre><code>[Failed to create Certificates deployer container on host [172.27.3.21]: Failed to create Docker container [cert-deployer] on host [172.27.3.21]: Error response from daemon: Conflict. The container name \"/cert-deployer\" is already in use by container \"c3b35c454d6000266098573949d021f45b13a3c7f7306d7fdb58a5766f2f3312\". You have to remove (or rename) that container to be able to reuse that name.]\n</code></pre>"},{"location":"kbs/000020046/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI, or Rancher v2.x provisioned Kubernetes cluster</li> <li>SSH access to affected node(s) and Docker CLI access</li> </ul>"},{"location":"kbs/000020046/#root-cause","title":"Root cause","text":"<p>This error can occur when an <code>rke up</code> invocation or Rancher cluster update process is interrupted, leaving temporary cluster deployment containers on nodes.</p>"},{"location":"kbs/000020046/#resolution","title":"Resolution","text":"<ol> <li>SSH into the node.</li> <li>Remove stuck containers:</li> </ol> <pre><code>docker rm -f file-deployer cert-deployer\n</code></pre> <ol> <li>Trigger a cluster update:</li> <li>For RKE CLI provisioned clusters, invoke <code>rke up</code>.</li> <li>For Rancher provisioned clusters:<ul> <li>Browse to the cluster in the Rancher UI</li> <li>Click <code>Edit</code> from the action menu</li> <li>Click <code>Edit as YAML</code></li> <li>Find the option <code>addon_job_timeout</code> and edit the value, incrementing it by one</li> <li>Click <code>Save</code></li> </ul> </li> </ol>"},{"location":"kbs/000020046/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020047/","title":"How to disable Calico Telemetry in a Kubernetes cluster with the Canal or Calico CNI","text":"<p>This document (000020047) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020047/#situation","title":"Situation","text":""},{"location":"kbs/000020047/#task","title":"Task","text":"<p>By default, Calico reports anonymous telemetry data, containing the Calico version number and cluster size, to an endpoint at projectcalico.org. This article provides details on how to disable this reporting.</p>"},{"location":"kbs/000020047/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster, provisioned with the Calico or Canal CNI</li> <li>kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user</li> </ul>"},{"location":"kbs/000020047/#resolution","title":"Resolution","text":"<p>To disable the Calico telemetry reporting, execute the following command against the cluster:</p> <pre><code>kubectl patch felixconfigurations.crd.projectcalico.org default  -p '{\"spec\":{\"usageReportingEnabled\": false}}' --type=merge\n</code></pre>"},{"location":"kbs/000020047/#further-reading","title":"Further reading","text":"<ul> <li>Calico configuration documentation</li> </ul>"},{"location":"kbs/000020047/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020048/","title":"How to increase the inotify.max_user_watches and inotify.max_user_instances sysctls on a Linux host","text":"<p>This document (000020048) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020048/#situation","title":"Situation","text":""},{"location":"kbs/000020048/#task","title":"Task","text":"<p>The sysctls <code>fs.inotify.max_user_instances</code> and <code>fs.inotify.max_user_watches</code> define user limits on the number of inotify resources and inotify file watches. If these limits are reached, you may experience processes failing with error messages related to the limits, for example:</p> <pre><code>ENOSPC: System limit for number of file watchers reached...\n</code></pre> <pre><code>The configured user limit (128) on the number of inotify instances has been reached\n</code></pre> <pre><code>The default defined inotify instances (128) has been reached\n</code></pre> <p>In the context of a Kubernetes cluster, this behaviour would exhibit as failing Pods, with inotify related errors in the Pod logs similar to the above. This article details how to check the current limits configured and how to increase these.</p>"},{"location":"kbs/000020048/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Linux host</li> </ul>"},{"location":"kbs/000020048/#resolution","title":"Resolution","text":""},{"location":"kbs/000020048/#check-current-limits","title":"Check current limits","text":"<p>You can check the current inotify user instance limit, with the following:</p> <pre><code>cat /proc/sys/fs/inotify/max_user_instances\n</code></pre> <p>Similarly, the current inotify user watch limit can be checked as follows:</p> <pre><code>cat /proc/sys/fs/inotify/max_user_watches\n</code></pre>"},{"location":"kbs/000020048/#update-the-limits","title":"Update the limits","text":"<p>You can update the limits temporarily, with the following commands (setting the values to 8192 and 524288 respectively in this example):</p> <pre><code>sudo sysctl fs.inotify.max_user_instances=8192\nsudo sysctl fs.inotify.max_user_watches=524288\nsudo sysctl -p\n</code></pre> <p>In order to make the changes permanent, i.e. to persist a reboot, you can set <code>fs.inotify.max_user_instances=8192</code> and <code>fs.inotify.max_user_watches=524288</code> in the file <code>/etc/sysctl.conf</code>.</p> <p>After updating the limits, you can validate these on the host again, as above, with <code>cat /proc/sys/fs/inotify/max_user_instances</code> and <code>cat /proc/sys/fs/inotify/max_user_watches</code>.</p> <p>To check the value as reflected in a running container, exec into the container and cat the files:</p> <pre><code>docker exec -it &lt;CONTAINER ID&gt; cat /proc/sys/fs/inotify/max_user_instances\n</code></pre> <p>and</p> <pre><code>docker exec -it &lt;CONTAINER ID&gt; cat /proc/sys/fs/inotify/max_user_watches\n</code></pre> <p>If the updated limits are not reflected on a host after running <code>sysctl -p</code>, reboot the host after setting the limits in <code>/etc/sysctl.conf</code>.</p>"},{"location":"kbs/000020048/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020049/","title":"How to perform packet captures?","text":"<p>This document (000020049) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020049/#situation","title":"Situation","text":""},{"location":"kbs/000020049/#task","title":"Task","text":"<p>It's often necessary to perform packet captures to debug an issue either in production or non-production setup. This article provides the steps to do the same.</p>"},{"location":"kbs/000020049/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Be able to pull the image <code>leodotcloud/swiss-army-knife</code> either directly or via HTTP/HTTPS proxy or using a registry mirror or via artifactory.</li> </ul>"},{"location":"kbs/000020049/#high-level-overview","title":"High-level overview","text":"<p>Here is a quick overview of the process involved: Identify the container or pod where packet capture is needed. SSH to the node where this particular container or pod is running. Figure out the id of the container or the pause container for the pod. Run the debug container attaching to the network namespace of the container identified in the previous step. Exec inside the debug container. Verify the network namespace by checking the IP address of the network interface. Perform the packet capture!</p> <pre><code>CONTAINER_ID=&lt;insert-value-here&gt;\nDEBUG_IMAGE=leodotcloud/swiss-army-knife\ndocker run -itd \\\n  --name debug_container \\\n  --net=container:$CONTAINER_ID \\\n  $DEBUG_IMAGE\ndocker exec -it debug_container bash\ntcpdump -i eth0 -w /tmp/debug_capture.pcap\n</code></pre>"},{"location":"kbs/000020049/#further-reading","title":"Further reading","text":"<p>The container image <code>leodotcloud/swiss-army-knife</code> is packaged with many tools needed in various debugging scenarios. Source code for this container image can be found here. Docker hub page can be found here. If you find any problems with this image, please file an issue on Github. You are also more than welcome to contribute to this repo by opening a PR (Pull Request)!</p>"},{"location":"kbs/000020049/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020050/","title":"How to migrate etcd data directory to a dedicated filesystem?","text":"<p>This document (000020050) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020050/#situation","title":"Situation","text":""},{"location":"kbs/000020050/#task","title":"Task","text":"<p>When running large Rancher installations or large clusters, it may be necessary to reduce IO contention on the disks for etcd. By default, etcd data is stored in folder <code>/var/lib/etcd</code>, which is most likely stored on the root file system. To avoid sharing the disk IOPS with other system components, it might be a good idea to migrate the etcd data directory to a dedicated file system to improve performance.</p>"},{"location":"kbs/000020050/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>RKE cluster</li> <li>Root access to all etcd nodes.</li> <li>A new file system with at least 2GB free, but we recommend 8GB or higher. Please work with your systems team to create and mount the file system.</li> <li>Etcd backups should be configured and verified.</li> <li>Schedule at least an hour of downtime during your change management maintenance window.</li> <li>It is highly recommended to pause/halt any new deployments and CI/CD jobs during this change window.</li> </ul>"},{"location":"kbs/000020050/#resolution","title":"Resolution","text":"<p>Before making any changes, please take an etcd snapshot using one of the following:</p> <ul> <li>Imported/local clusters</li> <li>Custom/downstream clusters</li> </ul>"},{"location":"kbs/000020050/#for-new-clusters","title":"For new clusters","text":"<p>For a new cluster, please see our installation documentation</p> <p>NOTE: Please make sure you have a file system mounted to \"/var/lib/etcd/\" before creating the cluster.</p>"},{"location":"kbs/000020050/#for-existing-clusters","title":"For existing clusters","text":""},{"location":"kbs/000020050/#option-a-in-place-migration","title":"Option A - In-place migration","text":"<ul> <li>SSH into the first etcd node and become root.</li> <li>Stop etcd container</li> </ul> <pre><code>docker update --restart=no etcd &amp;&amp; docker stop etcd\n</code></pre> <ul> <li>Verify etcd is stopped, and there are no open files.</li> </ul> <pre><code>lsof | grep '/var/lib/etcd/'\n</code></pre> <ul> <li>Move etcd data to a temporary location</li> </ul> <pre><code>mv /var/lib/etcd /var/lib/etcd_tmp\n</code></pre> <ul> <li>Create a new file system and mount it to \"/var/lib/etcd.\" Please work with your systems team for this step.</li> <li>Verify new file systems</li> </ul> <pre><code>df -H /var/lib/etcd\n</code></pre> <ul> <li>Move etcd data from temporary location to new file system</li> </ul> <pre><code>rsync -av --progress /var/lib/etcd_tmp/ /var/lib/etcd/\n</code></pre> <ul> <li>Restart etcd</li> </ul> <pre><code>docker update --restart=yes etcd &amp;&amp; docker start etcd\n</code></pre> <ul> <li>Verify etcd health</li> </ul> <pre><code>docker exec -it etcd member list\n</code></pre> <ul> <li>Repeat the process until all etcd nodes have been updated.</li> <li>Once all nodes have been updated, please cleanup the temporary data.</li> </ul> <pre><code>rm -rf /var/lib/etcd_tmp/\n</code></pre>"},{"location":"kbs/000020050/#option-b-rolling-replacement","title":"Option B - Rolling replacement","text":"<ul> <li>Create a new node with the dedicated file system mount at \"/var/lib/etcd/.\"</li> <li>Join the new nodes to the existing cluster.</li> <li>Waiting for cluster upgrade to finish.</li> <li>Verify etcd health</li> </ul> <pre><code>docker exec -it etcd member list\n</code></pre> <ul> <li>Remove old nodes from the cluster using documentation</li> <li>Repeat the process until all etcd nodes have been replaced.</li> </ul>"},{"location":"kbs/000020050/#further-reading","title":"Further reading","text":"<p>For additional disk tuning, please see etcd</p>"},{"location":"kbs/000020050/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020051/","title":"[JP] How to configure container log rotation for the Docker daemon","text":"<p>This document (000020051) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020051/#situation","title":"Situation","text":""},{"location":"kbs/000020051/#_1","title":"\u80cc\u666f","text":"<p>Docker\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u3067\u306f\u3001json-file\u30ed\u30b0\u30c9\u30e9\u30a4\u30d0\u30fc\u3092\u4f7f\u7528\u3057\u3066\u5236\u9650\u306a\u3057\u3067\u30b3\u30f3\u30c6\u30ca\u30fc\u30ed\u30b0\u3092\u8a18\u9332\u3059\u308b\u305f\u3081\u3001\u30ce\u30fc\u30c9\u3067disk-fill events\u304c\u767a\u751f\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001Docker\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9\u3092\u69cb\u6210\u3057\u3066\u3001\u30b3\u30f3\u30c6\u30ca\u30fc\u30ed\u30b0\u30b5\u30a4\u30ba\u3092\u5236\u9650\u3057\u3001\u53e4\u3044\u30b3\u30f3\u30c6\u30ca\u30fc\u30ed\u30b0\u3092\u30ed\u30fc\u30c6\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u624b\u9806\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020051/#_2","title":"\u4e8b\u524d\u6e96\u5099","text":"<ul> <li>json-file\u30ed\u30b0\u30c9\u30e9\u30a4\u30d0\u30fc\u3092\u4f7f\u7528\u3057\u3066Docker\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9</li> <li><code>/etc/docker/daemon.json</code> \u3092\u7de8\u96c6\u3057\u3001Docker\u30c7\u30fc\u30e2\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u6a29\u9650</li> </ul>"},{"location":"kbs/000020051/#_3","title":"\u6ce8\u610f","text":"<ul> <li>\u65b0\u3057\u304f\u4f5c\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30ca\u306b\u5909\u66f4\u3092\u6709\u52b9\u306b\u3059\u308b\u306b\u306f\u3001Docker\u30c7\u30fc\u30e2\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> <li>\u91cd\u8981\u3000\u65e2\u5b58\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30b3\u30f3\u30c6\u30ca\u30ed\u30b0\u8a2d\u5b9a\u306f\u5909\u66f4\u306a\u3057\u306e\u305f\u3081\u3001\u65e2\u5b58\u306e\u30b3\u30f3\u30c6\u30ca\u306f\u81ea\u52d5\u7684\u306b\u65b0\u3057\u3044\u30ed\u30b0\u8a2d\u5b9a\u3092\u9069\u7528\u3055\u308c\u307e\u305b\u3093\u3002\u65b0\u3057\u3044\u8a2d\u5b9a\u3092\u4f7f\u3046\u306b\u306f\u30b3\u30f3\u30c6\u30ca\u3092\u518d\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</li> </ul>"},{"location":"kbs/000020051/#_4","title":"\u89e3\u6c7a\u65b9\u6cd5","text":"<ol> <li>Docker\u30c7\u30fc\u30e2\u30f3\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u307e\u3059\uff1a</li> </ol> <pre><code>$ vim /etc/docker/daemon.json\n</code></pre> <ol> <li>\u6b21\u306e\u5185\u5bb9\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u8ffd\u52a0\u3057\u3066\u3001\u6700\u5927\u30b3\u30f3\u30c6\u30ca\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u309210MB\u306b\u3001\u30ed\u30fc\u30c6\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u6700\u591a\u306e\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u309210\u500b\u306b\u8a2d\u5b9a\u3057\u307e\u3059\u3002</li> </ol> <pre><code>{\n\"log-driver\": \"json-file\",\n\"log-opts\": {\n\"max-size\": \"10m\",\n\"max-file\": \"10\"\n}\n}\n</code></pre> <ol> <li>docker\u30c7\u30fc\u30e2\u30f3\u3092\u518d\u8d77\u52d5\u3057\u3066\u3001\u8a2d\u5b9a\u3092\u65b0\u3057\u3044\u30b3\u30f3\u30c6\u30ca\u30fc\u306b\u9069\u7528\u3057\u307e\u3059\uff08\u4e0a\u8a18\u306e \u91cd\u8981 \u6ce8\u610f\u3092\u53c2\u7167\uff09\u3002</li> </ol> <pre><code>$ systemctl restart docker\n</code></pre>"},{"location":"kbs/000020051/#_5","title":"\u30d2\u30f3\u30c8","text":"<p>\u3053\u306e\u30ed\u30b0\u30ed\u30fc\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8a2d\u5b9a\u3092\u30d3\u30eb\u30c9/\u69cb\u6210\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306b\u5c0e\u5165\u3059\u308b\u3068\u3001\u30ce\u30fc\u30c9\u3092\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6642\u306b\u81ea\u52d5\u7684\u306b\u9069\u7528\u3055\u308c\u3001\u624b\u52d5\u3067\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u306a\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"kbs/000020051/#_6","title":"\u53c2\u8003","text":"<p>Docker JSON file log driver documentation</p>"},{"location":"kbs/000020051/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020052/","title":"How to create a custom cluster role in Rancher v2.x to grant permission on the metrics endpoint of the kube-apiserver in a Rancher managed cluster","text":"<p>This document (000020052) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020052/#situation","title":"Situation","text":""},{"location":"kbs/000020052/#task","title":"Task","text":"<p>This article details how to create a cluster role to grant users access to the <code>/metrics</code> endpoint of the Kubernetes API Server, in Rancher-managed Kubernetes clusters.</p>"},{"location":"kbs/000020052/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>A Rancher-managed Kubernetes cluster, with Metrics Server deployed. This is deployed by default in Rancher-provisioned clusters.</li> </ul>"},{"location":"kbs/000020052/#resolution","title":"Resolution","text":"<p>In Rancher v2.4+ it should be possible to define a non-resource URL grant via role creation within the Rancher UI. However, this is affected by the issue tracked in Issue #30321 and use of the Rancher v3 API is therefore required to create the role.</p> <ol> <li>The first step is to create a custom cluster role within Rancher, that grants <code>get</code> permission on the non-resource URL <code>/metrics</code> endpoint.</li> </ol> <p>As an admin user, generate an un-scoped Rancher API token, and execute the following API request via cURL, to create the required role. You will need to set CATTLE_ACCESS_KEY, CATTLE_SECRET_KEY and RANCHER_URL to reflect the generated API token and your Rancher URL. You can also edit the role name, as desired, which is set to <code>kube-api metrics</code> in this example.</p> <pre><code>export CATTLE_ACCESS_KEY=token-8jn92\nexport CATTLE_SECRET_KEY=l2r4nq9sx6pdhpm4bgwntvgk49qn6rvvmtsvlvkmjk9rjsfd7n65fz\nexport RANCHER_URL=rancher.example.com\ncurl -k -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n-X POST \\\n-H 'Accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\"context\":\"cluster\",\"clusterCreatorDefault\":false,\" projectCreatorDefault\":false,\"name\":\"kube-api metrics\",\"rules\":[{\"nonResourceURLs\":[\"/metrics\"],\"type\":\"/v3/schemas/policyRule\",\"verbs\":[\"get\"]}]}' \\\n\"https://${RANCHER_URL}/v3/roletemplates\"\n</code></pre> <ol> <li> <p>After creating the cluster role, you can then grant this for a user or group. To do so, follow the steps in the Rancher documentation on assigning a cluster role to a user or group.</p> </li> <li> <p>Once the role is granted to a user, they will be able to test their access to the <code>/metrics</code> endpoint.</p> </li> </ol> <p>The user can access the endpoint, with the applicable cluster id, via the Rancher proxied Kubernetes API Server endpoint, by generating a cluster-scoped or un-scoped API token. The user will need to set CATTLE_ACCESS_KEY, CATTLE_SECRET_KEY, RANCHER_URL and CLUSTER_ID to reflect the generated API token, Rancher URL and cluster id.</p> <pre><code>export CATTLE_ACCESS_KEY=token-8jn92\nexport CATTLE_SECRET_KEY=l2r4nq9sx6pdhpm4bgwntvgk49qn6rvvmtsvlvkmjk9rjsfd7n65fz\nexport RANCHER_URL=rancher.example.com\nexport CLUSTER_ID=c-wwdjc\ncurl -k https://${RANCHER_URL}/k8s/clusters/${CLUSTER_ID}/metrics \\\n-H \"Authorization: Bearer ${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\"\n</code></pre> <p>For Rancher-provisioned Kubernetes clusters with Authorized Cluster Endpoint enabled, the user can also query the endpoint by connecting to the Kubernetes API Server on the cluster's controlplane nodes directly, using a cluster-scoped API token. The user will need to set CATTLE_ACCESS_KEY, CATTLE_SECRET_KEY and AUTHORIZED_ENDPOINT_ADDRESS to reflect the generated API token, and the authorized endpoint address.</p> <pre><code>export CATTLE_ACCESS_KEY=token-d6cls\nexport CATTLE_SECRET_KEY=b6gk6lmgrhsb4rjccktzkwxn5df7tm87msggq87lpmls2pkbpc5t5r\nexport AUTHORIZED_ENDPOINT_ADDRESS=controlplane-01.example.com\ncurl -k https://${AUTHORIZED_ENDPOINT_ADDRESS}:6443/metrics \\\n-H \"Authorization: Bearer ${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\"\n</code></pre>"},{"location":"kbs/000020052/#further-reading","title":"Further reading","text":"<ul> <li>Rancher documentation on custom roles</li> </ul>"},{"location":"kbs/000020052/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020053/","title":"How to scope a custom Tiller install to a Project","text":"<p>This document (000020053) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020053/#situation","title":"Situation","text":""},{"location":"kbs/000020053/#task","title":"Task","text":"<p>By default, Helm v2 will deploy Tiller into the kube-system namespace. Use of Tiller in this state to deploy charts requires more permissions than a Project Owner/Member would typically have.</p> <p>If, for some reason, you do not want to use Rancher Apps or you need to use the Helm v2 CLI to deploy/manage a chart in a downstream Project, then it is possible to create a custom Tiller deployment and scope it to your Project.</p>"},{"location":"kbs/000020053/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>kubectl access to the downstream cluster your Project resides in. For the initial setup, you will need full cluster-admin</li> <li>Helm v2 binary. See here for install information.</li> <li>Project created with namespaces you are planning on managing with Tiller. For the purposes of demonstration we are calling them <code>project-x-tiller-deploy</code>(the namespace we're installing Tiller into) and <code>project-x-namespaceA</code>(the namespace we want to manage with Tiller).</li> </ul>"},{"location":"kbs/000020053/#setup","title":"Setup","text":"<p>First, you will need to define a ServiceAccount and permissions for Tiller to use:</p> <ul> <li>Create a ClusterRole for Tiller(we will bind this to specific namespaces later on):</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\nname: tiller-project-x\nrules:\n- apiGroups: [\"\", \"batch\", \"extensions\", \"apps\"]\nresources: [\"*\"]\nverbs: [\"*\"]\nEOF\n</code></pre> <ul> <li>Create a ServiceAccount in the namespace you want Tiller to run in(within the same Project):</li> </ul> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: tiller\nnamespace: project-x-tiller-deploy\nEOF\n</code></pre> <ul> <li>Create RoleBindings to link the ClusterRole to the Tiller ServiceAccount:</li> </ul> <p><code>yaml   cat &lt;&lt;EOF | kubectl apply -f -   kind: RoleBinding   apiVersion: rbac.authorization.k8s.io/v1beta1   metadata:     name: tiller-rolebinding-project-x-namespaceA     namespace: project-x-namespaceA   subjects: - kind: ServiceAccount   name: tiller   namespace: project-x-tiller-deploy roleRef:   kind: ClusterRole   name: tiller-project-x   apiGroup: rbac.authorization.k8s.io EOF</code></p> <p>A separate RoleBinding is required for every namespace you want to manage with Tiller, so repeat the RoleBinding above for each namespace in your Project, changing <code>namespace: project-x-namespaceA</code> as needed.</p> <p>Once the ClusterRole, ServiceAccount, and RoleBindings are created, Helm can be instructed to deploy Tiller to the desired namespace using the ServiceAccount you created: <code>helm init --service-account tiller --tiller-namespace project-x-tiller-deploy</code></p> <p>You can now deploy using Helm. You will either need to set the environment variable <code>TILLER_NAMESPACE</code> to the namespace Tiller was deployed in, or specify it when running helm with <code>--tiller-namespace</code>. Not setting this will result in helm being unable to find Tiller and throwing the error <code>Error: could not find tiller</code></p>"},{"location":"kbs/000020053/#further-reading","title":"Further reading","text":"<ul> <li>Helm v2 docs</li> <li>Multiple Tillers gist</li> </ul>"},{"location":"kbs/000020053/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020054/","title":"[JP] How to grant users access to Grafana with minimal permissions","text":"<p>This document (000020054) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020054/#situation","title":"Situation","text":""},{"location":"kbs/000020054/#_1","title":"\u30bf\u30b9\u30af","text":"<p>Kubernetes\u30af\u30e9\u30b9\u30bf\u5185\u306e\u30af\u30e9\u30b9\u30bf\u76e3\u8996\u3068Grafana\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3059\u308b\u305f\u3081\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u306b\u5f93\u3063\u3066\u3001\u65b0\u3057\u3044\u30e6\u30fc\u30b6\u30fc\u3092\u4f5c\u6210\u3057\u3001\u6700\u5c0f\u9650\u306e\u6a29\u9650\u3092\u4ed8\u4e0e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020054/#_2","title":"\u4e8b\u524d\u6e96\u5099","text":"<ul> <li>Rancher v2.x</li> <li>\u30af\u30e9\u30b9\u30bf\u306eMonitoring\u304cenabled\u3067\u3042\u308b</li> </ul>"},{"location":"kbs/000020054/#_3","title":"\u80cc\u666f","text":"<p>\u3042\u308b\u30e6\u30fc\u30b6\u306b\u30af\u30e9\u30b9\u30bf\u76e3\u8996\u306e\u30e1\u30c8\u30ea\u30af\u30b9\u3084\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3059\u308b\u6a29\u9650\u3092\u4ed8\u4e0e\u3057\u305f\u3044\u304c\u3001\u305d\u306e\u30e6\u30fc\u30b6\u304c\u4ed6\u306e\u60c5\u5831\u3092\u898b\u305f\u308a\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3088\u3046\u306b\u3057\u305f\u3044\u6642\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3053\u306e\u30ca\u30ec\u30c3\u30b8\u30fc\u30d9\u30fc\u30b9\u3067\u306f\u3001\u3053\u308c\u3092\u5b9f\u73fe\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020054/#_4","title":"\u89e3\u6c7a\u65b9\u6cd5","text":"<p>\u30e6\u30fc\u30b6\u304c\u307e\u3060\u4f5c\u6210\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001Rancher \u3067\u65b0\u3057\u3044\u306e\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30b0\u30ed\u30fc\u30d0\u30eb\u30d3\u30e5\u30fc\u306b\u79fb\u52d5\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u30e1\u30cb\u30e5\u30fc\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\u53f3\u4e0a\u306e <code>\u30e6\u30fc\u30b6\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u540d\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3001\u8868\u793a\u540d\u3092\u9078\u629e\u3057\u307e\u3059\u3002 <code>\u30b0\u30ed\u30fc\u30d0\u30eb\u6a29\u9650</code> \u3067\u306f User-Base \u3092\u9078\u629e\u3057\u3001 <code>\u30ab\u30b9\u30bf\u30e0</code> \u306f\u3059\u3079\u3066\u30c1\u30a7\u30c3\u30af\u3092\u5916\u3057\u305f\u307e\u307e\u306b\u3057\u307e\u3059\u3002\u30d5\u30a9\u30fc\u30e0\u306e\u4e0b\u90e8\u306b\u3042\u308b <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3002\u3053\u3053\u3067\u306f\u3001\u30e6\u30fc\u30b6\u540d <code>johndoe</code> \u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3068\u3057\u307e\u3059\u3002</p> <p>\u300c\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u300d\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u300c\u30ed\u30fc\u30eb\u300d\u3001\u3055\u3089\u306b\u300cProjects\u300d\u30bf\u30d6\u3092\u9078\u629e\u3057\u3001 <code>Project \u30ed\u30fc\u30eb\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002 <code>\u540d\u524d</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u300cServices Proxy\u300d\u3068\u5165\u529b\u3057\u307e\u3059\u3002 <code>Grant Resources</code> \u306e\u4e0b\u3067\u3001 <code>+\u30ea\u30bd\u30fc\u30b9\u306e\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002 <code>\u53d6\u5f97</code> \u3068 <code>\u30ea\u30b9\u30c8</code> \u30dc\u30c3\u30af\u30b9\u306b\u30c1\u30a7\u30c3\u30af\u3092\u5165\u308c\u3001 <code>\u30ea\u30bd\u30fc\u30b9</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306b services/proxy\u3068\u5165\u529b\u3057\u307e\u3059\u3002\u3053\u308c\u304c\u81ea\u52d5\u7684\u306b <code>serivces/proxy (Custom)</code> \u306b\u5909\u66f4\u3055\u308c\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4e0b\u90e8\u306e <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u65b0\u3057\u3044\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30ed\u30fc\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <p>\u6b21\u306b\u3001\u30af\u30e9\u30b9\u30bf\u306e\u30af\u30e9\u30b9\u30bf\u30d3\u30e5\u30fc\u3092\u958b\u304d\u3001\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u300c\u30e1\u30f3\u30d0\u30fc\u300d\u3092\u9078\u629e\u3057\u307e\u3059\u3002\u53f3\u4e0a\u306e <code>\u30e1\u30f3\u30d0\u30fc\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\u30e1\u30f3\u30d0\u30fc\u306e\u30c9\u30ed\u30c3\u30d7\u30c0\u30a6\u30f3\u3067 <code>johndoe</code> \u3092\u9078\u629e\u3057\u3001\u30af\u30e9\u30b9\u30bf\u6a29\u9650\u306e <code>\u30e1\u30f3\u30d0\u30fc</code> \u3092\u9078\u629e\u3059\u308b\u3002\u30d5\u30a9\u30fc\u30e0\u306e\u4e0b\u90e8\u306b\u3042\u308b <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3002</p> <p>\u30af\u30e9\u30b9\u30bf\u306e <code>System</code> \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u79fb\u52d5\u3057\u3001 <code>\u30e1\u30f3\u30d0\u30fc</code> \u30e1\u30cb\u30e5\u30fc\u304b\u3089 <code>\u30e1\u30f3\u30d0\u30fc\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002 <code>\u30e1\u30f3\u30d0</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306b johndoe \u3068\u5165\u529b\u3057\u3001 <code>Project \u6a29\u9650</code> \u3067 <code>Services Proxy</code> \u3092\u9078\u629e\u3057\u307e\u3059\u3002\u30d5\u30a9\u30fc\u30e0\u306e\u4e0b\u90e8\u306b\u3042\u308b <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002</p> <p>\u3053\u308c\u3067\u3001 <code>johndoe</code> \u30e6\u30fc\u30b6\u30fc\u306f Rancher \u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3001Grafana \u30a2\u30a4\u30b3\u30f3\u304c\u8868\u793a\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002Grafana\u30a2\u30a4\u30b3\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3068\u3001\u65b0\u3057\u3044\u30d6\u30e9\u30a6\u30b6\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u958b\u304d\u3001\u30af\u30e9\u30b9\u30bf\u306e\u69d8\u3005\u306a\u30b0\u30e9\u30d5\u3084\u7d71\u8a08\u60c5\u5831\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u65b0\u3057\u3044\u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u306e\u8868\u793a\u3084\u8d77\u52d5\u306a\u3069\u306e\u4ed6\u306e\u64cd\u4f5c\u3092\u884c\u3046\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u3002</p>"},{"location":"kbs/000020054/#_5","title":"\u53c2\u8003","text":"<p>Rancher\u3084Kubernetes\u3067\u306eRBAC\u306e\u52d5\u4f5c\u65b9\u6cd5\u306e\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u30ea\u30f3\u30af\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li>Role-Based Access Control (RBAC) in Rancher</li> <li>Using RBAC Authorization in Kubernetes</li> </ul>"},{"location":"kbs/000020054/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020055/","title":"How to enable debug level logging for the kube-auth-api DaemonSet in Rancher v2.3+ provisioned Kubernetes clusters","text":"<p>This document (000020055) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020055/#situation","title":"Situation","text":""},{"location":"kbs/000020055/#task","title":"Task","text":"<p>The <code>kube-auth-api</code> DaemonSet is deployed to controlplane nodes, in Rancher v2.3+ provisioned Kubernetes clusters, to provide user authentication functionality for the authorized cluster endpoint. When troubleshooting an issue with authorized cluster endpoint authentication, it may be helpful to analyse the <code>kube-auth-api</code> logs at debug level, and this article details how to enable debug logging.</p>"},{"location":"kbs/000020055/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.3+ instance</li> <li>A Rancher provisioned Kubernetes cluster, either a custom cluster or on nodes in an infrastructure provider using a Node Driver</li> </ul>"},{"location":"kbs/000020055/#resolution","title":"Resolution","text":"<ol> <li> <p>Navigate to the workloads view of the System project, within the Rancher UI, for the relevant Rancher provisioned cluster.</p> </li> <li> <p>Locate the <code>kube-api-auth</code> DaemonSet, within the <code>cattle-system</code> namespace, click the vertial elipses and select <code>Edit</code>, per the following screenshot:</p> </li> </ol> <p></p> <ol> <li> <p>Click <code>Show advanced options</code> in the bottom left.</p> </li> <li> <p>Expand the <code>Command</code> section, enter <code>/usr/bin/kube-api-auth --debug serve</code> in the Command field, per the following screenshat, and click <code>Save</code>:</p> </li> </ol> <p></p> <ol> <li>The <code>kube-api-auth</code> pod(s) will restart with the new debug logging configuration. Viewing the <code>kube-api-auth</code> logs you should now obeserve log messages with <code>level=debug</code>.</li> </ol>"},{"location":"kbs/000020055/#further-reading","title":"Further reading","text":"<ul> <li>Rancher documentation on the <code>kube-api-auth</code> authentication webhook</li> <li>Rancher Server Architecture documentation</li> </ul>"},{"location":"kbs/000020055/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020056/","title":"How to Determine the Rancher Leader, in Rancher v2.x","text":"<p>This document (000020056) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020056/#situation","title":"Situation","text":""},{"location":"kbs/000020056/#task","title":"Task","text":"<p>During troubleshooting it may be useful to know which of the Rancher server pods, running as part of the <code>rancher</code> Deployment in the <code>cattle-system</code> namespace, is the current leader, and this article details the steps to determine this.</p>"},{"location":"kbs/000020056/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.x installed in a Kubernetes cluster</li> </ul>"},{"location":"kbs/000020056/#steps","title":"Steps","text":""},{"location":"kbs/000020056/#via-the-rancher-ui","title":"Via the Rancher UI","text":"<ol> <li> <p>Navigate to the local cluster in which Rancher is installed.</p> </li> <li> <p>Click the Projects/Namespaces tab, and select the System project.</p> </li> <li> <p>Under the Resources tab, select Config.</p> </li> <li> <p>Locate the <code>cattle-controllers</code> ConfigMap, under the <code>kube-system</code> namespace.</p> </li> <li> <p>Expand the Labels &amp; Annotations section and note the current leader pod name in the <code>holderIdentity</code> field.</p> </li> </ol>"},{"location":"kbs/000020056/#via-kubectl-with-a-script","title":"Via kubectl with a script","text":"<p>With the kubeconfig for the Rancher server cluster sourced, the leader can be determined using the following script:</p> <pre><code>curl https://raw.githubusercontent.com/rancherlabs/support-tools/master/troubleshooting-scripts/determine-leader/rancher2_determine_leader.sh | sh\n</code></pre>"},{"location":"kbs/000020056/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Documentation on Troubleshooting the Rancher Server Kubernetes Cluster</li> </ul>"},{"location":"kbs/000020056/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020057/","title":"[JP] How to collect and share Rancher logs with Support","text":"<p>This document (000020057) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020057/#situation","title":"Situation","text":""},{"location":"kbs/000020057/#_1","title":"\u80cc\u666f","text":"<p>\u3042\u308b\u554f\u984c\u306e\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3092Rancher\u30b5\u30dd\u30fc\u30c8\u306b\u4f9d\u983c\u3059\u308b\u5834\u5408\u3001\u8abf\u67fb\u3092\u7d99\u7d9a\u3059\u308b\u305f\u3081\u306b\u74b0\u5883\u60c5\u5831\u3084\u30ed\u30b0\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u30c7\u30fc\u30bf\u53ce\u96c6\u65b9\u6cd5\u306e\u7d71\u4e00\u5316\u3068\u7c21\u6613\u5316\u3059\u308b\u305f\u3081\u306b\u3001\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b58\u5728\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u60c5\u5831\u53ce\u96c6\u304c\u5fc5\u8981\u3068\u306a\u308b\u3001\u5404\u30ce\u30fc\u30c9\u304b\u3089\u30ed\u30b0\u304c\u53ce\u96c6\u3055\u308c\u305f\u5f8c\u3001Rancher Support \u306e\u6307\u793a\u306b\u5f93\u3044\u3001\u3053\u306e\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u3092\u30b5\u30dd\u30fc\u30c8\u30c1\u30b1\u30c3\u30c8\uff08\u4e0a\u965020MB\uff09\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3059\u308b\u304b\u3001\u30b5\u30dd\u30fc\u30c8\u304c\u63d0\u4f9b\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u30ea\u30f3\u30af\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"kbs/000020057/#_2","title":"\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3","text":""},{"location":"kbs/000020057/#rancher-v2x","title":"Rancher v2.x","text":"<p>Rancher v2.x \u30af\u30e9\u30b9\u30bf\u5185\u306e\u30ce\u30fc\u30c9\u304b\u3089\u30ed\u30b0\u3092\u53ce\u96c6\u3059\u308b\u306b\u306f\u3001 Rancher v2.x log collector script \u3092\u4f7f\u7528\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u53ce\u96c6\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s\n</code></pre>"},{"location":"kbs/000020057/#rancher-v16","title":"Rancher v1.6","text":"<p>Rancher v1.6 \u30af\u30e9\u30b9\u30bf\u5185\u306e\u30ce\u30fc\u30c9\u304b\u3089\u30ed\u30b0\u3092\u53ce\u96c6\u3059\u308b\u306b\u306f\u3001 Rancher v1.6 log collector script \u3092\u4f7f\u7528\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u53ce\u96c6\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v1.6/logs-collector/rancher16_logs_collector.sh | sudo bash -s\n</code></pre>"},{"location":"kbs/000020057/#rancheros","title":"RancherOS","text":"<p>RancherOS\u7279\u6709\u306e\u554f\u984c\u3092\u8abf\u67fb\u3059\u308b\u305f\u3081\u306b\u3001 RancherOS log collector script \u3067\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>sudo curl https://raw.githubusercontent.com/rancher/os/master/scripts/tools/collect_rancheros_info.sh | sh\n</code></pre>"},{"location":"kbs/000020057/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020058/","title":"How to change the kubelet root directory when provisioning a Kubernetes cluster with the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x","text":"<p>This document (000020058) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020058/#situation","title":"Situation","text":""},{"location":"kbs/000020058/#task","title":"Task","text":"<p>This article details how to change the path of the kubelet root directory, which defaults to <code>/var/lib/kubelet</code>, when provisioning a Kubernetes cluster via the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x.</p> <p>N.B. Updating the kubelet root directory is only supported on new cluster provisioning, and changing this path after initial cluster provisioning is not supported.</p>"},{"location":"kbs/000020058/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>The Rancher Kubernetes Engine (RKE) CLI or a Rancher v2.x instance.</li> </ul>"},{"location":"kbs/000020058/#resolution","title":"Resolution","text":"<p>If you wish to use a separate filesystem for the kubelet root directory, you will need to ensure that this is mounted at the desired path on nodes prior to provisioning.</p>"},{"location":"kbs/000020058/#provisioning-via-rancher-v2x","title":"Provisioning via Rancher v2.x","text":"<ol> <li>Click <code>Edit as YAML</code> when configuring the cluster in the <code>Add Cluster</code> view.</li> <li>Under <code>kubelet</code> in the <code>services</code> block, add the desired kubelet root directory path to the <code>root-dir</code> argument of the <code>extra_args</code> block, and in <code>extra_binds</code>, per the following example:</li> </ol> <pre><code>services:\nkubelet:\nextra_args:\nroot-dir: \"/my/new/folder\"\nextra_binds:\n- \"/my/new/folder:/my/new/folder:shared,z\"\n</code></pre> <ol> <li>After configuring other cluster options as desired, click <code>Create</code> or <code>Next</code>, respectively for Node Driver or Custom clusters, to save the new cluster configuration.</li> </ol>"},{"location":"kbs/000020058/#provisioning-via-the-rke-cli","title":"Provisioning via the RKE CLI","text":"<ol> <li>Open the cluster configuration YAML file for the new cluster.</li> <li>Under <code>kubelet</code> in the <code>services</code> block, add the desired kubelet root directory path to the <code>root-dir</code> argument of the <code>extra_args</code> block, and in <code>extra_binds</code>, per the following example:</li> </ol> <p><code>yaml    services:      kubelet:        extra_args:          root-dir: \"/my/new/folder\"        extra_binds:     - \"/my/new/folder:/my/new/folder:shared,z\"</code></p> <ol> <li>After configuring other cluster options as desired, provision the cluster by invoking <code>rke up --config &lt;cluster configuration YAML file&gt;</code>.</li> </ol>"},{"location":"kbs/000020058/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020059/","title":"Who creates user accounts on SUSE Rancher Hosted?","text":"<p>This document (000020059) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020059/#resolution","title":"Resolution","text":"<p>When a new SUSE Rancher Hosted environment is created, it will have a default admin account and a password will be provided to you for that account. You'll be asked to change the admin password when logging in for the first time. It is the customer's responsibility to create new user accounts. Most customers leverage Rancher's external authentication provider.</p>"},{"location":"kbs/000020059/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020061/","title":"Rancher Upgrade Checklist","text":"<p>This document (000020061) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020061/#situation","title":"Situation","text":""},{"location":"kbs/000020061/#task","title":"Task","text":"<p>This article details the high-level steps required when planning and performing a Rancher and/or Kubernetes upgrade.</p>"},{"location":"kbs/000020061/#planning","title":"Planning","text":"<p>The following are the high-level rules for planning a Rancher/Kubernetes/Docker upgrade.</p> <ul> <li>The support matrix gives a great idea of the versions that are certified by Rancher and work best together.</li> <li>The recommended order of upgrades is: Rancher, Kubernetes, and then Docker/OS</li> <li>All upgrades should be performed in a lab or non-prod environment first</li> <li>Please see the following recommendations when planning version upgrades:</li> <li> <p>Rancher: Do no skip any minor version when upgrading.</p> <ul> <li>For example: when upgrading from v2.1.x -&gt; v2.3.x we encourage upgrading v2.1.x -&gt; v2.2.x -&gt; v2.3.x</li> <li> <p>Kubernetes: Avoid skipping minor versions, as this can increase the chances of an issue due to accumulated changes.</p> </li> <li> <p>For example: when upgrading from v1.13.x -&gt; v1.16.x we encourage upgrading v1.13.x -&gt; v1.14.x -&gt; v1.15.x -&gt; v1.16.x</p> </li> <li> <p>RKE: perform one major RKE version jump at a time</p> </li> <li> <p>For example: when upgrading from v0.1.x -&gt; v1.1.0 instead do v0.1.x -&gt; v0.2.x -&gt; v.0.3.x -&gt; v1.0.x -&gt; v1.1.x</p> </li> <li>Adding a grace period between upgrades is recommended</li> <li>For example: add 24 hours between upgrading Rancher, the local cluster, and downstream clusters</li> <li>It is not required, but it is recommended to pause any application deployments using the Rancher API during an upgrade</li> </ul> </li> </ul>"},{"location":"kbs/000020061/#data-collection","title":"Data collection","text":"<p>Before you start your upgrade, please collect the following pieces of information to best prepare yourself in case you need to open a support ticket.</p> <ul> <li>Scheduled change window:</li> <li>Current Rancher version ( <code>rancher/rancher</code> image tag, or shown bottom left in the UI):</li> <li>Target Rancher version:</li> <li>Installation option (single install/HA):</li> <li>Current Kubernetes version of Rancher local cluster (use <code>kubectl version</code>):</li> <li>Current Docker version (use <code>docker version</code>):</li> </ul>"},{"location":"kbs/000020061/#rancher-pre-upgrade","title":"Rancher Pre-Upgrade","text":"<ul> <li>Check if the Rancher UI is accessible</li> <li>Check if all clusters in UI are in an Active state</li> <li>Check if all pods in <code>kube-system</code> and <code>cattle-system</code> namespaces are running (in both Rancher and downstream clusters)</li> </ul> <pre><code>kubectl get pods -n kube-system\nkubectl get pods -n cattle-system\n</code></pre> <ul> <li>Verify the datastore has scheduled snapshots configured, and these are working.</li> <li> <p>RKE: if Rancher is deployed on a Kubernetes cluster built with RKE, verify etcd snapshots are enabled and working, on etcd nodes you can confirm with the following:</p> <pre><code>ls -l /opt/rke/etcd-snapshots\ndocker logs etcd-rolling-snapshots\n</code></pre> </li> <li> <p>k3s: if Rancher is deployed on a k3s Kubernetes cluster, ensure scheduled backups are configured and working. Please see the k3s documentation pages for further information on this.</p> </li> <li>Create a one-time datastore snapshot, please see the following documentation for RKE and k3s, and the single node Docker install options for more information</li> <li>RKE: check for expired/expiring Kubernetes certs</li> </ul> <pre><code>for i in $(ls /etc/kubernetes/ssl/*.pem|grep -v key); do echo -n $i\" \"; openssl x509 -startdate -enddate -noout -in $i | grep 'notAfter='; done\n</code></pre>"},{"location":"kbs/000020061/#rancher-upgrade-steps","title":"Rancher Upgrade steps","text":"<ul> <li>The Rancher upgrade process is detailed in the upgrade documentation for both HA, and single node using Docker.</li> </ul>"},{"location":"kbs/000020061/#rancher-reviewverify","title":"Rancher Review/Verify","text":"<p>After the upgrade is completed, go through the following checklist to verify your environment is in working order.</p> <ul> <li>Check if the Rancher UI is accessible</li> <li>You should be able to login into Rancher, view clusters, and browse to workloads</li> <li>Verify the Rancher version has changed in UI</li> <li>After logging into Rancher, review the version in the bottom left corner of the page</li> <li>Check if all clusters in UI are in an Active state</li> <li>Check if all pods in <code>kube-system</code> and <code>cattle-system</code> are running (in both Rancher and downstream clusters)</li> <li> <p>Check the <code>cattle-cluster-agent</code> and <code>cattle-node-agent</code> pods are running in all downstream clusters and running the latest version</p> </li> <li> <p>The rollout of the updated agent versions can take some time if there are a lot of downstream clusters or nodes</p> </li> <li>Create a one-time datastore snapshot, please see the following documentation for RKE and k3s, and the single node Docker install options for more information</li> </ul>"},{"location":"kbs/000020061/#rancher-rollback-steps","title":"Rancher Rollback steps","text":"<p>The Rancher rollback process is detailed in the rollback documentation, please follow the relevant link for Rancher installed on a Kubernetes cluster, or Docker</p>"},{"location":"kbs/000020061/#follow-up-tasks-optional","title":"Follow-up tasks (optional)","text":"<ul> <li>Upgrade the Rancher management cluster, this is often a follow-up to the Rancher upgrade. Please see the RKE and k3s upgrade documentation for the upgrade process, as mentioned it is best to avoid skipping minor versions</li> <li>Upgrade the downstream clusters, please see the documentation for more information. A snapshot of both the local and downstream clusters before the upgrade is recommended to provides the maximum amount of recoverability options in the event of a rollback</li> <li>Docker/OS upgrades, please our article on performing rolling changes to nodes</li> </ul>"},{"location":"kbs/000020061/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020062/","title":"How to rotate the Rancher SSL certificate with a single node Docker installation","text":"<p>This document (000020062) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020062/#situation","title":"Situation","text":""},{"location":"kbs/000020062/#task","title":"Task","text":"<p>One installation method for Rancher 2.x is to run Rancher in a Docker container on a single node. This approach is designed for a short-lived development/test environment and bundles a minimal footprint of all the components needed by Rancher into the container image.</p> <p>When the default self-signed SSL certificate option is used, the lifetime of the SSL certificate is 1 year. If the container is run for a long period the certificate will need to be rotated. The below sections provide steps needed to rotate the certificate for different versions of Rancher.</p>"},{"location":"kbs/000020062/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x single node Docker installation</li> <li>Access to the node where Rancher is running to run Docker commands</li> <li>A backup of the Rancher container</li> </ul>"},{"location":"kbs/000020062/#resolution","title":"Resolution","text":"<p>To perform the certificate rotation, please ensure a backup of the Rancher container has been completed, this can be used as a rollback in the event any previous data needs to be restored.</p> <p>The process is different between different versions of Rancher, please select your version below as needed and set the container ID of the Rancher container.</p>"},{"location":"kbs/000020062/#rancher-v24x-and-above","title":"Rancher v2.4.x and above","text":"<p>If the certificate is expiring in less than 90 days, certificate rotation occurs automatically. When expiry falls within this period, certificates will be rotated on the next start of the Rancher container.</p> <pre><code>rancher_container_id=xxx\n\ndocker restart ${rancher_container_id}\n</code></pre>"},{"location":"kbs/000020062/#rancher-v23x","title":"Rancher v2.3.x","text":"<pre><code>rancher_container_id=xxx\n\ndocker exec -ti ${rancher_container_id} bash\ncp -rp /var/lib/rancher/k3s/server/tls /var/lib/rancher/k3s/server/tls.backup\ncd /var/lib/rancher/k3s/server/tls\nrm -rf *.crt *.key temporary-certs/\ncp -p /var/lib/rancher/k3s/server/tls.backup/*-ca.* .\nexit\n\ndocker restart ${rancher_container_id}\n</code></pre>"},{"location":"kbs/000020062/#rancher-v22x","title":"Rancher v2.2.x","text":"<pre><code>rancher_container_id=xxx\n\ndocker exec ${rancher_container_id} mv /var/lib/rancher/management-state/tls/localhost.crt /var/lib/rancher/management-state/tls/localhost.crt.backup\ndocker exec ${rancher_container_id} mv /var/lib/rancher/management-state/tls/localhost.key /var/lib/rancher/management-state/tls/localhost.key.backup\n\ndocker restart ${rancher_container_id}\n</code></pre>"},{"location":"kbs/000020062/#rancher-v2014-v219","title":"Rancher v2.0.14+, v2.1.9+","text":"<pre><code>rancher_container_id=xxx\n\ndocker exec ${rancher_container_id} mv /var/lib/rancher/management-state/certs/bundle.json /var/lib/rancher/management-state/certs/bundle.json.backup\n\ndocker restart ${rancher_container_id}\n</code></pre>"},{"location":"kbs/000020062/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020063/","title":"How to increase the log level for Calico components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020063) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020063/#situation","title":"Situation","text":""},{"location":"kbs/000020063/#task","title":"Task","text":"<p>During network troubleshooting it may be useful to increase the log level of the Calico components. This article details how to set verbose debug-level Calico component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020063/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Calico Network Provider</li> </ul>"},{"location":"kbs/000020063/#resolution","title":"Resolution","text":"<p>N.B. As these instructions involve editing the Calico DaemonSet directly, the change will not persist cluster update events, i.e. invocations of <code>rke up</code> for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting.</p>"},{"location":"kbs/000020063/#via-the-rancher-ui","title":"Via the Rancher UI","text":"<p>For a Rancher v2.x managed cluster, the Calico component log level can be adjusted via the Rancher UI, per the following process:</p> <ol> <li>Navigate to the <code>System</code> project of the relevant cluster within the Rancher UI.</li> <li>Locate the calico DaemonSet workload within the <code>kube-system</code> namespace, click the vertical elipses ( <code>\u22ee</code>) and select Edit.</li> <li>Click to Edit the <code>calico-node</code> container.</li> <li>Add <code>CALICO_STARTUP_LOGLEVEL = DEBUG</code> , <code>F\u200bELIX_LOGSEVERITYSCREEN  = Debug</code>, <code>BGP_LOGSEVERITYSCREEN = Debug</code> in Environment Variables section and click <code>Save</code>.</li> </ol>"},{"location":"kbs/000020063/#via-kubectl","title":"Via kubectl","text":"<p>With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Calico component log level can be adjusted via kubectl, per the following process:</p> <ol> <li>Run <code>kubectl -n kube-system edit daemonset calico-node</code>.</li> <li>In the <code>env</code> definition for the <code>calico-node</code> container add an environment variable with the name <code>CALICO_STARTUP_LOGLEVEL</code> and value <code>DEBUG</code>, <code>F\u200bELIX_LOGSEVERITYSCREEN</code> and value <code>Debug</code> and <code>BGP_LOGSEVERITYSCREEN</code> and value <code>Debug</code>, e.g.:</li> </ol> <pre><code>[...]\ncontainers:\n- env:\n[...]\n- name: CALICO_STARTUP_LOGLEVEL\nvalue: DEBUG\n- name: BGP_LOGSEVERITYSCREEN\nvalue: Debug\n- name: FELIX_LOGSEVERITYSCREEN\nvalue: Debug\n[...]\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"kbs/000020063/#further-reading","title":"Further reading","text":"<ul> <li>Calico configuration documentation</li> <li>Calico component logs</li> </ul>"},{"location":"kbs/000020063/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020064/","title":"The Rancher v2.x Windows log collector script","text":"<p>This document (000020064) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020064/#situation","title":"Situation","text":""},{"location":"kbs/000020064/#rancher-v2x-windows-worker-node-log-collection","title":"Rancher v2.x Windows worker node log collection","text":"<p>Logs can be collected from a Windows worker node within a Rancher v2.x cluster using the Rancher v2.x Windows worker node log collection script.</p> <p>N.B. The script needs to be downloaded and run directly on a Windows worker node using a Powershell session with Administrator Privileges.</p> <p>To run the script, open a new Powershell window with Administrator Privileges and run the following command:</p> <pre><code>Set-ExecutionPolicy Bypass | . {iwr -useb https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/windows-log-collector/win-log-collect.ps1} | iex\n</code></pre> <p>Upon successful completion, the log bundle will be written to the root of the C:\\ drive in a file named <code>rancher_&lt;hostname&gt;_&lt;datetime&gt;.tar.gz</code>.</p>"},{"location":"kbs/000020064/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020065/","title":"401 Unauthorized error using the Rancher2 Terraform Provider v1.4.0 with `access_key` and `secret_key` authentication","text":"<p>This document (000020065) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020065/#situation","title":"Situation","text":""},{"location":"kbs/000020065/#issue","title":"Issue","text":"<p>Executing Terraform actions against Rancher v2.x, with the Rancher2 Terraform Provider v1.4.0, results in an error of the following format, when using <code>access_key</code> and <code>secret_key</code> authentication:</p> <pre><code>Error: Bad response statusCode [401]. Status [401 Unauthorized]. Body: [message=must authenticate] from [https://xyz.rancher.com/v3]\n</code></pre>"},{"location":"kbs/000020065/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>The Rancher2 Terraform Provider v1.4.0</li> <li>Rancher2 Provider authentication with separate <code>access_key</code> and <code>secret_key</code></li> </ul>"},{"location":"kbs/000020065/#workaround","title":"Workaround","text":"<p>To workaround the issue use <code>token_key</code> authentication.</p>"},{"location":"kbs/000020065/#resolution","title":"Resolution","text":"<p>The issue was fixed in Rancher2 Terraform Provider v1.4.1, so you should upgrade to a later version of the provider.</p>"},{"location":"kbs/000020065/#further-reading","title":"Further reading","text":"<ul> <li>Rancher2 Terraform Provider Documentation</li> </ul>"},{"location":"kbs/000020065/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020066/","title":"nginx-ingress-controller pods failing to load configuration with \"client intended to send too large body\" error in nginx-ingress-controller &lt; nginx-0.32.0-rancher1","text":"<p>This document (000020066) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020066/#situation","title":"Situation","text":""},{"location":"kbs/000020066/#issue","title":"Issue","text":"<p><code>nginx-ingress-controller</code> pods fail to load configuration successfully, resulting in failure for some Ingress resources. Logs of the <code>nginx-ingress-controller</code> pods reveal error messages of the following format:</p> <pre><code>2020-09-22T19:06:19.696272452Z 2020/09/22 19:06:19 [error] 5832#5832: *28190476 client intended to send too large body: 10696855 bytes, client: unix:, server: , request: \"POST /configuration/servers HTTP/1.1\", host: \"nginx-status\"\n2020-09-22T19:06:19.718950185Z W0922 19:06:19.718851       8 controller.go:176] Dynamic reconfiguration failed: unexpected error code: 413\n</code></pre> <p>The <code>nginx-ingress-controller</code> dynamically updates its configuration by POST'ing the data to the <code>/configuration</code> endpoint. In <code>nginx-ingress-controller</code> versions lower than <code>nginx-0.26.0</code> the <code>client_max_body_size</code> for this endpoint is hardcoded to <code>10m</code>. As a result, if the configuration data is greater than <code>10m</code> the request will fail (in the log entry above the configuration body is <code>10696855</code> bytes, which is equal to <code>~10.2m</code>). The configuration request will be repeatedly retried, resulting in increased CPU usage by the <code>nginx-ingress-controller</code> pods.</p>"},{"location":"kbs/000020066/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> <li><code>nginx-ingress-controller</code> version lower than <code>nginx-0.32.0-rancher1</code></li> </ul>"},{"location":"kbs/000020066/#workaround","title":"Workaround","text":"<p>Remove some Ingress resources in the cluster to reduce the configuration size below the <code>10m</code> limit.</p> <p>N.B. To remove recently added Ingress resources that pushed the configuration size over the limit of 10m, check the age of the Ingresses.</p>"},{"location":"kbs/000020066/#resolution","title":"Resolution","text":"<p>In <code>ingress-nginx</code> versions <code>0.26.0</code> and above, the <code>client_max_body_size</code> for the <code>/configuration</code> endpoint is dynamic.</p> <p>To take advantage of this fix, upgrade the Kubernetes version of the cluster to one of the below or later, which use <code>nginx-ingress-controller:nginx-0.32.0-rancher1</code> or above:</p> <ul> <li><code>v1.15.12-rancher1-1</code></li> <li><code>v1.16.10-rancher2-1</code></li> <li><code>v1.17.11-rancher1-1</code></li> <li><code>v1.18.3-rancher2-1</code></li> </ul> <p>RKE CLI and Rancher v2.x provisioned Kubernetes clusters, with Kubernetes v1.19+ run a higher version of the <code>ingress-nginx</code>, which also includes the fix.</p>"},{"location":"kbs/000020066/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020067/","title":"Logs not forwarded by Rancher Logging in Rancher v2.x when Docker daemon logging driver is not set to json-file","text":"<p>This document (000020067) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020067/#situation","title":"Situation","text":""},{"location":"kbs/000020067/#issue","title":"Issue","text":"<p>The Rancher v2.x Logging feature enables you to configure log forwarding for Pods, as well as system component containers, in a cluster to a logging endpoint such as Elasticsearch or Splunk.</p> <p>This feature works by deploying a workload to each node in the cluster that mounts the container log directory from the host to parse the Docker container json log files. This is dependent upon use of the json-file Docker logging driver. In the event that the Docker daemon is configured with an alternative logging driver, the logging feature will be unable to parse the logs and will not forward these.</p> <p>In CentOS and RHEL packaged Docker 1.13.1, the default log driver configured is journald, which will prevent log forwarding functioning. Meanwhile, whilst json-file is the default log driver in the upstream Docker packages, if an alternative has been configured on nodes this will also prevent the correct functioning of the log forwarding.</p> <p>You can verify the currently configured Docker logging driver on a node by running <code>docker info | grep Logging</code>, which will show output of the following format: <code>Logging Driver: journald</code>.</p> <p>In the event that json-file is not the configured logging driver, the output of <code>ls -la /var/log/containers/</code> on the node should also be empty. With json-file configured this would display symoblic links to paths under <code>/var/log/pods</code>, containing symbolic links which in turn point to the Docker container json log files.</p>"},{"location":"kbs/000020067/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.x managed cluster with Rancher logging enabled</li> </ul>"},{"location":"kbs/000020067/#resolution","title":"Resolution","text":""},{"location":"kbs/000020067/#centos-or-rhel-packaged-docker","title":"CentOS or RHEL packaged Docker","text":"<ol> <li>Update <code>/etc/sysconfig/docker</code>, as shown in the screenshot below, to set <code>--log-driver=json-file</code> instead of <code>journald</code>.</li> </ol> <ol> <li> <p>Restart the Docker daemon: <code>systemctl restart docker</code></p> </li> <li> <p>You should now see symlinked logs created under <code>/var/log/containers</code></p> </li> </ol>"},{"location":"kbs/000020067/#upstream-docker","title":"Upstream Docker","text":"<ol> <li>Configure the json-file Docker logging driver in <code>/etc/docker/daemon.json</code> per the Docker documentation</li> <li>Restart the Docker daemon: <code>systemctl restart docker</code></li> <li>You should now see symlinked logs created under <code>/var/log/containers</code></li> </ol>"},{"location":"kbs/000020067/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020068/","title":"\"ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend\" error messages logged by the Docker daemon upon daemon startup","text":"<p>This document (000020068) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020068/#situation","title":"Situation","text":""},{"location":"kbs/000020068/#issue","title":"Issue","text":"<p>During startup of the Docker daemon, an error message of the following format is present in the system logs:</p> <pre><code>Jun  13 13:55:47 hostname container-storage-setup: ERROR: XFS filesystem  at /var has ftype=0, cannot use overlay backend; consider different  driver or separate volume or OS reprovision\n</code></pre>"},{"location":"kbs/000020068/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Docker daemon with the <code>overlay</code> or <code>overlay2</code> storage driver</li> </ul>"},{"location":"kbs/000020068/#resolution","title":"Resolution","text":"<p>An <code>xfs</code> formatted filesystem is only supported as backing for the <code>overlay</code> or <code>overlay2</code> Docker storage drivers if formatted with <code>d_type</code> set to <code>true</code>.</p> <p>The <code>d_type</code> value of an <code>xfs</code> filesystem can be verified with the <code>xfs_info</code> utility. Example output for this command can be found in the <code>xfs_info</code> man pages. If <code>ftype=1</code> the filesystem was formatted with <code>d_type</code> <code>true</code> and the filesystem is suitable for use as backing for the <code>overlay</code> or <code>overlay2</code> storage drivers. If the value is set to <code>0</code> the filesystem is not suitable for use with the <code>overlay</code> or <code>overlay2</code> storage drivers, and would need to be reformated with the flag <code>-n ftype=1</code>.</p> <p>Per the Docker documentation: \"Running on XFS without d_type support now causes Docker to skip the attempt to use the <code>overlay</code> or <code>overlay2</code> driver. Existing installs will continue to run, but produce an error. This is to allow users to migrate their data. In a future version, this will be a fatal error, which will prevent Docker from starting.\"</p>"},{"location":"kbs/000020068/#further-reading","title":"Further reading","text":"<p>Docker documentation on the <code>overlay</code> and <code>overlay2</code> storage drivers</p>"},{"location":"kbs/000020068/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020069/","title":"[JP] How to troubleshoot using the namespace of a container","text":"<p>This document (000020069) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020069/#situation","title":"Situation","text":""},{"location":"kbs/000020069/#_1","title":"\u80cc\u666f","text":"<p>\u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u5834\u5408\u3001\u767a\u751f\u3057\u305f\u554f\u984c\u3068\u4e00\u81f4\u3059\u308b\u518d\u73fe\u74b0\u5883\u304c\u5fc5\u8981\u3067\u3059\u3002\u305f\u3060\u3057\u30b3\u30f3\u30c6\u30ca\u30fc\u74b0\u5883\u3092\u5229\u7528\u3059\u308b\u5834\u5408\u3067\u306f\u3001\u30b3\u30f3\u30c6\u30ca\u30fc\u5185\u3067\u30c4\u30fc\u30eb\u3068\u30b7\u30a7\u30eb\u74b0\u5883\u304c\u7c21\u5358\u306b\u5229\u7528\u3067\u304d\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308a\u518d\u73fe\u74b0\u5883\u306e\u69cb\u7bc9\u304c\u56f0\u96e3\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"kbs/000020069/#_2","title":"\u624b\u9806","text":"<p>\u4e0a\u8a18\u306e\u8ab2\u984c\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u4ee5\u4e0b\u4e8c\u3064\u306e\u624b\u9806\u304c\u3042\u308a\u307e\u3059\u3002</p>"},{"location":"kbs/000020069/#sidecar-container","title":"Sidecar container\u00a0\u3092\u5229\u7528","text":"<p>\u554f\u984c\u3092\u6301\u3064\u30b3\u30f3\u30c6\u30ca\u30fc\u304c\u5c5e\u3059\u308bNamespace\u3067\u65b0\u3057\u3044sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3059\u308b\u3068\u3001\u3053\u306esidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u7528\u3044\u3066\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u306f\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3057\u306a\u304c\u3089\u3001\u540c\u3058Network\u3068PID\u306eNamespace\u3092\u4f7f\u7528\u3057\u3066\u8d77\u52d5\u3067\u304d\u307e\u3059\u3002</p> <ul> <li>\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>\u540c\u3058Network\u3001PID\u306eNamespace\u3068Volume\u3092\u4f7f\u7528\u3057sidecar\u30b3\u30f3\u30c6\u30ca\u30fc\u3092\u5b9f\u884c\u3057\u307e\u3059\u3002</li> </ul> <p><code>docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh</code></p> <ul> <li>\u3053\u308c\u304b\u3089\u554f\u984c\u304c\u3042\u308bcontainer\u3084Pod\u3068\u540c\u3058\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5185\u3067\u3001alpine\u30b3\u30f3\u30c6\u30ca\u30fc\u306e\u30b3\u30de\u30f3\u30c9\u3067\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3092\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul> <p>\u4f8b\u3048\u3070\u3001Pod\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3001Sidecar Container\u306b\u5165\u308a\u3001\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u306a\u304c\u3089\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u74b0\u5883\u306e\u8a2d\u5b9a\u3092\u78ba\u8a8d\u3057\u305f\u308a\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u3001alpine container\u3092\u5225\u306e\u30a4\u30e1\u30fc\u30b8\u3092\u7f6e\u304d\u63db\u3048\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u3068\u540c\u3058Volume\u3092Attach\u3067\u304d\u307e\u3059\u304c\u3001\u89aa\u30b3\u30f3\u30c6\u30ca\u30fc\u306eRead\uff0fWrite\u30ec\u30a4\u30e4\u306e\u30a2\u30af\u30bb\u30b9\u306f\u3067\u304d\u307e\u305b\u3093\u3002\u540c\u3058\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044\u5834\u5408\u306f\u4e0b\u8a18\u306ensenter\u306e\u4f8b\u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"kbs/000020069/#nsenter","title":"nsenter \u3092\u5229\u7528","text":"<p><code>nsenter</code> \u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u3066\u3001\u30ce\u30fc\u30c9\u4e0a\u3067\u5229\u7528\u53ef\u80fd\u306a\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002 <code>nsenter</code> \u30b3\u30de\u30f3\u30c9\u306f\u3001\u307b\u3068\u3093\u3069\u306eLinux\u30c7\u30a3\u30b9\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u7684\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305f\u3068\u3048\u3070\u3001Ubuntu\u3067\u306f\u3001util-linux\u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u3088\u3063\u3066\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308bContainer\u306eID\u307e\u305f\u306f\u540d\u524d\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>Container\u5185\u306e\u521d\u3081\u3066\u306e\u30d7\u30ed\u30bb\u30b9(PID 1)\u306e\u756a\u53f7\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</li> </ul> <p><code>PID=$(docker inspect --format '{{ .State.Pid }}' $ID)</code></p> <ul> <li>nsenter\u3092\u4f7f\u3063\u3066Container/Pod\u306e\u5168\u7a2e\u985e\u306eNamespace\u3067\u3001\u30ce\u30fc\u30c9\u4e0a\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</li> </ul> <p><code>nsenter -a -t $PID &lt;command&gt;</code></p> <p>\u4f8b\u3048\u3070\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u3092\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u6642\u3001\u30ce\u30fc\u30c9\u4e0a\u306b\u3042\u308b\u30b3\u30de\u30f3\u30c9\u3001tcpdump\u3001curl\u3001dig\u3084mtr\u306a\u3069\u304c\u4f7f\u7528\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p> <p><code>-a</code> \u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u6700\u8fd1\u306e <code>nsenter</code> \u3067\u5229\u7528\u53ef\u80fd\u3060\u304c\u3001\u3082\u3057\u3053\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u4f7f\u3048\u306a\u304b\u3063\u305f\u3089\u3001\u5358\u72ec\u306eNamespace\u306b\u5165\u308b\u3088\u3046\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3057\u3066\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f <code>nsenter --help</code> \u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"kbs/000020069/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020070/","title":"How to enable support for use-forwarded-headers in ingress-nginx","text":"<p>This document (000020070) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020070/#situation","title":"Situation","text":""},{"location":"kbs/000020070/#task","title":"Task","text":"<p>Per the [ingress-nginx documentation], the <code>use-forwarded-headers</code> configuration option enables passing \"the incoming X-Forwarded-* headers to upstreams. Use this option when NGINX is behind another L7 proxy / load balancer that is setting these headers.\"</p> <p>This article details how to enable the <code>use-forwarded-headers</code> option in the ingress-nginx instance of Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020070/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced</li> <li>For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"kbs/000020070/#resolution","title":"Resolution","text":""},{"location":"kbs/000020070/#configuration-for-rke-cli-provisioned-clusters","title":"Configuration for RKE CLI provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>use-forwarded-headers: true</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\nprovider: nginx\noptions:\nuse-forwarded-headers: true\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Verify the new configuration:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020070/#configuration-for-rancher-v2x-provisioned-clusters","title":"Configuration for Rancher v2.x provisioned clusters","text":"<ol> <li>Log in to the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name.</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the <code>use-forwarded-headers</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\nprovider: nginx\noptions:\nuse-forwarded-headers: true\n</code></pre> <ol> <li> <p>Click \"Save\" at the bottom of the page.</p> </li> <li> <p>Wait for cluster to finish upgrading.</p> </li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020070/#further-reading","title":"Further reading","text":"<ul> <li>ingress-nginx ConfigMap configuration documentation</li> </ul>"},{"location":"kbs/000020070/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020071/","title":"How to enable container log rotation with k3s or containerd","text":"<p>This document (000020071) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020071/#situation","title":"Situation","text":""},{"location":"kbs/000020071/#task","title":"Task","text":"<p>In a Kubernetes cluster running an alternative container runtime, such as containerd, instead of Docker, the kubelet manages container logs. The kubelet default values in relation to log rotation can be found in the upstream kubelet | Kubernetes\u00a0documentation,-%2D%2Dcontainer%2Druntime%20string). These values can be adjusted by adding flags to the kubelet process.</p>"},{"location":"kbs/000020071/#pre-requisites","title":"Pre-requisites","text":"<p>These steps have been validated for a k3s cluster using the default containerd runtime, in theory these same flags should work for any Kubernetes cluster which does not use Docker as the container runtime.</p>"},{"location":"kbs/000020071/#resolution","title":"Resolution","text":"<p>Two kubelet flags need to be added to configure log rotation, the flags will take effect only at start time.</p> <p>In the case of k3s, passing the needed flags can be done a number of ways, the most common perhaps is with the <code>INSTALL_K3S_EXEC</code> environment variable when installing k3s as a service. These same flags can be added to a previous install command to update the service configuration of an existing install of k3s.</p> <p>Note When updating an existing k3s install, the following command will restart k3s.</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--kubelet-arg \"container-log-max-files=4\" --kubelet-arg \"container-log-max-size=50Mi\"\" sh -\n</code></pre> <p>However, flags can also be supplied to the k3s binary directly if a service is not being used. A restart of k3s is required, using the updated flags.</p> <pre><code>k3s server --kubelet-arg container-log-max-files=4 --kubelet-arg container-log-max-size=50Mi\n</code></pre> <p>Note please adjust the values to suit your needs, for demonstration purposes the above commands used 4 log files of 50MB, allowing for 200MB of total space to be retained per container.</p>"},{"location":"kbs/000020071/#further-reading","title":"Further reading","text":"<p>Please reference the k3s and kubelet documentation pages to find more information on these flags.</p>"},{"location":"kbs/000020071/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020072/","title":"How to create an RKE template and template revision using the Rancher2 Terraform Provider","text":"<p>This document (000020072) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020072/#situation","title":"Situation","text":""},{"location":"kbs/000020072/#task","title":"Task","text":"<p>This article details how to create an RKE cluster template revision using the Rancher2 Terraform provider.</p>"},{"location":"kbs/000020072/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, from v2.3.0 and above</li> <li>Terraform and the Rancher2 Terraform Provider, authenticated with a Rancher user who has permission to create RKE Templates and RKE Template Revisions</li> </ul>"},{"location":"kbs/000020072/#resolution","title":"Resolution","text":"<p>RKE cluster templates can be created using the Rancher2 Terraform Provider per the documentation on the <code>rancher2_cluster_template</code> resource.</p> <p>An example of this resource can be found below:</p> <pre><code>resource \"rancher2_cluster_template\" \"foo\" {\n  name = \"foo\"\n  members {\n    access_type = \"owner\"\n    user_principal_id = \"local://user-XXXXX\"\n  }\n  template_revisions {\n    name = \"V1\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"6h\"\n            retention = \"24h\"\n          }\n        }\n      }\n    }\n    default = true\n  }\n  description = \"Terraform cluster template foo\"\n}\n</code></pre> <p>Having configured the Rancher2 Terraform Provider and added the above example resource, adjusting as desired and replacing <code>local://user-XXXXX</code> with a valid user prinical ID, run <code>terraform apply</code> to create the RKE template.</p> <p>N.B. the <code>default = true</code> flag, which will specify this <code>V1</code> revision as the the default revision.</p> <p>To add additional revisions, each one will be nested as a new <code>template_revisions</code> block for that resource. Here is an example <code>V2</code> revision:</p> <pre><code>template_revisions {\n  name = \"V2\"\n  cluster_config {\n    rke_config {\n      network {\n        plugin = \"canal\"\n      }\n      services {\n        etcd {\n          creation = \"3h\"\n          retention = \"12h\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>So, the full resource block would now look like this:</p> <pre><code>resource \"rancher2_cluster_template\" \"foo\" {\n  name = \"foo\"\n  members {\n    access_type = \"owner\"\n    user_principal_id = \"local://user-XXXXX\"\n  }\n  template_revisions {\n    name = \"V1\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"6h\"\n            retention = \"24h\"\n          }\n        }\n      }\n    }\n    default = true\n\n  }\n  template_revisions {\n    name = \"V2\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"3h\"\n            retention = \"12h\"\n          }\n        }\n      }\n    }\n  }\n  description = \"Terraform cluster template foo\"\n}\n</code></pre> <p>Run <code>terraform apply</code> and observe this second <code>V2</code> revision created for the RKE template.</p> <p>N.B. the <code>default</code> revision is still set to <code>V1</code>; this can be changed as needed.</p>"},{"location":"kbs/000020072/#further-reading","title":"Further reading","text":"<ul> <li>Rancher RKE Template documentation.</li> <li>Rancher2 Terraform Provider <code>rancher2_cluster_template</code> resource documentation.</li> </ul>"},{"location":"kbs/000020072/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020073/","title":"How to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed cluster","text":"<p>This document (000020073) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020073/#situation","title":"Situation","text":""},{"location":"kbs/000020073/#task","title":"Task","text":"<p>This article details how to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed Kubernetes cluster.</p>"},{"location":"kbs/000020073/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster</li> </ul>"},{"location":"kbs/000020073/#resolution","title":"Resolution","text":"<p>In Rancher v2.x you can create a custom Project Role that provides the permissions to enable a user to view Pods, Pod logs and to exec into Pods. You can then grant this role to users on Projects to provide them this access where necessary.</p> <p>Pod Reader Permissions in Rancher UI</p> <ol> <li> <p>Navigate to Security -&gt; Roles from the Global namespace.</p> </li> <li> <p>From the Projects tab, select Add Project Role.</p> </li> <li> <p>Provide a name for the role.</p> </li> <li> <p>Under Grant Resources, select Add Resource and fill in the information for each of the following:</p> </li> </ol> Permission(s) Resource Get, Create pods/exec Get, List pods Get, List pods/log <ol> <li>Select Create at the bottom.</li> </ol>"},{"location":"kbs/000020073/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Docs: Project Administration</li> <li>Rancher Docs: Cluster and Project Roles - Defining Custom Roles</li> </ul>"},{"location":"kbs/000020073/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020074/","title":"How to troubleshoot HTTP 400 response codes from ingress-nginx ingresses","text":"<p>This document (000020074) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020074/#situation","title":"Situation","text":""},{"location":"kbs/000020074/#task","title":"Task","text":"<p>This article details how to troubleshoot failing requests, with a HTTP 400 response code, when using an ingress to access a service in a Kubernetes cluster.</p> <p>This error message is typically returned due to a bad request, or as a result of an issue with the request headers or cookies.</p> <p>This article is not intended to be exhaustive as there are a wide variety of causes, however some possible issues are covered.</p>"},{"location":"kbs/000020074/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020074/#resolution","title":"Resolution","text":""},{"location":"kbs/000020074/#large-request-headers-exceeding-header-buffer","title":"Large request headers exceeding header buffer","text":"<p>Nginx has a client header buffer configuration of 4x 8KB by default. Per the Nginx documentation:</p> <ul> <li>\"A request line cannot exceed the size of one buffer, or the 414 (Request-URI Too Large) error is returned to the client.\"</li> <li>\"A request header field cannot exceed the size of one buffer as well, or the 400 (Bad Request) error is returned to the client.\"</li> </ul> <p>The buffer sizes can be extended by defining the <code>large-client-header-buffers</code> option in the <code>nginx-configuration</code> ConfigMap (see below).</p>"},{"location":"kbs/000020074/#large-uri-is-duplicated-into-a-new-header-passed-to-the-backend","title":"Large URI is duplicated into a new header passed to the backend","text":"<p>This issue is commonly seen where the app itself responds when queried directly with a large path, but returns a 400 error when queried through an ingress.</p> <p>By default, when ingress-nginx receives a request, it adds the original request's URI to the <code>X-Original-Uri</code> header that it passes on to the backend. This can result in the app being unable to handle the large sized headers, in addition to the long path.</p> <p>This behaviour can be disabled by setting the <code>proxy-add-original-uri-header</code> option to false in your <code>nginx-configuration</code> ConfigMap (see below).</p>"},{"location":"kbs/000020074/#adding-options-to-the-ingress-nginx-nginx-configuration-configmap","title":"Adding options to the ingress-nginx nginx-configuration ConfigMap","text":"<p>This configuration map is populated by RKE from configuration defined in the cluster config:</p>"},{"location":"kbs/000020074/#if-the-cluster-is-provisioned-by-rancher-v2x","title":"If the cluster is provisioned by Rancher v2.x:","text":"<ol> <li>Edit the cluster (navigate to the cluster within Rancher, select the triple-dot button and then \"Edit\")</li> <li>Select \"Edit as YAML\" to open the cluster configuration as YAML, instead of a form.</li> <li>Add the desired configuration within the ingress block in the following format:</li> </ol> <pre><code>     ingress:\nprovider: nginx\noptions:\nname: value\n</code></pre> <ol> <li>Save the cluster configuration changes. RKE will go through and apply the config defined during its update process.</li> </ol>"},{"location":"kbs/000020074/#if-the-cluster-is-provisioned-by-the-rke-cli","title":"If the cluster is provisioned by the RKE CLI:","text":"<p>The process is largely the same as the Rancher process above, but the configuration is defined in the cluster.yml for this cluster:</p> <ol> <li>Open the cluster configuration yaml with your editor and add the ingress.options block:</li> </ol> <p><code>yaml     ingress:        provider: nginx        options:          name: value</code></p> <p></p> <ol> <li>Apply this config with <code>rke up --config &lt;cluster configuration yaml&gt;</code></li> </ol> <p>Ensure you have an up-to-date cluster.rkestate within the same directory before running <code>rke up</code></p>"},{"location":"kbs/000020074/#further-reading","title":"Further reading","text":"<ul> <li>ingress-nginx ConfigMap documentation</li> <li>RKE documentation on ingress-nginx configuration</li> </ul>"},{"location":"kbs/000020074/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020075/","title":"How to increase the log level for Canal components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020075) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020075/#situation","title":"Situation","text":""},{"location":"kbs/000020075/#task","title":"Task","text":"<p>During network troubleshooting it may be useful to increase the log level of the Canal components. This article details how to set verbose debug-level Canal component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020075/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Canal Network Provider</li> </ul>"},{"location":"kbs/000020075/#resolution","title":"Resolution","text":"<p>N.B. As these instructions involve editing the Canal DaemonSet directly, the change will not persist cluster update events, i.e. invocations of <code>rke up</code> for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting.</p>"},{"location":"kbs/000020075/#via-the-rancher-ui","title":"Via the Rancher UI","text":"<p>For a Rancher v2.x managed cluster, the Canal component log level can be adjusted via the Rancher UI, per the following process:</p> <ol> <li>Navigate to the <code>System</code> project of the relevant cluster within the Rancher UI.</li> <li>Locate the canal DaemonSet workload within the <code>kube-system</code> namespace, click the vertical elipses ( <code>\u22ee</code>) and select Edit.</li> <li>Click to Edit the <code>calico-node</code> container.</li> <li>Add <code>CALICO_STARTUP_LOGLEVEL = DEBUG</code> in the Environment Variables section and click <code>Save</code>.</li> <li>Click Edit for the canal DaemonSet again.</li> <li>This time click to Edit the <code>kube-flannel</code> container.</li> <li>Click <code>Show advanced options</code>.</li> <li>In the Command section add <code>--v=10</code> to the <code>Entrypoint</code> e.g.: <code>/opt/bin/flanneld --ip-masq --kube-subnet-mgr --v=10</code>, and click <code>Save</code>.</li> </ol>"},{"location":"kbs/000020075/#via-kubectl","title":"Via kubectl","text":"<p>With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Canal component log level can be adjusted via kubectl, per the following process:</p> <ol> <li>Run <code>kubectl -n kube-system edit daemonset canal</code>.</li> <li>In the <code>env</code> definition for the <code>calico-node</code> container add an environment variable with the name <code>CALICO_STARTUP_LOGLEVEL</code> and value <code>DEBUG</code>, e.g.:</li> </ol> <pre><code>[...]\ncontainers:\n- env:\n[...]\n- name: CALICO_STARTUP_LOGLEVEL\nvalue: DEBUG\n[...]\n</code></pre> <ol> <li>In the <code>command</code> definition for the <code>kube-flannel</code> container add <code>--v=10</code> to the command, e.g.:</li> </ol> <p><code>yaml    [...]       - commmand:         - /opt/bin/flanneld         - --ip-masq         - --kube-subnet-mgr         - --v=10 [...]</code></p> <ol> <li>Save the file.</li> </ol>"},{"location":"kbs/000020075/#further-reading","title":"Further reading","text":"<ul> <li>Calico configuration documentation</li> <li>Flannel troubleshooting documentation</li> </ul>"},{"location":"kbs/000020075/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020076/","title":"How to enable legacy TLS versions for ingress-nginx in Rancher Kubernetes Engine (RKE) CLI and Rancher v2.x provisioned Kubernetes clusters","text":"<p>This document (000020076) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020076/#situation","title":"Situation","text":""},{"location":"kbs/000020076/#task","title":"Task","text":"<p>This article details how to enable TLS 1.1 on the ingress-nginx controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020076/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced</li> <li>For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"kbs/000020076/#resolution","title":"Resolution","text":""},{"location":"kbs/000020076/#configuration-for-rke-provisioned-clusters","title":"Configuration for RKE provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>ssl-protocols</code> option for the ingress, as follows:</li> </ol> <pre><code>     ingress:\nprovider: nginx\noptions:\nssl-protocols: \"TLSv1.1 TLSv1.2\"\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Verify the new configuration:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020076/#configuration-for-rancher-provisioned-clusters","title":"Configuration for Rancher provisioned clusters","text":"<ol> <li>Login into the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the <code>ssl-protocols</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\nprovider: nginx\noptions:\nssl-protocols: \"TLSv1.1 TLSv1.2\"\n</code></pre> <ol> <li> <p>Click \"Save\" at the bottom of the page.</p> </li> <li> <p>Wait for cluster to finish upgrading.</p> </li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020076/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020077/","title":"How to disable Grafana usage analytics reporting for cluster and project monitoring in Rancher v2.2.x - v2.4.x","text":"<p>This document (000020077) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020077/#situation","title":"Situation","text":""},{"location":"kbs/000020077/#task","title":"Task","text":"<p>By default, Grafana will report usage analytics to the endpoint at stats.grafana.org. In an air-gapped environment this can result in many connection timeout or proxy certificate errors in the Grafana pod logs of cluster and project monitoring, as in the following example:</p> <pre><code>lvl=eror msg=\"Failed to send usage stats\" logger=metrics err=\"Post https://stats.grafana.org/grafana-usage-report: x509: certificate signed by unknown authority\"\n</code></pre> <p>This article outlines how to disable this usage analytics reporting in the Grafana instance of cluster and project monitoring for Rancher v2.2.x - v2.4.x.</p>"},{"location":"kbs/000020077/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.2.x - v2.4.x</li> <li>Cluster or project monitoring enabled</li> </ul>"},{"location":"kbs/000020077/#resolution","title":"Resolution","text":"<ol> <li>Navigate to either the main cluster page or a project for which you have configured monitoring.</li> <li>Click \"Tools\" in the top menu bar and then \"Monitoring\".</li> <li>Click \"Show advanced options\" at the bottom of the page to reveal the \"Answers\" fields and add the following two answers:</li> </ol> <pre><code>grafana.extraVars[0].name=GF_ANALYTICS_REPORTING_ENABLED\ngrafana.extraVars[0].value='false'\n</code></pre> <ol> <li> <p>Click \"Save\".</p> </li> <li> <p>You can verify this by checking the logs for the Grafana Pod, which should show the following near the top of the logs at container startup:</p> </li> </ol> <pre><code>lvl=info msg=\"Config overridden from Environment variable\" logger=settings var=\"GF_ANALYTICS_REPORTING_ENABLED='false'\"\n</code></pre>"},{"location":"kbs/000020077/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Cluster Monitoring documentation</li> <li>Grafana analytics configuration documentation</li> </ul>"},{"location":"kbs/000020077/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020078/","title":"How to confirm a version upgrade of Rancher v2.x is completed successfully","text":"<p>This document (000020078) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020078/#situation","title":"Situation","text":""},{"location":"kbs/000020078/#task","title":"Task","text":"<p>This article details how to confirm that a Rancher version upgrade has successfully completed.</p>"},{"location":"kbs/000020078/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, either a single Docker container or a Highly Available (HA) installation in Kubernetes</li> <li>A Rancher version upgrade performed per the Rancher upgrade documentation</li> </ul>"},{"location":"kbs/000020078/#resolution","title":"Resolution","text":"<p>The following can be verified to confirm that the Rancher component containers have all been successfully upgrade to the newer version:</p> <ul> <li>Within the Rancher UI, confirm the version in the bottom-left corner displays the newer version.</li> <li>For a HA installation, confirm the rancher Deployment Pods within the cattle-system namespace of the Rancher cluster have all been updated to the newer version.</li> <li>Confirm that the Rancher agent workloads (the cattle-node-agent DaemonSet and cattle-cluster-agent Deployment in the cattle-system namespace) in all of the Rancher managed clusters have been updated to the newer version.</li> </ul>"},{"location":"kbs/000020078/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020079/","title":"How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile","text":"<p>This document (000020079) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020079/#situation","title":"Situation","text":""},{"location":"kbs/000020079/#question","title":"Question","text":"<p>How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile</p>"},{"location":"kbs/000020079/#answer","title":"Answer","text":"<p>About the parameters;</p> <p>worker_processes</p> <p>This parameter determines the number of Nginx worker processes to spawn during startup.</p> <p>worker_rlimit_nofile</p> <p>This parameter controls the open file limit per worker process.</p> <p>More details can be found on Nginx documentation</p> <p>Both <code>worker_processes</code> and <code>worker_rlimit_nofile</code> are calculated dynamically by Nginx Ingress during startup.</p> <p>Based on the source code of Ingress Nginx;</p> <pre><code>worker_processes = Number of CPUs ($ grep -c processor /proc/cpuinfo)\nworker_rlimit_nofile = ( RLIMIT_NOFILE / worker_processes ) - 1024\n</code></pre> <p>where RLIMIT_NOFILE is the maximum allowed open files by the process ( <code>ulimit -n</code> )</p> <p>From Nginx Ingress shell, you can verify the same.</p> <pre><code># kubectl exec -it  -n ingress-nginx nginx-ingress-controller-8ln2b -- bash\nbash-5.0$ ulimit -n\n1048576\nbash-5.0$\nbash-5.0$ grep -c processor /proc/cpuinfo\n2        &lt;&lt;---- worker_processes\nbash-5.0$\nbash-5.0$ echo $(((1048576/2)-1024))\n523264    &lt;&lt;--- worker_rlimit_nofile\nbash-5.0$\nbash-5.0$ egrep \"worker_processes|worker_rlimit_nofile\" /etc/nginx/nginx.conf\nworker_processes 2;\nworker_rlimit_nofile 523264;\nbash-5.0$\n</code></pre>"},{"location":"kbs/000020079/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020080/","title":"How can I tell whether my app is installed with Helm v2 or Helm v3?","text":"<p>This document (000020080) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020080/#situation","title":"Situation","text":""},{"location":"kbs/000020080/#question","title":"Question","text":"<p>How can I tell whether my app was installed with Helm v2 or Helm v3?</p>"},{"location":"kbs/000020080/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>kubectl access to the cluster the app is deployed in</li> </ul>"},{"location":"kbs/000020080/#answer","title":"Answer","text":"<p>The easiest way is to check what version of Helm was used to deploy resources is to look at the <code>heritage</code> label. For example, to check whether Rancher was installed via Helm v2 or v3, run:</p> <pre><code>kubectl get deployment -n cattle-system rancher -o yaml | grep heritage\n</code></pre> <p>The heritage version defines what version of helm was used to install this chart.</p> <p><code>heritage: Tiller</code> - This is a Helm v2 resource</p> <p><code>heritage: Helm</code> - This is a Helm v3 resource</p>"},{"location":"kbs/000020080/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020081/","title":"Troubleshooting - Nodes wont join cluster or show unavailable","text":"<p>This document (000020081) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020081/#situation","title":"Situation","text":""},{"location":"kbs/000020081/#issue-nodes-are-not-added-to-rancher-or-are-not-provisioned-correctly","title":"Issue - Nodes are not added to Rancher or are not provisioned correctly","text":"<p>The following article should help empower Rancher administrators diagnose and troubleshoot when a node is not added to Rancher or when a node is not provisioned correctly. We'll outline the process nodes undergo when they are added to a cluster.</p>"},{"location":"kbs/000020081/#scope","title":"Scope","text":"<p>We'll kick off by scoping what cluster types this document might pertain to. We're speaking specifically about custom clusters and clusters launched with a node driver. Mention of node driver will be synonymous with 'With RKE and new nodes in an infrastructure provider' in the Rancher UI.</p>"},{"location":"kbs/000020081/#tracing-the-steps-during-the-bootstrapping-of-a-node","title":"Tracing the steps during the bootstrapping of a node.","text":"<p>Whether you're selecting custom clusters or clusters launched with a node driver, the way to add nodes to the cluster is by executing a docker run command generated for the created cluster. In case of a custom cluster, the command will be generated and displayed on the final step of cluster creation. In case of a cluster launched with a node driver, the command is generated and executed as final command after creating the node and installing Docker.</p> <p>Note: not all roles may be present in the generated command, depending on what role(s) is/are selected.</p> <pre><code>sudo docker run -d \\\n --privileged \\\n --restart=unless-stopped \\\n --net=host \\\n -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:&lt;version&gt; --server https://&lt;server_url&gt; \\\n --token &lt;token&gt; \\\n --ca-checksum &lt;checksum_value&gt; \\\n --etcd \\\n --controlplane \\\n --worker\n</code></pre> <p>What happens next: 1. The <code>docker run</code> command launches a bootstrap agent container. It will be identified with a randomly generated name - The entrypoint is a shell script which parses the flags and runs some validation tests on said flags and their provided values - A token is then used to authenticate against your Rancher server in order to interact with it. - The agent retrieves the CA certificate from the Rancher server and places it in /etc/kubernetes/ssl/certs/serverca, then the checksum is used to validate if the CA certificate retrieved from Rancher matches. This only applies when a self signed certificate is in use. - Runs an agent binary and connects to Rancher using a WebSocket connection - Agent then checks in with the Rancher server to see if the node is unique, and gets a node plan - Agent executes the node plan provided by the Rancher server - Docker run command will create the path <code>/etc/kubernetes</code> if it doesn't exist - Rancher will run cluster provisioning/reconcile based on the desired role for the node being added (etcd and control plane nodes only). This process will copy certificates down from the server via the built in rke cluster provisioning. - On worker nodes, the process is slightly different. The agent requests a node plan from the Rancher server. The Rancher server generates the node config then sends it back down to the agent. The agent then executes the plan contained in the node config. This involves; certificate generation for the Kubernetes components, and the container create commands to create the following services; kubelet, kube-proxy, and nginx-proxy. - The Rancher agent uses the node plan to write out a cloud-config to configure cloud provider settings.</p> <ol> <li> <p>If provisioning of the node succeeds, the node will be registering to the Kubernetes cluster and cattle-node-agent DaemonSet pods will be scheduled to the node, and the pod will remove and replace the agent container that was created via the Docker run command</p> </li> <li> <p>The <code>share-mnt</code> binary (aka bootstrap phase 2) - The share-mnt container runs the share-root.sh which creates filesystem resources that other container end up using. Certificate folders, configuration files, etc... - The container spings up another container that runs a share mount binary. This container makes sure /var/lib/kubelet or /var/lib/rancher have the right share permissions for systems like boot2docker.</p> </li> </ol> <p>Note: All Kubernetes control plane components talk directly with the Kubernetes API server that's housed on the same node. This proxy is configured to front all k8s API servers within the cluster. It's nginx.conf should reflect that.</p> <ol> <li>If all goes well, the share-mnt bootstrap and share-root container exit and the share-root container gets removed. The kubelet starts, registers with Kubernetes, and cattle-node-agent <code>DaemonSet</code> schedules a pod. The pod should then take over the websocket connection to the rancher server. This should end our provisioning journey and hopefully lead to a functional, happy cluster.</li> </ol>"},{"location":"kbs/000020081/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020082/","title":"How to deploy Nginx instead of Traefik as your ingress controller on K3s","text":"<p>This document (000020082) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020082/#situation","title":"Situation","text":""},{"location":"kbs/000020082/#task","title":"Task","text":"<p>This knowledge base article will provide the directions for deploying NGINX instead of Traefik as your Kubernetes ingress controller on K3s. Please note that Traefik is the support ingress controller for K3s and NGINX is not officially supported by SUSE Rancher support.</p>"},{"location":"kbs/000020082/#requirements","title":"Requirements","text":"<ul> <li>K3s 1.17+ (may apply to other versions)</li> </ul>"},{"location":"kbs/000020082/#background","title":"Background","text":"<p>By default, K3s uses Traefik as the ingress controller for your cluster. The decision to use Traefik over NGINX was based on multi-architecture support across x86 and ARM based platforms. Normally Traefik meets the needs of most Kubernetes clusters. However, there are unique use cases where NGINX may be required or preferred. If you don't think you need NGINX, it's recommended to stick with Traefik.</p>"},{"location":"kbs/000020082/#solution","title":"Solution","text":"<p>The first step to using NGINX or any alternative ingress controller is to tell K3s that you do not want to deploy Traefik. When installing K3s add the following <code>--no-deploy traefik</code> flag to the <code>INSTALL_K3S_EXEC</code> environment variable:</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--no-deploy traefik\" sh -s -\n</code></pre> <p>If you have already downloaded the k3s.sh install script, you can run the following:</p> <pre><code>INSTALL_K3S_EXEC=\"--no-deploy traefik\" k3s.sh\n</code></pre> <p>This will install the K3s server and form a single node cluster. You can confirm the cluster is operational (\"Ready\") by running:</p> <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES    AGE    VERSION\nip-10-0-0-100       Ready    master   1m     v1.18.4+k3s1\n</code></pre> <p>Note, if you already had the kubectl binary installed on your host and it is not configured correctly, you may need to run <code>k3s kubectl</code> instead of <code>kubectl</code>.</p> <p>Next, confirm your out-of-box pods are running and Traefik is not running:</p> <pre><code>$ kubectl get pods -A\nNAMESPACE                   NAME                                                      READY   STATUS      RESTARTS   AGE\nkube-system                 local-path-provisioner-58fb86bdfd-vt57d                   1/1     Running     0          1m\nkube-system                 metrics-server-6d684c7b5-qmlcn                            1/1     Running     0          1m\nkube-system                 coredns-d798c9dd-72qrq                                    1/1     Running     0          1m\n</code></pre> <p>K3s has a nice feature that allows you to deploy Helm Charts by placing a <code>HelmChart</code> YAML in <code>/var/lib/rancher/k3s/server/manifests</code>. Create this file by running:</p> <pre><code>cat &gt;/var/lib/rancher/k3s/server/manifests/ingress-nginx.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\nname: ingress-nginx\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\nname: ingress-nginx\nnamespace: kube-system\nspec:\nchart: ingress-nginx\nrepo: https://kubernetes.github.io/ingress-nginx\ntargetNamespace: ingress-nginx\nversion: v3.29.0\nset:\nvaluesContent: |-\nfullnameOverride: ingress-nginx\ncontroller:\nkind: DaemonSet\nhostNetwork: true\nhostPort:\nenabled: true\nservice:\nenabled: false\npublishService:\nenabled: false\nmetrics:\nenabled: true\nserviceMonitor:\nenabled: true\nconfig:\nuse-forwarded-headers: \"true\"\nEOF\n</code></pre> <p>K3s periodically polls the manifests folder and applies the YAML in these files. After about a minute, you should see new pods running, including the NGINX Ingress Controller and default backend:</p> <pre><code>$ kubectl get pods -A\nNAMESPACE                   NAME                                                      READY   STATUS      RESTARTS   AGE\nkube-system                 local-path-provisioner-58fb86bdfd-vt57d                   1/1     Running     0          2m\nkube-system                 metrics-server-6d684c7b5-qmlcn                            1/1     Running     0          2m\nkube-system                 coredns-d798c9dd-72qrq                                    1/1     Running     0          2m\nkube-system                 helm-install-ingress-nginx-s99ct                          0/1     Completed   0          1m\ningress-nginx               ingress-nginx-default-backend-7fb8995f4d-h6rkb            1/1     Running     0          1m\ningress-nginx               ingress-nginx-controller-c8mkg                            1/1     Running     0          1m\n</code></pre> <p>You'll also see a <code>helm-install-ingress-nginx</code> pod in your environment. K3s uses this pod to deploy the Helm Chart and it's normal for it to be in a READY=0/1 and STATUS=Completed state once the Helm Chart has been successfully deployed. In the event your Helm Chart failed to deploy, you can view the logs of this pod to troubleshoot further.</p>"},{"location":"kbs/000020082/#reference","title":"Reference","text":"<ul> <li>K3s documentation</li> <li>NGINX Ingress Controller</li> </ul>"},{"location":"kbs/000020082/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020083/","title":"How to deploy the AWS EBS CSI driver on K3s","text":"<p>This document (000020083) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020083/#situation","title":"Situation","text":""},{"location":"kbs/000020083/#task","title":"Task","text":"<p>This knowledge base article will provide the directions for deploying and testing the AWS EBS CSI driver and storage class on K3s.</p>"},{"location":"kbs/000020083/#requirements","title":"Requirements","text":"<ul> <li>K3s 1.18+ (may apply to other versions)</li> <li>Amazon Web Services (AWS) account with privileges to launch EC2 instances and create IAM policies.</li> </ul>"},{"location":"kbs/000020083/#background","title":"Background","text":"<p>K3s has all in-tree storage providers removed since Kubernetes is shifting to out of tree providers for Container Storage Interface (CSI) and Cloud Provider Interface (CPI). While in-tree providers are convenient, they add a lot of bloat to Kubernetes and will eventually be removed from upstream Kubernetes, possibly in 2021.</p> <p>This how-to guide will instruct you on installing and configuring the AWS EBS CSI driver and storage class. This will allow you to dynamically provision and attach an EBS volume to your pod without having to manually create a persistent volume (PV) and EBS volume in advance. In the event that your node crashes and your pod is re-launched on another node, your pod will be reattached to the volume assuming that node is running in the same availability zone used by the defunct node.</p>"},{"location":"kbs/000020083/#solution","title":"Solution","text":"<p>Assuming you want the CSI and storage class automatically deployed by K3s, copy the following YAML to a file in your manifests folder on one or all of your K3s servers. For example, <code>/var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml</code>:</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\nname: aws-ebs-csi-driver\nnamespace: kube-system\nspec:\nchart: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/v0.5.0/helm-chart.tgz\nversion: v0.5.0\ntargetNamespace: kube-system\nvaluesContent: |-\nenableVolumeScheduling: true\nenableVolumeResizing: true\nenableVolumeSnapshot: true\nextraVolumeTags:\nName: k3s-ebs\nanothertag: anothervalue\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\nname: ebs-storageclass\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>First, note at the time of this writing, v0.5.0 is the latest version of the driver. If there is a newer version available, you can replace this in the chart and version tags. See the AWS EBS CSI readme for documentation on the versions currently available. Second, you can customize the <code>enableVolumeScheduling</code>, <code>enableVolumeResizing</code>, <code>enableVolumeSnaphost</code>, and <code>extraVolumeTags</code> based on your needs. These parameters and others are documented in the Helm chart.</p> <p>Next, you need to give the driver IAM permissions to manage EBS volumes. This can be done one of two ways. You can either feed your AWS access key and secret key as a Kubernetes secret, or use an AWS instance profile. Since the first option involves passing sensitive keys in clear text and storing them directly in Kubernetes, the second option is usually preferred. I will go over both options. For either option, make sure your access keys or instance profile has the following permissions set in IAM:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:AttachVolume\",\n\"ec2:CreateSnapshot\",\n\"ec2:CreateTags\",\n\"ec2:CreateVolume\",\n\"ec2:DeleteSnapshot\",\n\"ec2:DeleteTags\",\n\"ec2:DeleteVolume\",\n\"ec2:DescribeAvailabilityZones\",\n\"ec2:DescribeInstances\",\n\"ec2:DescribeSnapshots\",\n\"ec2:DescribeTags\",\n\"ec2:DescribeVolumes\",\n\"ec2:DescribeVolumesModifications\",\n\"ec2:DetachVolume\",\n\"ec2:ModifyVolume\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n</code></pre> <p>Reference: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/example-iam-policy.json</p>"},{"location":"kbs/000020083/#option-1-kubernetes-secret","title":"Option 1: Kubernetes Secret","text":"<p>You can place your AWS access key and secret key into a Kubernetes secret. Create a YAML file with the following contents and run a kubectl apply. You can also place this inside your <code>/var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml</code> file. Keep in mind this is not a terribly secure option and anyone with access to these files or secrets in the kube-system namespace will be able to obtain your AWS access keys.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: aws-secret\nnamespace: kube-system\nstringData:\nkey_id: \"AKI**********\"\naccess_key: \"**********\"\n</code></pre>"},{"location":"kbs/000020083/#option-2-instance-profile","title":"Option 2: Instance Profile","text":"<p>This option to more secure and should not expose your keys in clear text or in a Kubernetes secret object. You'll need to make sure when your EC2 instances are launched, you've attached an instance profile that has the permissions defined above in the JSON block.</p>"},{"location":"kbs/000020083/#verifying-and-testing","title":"Verifying and Testing","text":"<p>You can now check your pods to see if the CSI pods are running. You should see something like this:</p> <pre><code># kubectl get pods -n kube-system | grep ebs\nebs-snapshot-controller-0                1/1     Running   0          15m\nebs-csi-node-k2gh5                       3/3     Running   0          15m\nebs-csi-node-xdcvn                       3/3     Running   0          15m\nebs-csi-controller-6f799b5548-46jqr      6/6     Running   0          15m\nebs-csi-controller-6f799b5548-h4nbb      6/6     Running   0          15m\n</code></pre> <p>Time to test things out. The following command can be run that should provision a 1GB EBS and attach it to your pod:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: myclaim\nspec:\naccessModes:\n- ReadWriteOnce\nstorageClassName: ebs-storageclass\nresources:\nrequests:\nstorage: 1Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: storage-test\nspec:\ncontainers:\n- name: \"storage-test\"\nimage: \"ubuntu:latest\"\ncommand: [\"/bin/sleep\"]\nargs: [\"infinity\"]\nvolumeMounts:\n- name: myebs\nmountPath: /mnt/test\nvolumes:\n- name: myebs\npersistentVolumeClaim:\nclaimName: myclaim\nEOF\n</code></pre> <p>In your AWS console, you should see a new EBS volume has been created. After about a minute, you should be able to exec into your pod and see the volume mounted in your pod:</p> <pre><code># kubectl exec storage-test -- df -h\nFilesystem      Size  Used Avail Use% Mounted on\noverlay          31G  6.2G   25G  20% /\ntmpfs            64M     0   64M   0% /dev\ntmpfs           3.8G     0  3.8G   0% /sys/fs/cgroup\n/dev/nvme2n1    976M  2.6M  958M   1% /mnt/test\n/dev/root        31G  6.2G   25G  20% /etc/hosts\nshm              64M     0   64M   0% /dev/shm\ntmpfs           3.8G   12K  3.8G   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs           3.8G     0  3.8G   0% /proc/acpi\ntmpfs           3.8G     0  3.8G   0% /proc/scsi\ntmpfs           3.8G     0  3.8G   0% /sys/firmware\n</code></pre>"},{"location":"kbs/000020083/#cleaning-up","title":"Cleaning Up","text":"<p>Remove the test pod by running the following:</p> <pre><code>kubectl delete pod storage-test\n</code></pre> <p>Remove the PVC by running:</p> <pre><code>kubectl delete pvc myclaim\n</code></pre> <p>Check the AWS console and you should see your EBS volume has been removed automatically by the AWS EBS CSI driver.</p>"},{"location":"kbs/000020083/#reference","title":"Reference","text":"<ul> <li>K3s documentation</li> <li>AWS EBS CSI documentation</li> </ul>"},{"location":"kbs/000020083/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020084/","title":"How to perform a rolling change to nodes","text":"<p>This document (000020084) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020084/#situation","title":"Situation","text":""},{"location":"kbs/000020084/#task","title":"Task","text":"<p>In a Kubernetes cluster nodes can be treated as ephemeral building blocks providing the resources necessary for all workloads. Managing nodes in an immutable way is particularly common in a cloud environment.</p> <p>In an on premise environment however, nodes can be recycled and updated, in general it's typical that nodes have a longer lifecycle.</p> <p>There may be significant changes to nodes over time, for example: IP addresses, storage/filesystems migration to other hypervisors, data centers, large OS updates, or even migration between clusters.</p> <p>To perform large changes like this, this article aims to provide example steps to apply large changes like this safely in a rolling fashion.</p>"},{"location":"kbs/000020084/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A custom or imported cluster managed by Rancher, or an RKE/k3s cluster</li> <li>Access to the nodes in the cluster with sudo/root</li> <li>Permission to perform drain and delete actions on the nodes</li> </ul> <p>If there are any single replica workloads, whenever possible it is ideal to ensure at least 2 replicas are configured for availablity during rolling changes. These are best scheduled on separate nodes, a preferred anti-affinity can help with this.</p>"},{"location":"kbs/000020084/#steps","title":"Steps","text":"<p>While performing a rolling change to nodes you will need to determine a batch size, effectively how many nodes you wish to take out of service at a time. Initially, it is recommended to perform the change on one node as a canary first, and testing the change has the desired outcome before doing more at once.</p> <ol> <li> <p>If you wish to maintain the number of nodes in the cluster while performing the rolling change, at this point you may wish to add new nodes, this ensures that when nodes are out of service the cluster maintains at least the original number of available nodes.</p> </li> <li> <p>Drain the node, this can be done with <code>kubectl drain &lt;node&gt;</code>, or in the Rancher UI.</p> </li> </ol> <p>This is particularly important to avoid disruptions to services, by draining first, service endpoints are updated to remove the pods from services, stopped, started on a new node in the cluster, and added back to the service safely.</p> <p>If there are pods using local storage (commonly <code>emptyDir</code> volumes), and these should be drained, the <code>--delete-local-data=true</code> will be needed, beware: the data will be lost.</p> <ol> <li>Optional Delete the node(s) from the cluster, this can be done with <code>kubectl delete &lt;node&gt;</code>. This is needed for changes that cannot be performed on existing nodes, such as IP address, hostnames, moving nodes to another cluster, and large configuration updates. Any pods and Kubernetes components running on the nodes will be removed.</li> </ol> <p>Note: if this is an <code>etcd</code> node, ensure that the cluster has quorum and at least two remaining <code>etcd</code> nodes to maintain HA before performing this step.</p> <ul> <li>For an imported cluster, there is no automated cleanup so at this point you would remove the node from the cluster configuration<ul> <li>RKE, remove the node from the cluster.yaml file followed by an <code>rke up</code></li> <li>k3s, stop the k3s service and uninstall k3s using the script</li> </ul> </li> <li> <p>Optional If the node has been deleted in step 3, cleaning the node is important\u00a0to ensure all previous history of the cluster, CNI devices, volumes, and containers are removed. This is especially important if the node is to be re-used in another cluster.</p> </li> <li> <p>Perform the changes to the node, this could be automated with configuration management, scripted or manual steps.</p> </li> <li> <p>Once step 5 is complete, add the node back to the desired cluster.</p> </li> <li>In a custom cluster this can be done with the <code>docker run</code> command supplied in the Rancher UI</li> <li>For an imported cluster the steps are different<ul> <li>RKE, you would add this node to the cluster by configuring it in the cluster.yaml file, followed with an <code>rke up</code></li> <li>k3s, re-install k3s using the correct flags/variables</li> </ul> </li> <li> <p>Test the nodes with running workloads, and monitor before proceeding with the next node, or a larger batch size of nodes.</p> </li> <li> <p>If additional nodes were added in step 1, these can be removed from the cluster at this point by following steps 2, 3, and 4.</p> </li> </ul>"},{"location":"kbs/000020084/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020085/","title":"How is my SUSE Rancher Hosted environment monitored?","text":"<p>This document (000020085) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020085/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is monitored by multiple systems which will trigger an email/Slack/SMS notification to a SUSE Rancher Hosted DevOps on-call engineer in the event there's a problem with your environment. Prometheus and Grafana are used to monitor the health of the VMs running SUSE Rancher Hosted and look at CPU, load, memory, and disk metrics. CloudWatch is used to monitor response times and database health inside the cloud infrastructure. Pingdom and Datadog are used to monitor uptime and availability from multiple geographies. If at any time you notice a performance or availability problem with SUSE Rancher Hosted, please open a support case on our support portal.</p>"},{"location":"kbs/000020085/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020086/","title":"How often is maintenance performed on SUSE Rancher Hosted?","text":"<p>This document (000020086) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020086/#resolution","title":"Resolution","text":"<p>Paid customers with an SLA are given the choice of a one hour weekly maintenance window, so maintenance is done at most on a weekly basis with the exception of emergency maintenance to address an outage or high severity issue. Maintenance typically involves upgrading the underlying Kubernetes cluster or operating system patches and updates. Most maintenance can be done with little or no interruption to service.</p>"},{"location":"kbs/000020086/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020087/","title":"Is it possible to have alpha, beta, or release candidate (RC) available on SUSE Rancher Hosted?","text":"<p>This document (000020087) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020087/#situation","title":"Situation","text":""},{"location":"kbs/000020087/#resolution","title":"Resolution","text":"<p>While it may be technically possible to run an alpha, beta, or release candidate version of Rancher on SUSE Rancher Hosted, we don't typically offer it so that we can deliver our 99.9% uptime SLA. If you want to test a version of Rancher that is not GA, it's recommended that you use your own on-premise or cloud infrastructure.</p>"},{"location":"kbs/000020087/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020088/","title":"Can the admin password be reset if I\u2019m locked out of my SUSE Rancher Hosted environment?","text":"<p>This document (000020088) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020088/#resolution","title":"Resolution","text":"<p>Yes, if you find yourself locked out of the admin account on Hosted Rancher, the Rancher operations team can reset it for you using the method defined in our documentation. To initiate this request, please file a support case through the Rancher Support Portal.</p>"},{"location":"kbs/000020088/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020089/","title":"Who upgrades Kubernetes on my SUSE Rancher Hosted downstream clusters?","text":"<p>This document (000020089) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020089/#resolution","title":"Resolution","text":"<p>Customers are responsible for upgrading all downstream clusters that SUSE Rancher Hosted manages. RKE clusters can be easily upgraded and rolled back by using the UI or API. See Rancher docs for more details. K3s, RKE2, EKS, AKS, and GKE clusters can also be easily upgraded in the UI.</p>"},{"location":"kbs/000020089/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020090/","title":"Can SUSE Rancher Hosted manage my on-premise clusters running on VMWare or bare metal servers?","text":"<p>This document (000020090) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020090/#resolution","title":"Resolution","text":"<p>Yes, there are a variety of ways you can accomplish this. You can provision your VMs or bare metal servers ahead of time and use the Custom Cluster option when creating your Kubernetes cluster. A shell command will be provided which you can run on each server to join the cluster. Your on-premise servers will only require outbound (egress) access to SUSE Rancher Hosted.</p> <p>If you want to use the vSphere node driver to have SUSE Rancher Hosted provision your infrastructure, SUSE Rancher Hosted will need inbound (ingress) access to your on-premise infrastructure. This can be accomplished one of three ways:</p> <ol> <li>Open firewall rules on your corporate network.</li> <li>Establish a VPC peering connection between SUSE Rancher Hosted and your AWS cloud account. This requires that your AWS cloud account is connected to your on-premise infrastructure through Direct Connect or VPN.</li> <li>Establish a VPN connection between SUSE Rancher Hosted and your on-premise network.</li> </ol> <p>More details can be provided on each of these three options. See also SUSE Rancher Hosted Whitepaper.</p>"},{"location":"kbs/000020090/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020091/","title":"Is there any limit on the number of downstream clusters or nodes SUSE Rancher Hosted can manage?","text":"<p>This document (000020091) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020091/#situation","title":"Situation","text":""},{"location":"kbs/000020091/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted can manage up to 2,000 downstream clusters and a total of 20,000 nodes across all clusters. Check with your account executive on the node and cluster limits for your support contract.</p>"},{"location":"kbs/000020091/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020092/","title":"How often is SUSE Rancher Hosted upgraded?","text":"<p>This document (000020092) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020092/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is upgraded normally within two weeks after a stable release. There are typically one or two Rancher releases a quarter. SUSE will contact you to schedule the upgrade. In the future, we plan to have upgrades self-service by letting customers trigger an upgrade through the UI.</p>"},{"location":"kbs/000020092/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020093/","title":"How is SUSE Rancher Hosted different than the open-source Rancher I can download for free?","text":"<p>This document (000020093) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020093/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is built on the same Rancher open-source software that can be downloaded for free. SUSE Rancher Hosted's value proposition is that SUSE installs, upgrades, backs up, monitors, and completely manages the software for you.</p>"},{"location":"kbs/000020093/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020094/","title":"Can I integrate SUSE Rancher Hosted with my Active Directory, SAML, or LDAP based directory service?","text":"<p>This document (000020094) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020094/#resolution","title":"Resolution","text":"<p>Yes, the option to do authentication integration is available in SUSE Rancher Hosted and you can find all the options and directions in our documentation. Integration with external authentication services such as Okta or Azure Active Directory are fairly trivial. For integration with a private or on-premise directory service, you may need to open ports in your firewall or use SUSE Rancher Hosted's network peering or VPN capabilities. SUSE can guide you through this setup if needed.</p>"},{"location":"kbs/000020094/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020095/","title":"Does SUSE Rancher Hosted support multi-factor authentication (MFA)?","text":"<p>This document (000020095) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020095/#resolution","title":"Resolution","text":"<p>Local user accounts in SUSE Rancher Hosted authenticate only using a login and password, so multi-factor authentication (MFA) is not supported. However, SUSE Rancher Hosted can be integrated with many authentication providers that do support MFA, such as Microsoft Azure Active Directory. For a full list of authentication providers, see the Rancher 2.x Authentication Documentation.</p>"},{"location":"kbs/000020095/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020096/","title":"Do I have access to the \"local\" cluster in the management UI for my SUSE Rancher Hosted environment?","text":"<p>This document (000020096) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020096/#resolution","title":"Resolution","text":"<p>No, your SUSE Rancher Hosted environment will not display the \"local\" cluster in the UI and it is not accessible through the API. The local cluster is the Kubernetes cluster that is running the Rancher server workloads and is fully managed by the SUSE Rancher Hosted DevOps team.</p>"},{"location":"kbs/000020096/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020097/","title":"What are the \"-promoted\" Cluster Roles in Rancher?","text":"<p>This document (000020097) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020097/#situation","title":"Situation","text":""},{"location":"kbs/000020097/#question","title":"Question","text":"<p>When I query for Cluster Roles via kubectl, I see some entries with \"-promoted\" appended to them. What are these and why is Rancher creating them?</p>"},{"location":"kbs/000020097/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher server with RKE clusters added</li> <li>Users added to a Project</li> </ul>"},{"location":"kbs/000020097/#answer","title":"Answer","text":"<p>The ClusterRole with \"-promoted\" at the end, is created if the Project role given to a Project member contains any of these resources: storageClass, persistentVolumes, and apiServices.</p> <p>These resources are not scoped to a namespace. They do not belong to any Project but the entire Cluster. That is why Rancher creates an additional ClusterRole.</p>"},{"location":"kbs/000020097/#further-reading","title":"Further Reading","text":"<ul> <li>https://rancher.com/docs/rancher/v2.x/en/admin-settings/rbac/</li> </ul>"},{"location":"kbs/000020097/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020098/","title":"Can Rancher migrate my helm2 app to helm3?","text":"<p>This document (000020098) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020098/#situation","title":"Situation","text":""},{"location":"kbs/000020098/#question","title":"Question","text":"<p>Can I use Rancher to migrate a Rancher app I deployed from a Helm v2 catalog to Helm v3?</p>"},{"location":"kbs/000020098/#answer","title":"Answer","text":"<p>No, Rancher currently does not support migrating an app from Helm v2 to Helm v3. To migrate an app from Helm v2 to Helm v3, you would need to delete the app, re-add the catalog as a helm_v3 catalog and re-install the app.</p>"},{"location":"kbs/000020098/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020099/","title":"Loading the new Rancher Dashboard in an airgapped environment redirects to /fail-whale","text":"<p>This document (000020099) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020099/#situation","title":"Situation","text":""},{"location":"kbs/000020099/#issue","title":"Issue","text":"<p>When attempting to view the new Rancher dashboard in an airgapped environment, or one that requires a proxy to access the internet, the dashboard eventually times out and the user is redirected to https://rancher_server/fail-whale</p> <p></p>"},{"location":"kbs/000020099/#root-cause","title":"Root cause","text":"<p>As the dashboard is currently in beta testing, the code for it resides in our CDN instead of being included in our images. The service responsible for pulling this code currently does not support proxy configuration.</p>"},{"location":"kbs/000020099/#resolution","title":"Resolution","text":"<p>Until the dashboard goes into a General Release status, there is a requirement for internet connectivity to https://releases.rancher.com and https://github.com from both the Rancher cluster and also downstream controlplane nodes for this functionality to work.</p>"},{"location":"kbs/000020099/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020100/","title":"Slow etcd performance (performance testing and optimization)","text":"<p>This document (000020100) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020100/#situation","title":"Situation","text":""},{"location":"kbs/000020100/#issue","title":"Issue","text":"<p>If your etcd logs start showing messages like the following, your storage might be too slow for etcd or the server might be doing too much for etcd to operate properly:</p> <pre><code>2019-08-11 23:27:04.344948 W | etcdserver: read-only range request \"key:\\\"/registry/services/specs/default/kubernetes\\\" \" with result \"range_response_count:1 size:293\" took too long (1.530802357s) to execute\n</code></pre> <p>If your storage is really slow you will even see it throwing alerts in your monitoring system. What can you do to the verify the performance of your storage? If the storage is is not performing correctly, how can you fix it? After researching this I found an IBM article that went over this extensively. Their findings on how to test were very helpful. The biggest factor is your storage latency. If it is not well below 10ms in the 99th percentile, you will see warnings in the etcd logs. We can test this with a tool called fio which I will outline below.</p>"},{"location":"kbs/000020100/#testing-etcd-performance","title":"Testing etcd performance","text":"<ol> <li>Download and install the latest version of fio. This is important because older versions do not provide storage latency. I have a very simple script below to download and install this.</li> </ol> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/instant-fio-master/instant-fio-master.sh\nbash instant-fio-master.sh\n</code></pre> <ol> <li>Test the storage, create a directory on the device you want to test then run the fio command as shown below.</li> </ol> <pre><code>export PATH=/usr/local/bin:$PATH\nmkdir test-data\nfio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest\n</code></pre> <ol> <li>Below is an example output from an etcd,controlplane,worker node of a Rancher installation cluster running on an AWS ec2 instance type of t2.large.</li> </ol> <pre><code>[root@ip-172-31-14-184 ~]# fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest\nmytest: (g=0): rw=write, bs=(R) 2300B-2300B, (W) 2300B-2300B, (T) 2300B-2300B, ioengine=sync, iodepth=1\nfio-3.15-23-g937e\nStarting 1 process\nmytest: Laying out IO file (1 file / 100MiB)\nJobs: 1 (f=1): [W(1)][100.0%][w=2684KiB/s][w=1195 IOPS][eta 00m:00s]\nmytest: (groupid=0, jobs=1): err= 0: pid=21203: Sun Aug 11 23:47:30 2019\n     write: IOPS=1196, BW=2687KiB/s (2752kB/s)(99.0MiB/38105msec)\n       clat (nsec): min=2840, max=99026, avg=8551.56, stdev=3187.53\n         lat (nsec): min=3337, max=99664, avg=9191.92, stdev=3285.92\n       clat percentiles (nsec):\n         |  1.00th=[ 4640],  5.00th=[ 5536], 10.00th=[ 5728], 20.00th=[ 6176],\n         | 30.00th=[ 6624], 40.00th=[ 7264], 50.00th=[ 7968], 60.00th=[ 8768],\n         | 70.00th=[ 9408], 80.00th=[10304], 90.00th=[11840], 95.00th=[13760],\n         | 99.00th=[19328], 99.50th=[23168], 99.90th=[35584], 99.95th=[44288],\n         | 99.99th=[63744]\n       bw (  KiB/s): min= 2398, max= 2852, per=99.95%, avg=2685.79, stdev=104.84, samples=76\n       iops        : min= 1068, max= 1270, avg=1195.96, stdev=46.66, samples=76\n     lat (usec)   : 4=0.52%, 10=76.28%, 20=22.34%, 50=0.82%, 100=0.04%\n     fsync/fdatasync/sync_file_range:\n       sync (usec): min=352, max=21253, avg=822.36, stdev=652.94\n       sync percentiles (usec):\n         |  1.00th=[  400],  5.00th=[  420], 10.00th=[  437], 20.00th=[  457],\n         | 30.00th=[  478], 40.00th=[  529], 50.00th=[  906], 60.00th=[  947],\n         | 70.00th=[  988], 80.00th=[ 1020], 90.00th=[ 1090], 95.00th=[ 1156],\n         | 99.00th=[ 2245], 99.50th=[ 5932], 99.90th=[ 8717], 99.95th=[11600],\n         | 99.99th=[16581]\n     cpu          : usr=0.79%, sys=7.38%, ctx=119920, majf=0, minf=35\n     IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n         submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         issued rwts: total=0,45590,0,0 short=45590,0,0,0 dropped=0,0,0,0\n         latency   : target=0, window=0, percentile=100.00%, depth=1\nRun status group 0 (all jobs):\n     WRITE: bw=2687KiB/s (2752kB/s), 2687KiB/s-2687KiB/s (2752kB/s-2752kB/s), io=99.0MiB (105MB), run=38105-38105msec\nDisk stats (read/write):\n     xvda: ios=0/96829, merge=0/3, ticks=0/47440, in_queue=47432, util=92.25%\n</code></pre> <p>In the fsync data section you can see that the 99th percentile is 2245 or about 2.2ms of latency. This storage is well suited for an etcd node. The etcd documentation suggests that for storage to be fast enough, the 99th percentile of fdatasync invocations when writing to the WAL file must be less than 10ms.</p>"},{"location":"kbs/000020100/#resolution","title":"Resolution","text":"<p>What if your node's storage isn't fast enough? The simple solution is to upgrade the storage but that isn't always an option. If you are on the cusp of acceptable, there are things you can do to optimize your storage so that etcd is happy.</p> <ol> <li> <p>Don't run etcd on a node with other roles. A general rule of thumb is to never have the worker role on the same node as etcd. However many environments have etcd and controlplane roles on the same node and run just fine. If this is the case for your environment then you should consider separating etcd and controlplane nodes.</p> </li> <li> <p>If you've separated etcd and the controlplane node and are still having issues, you can mount a separate volume for etcd so that read write operations for everything else on the node do not impact etcd's performance. This is mostly applicable to Cloud hosted nodes since each volume mounted has its own allocated set of resources.</p> </li> <li> <p>If you are on a dedicated server and would like to separate etcd read write operations from the rest of the server, you should install a new storage device for etcd mounts.</p> </li> <li> <p>Always use SSD's for your etcd nodes, whether it is dedicated or in the cloud.</p> </li> <li> <p>Set the priority of the etcd container so that it is higher than other processes but not too high that it overwhelms the server.</p> </li> </ol> <pre><code>ionice -c2 -n0 -p `pgrep -x etcd`\n</code></pre>"},{"location":"kbs/000020100/#further-reading","title":"Further reading","text":"<p>Below is a list of links that I used for my research. I highly recommend reading these as they contain more information than I've posted in this article.</p> <ul> <li>IBM blog post on use of fio to test etcd storage performance</li> <li>etcd performance documentation</li> <li>etcd documentation on node sizing examples</li> <li>etcd metrics documentation</li> <li>etcd tuning documentation</li> <li>AWS blog post on the difference between burst and baseline performance in EC2 storage</li> </ul>"},{"location":"kbs/000020100/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020101/","title":"How to create docker goroutine, and memory heap, dumps","text":"<p>This document (000020101) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020101/#situation","title":"Situation","text":""},{"location":"kbs/000020101/#task","title":"Task","text":"<p>It's important to observe Docker as it operates to help drive troubleshooting an issue. Here are some commands to generate memory heap and goroutine dumps without killing the Docker process.</p>"},{"location":"kbs/000020101/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Docker with an exposed socket (typically found at <code>/var/run/docker.sock</code>)</li> </ul>"},{"location":"kbs/000020101/#collecting-dumps","title":"Collecting dumps","text":""},{"location":"kbs/000020101/#heap-dump","title":"Heap dump","text":"<p>Heap dumps report a sampling of memory allocations of live objects.</p> <pre><code>curl --unix-socket /var/run/docker.sock http://./debug/pprof/heap?debug=2\n</code></pre>"},{"location":"kbs/000020101/#goroutine-dump","title":"Goroutine dump","text":"<p>The goroutine dump reports stack traces of all current goroutines for the docker process.</p> <pre><code>curl --unix-socket /var/run/docker.sock http://./debug/pprof/goroutine?debug=2\n</code></pre> <p>The output normally is output to <code>stdout</code>, where it can be redirected to a file.</p> <p>Depending on how Docker is configured, and where its configured to log to, the traces could end up in the <code>docker.log</code> file or with the system logs (syslog, journalctl, kern.log, messages, etc...).</p>"},{"location":"kbs/000020101/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020102/","title":"Are API audit logs enabled in SUSE Rancher Hosted?","text":"<p>This document (000020102) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020102/#resolution","title":"Resolution","text":"<p>Yes, API audit logs are enabled in SUSE Rancher Hosted at level 2. Level 2 includes log event metadata and request body, but does not include response metadata and response body. Rancher APIs typically do not contain personally identifiable information (PII). One exception is the user API which can contain a user's full name. More details on Rancher's API audit logging can be found in the Rancher Documentation . Audit logs are stored in the same region as your SUSE Rancher Hosted environment and retained for 1 month. Only the SUSE Rancher team has access to these logs for troubleshooting purposes. Customers may request logs by filing a support case on SCC and providing a date and time range in UTC.</p>"},{"location":"kbs/000020102/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020103/","title":"What information is stored in SUSE Rancher Hosted and where is it stored?","text":"<p>This document (000020103) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020103/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted stores the following information:</p>"},{"location":"kbs/000020103/#user-data","title":"User Data","text":"<ul> <li>First and last name of users (aka Display Name)</li> <li>Login id and password. Password is stored using one-way encryption and transported using TLS.</li> <li>Other user information from GitHub, Okta, Microsoft Active Directory, etc. if authentication integration is enabled.</li> </ul>"},{"location":"kbs/000020103/#cloud-provider-credentials-if-provided","title":"Cloud Provider Credentials (if provided)","text":"<ul> <li>Amazon Web Services Access Key and Secret Key</li> <li>Microsoft Azure Subscription ID, Client ID, Client Secret</li> <li>DigitalOcean Access Token</li> <li>Linode Access Token</li> <li>VMWare vSphere endpoint, Username, and Password</li> <li>Similar types of keys, tokens, or credentials for other cloud providers that are enabled by the customer.</li> </ul>"},{"location":"kbs/000020103/#other-application-data","title":"Other Application Data","text":"<ul> <li>Catalogs and Helm Charts</li> <li>CIS Scan Results</li> <li>Cluster Monitoring Metrics (if turned on)</li> <li>Cluster infrastructure, including node roles, node hardware specs, node software versions, workload metadata, workload logs.</li> <li>Anything else entered by the end-user in the Rancher user interface, API, or CLI which could change from version to version.</li> </ul> <p>Data is stored in our third-party cloud service provider on virtual machines managed by the SUSE Rancher Hosted operations team in the region/country selected by the customer.</p>"},{"location":"kbs/000020103/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020104/","title":"Filesystem actions in containers fail with `Too many levels of symbolic links`","text":"<p>This document (000020104) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020104/#situation","title":"Situation","text":""},{"location":"kbs/000020104/#issue","title":"Issue","text":"<p>When attempting to perform a filesystem action inside a container with a volume located on an autofs directory, the error <code>Too many levels of symbolic links</code> is thrown and the action fails.</p> <pre><code>bash: cd: /data: Too many levels of symbolic links\n</code></pre>"},{"location":"kbs/000020104/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Docker container or Kubernetes Pod with a volume defined that is mounted on the host with autofs, typically backed by NFS</li> </ul>"},{"location":"kbs/000020104/#root-cause","title":"Root cause","text":"<p>As the share that backs the autofs volume isn't mounted until the directory specified is accessed, it is typically not mounted when a container is run.</p> <p>With the default Docker bind-mount propagation of <code>rprivate</code>, containers do not receive mount changes for volumes from the host.</p>"},{"location":"kbs/000020104/#resolution","title":"Resolution","text":"<p>Docker - Mount the volume in question with the flag <code>slave</code>, <code>rslave</code>, <code>shared</code>, or <code>rshared</code> to ensure that mount changes are propagated to the container.</p> <p>Example:</p> <p><code>docker run -d -v /path/to/autofs:/data:shared ubuntu</code></p> <p>See the links at the bottom of this article for info on what each of these flags does</p> <p>Kubernetes - Define mountPropagation for the volume in question as either <code>HostToContainer</code> (same as Docker's <code>rslave</code>) or <code>Bidirectional</code> (same as Docker's <code>rshared</code>):</p> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: test-app\nspec:\n  containers:\n    - name: test\n      image: busybox\n      volumeMounts:\n      - mountPath: \"/data\"\n        name: test-app-vol\n        mountPropagation: HostToContainer\n  volumes:\n    - name: test-app-vol\n      hostPath:\n        path: /data\n</code></pre>"},{"location":"kbs/000020104/#further-reading","title":"Further reading","text":"<p>Docker bind propagation - https://docs.docker.com/storage/bind-mounts/#configure-bind-propagation</p> <p>Kubernetes mountPropagation - https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation</p> <p>Linux Kernel Shared Subtree - https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt</p>"},{"location":"kbs/000020104/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020105/","title":"Best Practices Rancher","text":"<p>This document (000020105) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020105/#situation","title":"Situation","text":"<p>This article aims to provide a number of checks that can be evaluated to ensure best practices are in place when planning, building or preparing a Rancher 2.x and Kubernetes environment.</p>"},{"location":"kbs/000020105/#1-architecture","title":"1. Architecture","text":""},{"location":"kbs/000020105/#11-nodes","title":"1.1 Nodes","text":"<p>Understanding workload resource needs in downstream clusters upfront can help choose an appropriate node configuration; some nodes may need different configurations; however, all nodes of the same role are generally configured the same.</p> <p>Checks</p> <p>Standardize on supported versions and ensure minimum requirements are met:</p> <ul> <li>Confirm the OS is covered in the supported versions</li> <li>Resource needs can vary based on cluster size and workload, however, in general, no less than 8GB of memory and 2 vCPUs is recommended</li> <li>SSD storage is recommended, especially for nodes with the <code>etcd</code> role</li> <li>Firewall rules allow connectivity for nodes ( k3s,\u00a0RKE)</li> <li>A static IP for all nodes is required, if using DHCP, all nodes should have a reserved address</li> <li>Swap is disabled on the nodes</li> <li>NTP is enabled on the nodes</li> </ul>"},{"location":"kbs/000020105/#12-separation-of-concerns","title":"1.2 Separation of concerns","text":"<p>The Rancher management cluster should be dedicated to running the Rancher deployment, additional workloads added to the cluster can contend for resources and impact the performance and predictability of Rancher.</p> <p>This is also important to consider in downstream clusters, the etcd\u00a0and control plane\u00a0nodes (RKE), and server nodes (k3s) should be dedicated to the purpose. When possible, it is recommended that each node have a single role, for example, separate nodes for the etcd and control plane roles.</p> <p>Checks</p> <p>Using the following commands on each cluster, check and confirm for any unexpected workloads running on the Rancher management cluster, or running on the server or etcd/control plane nodes of a downstream cluster.</p>"},{"location":"kbs/000020105/#rancher-management-cluster","title":"Rancher management cluster","text":"<ul> <li>Check for any unexpected pods running in the cluster: <code>kubectl get pods --all-namespaces</code></li> <li>Check for any single points of failure or discrepancies in OS, kernel and CRI version: <code>kubectl get nodes -o wide</code></li> </ul>"},{"location":"kbs/000020105/#downstream-cluster","title":"Downstream cluster","text":"<p>k3sRKE</p> <ul> <li>Check for any unexpected pods running on server nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/master=true --no-headers | cut -d \" \" -f1)\ndo\nkubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre> <ul> <li>Check for any unexpected pods running on etcd nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/etcd=true --no-headers | cut -d \" \" -f1)\ndo\nkubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre> <ul> <li>Check for any unexpected pods running on control plane nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/controlplane=true --no-headers | cut -d \" \" -f1)\ndo\nkubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre>"},{"location":"kbs/000020105/#13-high-availability","title":"1.3 High Availability","text":"<p>Ensure nodes within a cluster are spread across separate failure boundaries as much as possible. This could mean VMs running on separate physical hosts, data centres, switches, storage pools, etc. If running in a cloud environment, instances in separate availability zones.</p> <p>For High Availability in Rancher, a Kubernetes install is required.</p> <p>Checks</p> <ul> <li>When deploying the Rancher management cluster it is recommended to use the following configuration:</li> </ul> Distribution Recommendation k3s 2 server nodes RKE 3 nodes with all roles <ul> <li>Confirm the components of all clusters and external datastores (k3s) are satisfying minimum HA requirements:</li> </ul> <p>k3sRKE</p> Component Minimum Recommended Notes external datastore 2 2 or greater The external datastore should provide failover to a standby using the datastore-endpoint server nodes 2 2 or greater Allow tolerance for at least 1 server node failure agent nodes 2 N/A Allow tolerance for at least 1 agent node failure, scale up to meet the workload needs Component Minimum Recommended Notes etcd nodes 3 3 To maintain quorum it is important to have an uneven # of nodes to provide tolerance for at least 1 node failure control plane nodes 2 2 Allow tolerance for at least 1 node failure worker nodes 2 N/A Allow tolerance for at least 1 worker node failure, scale up to meet the workload needs"},{"location":"kbs/000020105/#cloud-provider","title":"Cloud provider","text":"<p>The following commands can also be used with clusters configured with a cloud provider to review the instance type and availability zones of each node.</p> <ul> <li>Kubernetes v1.17 or earlier: <code>kubectl get nodes -L beta.kubernetes.io/instance-type -L failure-domain.beta.kubernetes.io/zone</code></li> <li>Kubernetes v1.17 or greater: <code>kubectl get nodes -L node.kubernetes.io/instance-type -L topology.kubernetes.io/zone</code></li> </ul> <p>These labels may not be available on all cloud providers.</p>"},{"location":"kbs/000020105/#14-load-balancer","title":"1.4 Load balancer","text":"<p>To provide a consistent endpoint for the Rancher management cluster, a load balancer is highly recommended to ensure the Rancher agents, UI, and API connectivity can effectively reach the Rancher deployment.</p> <p>Checks</p> <p>The load balancer is configured:</p> <ul> <li>Within close proximity of the Rancher management cluster to reduce latency</li> <li>For high availability, with all Rancher management nodes configured as upstream targets</li> <li>With a health check to the following path:</li> </ul> Distribution Health check path k3s <code>/ping</code> RKE <code>/healthz</code> <p>A health check interval is generally recommended at 30 seconds or less</p>"},{"location":"kbs/000020105/#15-proximity-and-latency","title":"1.5 Proximity and latency","text":"<p>For performance reasons, it is recommended to avoid spreading cluster nodes over long distances and unreliable networks. For example, nodes could be in separate AZs in the same region, the same datacenter, or separate nearby data centres.</p> <p>This is particularly important for etcd nodes\u00a0which are sensitive to network latency, the RTT between etcd nodes in the cluster will determine the minimum time to complete a commit .</p> <p>Checks</p> <ul> <li>Network latency and bandwidth is adequate between locations that the cluster nodes will be provisioned</li> </ul> <p>A tool like <code>mtr</code> to gather connectivity statistics between locations over a long sample period can be useful to report on the packet loss and latency.</p> <p>Generally latency between etcd nodes is recommended at 5s or less</p>"},{"location":"kbs/000020105/#16-datastore","title":"1.6 Datastore","text":"<p>It is important to ensure that the chosen datastore is capable of handling requests inline with the workload of the cluster.</p> <p>Allocation of resources, storage performance, and tuning of the datastore may be needed over time, this could be due to an increase in churn in a cluster, downstream clusters growing in size, or the number of downstream clusters Rancher is managing increases.</p> <p>Checks</p> <p>Confirm the recommended options are met for the distribution in use:</p> <p>k3sRKE</p> <p>With an external datastore the general performance requirements include:</p> <ul> <li>SSD or similar storage providing 1,000 IOPs or greater performance</li> <li>Datastore servers are assigned 2 vCPUs and 4GB memory or greater</li> <li>A low latency connection to the datastore endpoint from all k3s server nodes</li> </ul> <p>MySQL 5.7 is recommended . If running in a cloud provider, you may wish to utilise a managed database service .</p> <p>To confirm the storage performance of etcd nodes is capable of handling the workload, a benchmark tool like <code>fio</code> can be used.</p> <ul> <li>Nodes with the <code>etcd</code> role have SSD or similar storage providing high IOPs and low latency</li> </ul> <p>On large downstream or Rancher environments, tuning etcd may be needed, including adding dedicated disk for etcd.</p>"},{"location":"kbs/000020105/#17-cidr-selection","title":"1.7 CIDR selection","text":"<p>The cluster and service CIDRs cannot be changed once a cluster is provisioned.</p> <p>For this reason, it is important to future proof by changing the ranges to avoid routing overlaps with other areas of the network and potential cluster IP exhaustion if the defaults are not suitable.</p> <p>Checks</p> <ul> <li>The default CIDR ranges do not overlap with any area of the network</li> </ul> <p>The default CIDRs are below which often don't need to be changed, to ensure the are no issues with routing from or two pods you may wish to adjust these when creating clusters ( RKE, k3s).</p> Network Default CIDR Cluster 10.42.0.0/16 Service 10.43.0.0/16 <p>Reducing the CIDR sizes can lower the number of IPs available and therefore total number of pods and services in the cluster. In a large cluster, the CIDR ranges may need to be increased .</p>"},{"location":"kbs/000020105/#18-authorized-cluster-endpoint","title":"1.8 Authorized cluster endpoint","text":"<p>At times connecting directly to a downstream cluster may be desired, this could be to reduce latency, avoid interruption if Rancher is unavailable, or that a high frequency of external API calls occur, for example, external monitoring, or a CI/CD pipeline.</p> <p>Checks</p> <ul> <li>Check for any use cases where an authorized cluster endpoint is needed</li> </ul> <p>Access directly to the downstream cluster kube-apiserver can be configured using the secondary context in the kubeconfig file.</p>"},{"location":"kbs/000020105/#2-best-practices","title":"2. Best Practices","text":""},{"location":"kbs/000020105/#21-installing-rancher","title":"2.1 Installing Rancher","text":"<p>It is highly encouraged to install Rancher on a Kubernetes cluster in an HA configuration .</p> <p>If starting with small resource requirements, at the very minimum always install on a Kubernetes cluster with a single node, this provides a future path to adding nodes at a later date.</p> <p>The design of the single node Docker install is for short-lived testing environments, migration from a Docker to a Kubernetes install is not possible.</p> <p>Checks</p> <ul> <li>Rancher is installed on a Kubernetes cluster, even if that is a single node cluster</li> </ul>"},{"location":"kbs/000020105/#22-rancher-resources","title":"2.2 Rancher Resources","text":"<p>The minimum resource requirements for nodes in the Rancher management cluster need to scale to match the number of downstream clusters and nodes; this may change over time and need reviewing as changes occur in the environment.</p> <p>Checks</p> <ul> <li>Verify that nodes in the Rancher management cluster meet at least the minimum requirements:</li> </ul> Resource Requirements CPU/Memory Rancher v2.4.0 and greater CPU/Memory Rancher v2.4.0 and earlier Network Port requirements"},{"location":"kbs/000020105/#23-chart-options","title":"2.3 Chart options","text":"<p>When installing the Rancher helm chart, the default options may not always be the best fit for specific environments.</p> <p>Checks</p> <ul> <li> <p>The Rancher helm chart is installed with the desired options</p> </li> <li> <p><code>replicas</code> - the default number of Rancher replicas ( <code>3</code>) may not suit your cluster, for example, a k3s cluster with 2 x server nodes using a <code>replicas</code> value of <code>2</code> will ensure only one Rancher pod is running per node.</p> </li> <li><code>antiAffinity</code> - the default <code>preferred</code> scheduling can mean Rancher pods become imbalanced during the lifetime of a cluster, using <code>required</code> can ensure Rancher is always scheduled on unique nodes</li> </ul> <p>To confirm the options provided on an existing Rancher install with helm v3, the following command can be used <code>helm get values rancher -n cattle-system</code></p>"},{"location":"kbs/000020105/#24-supported-versions","title":"2.4 Supported versions","text":"<p>When choosing or maintaining the components for Rancher and Kubernetes clusters the product lifecycle and support matrix can be used to ensure the versions and OS configurations are certified and maintained.</p> <p>Checks</p> <ul> <li>All Rancher and Kubernetes cluster versions are under maintenance and certified</li> </ul> <p>As versions are a moving target, checking the current stable releases and planning for future upgrades on a schedule is recommended.</p>"},{"location":"kbs/000020105/#25-recurring-snapshots-and-backups","title":"2.5 Recurring snapshots and backups","text":"<p>It is important to configure snapshots on a recurring schedule and store these externally to the cluster for disaster recovery.</p> <p>Checks</p> <ul> <li>Recurring snapshots are configured for the distribution in use</li> </ul> Distribution Configuration k3s Configure snapshots and backups on the external datastore, this can differ depending on the chosen database RKE Configure recurring snapshots of etcd, with an S3 compatible endpoint for off-node copies <p>In addition to a recurring schedule, it's important to take one-time snapshots of etcd (RKE) , or datastore (k3s) before and after significant changes.</p> <p>The Rancher backup operator can also be used on any\u00a0distribution to backup the related objects that Rancher needs to function, this can be used to migrate Rancher between clusters.</p>"},{"location":"kbs/000020105/#26-provisioning","title":"2.6 Provisioning","text":"<p>Provisioning nodes and resources for Rancher and downstream clusters in a repeatable and automated way will greatly improve the supportability of Rancher and Kubernetes. This allows nodes to be replaced in a cluster easily, and new clusters created in a consistent way.</p> <p>Checks</p> <p>The below points can help prepare the Rancher and Kubernetes environment with integrations and modern approaches to managing resources, such as infrastructure as code, CI/CD, immutable infrastructure, and configuration management:</p> <ul> <li>Manifests and configuration data are stored in source control, treated as the source of truth for containerized applications</li> <li>Automated build, deployment and/or configuration management</li> </ul> <p>The rancher2 terraform provider and pulumi package can be used to manage clusters and resources as code.</p>"},{"location":"kbs/000020105/#27-managing-node-lifecycle","title":"2.7 Managing node lifecycle","text":"<p>When making significant planned changes it is important to drain nodes that are being affected to avoid disrupting in-flight connections, such as restarting Docker, patching, shutting down or removing nodes.</p> <p>For example, the <code>kube-proxy</code> component manages iptables rules on nodes to manage service endpoints, if a node is suddenly shutdown, stale endpoints and orphaned pods can be left in place for a period of time causing connectivity issues.</p> <p>In some cases during an unplanned issue, draining can be automated, such as when a node may be terminated, restarted, or shutdown.</p> <p>Checks</p> <ul> <li>A process is in place to drain before planned disruptive changes are performed on a node</li> <li>Where possible, node draining during the shutdown sequence is automated, for example, with a systemd or similar service</li> </ul>"},{"location":"kbs/000020105/#3-operating-kubernetes","title":"3. Operating Kubernetes","text":""},{"location":"kbs/000020105/#31-capacity-planning-and-monitoring","title":"3.1 Capacity planning and Monitoring","text":"<p>It is recommended to measure resource usage of all clusters by enabling monitoring in Rancher, or your chosen solution. It is recommended to alert on resource thresholds and events in the cluster.</p> <p>On supported platforms, using Cluster Autoscaler can be used to ensure the number of nodes is right-sized for the pod workload. Combining this with Horizontal Pod Autoscaler provides both application and infrastructure scaling capabilities.</p> <p>Checks</p> <ul> <li>Monitoring is enabled for the Rancher and downstream clusters</li> <li>Alert notifiers are configured to stay informed if an alarm or event occurs</li> <li>A process for adding/removing nodes is established, automated if possible</li> </ul>"},{"location":"kbs/000020105/#32-probes","title":"3.2 Probes","text":"<p>In the defence against service and pod related failures, liveness and readiness probes are very useful; these can be in the form of HTTP requests, commands, or TCP connections.</p> <p>Checks</p> <ul> <li>Liveness and Readiness probes are configured where necessary</li> <li>Probes do not rely on the success of upstream dependencies, only the running application in the pod</li> </ul>"},{"location":"kbs/000020105/#33-resources","title":"3.3 Resources","text":"<p>Assigning resource requests to pods allows the <code>kube-scheduler</code> to make more informed placement decisions, avoiding the \"bin packing\" of pods onto nodes and resource contention.</p> <p>Limits also offer value in the form of a safety net against pods consuming an undesired amount of resources.</p> <p>In addition to defining requests and limits for pods, it can also be useful to\u00a0reserve capacity\u00a0on nodes to prevent allocating resources that may be consumed by the kubelet and other system daemons, like Docker.</p> <p>Checks</p> <ul> <li>All pods define resource requests and have limits configured where necessary</li> <li>Nodes have system and daemon reservations where necessary</li> </ul> <p>When Rancher Monitoring is enabled, the graphs in Grafana\u00a0can be used to find a baseline of CPU and Memory for resource requests</p>"},{"location":"kbs/000020105/#34-os-limits","title":"3.4 OS Limits","text":"<p>Containerized applications can consume high amounts of OS resources, such as open files, connections, processes, filesystem space and inodes.</p> <p>Often the defaults are adequate; however, establishing a standardized image for all nodes can help establish a baseline for all configuration and tuning.</p> <p>Checks</p> <p>In general, the below can be used to confirm the OS limits allow for adequate headroom for the workloads</p> <ul> <li>File descriptor usage: <code>cat /proc/sys/fs/file-nr</code></li> <li> <p>User ulimits: <code>ulimit -a</code> Or, a particular process can be checked: <code>cat /proc/PID/limits</code></p> </li> <li> <p>Conntrack limits:</p> </li> </ul> <p><code>cat /proc/sys/net/netfilter/nf_conntrack_max</code></p> <p><code>cat /proc/sys/net/netfilter/nf_conntrack_count</code></p> <ul> <li>Filesystem space and inode usage: <code>df -h</code> and <code>df -ih</code></li> </ul> <p>Requirements for Linux can differ slightly depending on the distribution, refer to the Linux Requirements for more information.</p>"},{"location":"kbs/000020105/#35-log-rotation","title":"3.5 Log rotation","text":"<p>To prevent large log files from accumulating, and apply a desired retention period it is recommended to rotate OS, pod log files, and configure an external log service to stream logs off the nodes for a longer-term lifecycle and easier searching.</p> <p>Checks</p>"},{"location":"kbs/000020105/#containers","title":"Containers","text":"<p>k3sRKE</p> <ul> <li>Log rotation is configured for the container logs</li> <li>An external logging service is configured as needed</li> </ul> <p>The below arguments for the <code>INSTALL_K3S_EXEC</code> environment variable can be used as an example to rotate container logs:</p> <p><code>INSTALL_K3S_EXEC=\"--kubelet-arg container-log-max-files=5 --kubelet-arg container-log-max-size=100Mi\"</code></p> <ul> <li>Log rotation is configured for the container logs</li> <li>An external logging service is configured as needed</li> </ul> <p>Rotating container logs can be accomplished by configuring logrotate or the <code>/etc/daemon.json</code> file with a size and retention configuration.</p>"},{"location":"kbs/000020105/#os","title":"OS","text":"<p>Rotation of log files on nodes is also important, especially if a long node lifecycle is expected.</p>"},{"location":"kbs/000020105/#36-dns-scalability","title":"3.6 DNS scalability","text":"<p>DNS is a critical service running within the cluster. DNS queries are distributed throughout the cluster, where the availability depends on the accessibility of the CoreDNS pods in the service.</p> <p>The Nodelocal DNS cache is a redesign on the architecture and is recommended for clusters that may experience high DNS workload or issues.</p> <p>Checks</p> <p>If a cluster has experienced a DNS issue, or high DNS workload is expected:</p> <ul> <li>Check the output of <code>conntrack -S</code> on related nodes.</li> </ul> <p>High amounts of the <code>insert_failed</code> counter can be indicative of a conntrack race condition, Nodelocal DNS cache is recommended to mitigate this.</p>"},{"location":"kbs/000020105/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020105/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020106/","title":"What permissions are required to grant access to manage Cluster Logging in Rancher v2.x","text":"<p>This document (000020106) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020106/#situation","title":"Situation","text":""},{"location":"kbs/000020106/#question","title":"Question","text":"<p>By default, only Global Admins or Cluster Owners have access to configure and manage Cluster Logging in a Rancher v2.x managed cluster. This article details the permissions required to grant this access to other users.</p>"},{"location":"kbs/000020106/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster managed by Rancher v2.x</li> </ul>"},{"location":"kbs/000020106/#answer","title":"Answer","text":"<p>Cluster Logging configuration is managed by the ClusterLoggings Custom Resource in the management.cattle.io API Group. In order to create a role that grants permission to manage the logging configuration for a cluster, you should therefore grant all verbs on the CluserLoggings Resource in the management.cattle.io API group.</p> <p>You can define a custom Cluster Role via the Rancher UI, by navigating to the Global view, and selecting Security -&gt; Roles -&gt; Cluster, creating a custom role with these permissions. Granting this custom role on a cluster to a user or group will then provide access to manage the Cluster Logging configuration for that cluster.</p>"},{"location":"kbs/000020106/#further-reading","title":"Further Reading","text":"<ul> <li>Rancher v2.x Cluster Logging Documentation</li> <li>Rancher v2.x Role-Based Access Control (RBAC) Documentation</li> </ul>"},{"location":"kbs/000020106/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020107/","title":"How to pull the logs from the rancher-wins service on a Windows node in a Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020107) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020107/#situation","title":"Situation","text":""},{"location":"kbs/000020107/#task","title":"Task","text":"<p>In Windows Kubernetes clusters, available in Rancher v2.3.0 and above, the <code>rancher-wins</code> service provides a method for Rancher to operate the Windows host. Whilst troubleshooting a Windows cluster issue it may be necessary to pull the logs from this service, as documented in this article.</p>"},{"location":"kbs/000020107/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Windows Kubernetes cluster provisioned by Rancher v2.3.0 and above</li> </ul>"},{"location":"kbs/000020107/#steps","title":"Steps","text":"<p>To pull the logs from the <code>rancher-wins</code> service, execute the following command in a Powershell session on the node:</p> <pre><code>Get-EventLog -LogName Application -Source rancher-wins &gt; wins.log\n</code></pre> <p>This will write the logs to the file <code>wins.log</code> in the working directory, which you can then provide in your Rancher Support Ticket, for analysis.</p>"},{"location":"kbs/000020107/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020108/","title":"How to enable CoreDNS query logging in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020108) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020108/#situation","title":"Situation","text":""},{"location":"kbs/000020108/#task","title":"Task","text":"<p>By default, DNS query logging is disabled in CoreDNS, this article details the steps to enable query logging for CoreDNS in a Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x.</p>"},{"location":"kbs/000020108/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS dns add-on.</li> </ul>"},{"location":"kbs/000020108/#steps","title":"Steps","text":"<p>To enable DNS query logging the log plugin needs to be configured, by addition of <code>log</code> to the Corefile in the coredns ConfigMap of the kube-system Namespace.</p> <p>For example, to use the default log plugin configuration and log all queries, the Corefile definition would be updated as follows:</p> <pre><code>.:53 {\n    log\n    errors\n    health\n    ready\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n      pods insecure\n      fallthrough in-addr.arpa ip6.arpa\n    }\n    prometheus :9153\n    forward . \"/etc/resolv.conf\" {\n      policy random\n    }\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n</code></pre> <p>Steps to update the CoreDNS ConfigMap and persist these changes can be found in the article \"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster\".</p> <p>For the full list of available options when configuring the log plugin refer to the plugin documentation.</p>"},{"location":"kbs/000020108/#further-reading","title":"Further reading","text":"<ul> <li>CoreDNS log plugin documentation</li> </ul>"},{"location":"kbs/000020108/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020109/","title":"How to use External TLS Termination with AWS","text":"<p>This document (000020109) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020109/#situation","title":"Situation","text":""},{"location":"kbs/000020109/#task","title":"Task","text":"<p>This document covers setting up Rancher using an AWS SSL certificate and an ALB (Application Load Balancer).</p>"},{"location":"kbs/000020109/#requirements","title":"Requirements","text":"<ul> <li>Running Rancher management servers on AWS</li> </ul>"},{"location":"kbs/000020109/#resolution","title":"Resolution","text":""},{"location":"kbs/000020109/#configure-the-ssl-certificate","title":"Configure the SSL certificate","text":"<ul> <li>If you are using your own certificate follow the AWS documentation to import the certificate.</li> <li>If you are using an AWS certificate following the AWS documentation to request a public ACM certificate.</li> </ul>"},{"location":"kbs/000020109/#create-the-target-group","title":"Create the Target Group","text":"<ol> <li>Log into the AWS Console to get started.</li> <li>Use Create a Target Group to create a Target group using the data in the tables below to complete the procedure:</li> </ol> <p>- Target Group Name: rancher-http-80     - Protocol: http     - Port: 80     - Target type: instance     - VPC: Choose your VPC     - Protocol (Health Check): http     - Path (Health Check): /healthz</p> <ol> <li>Use Register Targets to Rancher management servers making sure to use the port 80.</li> </ol>"},{"location":"kbs/000020109/#create-the-alb","title":"Create the ALB","text":"<ol> <li>From your web browser, navigate to the Amazon EC2 Console.</li> <li>From the navigation pane, choose LOAD BALANCING &gt; Load Balancers.</li> <li>Click Create Load Balancer.</li> <li>Choose Application Load Balancer.</li> <li> <p>Complete the Step 1: Configure Load Balancer form:</p> <p>- Basic Configuration  - Name: rancher-http  - Scheme: internet-facing  - IP address type: ipv4  - Listeners  - Add the Load Balancer Protocols and Load Balancer Ports below.  - HTTP: 80  - HTTPS: 443  - Availability Zones  - Select Your VPC and Availability Zones.</p> </li> <li> <p>Complete the Step 2: Configure Security Settings form.</p> <p>- Configure the certificate you want to use for SSL termination.</p> </li> <li> <p>Complete the Step 3: Configure Security Groups form.</p> </li> <li> <p>Complete the Step 4: Configure Routing form.</p> <p>- From the Target Group drop-down, choose Existing target group.  - Add target group rancher-http-80.</p> </li> <li> <p>Complete Step 5: Register Targets. Since you registered your targets earlier, all you have to do it click Next: Review.</p> </li> <li> <p>Complete Step 6: Review. Look over the load balancer details and click Create when you\u2019re satisfied.</p> </li> <li>After AWS creates the ALB, click Close.</li> </ol>"},{"location":"kbs/000020109/#configure-external-tls-termination-for-rancher","title":"Configure External TLS Termination for Rancher","text":"<p>You need to add the option <code>--set tls=external</code> to your Rancher install, per the following example: <code>helm install rancher rancher-latest/rancher --namespace cattle-system --set hostname=mmattox-example.support.rancher.space --version 2.3.6 --set tls=external</code></p>"},{"location":"kbs/000020109/#verification","title":"Verification","text":"<p>Run the following command to verify new certificate:</p> <pre><code>curl --insecure -v https://&lt;&lt;Rancher Hostname&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n</code></pre> <p>Example output:</p> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host mmattox-example.support.rancher.space left intact\n</code></pre> <p>NOTE: Some browsers will cache the certificate. Details on how to clear the SSL state in a browser can be found here.</p>"},{"location":"kbs/000020109/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020110/","title":"Can I Use Rancher 2.4 Dashboard Feature in Air-Gapped environment ?","text":"<p>This document (000020110) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020110/#situation","title":"Situation","text":""},{"location":"kbs/000020110/#issue","title":"Issue","text":"<p>In an Air-Gapped environment, when I try to access the Dashboard Feature, it is not working.</p> <p>I keep having an <code>error 500</code> from Rancher.</p>"},{"location":"kbs/000020110/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher 2.4.x</li> <li>Air-Gapped environment</li> <li>Experimental Dashboard Feature enabled.</li> </ul>"},{"location":"kbs/000020110/#resolution","title":"Resolution","text":"<p>The Dashboard feature is still experimental and is currently not bundled in the Rancher releases.</p> <p>It is currently hosted on <code>https://releases.rancher.com/dashboard/latest/</code></p> <p>This allows our Engineers to do some changes outside of the regular release cycle.</p> <p>The same rules apply if the Kubernetes cluster is using a proxy.</p> <p>The proxy should allow external access to this URL.</p>"},{"location":"kbs/000020110/#further-reading","title":"Further Reading","text":"<p>https://rancher.com/docs/rancher/v2.x/en/installation/options/feature-flags/</p>"},{"location":"kbs/000020110/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020111/","title":"Kernel crash after \"unregister_netdevice: waiting for lo to become free. Usage count\"","text":"<p>This document (000020111) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020111/#situation","title":"Situation","text":""},{"location":"kbs/000020111/#issue","title":"Issue","text":"<p>In a Linux Kubernetes cluster that has frequent pod creations and deletions along with large amounts of pod network traffic, the following error may be logged to the host system logs:</p> <p><code>\"unregister_netdevice: waiting for lo to become free. Usage count = 1\"</code></p> <p>The kernel will typically be in a semi-hung state after this, causing major system instability.</p>"},{"location":"kbs/000020111/#resolution","title":"Resolution","text":"<p>This issue is fixed upstream in the Linux Kernel by this commit which was released in version 4.4.0.</p> <p>We recommend upgrading to the latest linux kernel available in your distribution.</p> <p>We have seen cases where certain kernel modules can cause this issue while loaded, even on a kernel that includes the fix above. If you are running a kernel higher than 4.4.0 and still seeing this issue, try disabling any third-party kernel modules to test.</p>"},{"location":"kbs/000020111/#projectos-specific-bugs","title":"Project/OS Specific bugs:","text":"<p>Docker - https://github.com/moby/moby/issues/5618</p> <p>Ubuntu - https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1403152</p> <p>- https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1711407</p> <p>RedHat - https://access.redhat.com/solutions/3105941 - https://access.redhat.com/solutions/3659011</p> <p>Centos - https://bugs.centos.org/view.php?id=12711</p>"},{"location":"kbs/000020111/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020112/","title":"Experimental Dashboard feature causes memory leak in rancher server and cattle-cluster-agent processes in Rancher v2.4.0 - v2.4.2","text":"<p>This document (000020112) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020112/#situation","title":"Situation","text":""},{"location":"kbs/000020112/#issue","title":"Issue","text":"<p>With the experimental Dashboard feature enabled in Rancher v2.4.0 through v2.4.2, the rancher server and cattle-cluster-agent processes will leak memory. As a result the memory usage of these Pods will grow over time, until they restart due to an Out of Memory (OOM) condition.</p>"},{"location":"kbs/000020112/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher install with either v2.4.0 or v2.4.2</li> </ul>"},{"location":"kbs/000020112/#workaround","title":"Workaround","text":"<p>Disable the experimental Dashboard feature within the Rancher UI:</p> <ol> <li>Navigate to the Global View -&gt; Settings -&gt; Feature Flags.</li> <li>Click the elipses for the <code>dashboard</code> entry and click <code>Deactivate</code>.</li> </ol>"},{"location":"kbs/000020112/#resolution","title":"Resolution","text":"<p>Upgrade to a newer version of Rancher v2.4.3+.</p>"},{"location":"kbs/000020112/#further-reading","title":"Further Reading","text":"<p>GitHub issue #26577 GitHub issue #26633</p>"},{"location":"kbs/000020112/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020113/","title":"Logging integration doesn't work if Docker Root is not default /var/lib/docker","text":"<p>This document (000020113) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020113/#situation","title":"Situation","text":""},{"location":"kbs/000020113/#issue","title":"Issue","text":"<p>As of the time of this writing, Rancher Logging is broken when the Docker root is configured to something other than <code>/var/lib/docker</code>.</p> <p>This issue is tracked in GitHub issue #21112.</p>"},{"location":"kbs/000020113/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher 2.x managed/imported cluster with logging enabled.</li> <li>Docker root configured to something other than <code>/var/lib/docker</code> on the nodes (confirmed with <code>docker info | grep Root</code>).</li> </ul>"},{"location":"kbs/000020113/#workaround","title":"Workaround","text":"<p>These steps will assume you have the Docker data root set to <code>/other-docker-root</code>. Change <code>/other-docker-root</code> to whatever your custom path is:</p> <ol> <li> <p>Rancher UI -&gt; Cluster -&gt; System Project -&gt; Workloads -&gt; cattle-logging Namespace</p> </li> <li> <p>Find workload rancher-logging-fluentd-linux</p> </li> <li> <p>Edit YAML</p> </li> <li> <p>Edit volume dockerroot</p> </li> <li> <p>Change \"Path on the Node\" from <code>/var/lib/docker</code> to <code>/other-docker-root</code></p> </li> <li> <p>Add volume (with the following details):</p> </li> </ol> <pre><code>Volume Name: dockerrootcustom\nType: bind-mount\nPath on the Node: /other-docker-root\nMount Point: /other-docker-root\n</code></pre> <ol> <li>Click Save</li> </ol> <p>At this point logging should be working with your non-default Docker root directory. You should be able to verify this on your logging target. Keep in mind it may take a few minutes for logs to show up there as fluentd is configured to clear its buffer every 60 seconds by default.</p>"},{"location":"kbs/000020113/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020114/","title":"Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge ' for InstanceType.","text":"<p>This document (000020114) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020114/#situation","title":"Situation","text":""},{"location":"kbs/000020114/#issue","title":"Issue","text":"<p>When trying to deploy a node in AWS EC2 on Rancher v1.6.x, you recieve an error like <code>Error in driver during machine creation: Error launching instance: InvalidParameterValue: Invalid value 'r5.12xlarge\\t' for InstanceType.</code></p>"},{"location":"kbs/000020114/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher UI v1.6.50 or earlier (included by default in Rancher v1.6.28 or earlier)</li> <li>Trying to deploy an AWS EC2 node of size <code>r5.12xlarge</code>, <code>r5.24xlarge</code>, <code>r5a.12xlarge</code>, or <code>r5a.24xlarge</code></li> </ul>"},{"location":"kbs/000020114/#resolution","title":"Resolution","text":"<p>The fix is in rancher ui v1.6.51 which is introduced in Rancher v1.6.29. Upgrade to v1.6.29 or later to deploy EC2 nodes of the affected sizes. It is advised to upgrade to the latest stable Rancher v1.6.x and migrate to latest stable Rancher v2.x</p>"},{"location":"kbs/000020114/#further-reading","title":"Further reading","text":"<p>The issue is a trailing tab in the name of the instance type. The commit that solves the issue can be seen here: https://github.com/rancher/ui/commit/5709e997aea949f41e299db8f519dc046d731cb9 rancher/ui v1.6.51: https://github.com/rancher/ui/tree/v1.6.51/app/components/machine rancher/rancher v1.6.29: https://github.com/rancher/rancher/releases/tag/v1.6.29</p>"},{"location":"kbs/000020114/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020115/","title":"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020115) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020115/#situation","title":"Situation","text":""},{"location":"kbs/000020115/#task","title":"Task","text":"<p>You might wish to update the Corefile configuration of CoreDNS, defined via the coredns ConfigMap in the kube-system Namespace, for example, in order to enable query logging or update the resolver policy. This article details how to update this ConfigMap and persist changes in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster.</p>"},{"location":"kbs/000020115/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS add-on.</li> <li>kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user.</li> </ul>"},{"location":"kbs/000020115/#steps","title":"Steps","text":"<ol> <li>Capture the current CoreDNS ConfigMap definition, with the following <code>kubectl</code> command:</li> </ol> <pre><code>kubectl -n kube-system get configmap coredns -o go-template={{.data.Corefile}}\n</code></pre> <p>The output should look like the following:</p> <pre><code>.:53 {\n       errors\n       health\n       ready\n       kubernetes cluster.local in-addr.arpa ip6.arpa {\n         pods insecure\n         fallthrough in-addr.arpa ip6.arpa\n       }\n       prometheus :9153\n       forward . \"/etc/resolv.conf\" {\n         policy random\n       }\n       cache 30\n       loop\n       reload\n       loadbalance\n}\n</code></pre> <ol> <li>Edit the cluster configuration YAML, to define a custom add-on containing the CoreDNS ConfigMap, with your desired changes. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click <code>Edit as YAML</code>.</li> </ol> <p>Create the add-on with the content below, replacing the Corefile definition with the existing configuration retrieved in step 1. Then make the desired changes, in this example the resolver policy is updated from random, in the existing configuration, to sequential.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n     name: coredns\n     namespace: kube-system\ndata:\n     Corefile: |\n       .:53 {\n           errors\n           health\n           ready\n           kubernetes cluster.local in-addr.arpa ip6.arpa {\n             pods insecure\n             fallthrough in-addr.arpa ip6.arpa\n           }\n           prometheus :9153\n           forward . \"/etc/resolv.conf\" {\n             policy sequential\n           }\n           cache 30\n           loop\n           reload\n           loadbalance\n       }\n</code></pre> <ol> <li>Update the cluster with the new configuration. For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>). For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol>"},{"location":"kbs/000020115/#further-reading","title":"Further reading","text":"<ul> <li>How to update CoreDNS's resolver policy</li> </ul>"},{"location":"kbs/000020115/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020116/","title":"How to run workloads on etcd or controlplane nodes, without the worker role, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020116) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020116/#situation","title":"Situation","text":""},{"location":"kbs/000020116/#task","title":"Task","text":"<p>Although it is normally not advised to run workloads on your controlplane and etcd nodes, there are occasionally scenarios when this is necessary. A few common examples are virus scanning, monitoring, and log collection workloads.</p>"},{"location":"kbs/000020116/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020116/#steps","title":"Steps","text":"<p>Both the controlplane and etcd nodes, which are not additionaly designated the worker role, have taints. When RKE or Rancher provisions these nodes, it adds these taints automatically. Workloads that need to run on these nodes require tolerations for these taints. For Rancher managed clusters you can see these taints within the Rancher UI on the cluster node view. The following kubectl command will also list the taints for each node.</p> <pre><code>$ kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\nNAME           TAINTS\nip-10-0-2-10   [map[effect:NoExecute key:node-role.kubernetes.io/etcd value:true]]\nip-10-0-2-11   [map[effect:NoSchedule key:node-role.kubernetes.io/controlplane value:true]]\nip-10-0-2-12   &lt;none&gt;\n</code></pre> <p>Per this output, each etcd node has the <code>NoExecute</code> taint <code>node-role.kubernetes.io/etcd=true</code> and each controlplane node has the <code>NoSchedule</code> taint <code>node-role.kubernetes.io/controlplane=true</code>.</p> <p>The Rancher UI does not have fields for adding tolerations, so you will need to specify the tolerations directly in the workload's YAML manifest. You can use the <code>Import YAML</code> button to deploy your workload and make sure to add the following tolerations block in your manifest:</p> <pre><code>spec:\n...\ntemplate:\n...\nspec:\n...\ntolerations:\n- operator: Exists\n...\n</code></pre> <p>If you have an existing workload, you can also select the <code>View/Edit YAML</code> option for the workload and apply the above change. This toleration will allow you to run the workload on any nodes with taints, so use with caution. If you are using Helm charts, you can also specify the same YAML in your Helm chart.</p>"},{"location":"kbs/000020116/#further-reading","title":"Further Reading","text":"<p>For more information on how taints and tolerations work in Kubernetes, see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</p>"},{"location":"kbs/000020116/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020117/","title":"How to override DNS results served by CoreDNS","text":"<p>This document (000020117) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020117/#situation","title":"Situation","text":""},{"location":"kbs/000020117/#task","title":"Task","text":"<p>By default, DNS requests for pods using CoreDNS will be made directly to the upstream nameservers configured in /etc/resolv.conf on the node.</p> <p>At times, it may not be possible to easily update records on the upstream nameservers, or specific records for the cluster may be needed. In these cases it's useful to override the results that CoreDNS will serve pods.</p>"},{"location":"kbs/000020117/#pre-requisites","title":"Pre-requisites","text":"<p>These steps should work for any cluster running CoreDNS where the <code>coredns</code> ConfigMap is used.</p>"},{"location":"kbs/000020117/#steps","title":"Steps","text":"<p>There are two approaches to achieve this, please read through both to understand which is best for your environment.</p> <p>Both approaches require editting the <code>coredns</code> ConfigMap, specifically the <code>Corefile</code> key. This can be done in the UI by clicking View/Edit YAML, Edit, or on the command line with kubectl.</p> <p>Along with these options, both plugins covered provide other features, like adjusting the TTL for records, see the documentation links for more information.</p>"},{"location":"kbs/000020117/#rewrite","title":"Rewrite","text":"<p>The rewrite plugin will perform a rewritten query to the upstream nameserver, and respond to the query with the results. The outcome would be similar to configuring a CNAME for the domain.</p> <pre><code>data:\nCorefile: |\n.:53 {\n[...]\nrewrite name archive.ubuntu.com internal-mirror.ubuntu.local\n}\n</code></pre> <p>In this example, pods configured with the default Ubuntu mirror are now resolving to the internal mirror without any custom configuration.</p> <p>The benefit of this approach is that the upstream nameserver remains the source of truth for the results.</p>"},{"location":"kbs/000020117/#hosts","title":"Hosts","text":"<p>The hosts plugin provides the ability to define a list of IPs and domains in the form of /etc/hosts to respond as query results.</p> <pre><code>data:\nCorefile: |\n.:53 {\n[...]\nhosts {\n10.0.0.1 archive.ubuntu.com\n10.0.0.2 testing.com\nfallthrough\n}\n}\n</code></pre> <p>A similar example, the internal IPs listed are provided as results.</p> <p>A downside to this approach is that the ConfigMap becomes a source of truth for these results, if changes in the environment are not reflected these entries could become stale. However, it does provide the most flexibility without needing to depend on any upstream nameserver to serve results.</p>"},{"location":"kbs/000020117/#persist-the-changes","title":"Persist the changes","text":"<p>In an RKE or Rancher environment, during cluster or addon upgrades, it's possible that changes to the <code>coredns</code> ConfigMap are updated to use the provided version.</p> <p>To persist the changes made to the ConfigMap, add the changes as a user-defined addon. The steps to do this are documented under How To Update CoreDNS's Resolver Policy article.</p>"},{"location":"kbs/000020117/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020118/","title":"How to monitor NTP on Linux nodes with Cluster Monitoring in Rancher v2.2.x+","text":"<p>This document (000020118) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020118/#situation","title":"Situation","text":""},{"location":"kbs/000020118/#task","title":"Task","text":"<p>Time drift between nodes in a Kubernetes cluster can create a range of issues, from a difficulty to correlate application log message timestamps across nodes, to a loss of etcd quorum (given the time sensitive nature of the consensus algorithm used in etcd).</p> <p>Using Rancher, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through integration with Prometheus, a leading open-source monitoring solution.</p> <p>This article details how to monitor time drift, via the Network Time Protocol (NTP), on Linux nodes within Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters.</p>"},{"location":"kbs/000020118/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, starting at v2.2.0 and above</li> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with Cluster Monitoring enabled, with Monitoring Version 0.2.0+</li> <li>ntp configured on Linux nodes in the cluster (refer to the documentation for your Linux distribution on enabling and configuring ntp)</li> </ul>"},{"location":"kbs/000020118/#steps","title":"Steps","text":""},{"location":"kbs/000020118/#enable-the-ntp-collector-on-the-node-exporter-daemonset","title":"Enable the NTP collector on the Node Exporter DaemonSet","text":"<ol> <li>Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -&gt; Monitoring</li> <li>In the bottom-right corner of the form, click <code>Show advanced options</code></li> <li>Click <code>Add Answer</code></li> <li>Configure the variable <code>exporter-node.collectors.ntp.enabled</code> with value <code>true</code></li> <li>Click <code>Save</code></li> </ol>"},{"location":"kbs/000020118/#configure-an-alert-for-ntp-time-drift","title":"Configure an alert for NTP time drift","text":"<ol> <li>Within the Rancher UI cluster view for the relevant cluster, navigate to Tools -&gt; Alerts</li> <li>On the <code>A set of alerts for node</code> Alert Group click <code>Add Alert Rule</code></li> <li>Set Name to <code>Node NTP time drift equal to or greater than 1 second</code></li> <li>Select <code>Expression</code> and enter <code>node_ntp_offset_seconds</code></li> <li>Click <code>Create</code></li> <li>Configure a Notifier for the <code>A set of alerts for node</code> Alert Group, by clicking the elipses for this Alert Group, and configuring the desired notifier in the <code>Alert</code> section at the bottom of the form.</li> </ol>"},{"location":"kbs/000020118/#further-reading","title":"Further Reading","text":"<ul> <li>Rancher Cluster Monitoring Documentation</li> <li>Prometheus Node Exporter README</li> </ul>"},{"location":"kbs/000020118/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020119/","title":"How to enable NGINX support for HTTP headers with underscores","text":"<p>This document (000020119) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020119/#situation","title":"Situation","text":""},{"location":"kbs/000020119/#task","title":"Task","text":"<p>This article details how to enable HTTP headers with underscores on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020119/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced.</li> <li>For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"kbs/000020119/#resolution","title":"Resolution","text":""},{"location":"kbs/000020119/#configuration-for-rke-provisioned-clusters","title":"Configuration for RKE provisioned clusters","text":"<ul> <li>Edit the cluster configuration YAML file to include the <code>enable-underscores-in-headers: true</code> option for the ingress, as follows:</li> </ul> <pre><code>ingress:\nprovider: nginx\noptions:\nenable-underscores-in-headers: true\n</code></pre> <ul> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ul> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ul> <li>Recycle the nginx pods in-order to pick up new argument:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <ul> <li>Verify the new configuration:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020119/#configuration-for-rancher-provisioned-clusters","title":"Configuration for Rancher provisioned clusters","text":"<ul> <li>Login into the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the <code>enable-underscores-in-headers</code> option for the ingress, as follows:</li> </ul> <pre><code>ingress:\nprovider: nginx\noptions:\nenable-underscores-in-headers: true\n</code></pre> <ul> <li>Click \"Save\" at the bottom of the page.</li> <li>Wait for cluster to finish upgrading.</li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <p>- Run the following inside the kubectl CLI to verify the new argument:</p> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020119/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020120/","title":"How to enable antiAffinity for Rancher v2.x server pods","text":"<p>This document (000020120) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020120/#situation","title":"Situation","text":""},{"location":"kbs/000020120/#task","title":"Task","text":"<p>By default the Rancher server pods are deployed without podAntiAffinity rules. As a result of this multiple Rancher pods may be scheduled onto a single node, potentially leading to temporary service disruption if the node is unavailable or gets rebooted.</p>"},{"location":"kbs/000020120/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> <li>kubectl access to the cluster</li> <li>Rancher Kubernetes Engine (RKE) to be installed, with access to the cluster.yml and correspoding cluster.rkestate file see the RKE documentation for more information</li> <li>helm v3</li> </ul>"},{"location":"kbs/000020120/#steps","title":"Steps","text":"<p>You need to add the option <code>--set-string antiAffinity=required</code> to your Rancher install. Details on how to add this option to both new installations of Rancher, as well as existing deployment are provided below.</p>"},{"location":"kbs/000020120/#new-rancher-installation","title":"New Rancher installation","text":"<p>For new installations of Rancher, add the antiAffinity option to the <code>helm install</code> command, per the following example:</p> <pre><code>helm install rancher rancher-latest/rancher \\\n--namespace cattle-system \\\n--set hostname=mmattox-example.support.rancher.space \\\n--version 2.3.6 \\\n--set-string antiAffinity=required\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade.</p>"},{"location":"kbs/000020120/#update-existing-rancher-deployments","title":"Update existing Rancher deployments","text":"<p>To add the antiAffinity option to an existing deployment of Rancher, follow the Rancher upgrade documentation, using the <code>--version</code> flag to pin to the running Rancher version, preventing a version upgrade.</p> <ol> <li>Run <code>helm get values rancher</code> to get the current Rancher helm chart values, which will be used to generate the <code>helm upgrade</code> command with matching values.</li> <li>Generate and run the <code>helm upgrade</code> command with the chart values, including the pinned version and antiAffinity option, per the following example:</li> </ol> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n--namespace cattle-system \\\n--set hostname=mmattox-example.support.rancher.space \\\n--version 2.3.6 \\\n--set-string antiAffinity=required\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade. NOTE: We recommend saving this command for future Rancher upgrades to save time.</p>"},{"location":"kbs/000020120/#verification","title":"Verification","text":"<p>Run the command <code>kubectl get deployment -n cattle-system rancher -o yaml</code> and verify the following <code>podAntiAffinity</code> spec has been added:</p> <pre><code>[...]\nspec:\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- rancher\ntopologyKey: kubernetes.io/hostname\n[...]\n</code></pre>"},{"location":"kbs/000020120/#rollback","title":"Rollback","text":"<p>To remove the antiAffnitiy configuration you should remove the <code>--set-string antiAffinity=required</code> option from the <code>helm upgrade</code> command and re-run this, per the following example:</p> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n--namespace cattle-system \\\n--set hostname=mmattox-example.support.rancher.space \\\n--version 2.3.6\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade.</p>"},{"location":"kbs/000020120/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020121/","title":"How to create a cluster in Rancher v2.x using the Rancher CLI or v3 API","text":"<p>This document (000020121) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020121/#situation","title":"Situation","text":""},{"location":"kbs/000020121/#task","title":"Task","text":"<p>The process for creating Kubernetes clusters via the Rancher v2.x UI is documented in \"Setting up Kubernetes Clusters in Rancher\".</p> <p>This article details the process for creating Kubernetes clusters in Rancher v2.x via the Rancher CLI or v3 API interfaces.</p>"},{"location":"kbs/000020121/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>The Rancher CLI installed for CLI cluster creation (this can be downloaded from the Rancher UI, via the <code>Download CLI</code> link in the lower-right corner)</li> <li>curl installed to make Rancher v3 API requests for API cluster creation</li> <li>A Rancher API Key for a user with cluster creation permissions</li> <li>A Rancher Kubernetes Engine (RKE) cluster config file in YAML or JSON format (optional)</li> </ul>"},{"location":"kbs/000020121/#steps","title":"Steps","text":"<p>The cluster creation process is detailed below for both the Rancher CLI and v3 API.</p>"},{"location":"kbs/000020121/#cluster-creation-via-the-rancher-cli","title":"Cluster creation via the Rancher CLI","text":"<ol> <li>Log in to your Rancher Server:</li> </ol> <pre><code>rancher login &lt;server_url&gt; --token &lt;token&gt;\n</code></pre> <ol> <li>Create the cluster:</li> </ol> <p>To create a cluster with the default cluster configuration:</p> <pre><code>rancher cluster create &lt;new_cluster_name&gt;\n</code></pre> <p>If you are passing in an RKE cluster config file, do so as follows:</p> <pre><code>rancher cluster create --rke-config &lt;rke_config_file&gt; &lt;new_cluster_name&gt;\n</code></pre>"},{"location":"kbs/000020121/#cluster-creation-via-the-rancher-v3-api","title":"Cluster creation via the Rancher v3 API","text":"<ol> <li>Create a Rancher API Key, and save the access key and secret key as environment variables ( <code>export CATTLE_ACCESS_KEY=&lt;access_key&gt; &amp;&amp; export CATTLE_SECRET_KEY=&lt;secret_key&gt;</code>). Alternatively you can pass these directly into the curl request in place of the <code>${CATTLE_ACCESS_KEY}</code> and <code>${CATTLE_SECRET_KEY}</code> variables in the examples below.</li> <li>Send a POST request to the <code>/v3/clusters</code> API endpoint of your Rancher server: To create a cluster with the default cluster configuration:</li> </ol> <pre><code>curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n-X POST \\\n-H 'Accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d '{\"name\":\"test-cluster\"}' \\\n'https://&lt;rancher_server&gt;/v3/clusters'\n</code></pre> <p>If you are passing in an RKE cluster config file, do so as follows:</p> <pre><code>curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n-X POST \\\n-H 'Accept: application/json' \\\n-H 'Content-Type: application/json' \\\n-d @&lt;rke_config_file&gt; \\\n'https://&lt;rancher_server&gt;/v3/clusters'\n</code></pre>"},{"location":"kbs/000020121/#additional-reading","title":"Additional Reading","text":"<ul> <li>The Rancher v2.x API Documentation</li> <li>The Rancher v2.x API Specification</li> </ul>"},{"location":"kbs/000020121/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020122/","title":"How to edit the upstream nameservers used by CoreDNS or kube-dns, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020122) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020122/#situation","title":"Situation","text":""},{"location":"kbs/000020122/#task","title":"Task","text":"<p>By default, CoreDNS and kube-dns Pods will inherit the nameserver configuration from the node. In certain circumstances it might be desired to override this, and use a specific set of nameservers for external queries.</p> <p>Note: These steps update the nameservers only for Pods that use either the <code>ClusterFirst</code> (default) or <code>ClusterFirstWithHostNet</code> DNS policy. Nameserver configuration for nodes and other Pods will not be affected.</p>"},{"location":"kbs/000020122/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, with the CoreDNS or kube-dns dns addon enabled.</li> </ul> <p>Note: New clusters can also be created using the same steps.</p>"},{"location":"kbs/000020122/#steps","title":"Steps","text":""},{"location":"kbs/000020122/#option-a-update-the-clusteryaml","title":"Option A: Update the cluster.yaml","text":"<p>The cluster configuration YAML provides the <code>upstreamnameservers</code> option, to configure a list of upstream nameservers, per the example below:</p> <ol> <li>Add the <code>upstreamnameservers</code> option, with the list of nameservers, to the cluster configuration YAML.     For RKE provisioned clusters, add this into the cluster.yml file.     For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click <code>Edit as YAML</code>.</li> </ol> <pre><code>dns:\nprovider: coredns\nupstreamnameservers:\n- 1.1.1.1\n- 8.8.8.8\n</code></pre> <ol> <li>Update the cluster with the new configuration.     For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>).     For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol> <p>Note: This option is recommended as it requires minimal change, see the RKE add-ons documentation for more information.</p>"},{"location":"kbs/000020122/#option-b-update-the-kubelet-resolvconf","title":"Option B: Update the kubelet resolv.conf","text":"<p>By default, the kubelet will refer to the <code>/etc/resolv.conf</code> file as the source for nameserver configuration.</p> <p>It is possible to override this by adding an <code>extra_args</code> option to the <code>kubelet</code> service, and this is also accomplished in the cluster configuration YAML.</p> <p>A custom resolv.conf file can then be used by the kubelet instead, per the example below:</p> <ol> <li>On each of the nodes in the cluster create the custom nameserver configuration file:</li> </ol> <pre><code>echo \"nameserver 8.8.8.8\" &gt; /etc/k8s-resolv.conf\n</code></pre> <ol> <li>Add <code>resolv-conf</code>, referencing the custom nameserver configuration file, to the <code>extra_args</code> option for the kubelet service, in the cluster configuration YAML.     For RKE provisioned clusters, add this into the cluster.yml file.     For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click <code>Edit as YAML</code>.</li> </ol> <pre><code>services:\nkubelet:\nextra_args:\nresolv-conf: /host/etc/k8s-resolv.conf\n</code></pre> <ol> <li>Update the cluster with the new configuration.     For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>).     For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol> <p>See the RKE services documentation for more information.</p> <p>Note: kubelet flags are being updated, as such a restart of the kubelet component will occur on each node.</p>"},{"location":"kbs/000020122/#option-c-update-the-node-resolvconf","title":"Option C: Update the node resolv.conf","text":"<p>If the nameserver configuration should be consistent between the OS and Kubernetes Pods, updating the node <code>/etc/resolv.conf</code> file is recommended.</p> <p>This could be because nameservers are changing or that the caching configuration (for example systemd-resolved) is not desired.</p> <p>Changes to a systemd managed resolv.conf can be dependent on the Linux distribution and you should refer to the documentation for the distribution used in the cluster.</p> <p>Note: The kubelet component caches the <code>/etc/resolv.conf</code> file at start time, as such a restart of the kubelet component needs to occur on each node manually, after updating the configuration.</p> <p>This can be accomplished a number of ways:</p> <ul> <li><code>docker restart kubelet</code> on each node</li> <li>A drain and restart of each node</li> <li>Replacing nodes in the cluster with the updated configuration</li> </ul>"},{"location":"kbs/000020122/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020123/","title":"How to change etcd cipher suite","text":"<p>This document (000020123) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020123/#situation","title":"Situation","text":""},{"location":"kbs/000020123/#hardening-etcd-cluster-communication","title":"Hardening ETCD cluster communication","text":""},{"location":"kbs/000020123/#synopsis","title":"Synopsis:","text":"<p>This article will walk Rancher administrators through hardening the cluster communication between etcd nodes. We'll go over configuring etcd to use specific ciphers which enable stronger encryption for securing intra-cluster etcd traffic.</p>"},{"location":"kbs/000020123/#configuring-etcd-rke-and-rancher-ui","title":"Configuring etcd (rke and Rancher UI):","text":"<p>To make the modifications we'll be configuring our rke cluster YAML spec. This setting would be defined, then applied at the command line with the rke CLI, or alternately via the Rancher UI. From within the Rancher UI, navigate to the cluster you're looking to modify, and click edit under the 3 dot menu. From there, you should see a button labeled 'Edit as Yaml'. At the cluster YAML spec view we define the cipher-suites parameter under the etcd service definition. We recommend testing this out in a non-vital cluster before rolling out on important clusters to become familiar with the process.</p> <pre><code>services:\n  etcd:\n    extra_args:\n      cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"\n      election-timeout: \"5000\"\n      heartbeat-interval: \"500\"\n</code></pre>"},{"location":"kbs/000020123/#note","title":"Note:","text":"<p>The cipher suites defined in the example could trade off speed for stronger encryption. Consider the level of ciphers in use and how they could impact the performance of an etcd cluster. Testing should be done to factor the spec of your hosts (cpu, memory, disk, network, etc...) and the typical types of interacting with kubernetes as well as the amount of resources under management within the k8s cluster.</p>"},{"location":"kbs/000020123/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020124/","title":"How to block external connectivity with Calico","text":"<p>This document (000020124) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020124/#situation","title":"Situation","text":""},{"location":"kbs/000020124/#task","title":"Task","text":"<p>In cases where it is desired to control external connectivity from the cluster, such as to deny or allow specific IP addresses or ports from Pods using the CNI network, a <code>GlobalNetworkPolicy</code> object can be used to control the rules applied to all nodes in the cluster.</p> <p>The <code>GlobalNetworkPolicy</code> is provided by the Calico CRD deployed on RKE clusters.</p>"},{"location":"kbs/000020124/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE cluster configured with the Canal or Calico CNI</li> </ul>"},{"location":"kbs/000020124/#steps","title":"Steps","text":"<p>Configure a YAML manifest the desired rules, using the <code>nets</code> and/or <code>ports</code> keys, the Calico documentation provides some more information on each field.</p> <p>In the below example the EC2 metadata is being denied to prevent Pods from accessing the IAM profile credentials of the instance.</p> <pre><code>apiVersion: crd.projectcalico.org/v1\nkind: GlobalNetworkPolicy\nmetadata:\nname: deny-ec2-metadata\nspec:\ntypes:\n- Egress\negress:\n- action: Deny\ndestination:\nnets:\n- 169.254.169.254/32\n- action: Allow\ndestination:\nnets:\n- 0.0.0.0/0\n</code></pre> <p>Deny 80/TCP connectivity external to the cluster</p> <pre><code>apiVersion: crd.projectcalico.org/v1\nkind: GlobalNetworkPolicy\nmetadata:\nname: deny-http\nspec:\ntypes:\n- Egress\negress:\n- action: Deny\nprotocol: TCP\ndestination:\nports:\n- 80\n- action: Allow\ndestination:\nnets:\n- 0.0.0.0/0\n</code></pre> <p>Apply the YAML file created and test connectivity from a Pod running within the cluster on the CNI network.</p> <p>Note: Pods running with <code>hostnetwork: true</code> will not be effected by the <code>GlobalNetworkPolicy</code> as these Pods do not use the CNI network.</p>"},{"location":"kbs/000020124/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020125/","title":"How to Enable Pod Presets","text":"<p>This document (000020125) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020125/#situation","title":"Situation","text":""},{"location":"kbs/000020125/#task","title":"Task","text":"<p>This how-to article outlines how to enable pod presets on your cluster. This is done by enabling the <code>PodPreset</code> admission plugin and the <code>settings.k8s.io/v1alpha1</code> API for the kube-apiserver.</p>"},{"location":"kbs/000020125/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes version 1.10 and above</li> <li>Access to edit the cluster in yaml or the cluster.yaml file you used with RKE.</li> </ul>"},{"location":"kbs/000020125/#resolution","title":"Resolution","text":"<p>Get to the cluster yaml in Rancher by editing the cluster and selecting \"edit as yaml\" or by opening the RKE cluster.yml file. Modify the kube-api section to resemble the following and hit save or running <code>rke up</code>:</p> <pre><code>services:\nkube-api:\nextra_args:\nruntime-config: authorization.k8s.io/v1beta1=true,settings.k8s.io/v1alpha1=true\nenable-admission-plugins: PodPreset,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize\n</code></pre> <p>Notice that <code>settings.k8s.io/v1alpha1/podpreset</code> and <code>PodPreset</code> is added to the runtime-config and admission plugins.</p>"},{"location":"kbs/000020125/#further-reading","title":"Further reading","text":"<p>You can test the ability to use pod presets with this guide.</p> <p>More details can be found in the kubernetes docs on pod presets.</p>"},{"location":"kbs/000020125/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020126/","title":"How to use nginx /dbg","text":"<p>This document (000020126) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020126/#situation","title":"Situation","text":""},{"location":"kbs/000020126/#what-is-the-dbg-command","title":"What is the /dbg command","text":"<p>/dbg is a program included in the ingress-nginx container image that can be used to show information about the nginx environment and the resulting nginx configuration, which can be helpful when debugging ingress issues in Kubernetes.</p>"},{"location":"kbs/000020126/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster that has ingress enabled with ingress-nginx as the ingress controller</li> <li>A cluster with Linux nodes, nginx will not run on Windows</li> <li>kubectl configured</li> </ul>"},{"location":"kbs/000020126/#using-dbg","title":"Using /dbg","text":"<p>This command needs to be run from inside one of the ingress-nginx pods, so first determine the pod to run it in.</p> <pre><code>&gt; kubectl get pods -n ingress-nginx\nNAME                                    READY   STATUS    RESTARTS   AGE\ndefault-http-backend-67cf578fc4-54jlz   1/1     Running   0          5d\nnginx-ingress-controller-56nss          1/1     Running   0          5d\nnginx-ingress-controller-hscfg          1/1     Running   0          4d21h\nnginx-ingress-controller-n4p22          1/1     Running   0          5d\n</code></pre> <pre><code>&gt; export NGINX_POD=nginx-ingress-controller-n4p22\n</code></pre> <p>If you are diagnosing specific connection issues, you can determine which controller is receiving the traffic by looking through the logs of each.</p>"},{"location":"kbs/000020126/#viewing-ingress-controller-status","title":"Viewing ingress-controller status","text":"<p>/dbg general will show the count of running controllers.</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg  general\n{\n  \"controllerPodsCount\": 3\n}\n</code></pre>"},{"location":"kbs/000020126/#viewing-backend-configuration","title":"Viewing backend configuration","text":"<p>/dbg backends list will list the discovered backends:</p> <p>```</p> <p>kubectl exec -n ingress-nginx $NGINX_POD /dbg backends list cattle-system-rancher-80 upstream-default-backend ```</p> <p>/dbg backends get will show the configuration for the named backend:</p> <pre><code> &gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg backends get cattle-system-rancher-80\n</code></pre>"},{"location":"kbs/000020126/#viewing-ingress-certificate-data","title":"Viewing ingress certificate data","text":"<p>/dbg certs will dump the x509 cert and key for a certificate that nginx has discovered from k8s secrets for the given hostname:</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg certs get &lt;fqdn&gt;\n</code></pre>"},{"location":"kbs/000020126/#viewing-dynamically-generated-nginx-configuration","title":"Viewing dynamically generated nginx configuration","text":"<p>/dbg conf will dump the dynamically generated nginx configuration. To view the configuration for a specific ingress hostname, you could run /dbg conf and then grep for the server_name:</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg conf | grep \"server_name example.com\" -B2 -A20\n</code></pre>"},{"location":"kbs/000020126/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020127/","title":"Where is SUSE Rancher Hosted hosted?","text":"<p>This document (000020127) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020127/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is hosted in the Cloud. You can have your choice of regions available in North America, EMEA, and APAC geographies.</p>"},{"location":"kbs/000020127/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020128/","title":"How is uptime measured for my SUSE Rancher Hosted environment?","text":"<p>This document (000020128) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020128/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted currently uses Pingdom for measuring uptime. Your Hosted Rancher endpoint is tested every one minute and is considered down if a response is not returned within five seconds. Pingdom tests endpoints from over 100 locations across the world. Uptime for the month is calculated by dividing the number of uptime minutes by the total number of minutes in the month. For example, in the month of May, there are 44,640 minutes (60 X 24 X 31). If there were 5 minutes of downtime and 44,635 minutes of uptime, the uptime measurement would be 44,635 / 44,640 = 99.989%. Please also refer to your Service Agreement for the legal definitions for uptime.</p>"},{"location":"kbs/000020128/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020129/","title":"Does SUSE Rancher Hosted offer an uptime SLA?","text":"<p>This document (000020129) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020129/#resolution","title":"Resolution","text":"<p>Yes, Hosted Rancher offers a 99.9% uptime Service Level Agreement (SLA). For the details on our SLA, check your master service agreement or contact your Account Executive.</p>"},{"location":"kbs/000020129/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020130/","title":"Can I have more than one SUSE Rancher Hosted environment?","text":"<p>This document (000020130) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020130/#resolution","title":"Resolution","text":"<p>Yes, you can have multiple SUSE Rancher Hosted environments. Check with your Account Executive for pricing details. There are several use cases where this is preferred, such as having separate environments for development, quality assurance, and production. You will also want multiple SUSE Rancher Hosted environments if you need to manage Kubernetes clusters in separate geographies, such as North America, Europe, and Asia.</p>"},{"location":"kbs/000020130/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020131/","title":"Does SUSE Rancher Hosted offer a support SLA?","text":"<p>This document (000020131) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020131/#resolution","title":"Resolution","text":"<p>Yes, SUSE Rancher Hosted offers the same support Service Level Agreement (SLA) to both SUSE Rancher Hosted and customers who manage their own Rancher server instance. Support cases can be opened on the Support Portal. Details of the support offering can be found on the SUSE support page.</p>"},{"location":"kbs/000020131/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020132/","title":"How often are backups taken and retained on Rancher Hosted Prime?","text":"<p>This document (000020132) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020132/#resolution","title":"Resolution","text":"<p>Backups on Rancher Hosted Prime are taken hourly and retained for up to 1 year.</p>"},{"location":"kbs/000020132/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020133/","title":"How to configure the CoreDNS Autoscaler in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020133) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020133/#situation","title":"Situation","text":""},{"location":"kbs/000020133/#task","title":"Task","text":"<p>During the life of a cluster, you may need to adjust the scaling parameters for the kube-dns or CoreDNS autoscaler. The autoscaler runs as an independant Deployment in the cluster, using the cluster-proportional-autoscaler container to scale up and down the related kube-dns or CoreDNS Deployment, using a linear or ladder pattern.</p>"},{"location":"kbs/000020133/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> <li>The cluster is configured with either the kube-dns or coredns provider (enabled by default)</li> </ul> <p>Note When running <code>rke up</code> commands, ensure the <code>.rkestate</code> file for the cluster is present in the working directory as per the documentation here.</p>"},{"location":"kbs/000020133/#steps","title":"Steps","text":"<p>Four approaches are provided, depending on the Rancher or RKE version in use.</p> <p>Note: When making the changes, the <code>coredns-autoscaler</code> or kube-dns-autoscaler` pod will be restarted with updated command arguments, this will not cause any disruption to DNS resolution.</p> <p>Note: Check the logs of the <code>kube-dns-autoscaler</code>, or <code>coredns-autoscaler</code> pod after making changes to confirm they have taken effect.</p>"},{"location":"kbs/000020133/#a-rancher-provisioned-cluster-managed-by-rancher-versions-after-v24x","title":"A Rancher provisioned cluster managed by Rancher versions after v2.4.x","text":"<ol> <li>Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'.</li> <li>Click 'Edit as YAML'.</li> <li>Locate or add the <code>dns</code> field, using the below as an example to add the desired parameters below:</li> </ol> <pre><code>rancher_kubernetes_engine_config:\n[...]\ndns:\nlinear_autoscaler_params:\ncores_per_replica: 128\nmax: 0\nmin: 1\nnodes_per_replica: 4\nprevent_single_point_failure: true\n</code></pre> <ol> <li>Click 'Save' to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020133/#a-rancher-provisioned-cluster-managed-by-rancher-versions-before-v24x","title":"A Rancher provisioned cluster managed by Rancher versions before v2.4.x","text":"<ol> <li>Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'.</li> <li>Click 'Edit as YAML'.</li> <li>Locate or add the <code>addons</code> field, using the below as an example to add the desired parameters:</li> </ol> <pre><code>rancher_kubernetes_engine_config:\n[...]\naddons: |-\napiVersion: v1\ndata:\nlinear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}'\nkind: ConfigMap\nmetadata:\nname: coredns-autoscaler\nnamespace: kube-system\n</code></pre> <ol> <li>Click 'Save' to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020133/#an-rke-provisioned-cluster-managed-by-rke-versions-after-v110","title":"An RKE provisioned cluster managed by RKE versions after v1.1.0","text":"<ol> <li>Edit the cluster configuration YAML file to configure the <code>dns</code> field, using the below as an example to add the desired parameters below:</li> </ol> <pre><code>dns:\nlinear_autoscaler_params:\ncores_per_replica: 128\nmax: 0\nmin: 1\nnodes_per_replica: 4\nprevent_single_point_failure: true\n</code></pre> <ol> <li>Invoke <code>rke up --config &lt;cluster configuration YAML file&gt;</code> to update the cluster.</li> </ol>"},{"location":"kbs/000020133/#an-rke-provisioned-cluster-managed-by-rke-versions-before-v110","title":"An RKE provisioned cluster managed by RKE versions before v1.1.0","text":"<ol> <li>Edit the cluster configuration YAML file to configure the ConfigMap addon, using the below as an example to add the desired parameters below:</li> </ol> <pre><code>addons: |-\napiVersion: v1\ndata:\nlinear: '{\"coresPerReplica\":128,\"min\":1,\"nodesPerReplica\":4,\"preventSinglePointFailure\":true}'\nkind: ConfigMap\nmetadata:\nname: coredns-autoscaler\nnamespace: kube-system\n</code></pre> <ol> <li>Invoke <code>rke up --config &lt;cluster configuration YAML file&gt;</code> to update the cluster.</li> </ol>"},{"location":"kbs/000020133/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020134/","title":"Do SUSE employees have a login account for my SUSE Rancher Hosted environment?","text":"<p>This document (000020134) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020134/#resolution","title":"Resolution","text":"<p>When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment. At your discretion, you can create an account for SUSE employees, with the permissions you feel comfortable with, to allow SUSE staff to perform troubleshooting activities. SUSE can also do a Teams or Zoom screen-sharing session to help troubleshoot any issues you have with SUSE Rancher Hosted.</p>"},{"location":"kbs/000020134/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020135/","title":"Do SUSE employees have the credentials to my \u201cadmin\u201d account?","text":"<p>This document (000020135) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020135/#resolution","title":"Resolution","text":"<p>When your SUSE Rancher Hosted environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your SUSE Rancher Hosted environment.</p>"},{"location":"kbs/000020135/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020136/","title":"What type of cluster is SUSE Rancher Hosted running on?","text":"<p>This document (000020136) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020136/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted runs on top of SUSE Rancher's k3s which is a fully compliant, lightweight Kubernetes distribution. The cluster consists of two k3s server nodes with a MySQL cluster backend for the datastore.</p>"},{"location":"kbs/000020136/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020137/","title":"Why does `kubectl get` show a different API group for a resource to the group originally applied in the resource specification?","text":"<p>This document (000020137) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020137/#situation","title":"Situation","text":""},{"location":"kbs/000020137/#question","title":"Question","text":"<p>The Kubernetes API group and version returned for a resource from the Kubernetes API (using for example the <code>kubectl</code> CLI) may show as different to the original group and version defined in the resource specification via the <code>apiVersion</code>. For example, when creating a Deployment resource in the v1 version of the apps API group, the output of <code>kubectl get deployment -o yaml</code> may show the Deployment resource in the v1beta1 version of the extensions API group, per the below:</p> <p>Original Deployment YAML:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\n[...]\n</code></pre> <p>Output of <code>kubectl get deployment -o yaml</code> for this Deployment resource post-creation:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\n[...]\n</code></pre> <p>This article explains the cause of this behaviour and how to ensure resources are returned by the API under a specific API group and version.</p>"},{"location":"kbs/000020137/#answer","title":"Answer","text":"<p>A Kubernetes resource type, such as Deployment, can exist within multiple API groups. Where this is the case, and no API group and version is specified in the command, <code>kubectl</code> will use the first group listed in the discovery docs published by the Kubernetes API server that you are querying. In the instance of the above example, the Deployment resource exists under boths the <code>apps/v1</code> and <code>extensions/v1beta1</code> API groups, but for backwards compatability the API server lists this first under the <code>extensions/v1beta1</code> group.</p> <p>To ensure that the resource retrieved is in a particular API group, you should fully qualify the resource type in the <code>kubectl</code> command, i.e. to query Deployment resources in the apps API group run <code>kubectl get deployments.apps -o yaml</code>. Additionally you can provide an explicit version of the API group, i.e. to query Deployment resources in the v1 version of the apps API group run <code>kubectl get deployments.v1.apps -o yaml</code>.</p>"},{"location":"kbs/000020137/#further-reading","title":"Further Reading","text":"<p>You can find a good discussion on this behaviour in the Kubernetes GitHub Issue #58131.</p> <p>The Kubernetes developer documentation on API resource versioning can be found here.</p>"},{"location":"kbs/000020137/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020138/","title":"How can I audit or examine RBAC Roles for different accounts within a Kubernetes cluster?","text":"<p>This document (000020138) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020138/#situation","title":"Situation","text":""},{"location":"kbs/000020138/#question","title":"Question","text":"<p>Access to different resources within Kubernetes is handled by role-based access control (RBAC).</p> <p>These resources are referenced by the resource name and API group, for example pods within the core/v1 Kubernetes API group or clusters within the management.cattle.io/v3 API group.</p> <p>A role can be applied (or bound) to different subjects, like a user, group or service account via role bindings, to grant varying degress of access to these resource types at a cluster or namespace level. The access a role grants on a particular resource type is defined by verbs, e.g. get, create, list, watch, delete, and patch etc.</p> <p>This article details methods by which you can audit or examine role-based access control (RBAC) roles for different accounts within a Kubernetes cluster.</p>"},{"location":"kbs/000020138/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster</li> <li>kubectl access to the cluster</li> </ul>"},{"location":"kbs/000020138/#answer","title":"Answer","text":"<p>To audit a specific account, the kubectl command can use the can-i option with the impersonation API to examine what verbs a user has access to, given a specific namespace.</p>"},{"location":"kbs/000020138/#basic-usage","title":"Basic Usage","text":"<p>Basic usage of the kubectl can-i option takes the following form:</p> <pre><code>kubectl auth can-i &lt;verb&gt; &lt;resource&gt; --as account --namespace=&lt;namespace&gt;\n</code></pre>"},{"location":"kbs/000020138/#can-my-user-perform-all-verbs-on-all-resources-am-i-an-admin","title":"Can my user perform all verbs on all resources? Am I an admin?","text":"<pre><code>kuboectl auth can-i \"*\" \"*\"\n</code></pre>"},{"location":"kbs/000020138/#can-the-helm-serviceaccount-delete-pods-in-the-current-namespace-or-cluster-wide","title":"Can the helm serviceaccount delete pods in the current namespace or cluster-wide?","text":"<pre><code>kubectl auth can-i delete pods --as helm\n</code></pre>"},{"location":"kbs/000020138/#is-user1234-an-admin-in-the-testing-namespace-can-they-perform-all-verbs-on-all-resources","title":"Is user1234 an admin in the \"testing\" namespace? Can they perform all verbs on all resources?","text":"<pre><code>kubectl auth can-i \"*\" \"*\" --namespace=testing --as user1234\n</code></pre>"},{"location":"kbs/000020138/#list-option-gives-insight-into-permissions-for-a-user-or-account","title":"List option gives insight into permissions for a user or account","text":"<pre><code>kubectl auth can-i --list --namespace=testing --as user1234\n</code></pre>"},{"location":"kbs/000020138/#additional-tools-for-querying-rbac","title":"Additional tools for querying RBAC","text":"<p>Other open-source third-party tools exist for auditing RBAC, many of which use the Krew plugin framework:</p> <ul> <li>access-matrix - output a CLI matrix of what users or roles have permissions</li> <li>rbac-lookup - perform lookups given subject queries</li> <li>who-can - see \"who-can\" perform a certain verb on a resource, like an opposite view of \"can-i\"</li> </ul> <p>Third-party tools also exist for creating visualizations of the RBAC configuration:</p> <ul> <li>RBack - parse the output from the kubectl commands as json, import into visualization in different formats</li> <li>RBAC-view - visualizing RBAC relationships via a dashboard interface</li> </ul>"},{"location":"kbs/000020138/#further-reading","title":"Further Reading","text":"<ul> <li>Offical Kubernetes RBAC documentation</li> <li>CNCF RBAC Blog post</li> <li>NCCGROUP Examples</li> <li>Krew Plugin Framework</li> <li>RBAC-View</li> <li>RBack</li> <li>who-can</li> <li>rakksess, acess-matrix plugin</li> <li>rbac-lookup</li> </ul>"},{"location":"kbs/000020138/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020139/","title":"What is the kube_config_cluster.yml file that is created after provisioning a cluster with the Rancher Kubernetes Engine (RKE) CLI?","text":"<p>This document (000020139) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020139/#situation","title":"Situation","text":""},{"location":"kbs/000020139/#question","title":"Question","text":"<p>The Rancher Kubernetes Engine (RKE) documentation references a file <code>kube_config_cluster.yml</code> that is generated after running <code>rke up</code>, this article explains what this file is and how to use it.</p>"},{"location":"kbs/000020139/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI provisioned Kubernetes cluster</li> <li>kubectl installed</li> </ul>"},{"location":"kbs/000020139/#answer","title":"Answer","text":"<p>When you provision a Kubernetes cluster using RKE, a kubeconfig file is automatically generated for your cluster.</p> <p>This file is created and saved as <code>kube_config_&lt;cluster&gt;.yml</code>, where <code>&lt;cluster&gt;</code> is the filename of your cluster configuration YAML file. This kubeconfig defines the connection and authentication details to interact with your cluster, using tools such as <code>kubectl</code>.</p> <p>By default, kubectl checks <code>~/.kube/config</code> for a kubeconfig file, but you can specify a different kubeconfig file using the --kubeconfig flag. For example:</p> <pre><code>kubectl --kubeconfig /custom/path/rke/kube_config_cluster.yml get pods\n</code></pre> <p>Or you can export the config path into the KUBECONFIG environment variable, removing the requirement to specify the --kubeconfig flag each time you run kubectl:</p> <pre><code>export KUBECONFIG=\"/custom/path/rke/kube_config_cluster.yml\"\n</code></pre>"},{"location":"kbs/000020139/#further-reading","title":"Further Reading","text":"<ul> <li> <p>RKE Documentation on the kubeconfig</p> </li> <li> <p>Kubernetes Documentation on kubeconfig files</p> </li> </ul>"},{"location":"kbs/000020139/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020140/","title":"What permissions are required for the API token when configuring the Rancher2 Terraform Provider?","text":"<p>This document (000020140) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020140/#situation","title":"Situation","text":""},{"location":"kbs/000020140/#question","title":"Question","text":"<p>When configuring the Rancher2 Terraform Provider, what permissions are required for the API token configured to authenticate with Rancher (as in the below example)?</p> <pre><code>provider \"rancher2\" {\napi_url    = \"https://rancher.my-domain.com\"\naccess_key = \"${var.rancher2_access_key}\"\nsecret_key = \"${var.rancher2_secret_key}\"\n}\n</code></pre>"},{"location":"kbs/000020140/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>The Rancher2 Terraform Provider</li> </ul>"},{"location":"kbs/000020140/#answer","title":"Answer","text":"<p>The user account for which you generate the API token, to configure the Terraform provider, will need permissions granted on any resources that you intend to configure and manage via Terraform.</p>"},{"location":"kbs/000020140/#further-reading","title":"Further Reading","text":"<p>Rancher2 Terraform Provider Documentation</p>"},{"location":"kbs/000020140/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020141/","title":"How to configure container log rotation for the Docker daemon","text":"<p>This document (000020141) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020141/#situation","title":"Situation","text":""},{"location":"kbs/000020141/#task","title":"Task","text":"<p>As the default setting on Docker is to log using the json-file log driver, without a container log limit, this can lead to disk-fill events on nodes. This article provides steps to configure any nodes running Docker to have a limited container log size and rotate out older container logs.</p>"},{"location":"kbs/000020141/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Node(s) running Docker, using the json-file log driver</li> <li>Permission to edit the <code>/etc/docker/daemon.json</code> and to restart the Docker daemon</li> </ul>"},{"location":"kbs/000020141/#warning","title":"Warning","text":"<ul> <li>You must restart the Docker daemon for the changes to take effect for newly created containers</li> <li>N.B. As the container logging configuration for pre-existing container is immutable, existing containers do not use the new logging configuration and would need to be redeployed to take on this new configuration.</li> </ul>"},{"location":"kbs/000020141/#resolution","title":"Resolution","text":"<ol> <li>Edit the Docker daemon configuration file:</li> </ol> <pre><code>$ vim /etc/docker/daemon.json\n</code></pre> <ol> <li>Add the following lines to the file, to configure a maximum container log file size of 10MB and maintain only 10 of these before deleting the oldest:</li> </ol> <pre><code>{\n\"log-driver\": \"json-file\",\n\"log-opts\": {\n\"max-size\": \"10m\",\n\"max-file\": \"10\"\n}\n}\n</code></pre> <ol> <li>Restart the docker daemon to apply the settings to new containers (see Warnings above):</li> </ol> <pre><code>$ systemctl restart docker\n</code></pre>"},{"location":"kbs/000020141/#tips","title":"Tips","text":"<p>You could include this Docker daemon container log rotation configuration in your build/connfiguration management systems, to ensure this is automatically applied to nodes on provisioning, removing any requirement for manual configuration.</p>"},{"location":"kbs/000020141/#further-reading","title":"Further reading","text":"<p>Docker JSON file log driver documentation</p>"},{"location":"kbs/000020141/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020142/","title":"How to configure an internal Elastic Load Balancer (ELB) or Network Load Balancer (NLB) with an Istio Ingress Gateway in Rancher v2.3+","text":"<p>This document (000020142) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020142/#situation","title":"Situation","text":""},{"location":"kbs/000020142/#task","title":"Task","text":"<p>When configuring an Istio Ingress Gateway, a <code>LoadBalancer</code> type service is commonly configured to provide external access to the cluster.</p> <p>By default Kubernetes will provision an internet-facing Classic Load Balancer (CLB). The below steps provide guidance on the annotations needed to configure an internal CLB or Network Load Balancer (NLB) using private subnets.</p>"},{"location":"kbs/000020142/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.3+ managed Kubernetes cluster, runnning in AWS, with the AWS cloud provider configured</li> <li>Istio enabled in the cluster</li> <li>Tagging configured for the VPC and Subnets that will be used for the ELB or NLB</li> </ul> <p>Note: When using Load Balancers with the AWS cloud provider, it is important tag the private and public subnets in the VPC so that kube-controller-manager can correctly discover the specific subnets intended for use.</p> <p>For example the <code>kubernetes.io/role/internal-elb</code> and <code>kubernetes.io/role/elb</code> keys configured respectively, with the value of <code>1</code>.</p>"},{"location":"kbs/000020142/#steps","title":"Steps","text":""},{"location":"kbs/000020142/#enable-the-istio-ingress-gateway","title":"Enable the Istio Ingress Gateway","text":"<p>If the not already enabled, enable the Istio Ingress Gateway. In the drop down list for 'Service Type of Ingress Gateway', select <code>LoadBalancer</code>.</p>"},{"location":"kbs/000020142/#use-an-internal-load-balancer","title":"Use an internal Load Balancer","text":"<p>When editing the Istio Ingress Gateway, click the drop down for Custom Answers.</p> <p>Paste the below in the Variable field, this will automatically populate the value:</p> <pre><code>gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-internal\" = \"true\"\n</code></pre>"},{"location":"kbs/000020142/#use-an-nlb","title":"Use an NLB","text":"<p>To use an NLB, click 'Add Answer' and paste the below in the Variable field:</p> <pre><code>gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-type\" = nlb\n</code></pre> <p>Note: An NLB can be used as an internet-facing loadbancer by using only the above annotation, without adding the aws-load-balancer-internal annotation.</p>"},{"location":"kbs/000020142/#references","title":"References","text":"<p>Istio install options documentation</p> <p>Kubernetes load balancer documentation</p>"},{"location":"kbs/000020142/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020143/","title":"How to set server-tokens to false, to disable the the NGINX header in ingress-nginx responses, within a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster","text":"<p>This document (000020143) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020143/#situation","title":"Situation","text":""},{"location":"kbs/000020143/#task","title":"Task","text":"<p>The ingress-nginx server-tokens option controls display of the NGINX server header, including version information, in the response to ingress requests. By default this header is enabled; however, due to security concerns in exposing version information, a user might want to disable this on the nginx-ingress-controllers of their Kubernetes cluster(s). This article details how to disable the header, via the server-tokens option, in Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters.</p>"},{"location":"kbs/000020143/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020143/#resolution","title":"Resolution","text":""},{"location":"kbs/000020143/#rke-provisioned-clusters","title":"RKE provisioned clusters","text":"<ol> <li>Add the <code>server-tokens: \"false\"</code> option for nginx into the cluster configuration YAML file as follows:</li> </ol> <pre><code>ingress:\nprovider: nginx\noptions:\nserver-tokens: \"false\"\n</code></pre> <p>Example:</p> <pre><code>nodes:\n- address: x.x.x.x\ninternal_address: x.x.x.x\nuser: ubuntu\nrole: [controlplane,worker,etcd]\ningress:\nprovider: nginx\noptions:\nserver-tokens: \"false\"\nservices:\netcd:\nsnapshot: true\ncreation: 6h\nretention: 24h\n</code></pre> <ol> <li>Execute <code>rke up</code> to update the cluster with the new configuration. N.B. Ensure the <code>.rkestate</code> file for the cluster is present in the working directory when invoking <code>rke up</code> per the documentation here:</li> </ol> <pre><code>rke up --config &lt;cluster configuration YAML file&gt;\n</code></pre>"},{"location":"kbs/000020143/#rancher-v2x-provisioned-clusters","title":"Rancher v2.x provisioned clusters","text":"<ol> <li>Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'.</li> <li>Click 'Edit as YAML'.</li> <li>Add the <code>server-tokens: \"false\"</code> option for nginx into the cluster configuration YAML file as follows:</li> </ol> <pre><code>rancher_kubernetes_engine_config:\n[...]\ningress:\nprovider: nginx\noptions:\nserver-tokens: \"false\"\n</code></pre> <ol> <li>Click 'Save' to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020143/#further-reading","title":"Further reading","text":"<p>ingress-nginx documentation on the server-tokens options</p>"},{"location":"kbs/000020143/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020144/","title":"How to enable debug level logging for the Rancher Cluster/Project Alerting Alertmanager instance, in a Rancher v2.x managed cluster?","text":"<p>This document (000020144) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020144/#situation","title":"Situation","text":""},{"location":"kbs/000020144/#task","title":"Task","text":"<p>This article details how to enable debug level logging on the Alertmanager instance in a Rancher v2.x managed Kubernetes cluster, which may assist when troubleshooting cluster or project alerting.</p>"},{"location":"kbs/000020144/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster</li> <li>Cluster or project alerting configured</li> </ul>"},{"location":"kbs/000020144/#resolution","title":"Resolution","text":"<ol> <li>Within the Rancher UI navigate to the System Project of the relevant cluster and click on the Apps view.</li> <li>Click 'Upgrade' on the cluster-alerting app.</li> <li>In the Answers section click 'Add Answer' and add the variable <code>alertmanager.logLevel</code> with a value of <code>debug</code>.</li> <li>Click upgrade to save the change and update the Alertmanager instance with the debug log level.</li> <li>Navigate to the cattle-prometheus namespace within the System Project for the cluster, and view the logs of the alertmanager-cluster-alerting-0 Pod running for the alertmanager-cluster-alerting StatefulSet. You should see <code>level=debug</code> log messages, such as in the following example, confirming debug level logging has been successfully configured:</li> </ol> <p><code>plaintext     level=debug  ts=2019-07-09T15:03:37.511451301Z caller=dispatch.go:104  component=dispatcher msg=\"Received alert\" alert=[433a194][active]     level=debug  ts=2019-07-09T15:03:38.511774835Z caller=dispatch.go:430  component=dispatcher  aggrGroup=\"{}/{group_id=\\\"c-5h85q:event-alert\\\"}/{rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\"}:{event_message=\\\"Scaled  up replica set mynginx2-7994cd84ff to 1\\\",  resource_kind=\\\"Deployment\\\",  rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\",  target_name=\\\"mynginx2\\\", target_namespace=\\\"default\\\"}\" msg=flushing  alerts=[[433a194][active]]</code></p>"},{"location":"kbs/000020144/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020145/","title":"How to generate a Longhorn Support Bundle","text":"<p>This document (000020145) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020145/#situation","title":"Situation","text":""},{"location":"kbs/000020145/#task","title":"Task","text":"<p>When troubleshooting an issue with Longhorn, Rancher Support may request a Longhorn Support Bundle, which can be generated via the Longhorn UI, and contains system information and logs.</p>"},{"location":"kbs/000020145/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster with Longhorn deployed</li> </ul>"},{"location":"kbs/000020145/#steps","title":"Steps","text":""},{"location":"kbs/000020145/#generate-the-support-bundle","title":"Generate the Support Bundle","text":"<ol> <li>Log in into the Rancher UI.</li> <li>Select the cluster with Longhorn depoyed.</li> <li>Select the Project where Longhorn is deployed (typically under the System project).</li> <li>Click on \"Apps\" button.</li> <li> <p>Find the Longhorn system app and click on the index.html button</p> </li> <li> <p>Click on the \"Generate Support Bundle\" in the bottom left of the screen</p> </li> <li> <p>Type in a description and click generate (issue url is optional)</p> </li> </ol>"},{"location":"kbs/000020145/#upload-the-support-bundle","title":"Upload the Support Bundle","text":"<p>Generally Longhorn Support Bundles files are small in size; however, if the pack is too large to upload directly to the ticket, please request a temporary upload location.</p>"},{"location":"kbs/000020145/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020146/","title":"How to add additional scrape configs to a Rancher cluster or project monitoring prometheus","text":"<p>This document (000020146) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020146/#situation","title":"Situation","text":""},{"location":"kbs/000020146/#task","title":"Task","text":"<p>The Rancher cluster and project monitoring tools, allow you to monitor cluster components and nodes, as well as workloads and custom metrics from any HTTP or TCP/UDP metrics endpoint that these workloads expose.</p> <p>This article will detail how to manually define additional scrape configs for either the cluster or project monitoring prometheus instance, where you want to scrape other metrics.</p> <p>Whether to define the additional scrape config at the cluster or project level would depend on the desired scope for the metrics and possible alerts. If you wish to scope the metrics scraped, and thus possible alerts configured for these metrics, to a project, you could configure the additional scrape config at the project monitoring level. If you wish to scope the metrics at the cluster level, so only those with cluster admin access could see the metrics or configure alerts, you could configure the additional scrape config at the cluster monitoring level.</p>"},{"location":"kbs/000020146/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.2.x, v2.3.x or v2.4.x managed cluster, with cluster monitoring enabled (and optionally project monitoring enabled, if you wish to configure the additonal scrape config at the project scope).</li> </ul>"},{"location":"kbs/000020146/#resolution","title":"Resolution","text":"<p>For both cluster and project monitoring the additional scrape config(s) are defined in the Answers section of the Monitoring configuration. This can be found as follows:</p> <ul> <li>Cluster Monitoring: As a user with permissions to edit cluster monitoring (global admins and cluster owners by default), navigate to the cluster view and click Tools -&gt; Monitoring from the menu bar. Click 'Show advanced options' at the bottom right.</li> <li>Project Monitoring: As a user with permissions to edit project monitoring (global admins, cluster owners and project owners by default), navigate to the project and click Tools -&gt; Monitoring from the menu bar. Click 'Show advanced options' at the bottom right.</li> </ul> <p>You can add an array of prometheus.additionalScrapeConfigs in the Answers section here.</p> <p>For example to define a scrape job of the following:</p> <pre><code> - job_name: \"prometheus\"\n   static_configs:\n   - targets:\n     - \"localhost:9090\"\n</code></pre> <p>You would add the following two definitions to the Answers section:</p> <p>prometheus.additionalScrapeConfigs[0].job_name = prometheus prometheus.additionalScrapeConfigs[0].static_configs[0].targets[0] = localhost:9090</p> <p>After adding the answers, click 'Save' and you should now be able to view the target and its status within the Prometheus UI under Status -&gt; Targets.</p>"},{"location":"kbs/000020146/#further-reading","title":"Further reading","text":"<p>Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here.</p>"},{"location":"kbs/000020146/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020147/","title":"How to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters","text":"<p>This document (000020147) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020147/#situation","title":"Situation","text":""},{"location":"kbs/000020147/#task","title":"Task","text":"<p>This article details how to enable SSL passthrough on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020147/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Enginer (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced.</li> <li>For Rancher v.2x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"kbs/000020147/#resolution","title":"Resolution","text":""},{"location":"kbs/000020147/#configuration-for-rke-provisioned-clusters","title":"Configuration for RKE provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>enable-ssl-passthrough: true</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\nprovider: nginx\nextra_args:\nenable-ssl-passthrough: true\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Recycle the nginx pods in-order to pick up new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <ol> <li>Verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo \"Good\" || echo \"Bad\"; done\n</code></pre> <ol> <li>Edit the ingress to include the new annotations:</li> </ol> <pre><code>kubectl -n default edit ingress hello-world-lb\n</code></pre> <p>Example:</p> <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\nannotations:\nnginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nname: hello-world-lb\nnamespace: default\n</code></pre>"},{"location":"kbs/000020147/#configuration-for-rancher-provisioned-clusters","title":"Configuration for Rancher provisioned clusters","text":"<ol> <li>Login into the Rancher UI.</li> <li> <p>Go to Global -&gt; Clusters -&gt; &lt;&gt;.</p> </li> <li> <p>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</p> </li> <li>Click \"Edit as YAML\".</li> <li> <p>Enclude the <code>enable-ssl-passthrough: true</code> option for the ingress, as follows:</p> <pre><code>ingress:\nprovider: nginx\nextra_args:\nenable-ssl-passthrough: true\n</code></pre> </li> <li> <p>Click \"Save\" at the bottom of the page.</p> </li> <li> <p>Wait for cluster to finish upgrading.</p> </li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li> <p>Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument:</p> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <p>9. Run the following inside the kubectl CLI to verify the new argument:</p> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"ps aux | grep -v grep | grep enable-ssl-passthrough=true\" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo \"Good\" || echo \"Bad\"; done\n</code></pre> </li> <li> <p>Browse to the ingress in question and click edit.</p> </li> <li> <p>Expand \"Labels &amp; Annotations\".</p> </li> <li>Click \"Add annotation\" and add <code>nginx.ingress.kubernetes.io/ssl-passthrough=true</code> under \"Annotations\".</li> <li>Click \"Save\".</li> </ol>"},{"location":"kbs/000020147/#verification-steps","title":"Verification Steps","text":"<p>Run the following command to verify new certificate:</p> <pre><code>```bash\ncurl --insecure -v https://&lt;&lt;APP URL&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n```\n</code></pre> <p>Example output:</p> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host lab.rancher.tools left intact\n</code></pre> <p>N.B. Some browsers will cache the certificate, as a result you might need to close and re-open the browser in order to get the new certificate. How to clear the SSL state in a browser.</p>"},{"location":"kbs/000020147/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020148/","title":"Istio fails to deploy with restricted PodSecurityPolicy in Rancher v2.3 and v2.4","text":"<p>This document (000020148) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020148/#situation","title":"Situation","text":""},{"location":"kbs/000020148/#issue","title":"Issue","text":"<p>Attempting to enable Istio in a Rancher v2.3 or v2.4 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the istio-galley, istio-pilot, istio-policy, istio-sidecar-injector and istio-telemtry Deployments in a CrashLoopBackOff, with log messages of the formats below:</p> <pre><code>fatal   validation  admission webhook ListenAndServeTLS failed: listen tcp :443: bind: permission denied\n</code></pre> <p>or</p> <pre><code>nginx: [emerg] chown(\"/tmp/nginx\", 101) failed (1: Operation not permitted)\n</code></pre> <p>In addition in namespaces with Istio sidecar auto injection enabled, an error of the following format will show for Pods upon scheduling:</p> <pre><code>Pods \"nginx-7f4c54479d-\" is forbidden: unable to validate against any pod security policy: [spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_ADMIN\": capability may not be added spec.initContainers[0].securityContext.capabilities.add: Invalid value: \"NET_RAW\": capability may not be added]\n</code></pre> <p>This is a result of the system capabilities required by the Istio system components ( <code>CHOWN</code> and <code>NET_BIND_SERVICE</code>), as well as the Istio sidecar containers ( <code>NET_ADMIN</code> and <code>NET_RAW</code>), in the default Istio configuration and which are blocked by the restricted PSP.</p>"},{"location":"kbs/000020148/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.3.x or v2.4.x with a restricted PSP configured as the default and Istio enabled</li> </ul>"},{"location":"kbs/000020148/#resolution","title":"Resolution","text":"<p>The steps to configure Istio in a cluster with restrictive Pod Security Policies enabled can be found in the Rancher documentation \"Enable Istio with Pod Security Policies\".</p>"},{"location":"kbs/000020148/#futher-reading","title":"Futher Reading","text":"<ul> <li> <p>Rancher Documentation on Istio</p> </li> <li> <p>Kubernetes Documentation on PSP Capabilities</p> </li> <li> <p>Istio Requirements Documentation</p> </li> </ul>"},{"location":"kbs/000020148/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020149/","title":"Resolving conntrack table full error messages: 'nf_conntrack: table full, dropping packets'","text":"<p>This document (000020149) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020149/#situation","title":"Situation","text":""},{"location":"kbs/000020149/#issue","title":"Issue","text":"<p>When investigating a connectivity issue, you may experience errors like the below in the system logs:</p> <pre><code>nf_conntrack: table full, dropping packets\n</code></pre> <p>This error indicates the connection tracking table size has been exhausted. This can manifest with different symptoms, such as intermittent or consistent network timeouts.</p> <p>The conntrack table keeps state on open connections that the kernel is translating. This occurs often in a Kubernetes cluster when pods access an external endpoint, or another service within the cluster. These scenarios use NAT and stateful firewall rules which are maintained as entries in the conntrack table.</p>"},{"location":"kbs/000020149/#investigation","title":"Investigation","text":"<p>By default, the table size is calculated based on the memory allocated to the node. This does not fit all workloads demands, for example in a microservice environment typically a higher number of inter-service connections could be expected without consuming a high amount of memory.</p> <p>To output the current max table size:</p> <pre><code>cat /proc/sys/net/netfilter/nf_conntrack_max\n</code></pre> <p>To get a point in time count of the current entries in the table:</p> <pre><code>cat /proc/sys/net/netfilter/nf_conntrack_count\n</code></pre> <p>Note: With the <code>conntrack</code> package installed, you can also use <code>conntrack -C</code></p> <p>If the <code>nf_conntrack_count</code> and <code>nf_conntrack_max</code> are close, it is indicating that the current workload requires a larger table size.</p> <p>If the current number of entries are not approaching the table size, this could indicate that a burst of workload was experienced historically, in a containerized environment this can be common. For example, if the high-traffic Pods may now running on different nodes.</p>"},{"location":"kbs/000020149/#resolution","title":"Resolution","text":"<p>Increasing the conntrack table size is achieved with <code>sysctl</code>.</p> <p>Calculate a higher value, this can be applied to the node immediately with:</p> <pre><code>sysctl -w net.netfilter.nf_conntrack_max=&lt;value&gt;\n</code></pre> <p>To persist through reboot, add the tunable to either <code>/etc/sysctl.conf</code>, or a specific config file in <code>/etc/sysctl.d</code>.</p> <p>For example, if your Linux distribution follows the /etc/sysctl.d/ directory structure:</p> <pre><code>echo \"net.netfilter.nf_conntrack_max=&lt;value&gt;\" &gt; /etc/sysctl.d/10-conntrack-max.conf\nsysctl -p /etc/sysctl.d/10-conntrack-max.conf\n</code></pre> <p>This creates a new config file to set the table size at each boot.</p> <p>Additionally, if you configure nodes with configuration management, UserData, or build custom images etc., you may wish to add this to your usual approach to configure this for future nodes.</p>"},{"location":"kbs/000020149/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020150/","title":"Does SUSE Rancher Hosted provide downstream clusters?","text":"<p>This document (000020150) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020150/#resolution","title":"Resolution","text":"<p>No, currently our SUSE Rancher Hosted service only includes the Rancher multi-cluster management software. Downstream clusters are not provided as part of the service. You will either need to use Rancher to provision Kubernetes clusters on-premise or in a cloud provider account that you own. You can also import existing Kubernetes clusters into SUSE Rancher Hosted.</p>"},{"location":"kbs/000020150/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020151/","title":"How to grant users access to Grafana with minimal permissions","text":"<p>This document (000020151) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020151/#situation","title":"Situation","text":""},{"location":"kbs/000020151/#deprecation-note","title":"\\* Deprecation note *","text":""},{"location":"kbs/000020151/#there-is-now-a-view-monitoring-role-in-monitoring-v2-which-a-user-can-be-granted-on-the-system-project-this-grants-user-monitoring-access-thus-the-article-is-no-more-maintained-please-refer-to-this-for-more-information-on-rbac","title":"There is now a \"View Monitoring\" role in Monitoring v2, which a user can be granted on the System project. This grants user monitoring access. Thus, the article is no more maintained. Please, refer to this\u00a0for more information on RBAC.","text":""},{"location":"kbs/000020151/#task","title":"Task","text":"<p>You can follow these directions to create a new user and grant minimal permissions to view cluster monitoring and Grafana graphs in your Kubernetes cluster.</p>"},{"location":"kbs/000020151/#requirements","title":"Requirements","text":"<ul> <li>Rancher v2.x</li> <li>Monitoring enabled in your cluster</li> </ul>"},{"location":"kbs/000020151/#background","title":"Background","text":"<p>You may have a use case to grant permissions to a user to view cluster monitoring metrics and graphs, but don't want that same user to be able to see other information or perform any actions on your cluster. This how-to guide will show you how to achieve this.</p>"},{"location":"kbs/000020151/#solution","title":"Solution","text":"<ol> <li>If you have not already, create a new user in Rancher. Go to the Global view and click on the Users menu. Click the <code>Add Users</code> button in the top right corner. Select the desired Username, Password, and Display Name. For Global Permissions, select User-Base and leave all Custom permissions unchecked. Click the <code>Create</code> button at the bottom of the form. Let's assume we are using the username <code>johndoe</code>.</li> <li>Go to the Security menu and select Roles. Select the Projects tab and click the <code>Add Project Role</code> button. In the name field, enter Services Proxy. Under Grant Resources, click the <code>+ Add Resource</code> button. Check the Get and List boxes and enter <code>services/proxy</code> in the Resource field. Note, you'll see it changes this to <code>serivces/proxy (Custom)</code> which is normal. Click the <code>Create</code> button at the bottom to create the new project role.</li> <li>Next, go to the cluster view for your cluster and select Members from the menu. Click the <code>Add Members</code> button in the top right corner. In the Members dropdown, select <code>johndoe</code> and select Member for Cluster Permissions. Click the <code>Create</code> button at the bottom of the form.</li> <li>Now navigate to the System project in your cluster. Go to the Members menu and click the <code>Add Member</code> button. Enter <code>johndoe</code> in the Member field and select <code>Services Proxy</code> under Project Permissions. Click the <code>Create</code> button at the bottom of the form.</li> <li>The <code>johndoe</code> user should now be able to log into Rancher and see the cluster dashboard with the Grafana icons. Clicking the Grafana icons should open a new browser window that will show the user various graphs and statistics for the cluster. This user should not be able to perform other operations, like view or launch new workloads in the cluster.</li> </ol>"},{"location":"kbs/000020151/#further-reading","title":"Further Reading","text":"<p>For more detailed information on how RBAC works in Rancher and Kubernetes, see the following links:</p> <ul> <li>Role-Based Access Control (RBAC) in Rancher</li> <li>Using RBAC Authorization in Kubernetes</li> </ul>"},{"location":"kbs/000020151/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020152/","title":"Updating SSL cert in Rancher v2.x with the same CA","text":"<p>This document (000020152) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020152/#situation","title":"Situation","text":""},{"location":"kbs/000020152/#task","title":"Task","text":"<p>How do I renew my SSL/TLS certificate for Rancher?</p>"},{"location":"kbs/000020152/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> <li>Rancher on a Kubernetes Cluster see documentation for more information</li> <li>The new certificate must have the same root CA as the current certificate.</li> <li>Used the option \"Bring your own certificate\" when installing Rancher Doc</li> <li>A copy of the certificate and private key in Base64 format Doc</li> <li>A copy of the root and intermediate CA certificate (Sometimes called the certificate chain).</li> </ul>"},{"location":"kbs/000020152/#assumptions","title":"Assumptions","text":"<ul> <li>kubectl access to the Rancher local cluster</li> <li>The certificate is stored as server.crt</li> <li>The private key is stored as tls.key</li> <li>The root CA is stored as root-ca.crt</li> <li>The intermediate CA is stored as intermediate-ca.crt</li> </ul>"},{"location":"kbs/000020152/#resolution","title":"Resolution","text":""},{"location":"kbs/000020152/#install-steps","title":"Install Steps","text":"<ol> <li>Verify private key doesn't have a passphrase using the command listed below. If the following command asks for a passphrase then it is password protected and this must be removed.</li> </ol> <pre><code>openssl rsa -in tls.key -noout\n</code></pre> <ol> <li>Remove the passphrase (skip this step if the previous command didn't ask for a passphrase):</li> </ol> <pre><code>mv tls.key tls-pass.key\nopenssl rsa -in tls-pass.key -out tls.key\nEnter your passphrase here\n</code></pre> <ol> <li>Create the certificate chain. If you have additional intermediate certs please add them at this step.</li> </ol> <p>NB: Order is important!</p> <pre><code>cat server.crt intermediate-ca.crt root-ca.crt &gt; tls.crt\n</code></pre> <ol> <li>Backup the current certificate:</li> </ol> <pre><code>kubectl -n cattle-system get secret tls-rancher-ingress -o yaml &gt; tls-rancher-ingress-bk.yaml\n</code></pre> <ol> <li>Remove the current certificate:</li> </ol> <pre><code>kubectl -n cattle-system delete secret tls-rancher-ingress\n</code></pre> <ol> <li>Install the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n--cert=tls.crt \\\n--key=tls.key\n</code></pre>"},{"location":"kbs/000020152/#verification-steps","title":"Verification Steps","text":"<ul> <li>Run the following command to verify the new certificate. (Replace Rancher with your Rancher URL):</li> </ul> <pre><code>curl --insecure -v https://&lt;&lt;Rancher&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n</code></pre> <ul> <li>Example output:</li> </ul> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host lab.rancher.tools left intact\n</code></pre> <ul> <li>NOTE: Some browsers will cache the certificate. So you might to close the browser and reopen in order to get the new certificate. How to clear the SSL state in a browser.</li> </ul>"},{"location":"kbs/000020152/#rollback-steps","title":"Rollback Steps","text":"<ol> <li>Backup the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system get secret tls-rancher-ingress -o yaml &gt; tls-rancher-ingress-new.yaml\n</code></pre> <ol> <li>Remove the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system delete secret tls-rancher-ingress\n</code></pre> <ol> <li>Re-install the old certificate:</li> </ol> <pre><code>kubectl -n cattle-system apply -f tls-rancher-ingress-bk.yaml\n</code></pre>"},{"location":"kbs/000020152/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020154/","title":"How to troubleshoot SNI enabled endpoints with curl and openssl","text":"<p>This document (000020154) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020154/#situation","title":"Situation","text":""},{"location":"kbs/000020154/#issue","title":"Issue","text":"<p>A modern webserver hosting or proxying to multiple backend domain names will often be configured to use SNI (Server Name Indication).</p> <p>SNI allows multiple SSL-protected domains to be hosted on the same IP address, and is commonly used in Kubernetes with ingress controllers, for example, the nginx ingress controller.</p> <p>As the SNI extension requires a slight change to the conversation between client and server - the hostname must be provided in the <code>Hello</code> message to correctly access the associated domain name.</p> <p>This can present an issue when troubleshooting a node or pod directly, where an IP address is used.</p>"},{"location":"kbs/000020154/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>The <code>curl</code> and/or <code>openssl</code> command installed</li> <li>Network access to the endpoint you wish to troubleshoot</li> </ul>"},{"location":"kbs/000020154/#steps","title":"Steps","text":"<p>To perform an SNI-compliant request using an IP address, use the following commands replacing the domain name and IP address.</p> <ul> <li>Using the <code>curl</code> command:</li> </ul> <pre><code>curl -v --resolve domain.com:443:&lt;ip address&gt; https://domain.com\n</code></pre> <ul> <li>Using <code>openssl</code> can be useful to obtain details about the certificate configured:</li> </ul> <pre><code>openssl s_client -showcerts -servername domain.com -connect &lt;ip address&gt;:443\n</code></pre>"},{"location":"kbs/000020154/#further-reading","title":"Further reading","text":"<p>More information on SNI can be found here.</p>"},{"location":"kbs/000020154/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020155/","title":"What is SUSE Rancher Hosted?","text":"<p>This document (000020155) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020155/#resolution","title":"Resolution","text":"<p>SUSE Rancher Hosted is a software-as-a-service offering from SUSE. As the name implies, Rancher is completely hosted for you in the cloud. SUSE takes care of the installation, upgrade, and day-to-day operations of your Rancher control plane. Using Rancher, you can then add your own cloud-based, on-premise, AKS, GKE, or EKS clusters. For more information, see the Rancher v2.4 announcement, blog announcement, or SUSE Rancher Hosted product page .</p>"},{"location":"kbs/000020155/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020156/","title":"Can I move an existing Kubernetes cluster to SUSE Rancher Hosted?","text":"<p>This document (000020156) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020156/#resolution","title":"Resolution","text":"<p>Existing Kubernetes clusters can be imported into SUSE Rancher Hosted. However, you cannot currently move a cluster that is already managed by Rancher to SUSE Rancher Hosted. We are currently looking to enhance our management capabilities to allow users to move clusters between Rancher clusters. See GitHub issue 16471. As a workaround, you can redeploy workloads running in an existing cluster over to a new SUSE Rancher Hosted managed cluster.</p>"},{"location":"kbs/000020156/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020157/","title":"Where are the links for Prometheus and Grafana in Rancher v2.x Cluster and Project Monitoring?","text":"<p>This document (000020157) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020157/#situation","title":"Situation","text":""},{"location":"kbs/000020157/#question","title":"Question","text":"<p>Using Rancher v2.x, from v2.2.4 and above, you can monitor the state and processes of your cluster nodes, Kubernetes components, and software deployments through the use of the built-in Rancher cluster and project monitoring. Rancher monitoring deploys the open-source Grafana and Prometheus projects, and this article details how to access the UI for these components, so you can view the monitoring dashboards and query metrics.</p>"},{"location":"kbs/000020157/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster managed by Rancher v2.x, from v2.2.4 and above</li> <li>Cluster monitoring enabled: Go to your Cluster --&gt; Tools --&gt; Monitoring --&gt; Enable</li> <li>(Optionally) Project monitoring enabled: Go to your Project --&gt; Tools --&gt; Monitoring --&gt; Enable</li> </ul>"},{"location":"kbs/000020157/#answer","title":"Answer","text":""},{"location":"kbs/000020157/#cluster-level","title":"Cluster level","text":""},{"location":"kbs/000020157/#grafana","title":"Grafana","text":"<p>To access the Grafana UI for Rancher cluster monitoring, from the main dashboard for your cluster, click the three-dot option menu in the top-right and click the button \"Go to Grafana\".</p> <p></p> <p>You should also see Grafana logos next to the system components on the main dashboard for your cluster. Click on any of them to take you to the Grafana dashboard for that particular component.</p> <p></p>"},{"location":"kbs/000020157/#prometheus","title":"Prometheus","text":"<p>To access the Prometheus UI for the cluster monitoring, navigate to the Apps page in the System project for your cluster. You will see an app called \"cluster-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus.</p> <p></p>"},{"location":"kbs/000020157/#project-level","title":"Project level","text":""},{"location":"kbs/000020157/#grafana-and-prometheus","title":"Grafana and Prometheus","text":"<p>To access the Grafana and Prometheus UIs, for a project with project monitoring enabled, navigate to the Apps page within the project. You will see an app called \"project-monitoring\" deployed, listing \"/index.html\" links for Grafana and Prometheus.</p> <p></p>"},{"location":"kbs/000020157/#further-reading","title":"Further Reading","text":"<p>Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here.</p>"},{"location":"kbs/000020157/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020158/","title":"How to use the calicoctl CLI in an RKE or Rancher provisioned Kubernetes cluster","text":"<p>This document (000020158) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020158/#situation","title":"Situation","text":""},{"location":"kbs/000020158/#task","title":"Task","text":"<p>The <code>calicoctl</code> CLI provides an interface for managing calico network and security policy.</p> <p>In Kubernetes clusters provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, and which use the Calico or Canal Container Networking Interface (CNI) Plugin, <code>calicoctl</code> can be used to configure Calico GlobalNetworkPolicy and NetworkPolicy resources.</p>"},{"location":"kbs/000020158/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned with Rancher Kubernetes Engine (RKE) v0.x.x or v1.x.x, or Rancher v2.x.x</li> <li>The Calico or Canal Container Networking Interface (CNI) Plugin (Canal is the default in both RKE and Rancher provisioned clusters).</li> <li>A cluster-admin level kube config sourced via $KUBECONFIG on a host running Docker</li> </ul>"},{"location":"kbs/000020158/#resolution","title":"Resolution","text":"<p>N.B. The commands in this section should be run from a host running Docker, with a cluster-admin level kube config sourced.</p> <p>For the purpose of this example, we will demonstrate creating an empty GlobalNetworkPolicy resource via <code>calicoctl</code>.</p>"},{"location":"kbs/000020158/#set-kubeconfig-environment-variable-to-the-cluster-admin-kube-config","title":"Set $KUBECONFIG environment variable to the cluster-admin kube config","text":"<p>With the cluster-admin level kube config file present on the host, execute <code>export KUBECONFIG=&lt;full path to cluster-admin kube config&gt;</code> replacing with the full path of the kube config.</p>"},{"location":"kbs/000020158/#create-the-desired-resource-in-the-working-directory","title":"Create the desired resource in the working directory","text":"<p>Create a YAML file in the working directory with the NetworkPolicy resource definition(s) you want to apply to the cluster.</p> <p>For this example create a file named <code>globalpolicy.yaml</code> in the working directory with the following contents:</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: GlobalNetworkPolicy\nmetadata:\nname: allow-tcp-port-6379\n</code></pre>"},{"location":"kbs/000020158/#determine-the-calico-node-version-of-the-cluster","title":"Determine the calico-node version of the cluster","text":"<p>First get the version of the <code>calico-node</code> container running in the cluster.</p> <p>In a cluster with the Canal CNI Network Provider, run the following, with the admin kube config sourced:</p> <pre><code>CALICOVERSION=`kubectl -n kube-system get daemonset canal -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3`\necho $CALICOVERSION\n</code></pre> <p>In a cluster with the Calico CNI Network Provider, run the following, with the admin kube config sourced:</p> <pre><code>CALICOVERSION=`kubectl -n kube-system get daemonset calico-node -o yaml | grep 'rancher/calico-node:v' | tail -n1 | cut -d: -f3`\necho $CALICOVERSION\n</code></pre>"},{"location":"kbs/000020158/#run-calicoctl","title":"Run <code>calicoctl</code>","text":"<p>With the <code>calico-node</code> version determined and now set in the variable <code>$CALICOVERSION</code>, <code>calicoctl</code> can be invoked. This is done by running the <code>calico/ctl</code> image, with the version matching the <code>calico-node</code>. The kube config file is mounted into the container, as is the present working directory (at the path <code>/host</code>), so that the desired resource (in this example in the file globalpolicy.yaml) is available.</p> <p>To execute <code>calicoctl</code> run the following command, altering the filename as applicable to the resource you have created in the working directory:</p> <pre><code>docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION apply -f /host/globalpolicy.yaml\n</code></pre> <p>We can now view the GlobalNetworkPolicy resource by using <code>calicoctl get</code> as follows:</p> <pre><code>docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION get globalnetworkpolicy allow-tcp-port-6379 -o yaml\n</code></pre> <p>This should return output similar to the following:</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: GlobalNetworkPolicy\nmetadata:\ncreationTimestamp: \"2020-04-08T15:12:45Z\"\nname: allow-tcp-port-6379\nresourceVersion: \"9033\"\nuid: df2875a6-1142-4fe0-9f0c-5dc1372bd2c5\nspec:\ntypes:\n- Ingress\n</code></pre>"},{"location":"kbs/000020158/#further-reading","title":"Further reading","text":"<ul> <li>Calico's Get started with Calico network policy.</li> <li>The <code>calicoctl</code> user reference documentation.</li> </ul>"},{"location":"kbs/000020158/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020159/","title":"How to switch from private mirror to bundled system-charts in Rancher v2.3 and v2.4","text":"<p>This document (000020159) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020159/#situation","title":"Situation","text":""},{"location":"kbs/000020159/#task","title":"Task","text":"<p>The Rancher system-charts repository contains all the catalog items required for features such as monitoring, logging, alerting and global DNS.</p> <p>In air gapped installations prior to Rancher v2.3.0 it was necessary to host a private mirror of this repository. However, since v2.3.0 a copy of these charts has been bundled with the Rancher image.</p> <p>This article outlines how to switch from using a private mirror to the bundled system-charts that are included in the Rancher v2.3 and v2.4 images, removing the requirement to host a private mirror.</p>"},{"location":"kbs/000020159/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.3.x or v2.4.x instance, provisioned as a Highly Available (HA) install on a Kubernetes cluster, or running as a Single Node install using Docker.</li> </ul>"},{"location":"kbs/000020159/#resolution","title":"Resolution","text":""},{"location":"kbs/000020159/#rancher-ha-install","title":"Rancher HA Install","text":"<p>For a Rancher HA install this follows the same steps as the upgrade documentation.</p> <ol> <li>Get your current helm deployment values with <code>helm get values rancher</code>. Example output:</li> </ol> <pre><code>helm get values rancher\n\nhostname: rancher.my.org\n</code></pre> <ol> <li>Append <code>--set useBundledSystemChart=true</code> to your values, set the <code>--version</code> value to your current Rancher version, e.g. 2.4.2, (to pin the version and prevent an actual upgrade) and run the <code>helm upgrade</code> command. Example:</li> </ol> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n--namespace cattle-system \\\n--set hostname=rancher.my.org \\\n--set useBundledSystemChart=true \\\n--version 2.4.2\n</code></pre> <p>At this point your HA Rancher should be using the bundled charts for the system-charts.</p>"},{"location":"kbs/000020159/#rancher-single-node-install-using-docker","title":"Rancher Single Node Install Using Docker","text":"<p>To accomplish this for single node installations using Docker, you will be re-creating the Rancher container, with its current data, to add the <code>CATTLE_SYSTEM_CATALOG=bundled</code> environment variable. This closely follows the upgrade documentation.</p> <ol> <li>Create a copy of the data from your Rancher server container. <code>&lt;RANCHER_CONTAINER_NAME&gt;</code> is the name of your container as shown with <code>docker ps</code> and the <code>&lt;RANCHER_CONTAINER_TAG&gt;</code> is the version of Rancher ( <code>v2.3.0</code> for example):</li> </ol> <pre><code>docker stop &lt;RANCHER_CONTAINER_NAME&gt;\ndocker create --volumes-from &lt;RANCHER_CONTAINER_NAME&gt; --name rancher-data rancher/rancher:&lt;RANCHER_CONTAINER_TAG&gt;\n</code></pre> <ol> <li>Create a backup tarball:</li> </ol> <pre><code>docker run --volumes-from rancher-data -v $PWD:/backup busybox tar zcvf /backup rancher-data-backup-&lt;RANCHER_VERSION&gt;-&lt;DATE&gt;.tar.gz /var/lib/rancher\n</code></pre> <ol> <li>Start a new Rancher container with the added environment variable. The important thing to note here is that you use all of the same flags as when you initially started Rancher and append <code>-e CATTLE_SYSTEM_CATALOG=bundled</code> before the Rancher image. Example:</li> </ol> <pre><code>docker run -d --volumes-from rancher-data \\\n--restart=unless-stopped \\\n-p 80:80 -p 443:443 \\\n-e CATTLE_SYSTEM_CATALOG=bundled \\\nrancher/rancher:&lt;RANCHER_VERSION_TAG&gt;\n</code></pre> <p>At this point your single node Rancher installation using Docker should be using the bundled charts for the system-charts.</p>"},{"location":"kbs/000020159/#further-reading","title":"Further reading","text":"<p>Documentation on Setting up Local System Charts for Air Gapped Installations</p>"},{"location":"kbs/000020159/#documentation-for-rancher-ha-installs","title":"Documentation for Rancher HA Installs","text":"<p>Rancher HA Upgrade Documentation</p> <p>Rancher HA Helm Chart Options Documentation</p>"},{"location":"kbs/000020159/#documentation-for-single-node-installs","title":"Documentation for Single Node Installs","text":"<p>Rancher Single Node Upgrade Documentation</p>"},{"location":"kbs/000020159/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020160/","title":"How to run multiple ingress controllers","text":"<p>This document (000020160) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020160/#situation","title":"Situation","text":""},{"location":"kbs/000020160/#why-use-multiple-ingress-controllers","title":"Why use multiple ingress controllers?","text":"<p>At large numbers of ingresses and related workloads, a single ingress-controller can be a bottleneck in both throughput and reliability. It is recommended to shard ingresses across multiple ingress controllers in these scenarios.</p>"},{"location":"kbs/000020160/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster created by Rancher v2.x or RKE</li> <li>A Linux cluster, Windows is currently not supported</li> <li>Helm installed and configured</li> </ul>"},{"location":"kbs/000020160/#overview","title":"Overview","text":"<p>At a high level, the process for sharding ingresses is to build out one or more extra ingress controllers and logically separate your ingresses to evenly split the load between your ingress controllers. This separation is handled through annotations on the ingresses. When an nginx-ingress-controller pod starts up with an ingressClass set, it will only try to satisfy ingresses that are annotated with the same ingressClass. This allows you to run as many ingress-controllers as needed to satisfy your ingress needs.</p>"},{"location":"kbs/000020160/#creating-extra-nginx-ingress-controller-charts","title":"Creating extra nginx-ingress-controller charts","text":"<p>It is recommended to use the community nginx-ingress helm chart to install the extra ingress-controllers with NodePort services.</p> <p>This deployment method allows you to run multiple ingress controllers on a single node, as there are no conflicting ports. You are required to route traffic to the correct ingress controller ports through an external load balancer.</p> <p>Deploy a second default backend and ingress-controller from the nginx-ingress helm chart with the following values: <code>controller.ingressClass</code> - unique name of the ingress class, such as <code>ingress-nginx-2</code> <code>controller.service.type=NodePort</code></p> <p><code>controller.service.nodePorts.http</code> - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned</p> <p><code>controller.service.nodePorts.https</code> - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned</p> <p><code>controller.kind=DaemonSet</code></p> <p>For more configuration options, see the chart readme.</p> <p>An example daemonset install would be:</p> <pre><code>helm repo add stable https://kubernetes-charts.storage.googleapis.com\nhelm install nginx-ingress-second -n ingress-nginx stable/nginx-ingress --set controller.ingressClass=\"ingress-class-2\" --set controller.service.type=NodePort --set controller.kind=DaemonSet\n</code></pre> <p>This will create an ingress-nginx daemonset and service. This ingress controller will handle any ingress routed to it tagged with the annotation <code>kubernetes.io/ingress.class: ingress-class-2</code></p>"},{"location":"kbs/000020160/#sharding-ingresses","title":"Sharding Ingresses","text":"<p>It is recommended to shard (split) your ingresses in a way that evenly splits load and configuration size between ingress controllers.</p> <p>Sharding in this way does mean changing dns and ingress hosts so that traffic for ingresses is sent to the correct ingress controllers, typically through an external load balancer.</p> <p>The process for sharding ingresses is to tag each ingress with the ingressClass for the ingress controller you want to route them through. For example:</p> <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\nname: app_1_ingress\nannotations:\nkubernetes.io/ingress.class: \"ingress-class-2\"\nspec:\n</code></pre> <p>Once annotated with an ingressClass, these ingresses are now only handled by the ingress-controller that has that ingressClass.</p> <p>In the default configuration, the Rancher-provided nginx-ingress-controller will only handle ingresses that either have the default ingress.class annotation of <code>nginx</code> or do not have an ingress.class annotation at all.</p>"},{"location":"kbs/000020160/#next-steps","title":"Next steps","text":"<p>From here it is just a matter of ensuring that the traffic for each ingress is routed to the correct nodePort on the nodes that the daemonset is targeted against.</p> <p>If you did not specify a nodePort when deploying the chart, you can determine the nodePort that was assigned by checking the service created:</p> <pre><code>$ kubectl describe svc -n ingress-nginx nginx-ingress-second\nName:                     nginx-ingress-second-controller\nNamespace:                ingress-nginx\nLabels:                   app=nginx-ingress\n                          chart=nginx-ingress-1.35.0\n                          component=controller\n                          heritage=Helm\n                          release=nginx-ingress-second\nAnnotations:              field.cattle.io/publicEndpoints:\n                            [{\"addresses\":[\"13.210.157.241\"],\"port\":30155,\"protocol\":\"TCP\",\"serviceName\":\"ingress-nginx:nginx-ingress-second-controller\",\"allNodes\":tr...\nSelector:                 app.kubernetes.io/component=controller,app=nginx-ingress,release=nginx-ingress-second\nType:                     NodePort\nIP:                       10.43.139.23\nPort:                     http  80/TCP\nTargetPort:               http/TCP\nNodePort:                 http  30155/TCP\nEndpoints:                &lt;none&gt;\nPort:                     https  443/TCP\nTargetPort:               https/TCP\nNodePort:                 https  30636/TCP\nEndpoints:                &lt;none&gt;\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n</code></pre> <p>In this example, the service is exposed on every node on ports 30155 for http and 30636 for https</p>"},{"location":"kbs/000020160/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020161/","title":"Why does the kubelet certificate still show as expired after performing a cluster certificate rotation in an Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster?","text":"<p>This document (000020161) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020161/#situation","title":"Situation","text":""},{"location":"kbs/000020161/#question","title":"Question","text":"<p>Why is Kubelet certificate still indicating expired after performing a cluster certificate rotation?</p>"},{"location":"kbs/000020161/#pre-requisite","title":"Pre-requisite","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020161/#answer","title":"Answer","text":"<p>Before Rancher v2.3.3 and RKE v1.0.0, cluster provisioning did not supply the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> arguments to the Kubelet container. As a result, the kubelet automatically generates the <code>kubelet.crt</code>, and <code>kubelet.key</code> files under the <code>/var/lib/kubelet/pki</code> \u200bdirectory and the certificate is not rotated during the certificate rotation.</p>"},{"location":"kbs/000020161/#how-to-verify-the-kubelet-certificate","title":"How to verify the Kubelet certificate","text":"<ul> <li> <p><code>openssl s_client -connect &lt;NODE IP&gt;:10250 | openssl x509 -text</code></p> </li> <li> <p><code>curl -vk https://&lt;NODE IP&gt;:10250</code></p> </li> </ul>"},{"location":"kbs/000020161/#resolution","title":"Resolution","text":"<p>You can rotate the kubelet certificate in RKE and Rancher provisioned clusters as follows:</p>"},{"location":"kbs/000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v220-v230-and-rke-v020-v032-provisioned-clusters","title":"How to rotate the kubelet certificate in Rancher v2.2.0 - v2.3.0 and RKE v0.2.0 - v0.3.2 provisioned clusters","text":"<p>For clusters provisioned and managed by Rancher prior to v2.3.3 or RKE prior to v1.0.0, you will need to manually delete the <code>kubelet.crt</code> and <code>kubelet.key</code> in <code>/var/lib/kubelet/pki</code> and restart the Kubelet container:</p> <pre><code>docker exec kubelet rm /var/lib/kubelet/pki/kubelet.crt\ndocker exec kubelet rm /var/lib/kubelet/pki/kubelet.key\ndocker restart kubelet\n</code></pre>"},{"location":"kbs/000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v232-provisioned-clusters","title":"How to rotate the kubelet certificate in Rancher v2.3.2+ provisioned clusters","text":"<p>For Rancher provisioned clusters managed by Rancher v2.3.3 and above, you can set the <code>generate_serving_certificate</code> kubelet option to <code>true</code> in the cluster configuration YAML to rotate the kubelet certificate.</p> <p>N.B. If <code>hostname_override</code> is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding.</p> <ol> <li> <p>For the affected cluster click 'Edit Cluster' from within the Rancher UI cluster view.</p> </li> <li> <p>Click 'Edit as YAML'.</p> </li> <li> <p>Set the <code>generate_serving_certificate</code> option to true for the kubelet, per the below:</p> </li> </ol> <pre><code>services:\nkubelet:\ngenerate_serving_certificate: true\n</code></pre> <ol> <li>Click 'Save' to intitate a cluster reconciliation and trigger rotation of the kubelet certificate.</li> </ol>"},{"location":"kbs/000020161/#how-to-rotate-the-kubelet-certificate-in-rke-v100-provisioned-clusters","title":"How to rotate the kubelet certificate in RKE v1.0.0+ provisioned clusters","text":"<p>For clusters managed by RKE v1.0.0 and above, you can set the <code>generate_serving_certificate</code> kubelet option to <code>true</code> in the cluster configuration YAML and invoke <code>rke up</code> to rotate the kubelet certificate.</p> <p>N.B. If <code>hostname_override</code> is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding.</p> <ol> <li>Set the <code>generate_serving_certificate</code> option to true for the kubelet, within the cluster configuration YAML file, per the below:</li> </ol> <pre><code>services:\nkubelet:\ngenerate_serving_certificate: true\n</code></pre> <ol> <li>Invoke <code>rke up --config &lt;cluster configuration yaml&gt;</code> to update the cluster configuration with the new kubelet option and trigger rotation of the kubelet certificate.</li> </ol>"},{"location":"kbs/000020161/#further-reading","title":"Further Reading","text":"<p>RKE Certificate Rotation Documentation. Rancher v2.x Certificate Rotation Documentation. Kubelet Service Certificate Requirements Documentation.</p>"},{"location":"kbs/000020161/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020162/","title":"How to clean a Rancher 2.x node","text":"<p>This document (000020162) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020162/#situation","title":"Situation","text":""},{"location":"kbs/000020162/#task","title":"Task","text":"<p>At times a node may need to be cleaned of all state to ensure it is consistent for further use in a cluster. This article and script are for Rancher 2.x.</p> <p>Please note, this script will delete all containers, volumes, images, network interfaces, and directories that relate to Rancher and Kubernetes. It can also optionally flush all iptables rules and delete container images. It is important to perform pre-checks, and backup the node as needed before proceeding with any steps below.</p>"},{"location":"kbs/000020162/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A node provisioned with the RKE distribution using Rancher or the RKE CLI.</li> <li>The node should no longer be a member of any cluster.</li> <li>A copy of the cleanup script, and root/sudo access.</li> <li>Check the running containers or Pods, these will be forcefully deleted in the following steps.</li> <li>Confirm you are on the correct node and are ready to proceed with cleaning all containers and all data specific to Kubernetes and Rancher/RKE.</li> </ul> <p>Note, for RKE2 and K3s use the uninstall script deployed on the node during install.</p>"},{"location":"kbs/000020162/#resolution","title":"Resolution","text":"<p>The below steps use a script to automate the clean of a node, the commands used can be run manually as needed, follow the steps below cleaning a node that has been used previously in a cluster.</p> <ul> <li>Login to the node and download the cleanup script:</li> </ul> <p><code>curl -sLO https://github.com/rancherlabs/support-tools/raw/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh</code></p> <p>You should now have a copy of the script in the current directory.</p> <ul> <li>Run the script:</li> </ul> <p><code>sudo bash extended-cleanup-rancher2.sh</code></p> <p>If desired, the optional -f and -i flags can be used together or individually to flush iptables (-f) and delete container images (-i).</p> <p><code>sudo bash extended-cleanup-rancher2.sh -f -i</code></p> <ul> <li>Restart the node</li> </ul> <p>The node is now in a clean consistent state to be reused in a cluster.</p>"},{"location":"kbs/000020162/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020163/","title":"How to troubleshoot using the namespace of a container","text":"<p>This document (000020163) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020163/#situation","title":"Situation","text":""},{"location":"kbs/000020163/#task","title":"Task","text":"<p>When troubleshooting an issue, often a faithful reproduction and exact environment are needed. This can be a challenge in a containerized environment, where tools and a shell environment may not be easily available within containers of a Pod.</p>"},{"location":"kbs/000020163/#steps","title":"Steps","text":"<p>There are two approaches that can be taken:</p>"},{"location":"kbs/000020163/#sidecar-container","title":"Sidecar container","text":"<p>By running a container in the same namespaces as another, it's possible to use that container for troubleshooting.</p> <p>The sidecar container can be started using the same network and PID namespaces while attaching the same volumes:</p> <ul> <li>Set the ID or name of the container you wish to troubleshoot:</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>Run the sidecar container using the network, PID and volumes</li> </ul> <p><code>docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh</code></p> <ul> <li>It is now possible to troubleshoot with commands from the alpine container, within the context of the container or Pod with the issue.</li> </ul> <p>For example, if you were experiencing a network issue from this Pod, it is now possible to use tools available in the sidecar container to simulate the connection, view the network configuration and troubleshoot interactively.</p> <p>Substitute the alpine container as needed with an image of your choice.</p> <p>Note, this will attach the same volumes as the parent container, but the parent container read/write layers will not be accesible - to access the same container filesystem, see the nsenter example below.</p>"},{"location":"kbs/000020163/#use-the-host-tools-with-nsenter","title":"Use the host tools with nsenter","text":"<p>Alternatively you can use tools available on the host for the same usecase with the <code>nsenter</code> command. The <code>nsenter</code> command is standard on most Linux distributions, for example on Ubuntu it is provided by the util-linux package.</p> <ul> <li>Set the ID or name of the container you wish to troubleshoot:</li> </ul> <p><code>ID=&lt;container ID or name&gt;</code></p> <ul> <li>Obtain the first process in the container (PID 1):</li> </ul> <p><code>PID=$(docker inspect --format '{{ .State.Pid }}' $ID)</code></p> <ul> <li>Run commands available on the node within the context of all of the container/Pod namespaces with nsenter:</li> </ul> <p><code>nsenter -a -t $PID &lt;command&gt;</code></p> <p>For example, if troubleshooting a network issue, tools from the node like tcpdump, curl, dig and mtr can be used to troubleshoot the issue interactively.</p> <p>Note, the <code>-a</code> flag is available in recent versions of <code>nsenter</code>, if this does not succeed, use a flag for a specific namespace, check the <code>nsenter --help</code> output.</p>"},{"location":"kbs/000020163/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020164/","title":"How to troubleshoot HTTP request performance with curl statistics","text":"<p>This document (000020164) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020164/#situation","title":"Situation","text":""},{"location":"kbs/000020164/#task","title":"Task","text":"<p>When troubleshooting a performance issue with a web-based endpoint, it's important to have metrics that assist in understanding what areas are related.</p> <p>This is where using a lightweight tool like curl, and it's ability to write out the statistics of a request can be very useful.</p>"},{"location":"kbs/000020164/#pre-requisites","title":"Pre-requisites","text":"<p>You will just need curl installed and available from the location performing the test.</p>"},{"location":"kbs/000020164/#steps","title":"Steps","text":"<p>Download the format file to use with curl:</p> <p><code>curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/files/curl-format.txt</code></p> <p>You should now have a <code>curl-format.txt</code> file locally in the current directory.</p> <p>Using the file and the <code>-w</code> flag, perform the desired request to the service, the example below displays the headers and statistics.</p> <p><code>curl -I -w \"@curl-format.txt\" https://rancher.com</code></p> <p>Timing statistics will be output with each run of the command, measurements are recorded in seconds.</p> <p>Note: run the command from a location that provides an accurate reproduction of the issue, to simulate the issue as closely as possible. Using the same request parameters are important - like the path and headers that might be used by client applications.</p>"},{"location":"kbs/000020164/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020165/","title":"How to recover an RKE v0.2.x, v0.3.x or v1.x.x cluster after restoration with an incorrect or missing rkestate file","text":"<p>This document (000020165) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020165/#situation","title":"Situation","text":""},{"location":"kbs/000020165/#issue","title":"Issue","text":"<p>When using RKE (Rancher Kubernetes Engine) v0.2.x, v0.3.x, v1.0.x or v1.1.0, if you have restored a cluster with the incorrect or missing rkestate file you will end up in a state where your infrastructure pods will not start. This includes all pods in the kube-system, cattle-system and ingress-nginx namespaces. As a result of these stopped infrastructure pods, workload pods will not function correctly. If you find yourself in this situation you can use the directions below to fix the cluster. For more information about the cluster state file, please see RKE documentation on Kubernetes Cluster State.</p>"},{"location":"kbs/000020165/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>RKE v0.2.x, v0.3.x, v1.0.x or v1.1.0</li> <li>A cluster restoration performed with the incorrect or missing rkestate file</li> </ul>"},{"location":"kbs/000020165/#workaround","title":"Workaround","text":"<ol> <li>Delete all service-account-token secrets in kube-system, cattle-system and ingress-nginx namespaces:</li> </ol> <pre><code>kubectl get secret -n cattle-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cattle-system delete secret \" $1) }'\nkubectl get secret -n kube-system | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n kube-system delete secret \" $1) }'\nkubectl get secret -n ingress-nginx | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n ingress-nginx delete secret \" $1) }'\nkubectl get secret -n cert-manager | awk '{ if ($2 == \"kubernetes.io/service-account-token\") system(\"kubectl -n cert-manager delete secret \" $1) }'\n</code></pre> <ol> <li>Restart Docker on all nodes in the cluster currently:</li> </ol> <pre><code>systemctl restart docker\n</code></pre> <ol> <li>Force delete all pods stuck in a CrashLoopBackOff, Terminating, Error and Evicted state:</li> </ol> <pre><code>kubectl get po --all-namespaces | awk '{ if ($4 ==\"CrashLoopBackOff\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\nkubectl get po --all-namespaces | awk '{ if ($4 ==\"Terminating\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\nkubectl get po --all-namespaces | awk '{ if ($4 ==\"Error\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\nkubectl get po --all-namespaces | awk '{ if ($4 ==\"Evicted\") system(\"kubectl delete po --force --grace-period=0 -n \" $1 \" \" $2) }'\n</code></pre> <ol> <li>Once your force delete has finished, restart Docker again to clear out any stale containers from the above force delete command:</li> </ol> <pre><code>systemctl restart docker\n</code></pre> <ol> <li>You may have to delete service account tokens more than once or delete pods more than once. After you go through the guide once, monitor pod statuses with a watch command in one terminal as shown below.</li> </ol> <pre><code>watch -n1 'kubectl get po --all-namespaces | grep -i  \"cattle-system\\|kube-system\\|ingress-nginx\\|cert-manager\"'\n</code></pre> <p>If you see any pods still in an error state, you can describe them to get idea of what is wrong. Most likely you'll see an error like the following which indicates that you need to delete its service account tokens again.</p> <pre><code>Warning  FailedMount  7m23s (x126 over 4h7m)  kubelet, 18.219.82.148  MountVolume.SetUp failed for volume \"rancher-token-tksxr\" : secret \"rancher-token-tksxr\" not found\nWarning  FailedMount  114s (x119 over 4h5m)   kubelet, 18.219.82.148  Unable to attach or mount volumes: unmounted volumes=[rancher-token-tksxr], unattached volumes=[rancher-token-tksxr]: timed out waiting for the condition\n</code></pre> <p>Delete the service account tokens again for that one namespace so that pods in other namespaces don't have to be disturbed if they are good. Once the service account tokens are deleted, run a delete pod command for just the namespace with pods still in an error state. cattle-node-agent and cattle-cluster-agent depend on the Rancher pod to be online, so you can ignore those until the very end. Once Rancher pods are stable, go back in and delete all the agents again to get them to restart more quickly.</p>"},{"location":"kbs/000020165/#resolution","title":"Resolution","text":"<p>An update to enable successful restoration of an RKE provisioned cluster without the correct rkestate file is targetted for an RKE v1.1.x patch release. For more information please see RKE GitHub issue #1336.</p>"},{"location":"kbs/000020165/#further-reading","title":"Further Reading","text":"<ul> <li>RKE documentation on Kubernetes Cluster State</li> <li>RKE documentation on etcd snapshots</li> </ul>"},{"location":"kbs/000020165/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020166/","title":"Pod network connectivity non-functional as a result of sysctl net.ipv4.ip_forward=0","text":"<p>This document (000020166) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020166/#situation","title":"Situation","text":""},{"location":"kbs/000020166/#issue","title":"Issue","text":"<p>If the sysctl <code>net.ipv4.ip_forward</code> is set to 0 (disabled) on a Linux host, then IPv4 packet forwarding is disabled.</p> <p>As a result, on a Kubernetes nodes this will prevent Pod networking from functioning.</p> <p>You can confirm the current value of this sysctl on a Linux host, if you are experiencing a network issue, with the following:</p> <p><code>sysctl net.ipv4.ip_forward</code></p> <p>The output should show 1, for enabled.</p>"},{"location":"kbs/000020166/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster with a CNI (Container Network Interface) plugin configure, e.g. an RKE (Rancher Kubernetes Engine) or Rancher launched cluster.</li> <li>The systctl net.ipv4.ip_forward set to 0 (disabled) on the cluster hosts.</li> </ul>"},{"location":"kbs/000020166/#resolution","title":"Resolution","text":"<p>Check if the kernel parameter <code>net.ipv4.ip_forward</code> is set to 1 with:</p> <p><code>sysctl net.ipv4.ip_forward</code></p> <p>If the current value of net.ipv4.ip_forward is 0, then set to this to 1 with the following:</p> <p><code>sysctl net.ipv4.ip_forward=1</code></p> <p>To make it permanent across reboot, add the following line in <code>/etc/sysctl.conf</code>:</p> <p><code>net.ipv4.ip_forward=1</code></p> <p>With this sysctl correctly enabled, Pod ingress and egress will be able to function as expected.</p>"},{"location":"kbs/000020166/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020167/","title":"How to setup your network CIDR for a large cluster","text":"<p>This document (000020167) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020167/#situation","title":"Situation","text":""},{"location":"kbs/000020167/#task","title":"Task","text":"<p>If you are expecting to use Rancher to deploy a Kubernetes cluster with more than 256 nodes, you'll need to make sure you adjust the default cluster CIDR settings. The default settings only allows clusters of 256 nodes or less.</p>"},{"location":"kbs/000020167/#requirements","title":"Requirements","text":"<ul> <li>Rancher v2.x</li> <li>A lot of hardware or VMs!</li> </ul>"},{"location":"kbs/000020167/#background","title":"Background","text":"<p>Kubernetes provides each pod with an IP address and each node with a block of IP addresses. Each cluster is also provided a block of IP addresses that is distributed to each node.</p> <p>This is controlled by two settings, the <code>cluster_cidr</code> block and <code>node-cidr-mask-size</code>. By default, the <code>cluster_cidr</code> block is 10.42.0.0/16 and the <code>node-cidr-mask-size</code> is 24. This gives the cluster 256 blocks of /24 networks to distribute out to the pool of nodes. For example, node1 will get 10.24.0.0/24, node2 will get 10.42.1.0/24, node3 will get 10.42.2.0/24 and so on.</p>"},{"location":"kbs/000020167/#solution","title":"Solution","text":"<p>To support more than 256 nodes, you will need to use a larger cluster_cidr block, a smaller node-cidr-mask-size, or adjust both. For example, if you want to support up to 512 nodes you can set:</p> <ul> <li><code>cluster_cidr</code> to 10.40.0.0/15</li> <li><code>node-cidr-mask-size</code> to 24</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.42.0.0/16</li> <li><code>node-cidr-mask-size</code> to 25</li> </ul> <p>To support up to 1024 nodes, you can use a larger <code>cluster_cidr</code>, smaller <code>node-cidr-mask-size</code>, or combination of both:</p> <ul> <li><code>cluster_cidr</code> to 10.38.0.0/14</li> <li><code>node-cidr-mask-size</code> to 24</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.42.0.0/16</li> <li><code>node-cidr-mask-size</code> to 26</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.40.0.0/15</li> <li><code>node-cidr-mask-size</code> to 25</li> </ul> <p>You should be aware of the following caveats when specifying your <code>cluster_cidr</code> and <code>node-cidr-mask-size</code> settings:</p> <ul> <li>Make sure you don't set your <code>cluster_cidr</code> to overlap with the default cluster service network of 10.43.0.0/16. That's why the examples above used 10.40.0.0/15 and 10.38.0.0/14. A CIDR of 10.42.0.0/15 will clash with the default cluster service CIDR.</li> <li>Make sure you don't set your <code>cluster_cidr</code> to overlap with IP address ranges already used in your enterprise infrastructure such as your node IPs, firewalls, load balancers, DNS, or other internal networks.</li> <li>Make sure your <code>node-cidr-mask-size</code> is large enough to accommodate the number of pods you want to run on each node. A size of 24 will give enough IP addresses for about 250 pods per node, which is well above the 110 maximum. However a size of 26 will only give you about 60 IPs, which is below the 110 maximum. If you plan to raise the default pod per node limit beyond 110, make sure sure your <code>node-cidr-mask-size</code> is large enough to support it. Note that pods that have <code>hostNetwork: true</code> do not count toward this total.</li> <li>Set it right the first time! Once your cluster has been deployed, these values cannot change. You'll need to decommission your cluster and start over again if you don't set it right.</li> <li>As of v1.17, Kubernetes supports clusters up to 5000 nodes. If you plan to go beyond this, you're venturing into unknown territory. For the latest large cluster best practices, see https://kubernetes.io/docs/setup/best-practices/cluster-large/</li> </ul> <p>Setting these values can be done when first creating the cluster. You'll need to click on the <code>Edit as YAML</code> button and merge in the following YAML:</p> <pre><code>rancher_kubernetes_engine_config:\nservices:\nkube-controller:\ncluster_cidr: 10.40.0.0/15\nextra_args:\nnode-cidr-mask-size: 25\n</code></pre> <p>The above configuration should allow you to have about 120 pods per node and 1024 nodes in your cluster. That's over 100,000 pods, wow!</p>"},{"location":"kbs/000020167/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020168/","title":"Update self signed certificate on single install of Rancher 2.x","text":"<p>This document (000020168) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020168/#situation","title":"Situation","text":""},{"location":"kbs/000020168/#task","title":"Task","text":"<p>Update/renew self signed certificates to ten year expiration on Single Server Install of Rancher 2.x</p>"},{"location":"kbs/000020168/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>rancher-single-tool</li> </ul>"},{"location":"kbs/000020168/#resolution","title":"Resolution","text":"<ol> <li>Download Rancher single tool on the server that is running your Rancher container:</li> </ol> <pre><code>curl -LO https://github.com/patrick0057/rancher-single-tool/raw/master/rancher-single-tool.sh\n</code></pre> <ol> <li>Run script so that it upgrades your installation (you can upgrade to the same version) and pass flags to indicate that you want to regenerate your self signed certificate. The most reliable way is to just specify all of your options on the command line but the script does have an easy to use automated system as well as shown in option b.</li> </ol> <p>a. Specify all flags on command line, including any rancher options you had and docker options. Option -s is required for generating new 10 year self signed SSL certificates.</p> <pre><code>bash rancher-single-tool.sh -f -c'&lt;container_id&gt;' -t'upgrade' -v'&lt;rancher_version&gt;' -d'&lt;docker_options&gt;' -r'&lt;rancher_options&gt;' -s'&lt;self_signed_ssl_hostname&gt;'\n</code></pre> <p>For example:</p> <pre><code>bash rancher-single-tool.sh -f -c'984f2fe62f6a' -t'upgrade' -v'v2.2.4' -d'-d --restart=unless-stopped -p 80:80 -p 443:443' -r'none' -s'company.domain.com'\n</code></pre> <p>b. Let the script prompt you for answers and autodetect docker and rancher options when asked to.</p> <pre><code>bash rancher-single-tool.sh -s'&lt;self_signed_ssl_hostname&gt;'\n</code></pre> <p>For example:</p> <pre><code>bash rancher-single-tool.sh -s'company.domain.com'\n</code></pre> <ol> <li>In order to see the new SSL you need to completely quit your browser and start it back up, otherwise it might still show you the old certificate. Alternatively you can consistently check this using openssl instead of using your browser.</li> </ol> <pre><code>openssl s_client -connect company.domain.com:443 | openssl x509 -noout -text -startdate -enddate\n</code></pre> <ol> <li>If you have any downstream clusters attached to this Rancher installation you will need to update their Rancher agent deployment which will be covered in https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool</li> </ol>"},{"location":"kbs/000020168/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020169/","title":"How to troubleshoot IPsec stability issues in a Rancher v1.6 cluster","text":"<p>This document (000020169) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020169/#situation","title":"Situation","text":""},{"location":"kbs/000020169/#troubleshooting","title":"Troubleshooting","text":"<p>If you are experiencing issues with containers communicating to each other in your Rancher 1.6 environment, your ipsec might be having some issues. In this article I will go over common troubleshooting steps and procedures to correct the problem.</p> <ul> <li>exec into one of your ipsec-router containers and run the following ipsec test</li> </ul> <pre><code>for i in `curl -s rancher-metadata/latest/self/service/containers/| cut -f1 -d=` ; do ping -c2 `curl -s curl rancher-metadata/latest/self/service/containers/$i/primary_ip` ; done\n</code></pre> <p>If all containers or a majority are not responding then there is likely an issue with ipsec that needs to be addressed. Usually when there are ipsec issues, it is because metadata is having issues getting in sync. To confirm this, check your metadata logs (Infrastructure stacks&gt; network-services&gt; metadata&gt;) and look at the \"Download and reload in\" time. If it is hovering around 10 seconds or greater then this is most likely your problem. We generally want this value to be 1-2 seconds. Below is a sample of what this looks like. </p>"},{"location":"kbs/000020169/#information","title":"Information","text":"<p>The metadata container is a database that runs on every host in an environment. Infrastructure containers on each host rely on their local metadata database for information that allows them to run correctly. The data that is retrieved by metadata is serialized, so if it detects that it is out of date it will grab the data again until it is in sync. On a system that downloads and reloads in 10 seconds, the metadata container will be stuck in a perpetual loop of not having the correct data. This will result in infrastructure containers on that host to not work as expected.</p>"},{"location":"kbs/000020169/#repair","title":"Repair","text":"<p>IPsec usually has issues when there are more than 50 hosts in an environment. Rancher's official recommendation is that you have no more than 50 hosts in an environment. If you need more, we recommend scaling your hosts vertically or creating a separate environment. If you are still having issues or cannot for some reason scale down your environment right away then you can try increasing the CPU allowance to the metadata stack.</p> <p>To check metadata CPU usage, we need to go to infrastructure stacks then click on network-services. In network-services click \"Up to date\" in the top right corner. Then select the latest template version in the drop down menu to reveal the settings. You should see settings similar to the screenshot below.</p> <p></p> <p>The number on the left is the CPU Period which indicates a number that represents a full CPU core. The number on the right is the CPU quota which indicates how much CPU we want to allow metadata to use. By default we only allow metadata to use 1/2 of a core. In larger environments you can increase this value to correct ipsec issues.</p> <p>To increase 1/2 core to 2 cores for example, you could change the above CPU Quota number from 200000 to 800000. Once you save changes the containers will go through a rolling upgrade, this can take a while depending on how overloaded your environment is and how many hosts are in it. Once the rolling update is complete, test your ipsec connectivity again to ensure that it is working as expected.</p>"},{"location":"kbs/000020169/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020170/","title":"How to upgrade Docker using Rancher's install script","text":"<p>This document (000020170) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020170/#situation","title":"Situation","text":""},{"location":"kbs/000020170/#task","title":"Task","text":"<p>Rancher provides quick scripts for installing Docker, which are available for the most recent versions of Docker https://rancher.com/docs/rancher/v2.x/en/installation/requirements/installing-docker/ Upgrading Docker on your machine using these scripts is equally as simple</p>"},{"location":"kbs/000020170/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A supported node with a version of Docker needing to be upgraded</li> <li>Curl or Wget installed</li> </ul>"},{"location":"kbs/000020170/#resolution","title":"Resolution","text":"<p>Just run the script with the version number you are trying to upgrade to. Let's say you're running 18.09 and want to upgrade to 19.03. Simply provide the version number as the name of the script to run. For example:</p> <p><code>curl https://releases.rancher.com/install-docker/19.03.sh | sh</code></p> <p>or</p> <p><code>wget -O- https://releases.rancher.com/install-docker/19.03.sh | sh</code></p> <p>This will throw a warning that Docker is already installed, stop the running Docker engine, and upgrade your version. Note that restarting Docker will also stop any running container or workloads running on this host.</p> <p>This procedure does not apply to RancherOS</p>"},{"location":"kbs/000020170/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020171/","title":"Information to provide when logging a support case","text":"<p>This document (000020171) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020171/#situation","title":"Situation","text":"<p>To allow us to provide quick and efficient support, we ask that customers provide as much information as possible when logging a ticket.</p> <p>Below are some things that we find we typically need from customers when more information is needed.</p>"},{"location":"kbs/000020171/#access","title":"Access","text":"<p>When troubleshooting interactively or gathering further information, access areas of the environment may be needed, such as:</p> <ul> <li>SSH access to the affected node(s)</li> <li>Access to the Rancher UI</li> <li>The kubectl CLI and kubeconfig available for the related cluster(s)</li> </ul>"},{"location":"kbs/000020171/#information-to-provide","title":"Information to provide","text":"<p>Please consider the key details and provide as much information as possible about the related areas, below are some common questions that can be asked to help form a description when raising a ticket:</p> <ul> <li>When did you first notice the issue? Please be as specific as possible, for example a time, date and timezone</li> <li>Is the issue related to Rancher, the management (local) cluster, or a downstream cluster managed by Rancher?</li> <li>Logs, screenshots, terminal output, error, etc. are very useful to assist troubleshooting:</li> <li>System logs are almost always required when diagnosing an issue, you can generate these using the Rancher log collector script</li> <li>For some issues it can help to capture traffic from the browser to the Rancher UI in a HAR file, the steps can be found here</li> <li>To capture as much context about the issue as possible, a screenshot to demonstrate the issue, or a copy/paste or attachment of errors or terminal output that demonstrate the issue can greatly reduce the time to resolution</li> <li>What are the installed versions? For example, Rancher version, Kubernetes version of the affected cluster(s)?</li> <li>The Rancher version can be found at the bottom left of the Rancher UI or by inspecting the container image tag of the Rancher pod/container</li> <li>The Kubernetes version can be found by:<ul> <li>Navigating to the relevant cluster in the Rancher dashboard</li> <li>Using kubectl get nodes</li> <li>Standalone RKE clusters can be checked also with kubectl, and rke version --config  <li>If the issue is related to an upgrade of Rancher or Kubernetes, what were the previous versions?</li> <li>If the issue is related to a downstream cluster, which distribution and how was the cluster built? RKE1, RKE2, k3s, hosted provider, imported or custom?</li> <li>Are there any GitHub issues or previous support tickets you think are related?</li> <li>Are there any events that you think may correlate with the issue? For example, an infrastructure outage, host reboot, OS upgrade, Docker upgrade, network changes, configuration changes?</li> <li>Have you taken any corrective action or steps?</li>"},{"location":"kbs/000020171/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020172/","title":"How to setup Rancher 2.x with Active Directory external authentication","text":"<p>This document (000020172) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020172/#situation","title":"Situation","text":""},{"location":"kbs/000020172/#overview-and-intention","title":"Overview and Intention","text":"<p>This is a quick guide aiming to get Rancher v2.x using external authentication via Active Directory with the least amount of effort. Of course there is much more to consider and configure in a production enterprise environment. For more detail on this function please refer to this Rancher article and fine tune as required. Configuring Active Directory</p>"},{"location":"kbs/000020172/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher v2.x, either a single node instance or High Availability (HA) cluster.</li> <li>Local account to log onto the Rancher Server (usually admin)</li> <li>A Windows Server running Active Directory</li> <li>Name of the domain you wish to join</li> <li>A restricted account that Rancher can use to bind and query the Directory with (Security Recommendations at the bottom of article)</li> <li>A standard user account that will be used to test and enable the authentication (i.e your domain account)</li> <li>Knowledge of where the users are in the Active Directory OU (Organisational Unit) structure</li> <li>Network connectivity from the Rancher worker nodes to the Active Directory Servers (There is probably more than one, run nslookup on the domain name)</li> <li>Also good to test ports 389 or 636 (TLS) as these need to be allowed</li> </ul>"},{"location":"kbs/000020172/#steps-on-how-to-get-rancher-talking-to-ad-quickly","title":"Steps on How To Get Rancher Talking to AD Quickly","text":""},{"location":"kbs/000020172/#tested-with-ad-running-windows-server-20162019","title":"(Tested with AD running Windows Server 2016/2019)","text":"<p>In this example I have used the below examples (yours will be different):</p> <ul> <li>my domain is 'rancher.local'</li> <li>All or my users are located under the Users OU in AD</li> <li>my bind account is 'svc-rancher'</li> </ul> <p>For more detail refer to Configuring Active Directory:</p> <ol> <li>Log into the Rancher UI using the initial local admin account.</li> <li>From the Global view, navigate to Security &gt; Authentication</li> <li>Select Active Directory. The Configure an AD server form will be displayed.</li> <li>Add in the Hostname or IP address into the Hostname field</li> <li>Add 'rancher/svc-rancher' to the Service Account Username field</li> <li>Add 'cn=users,dc=rancher,dc=local' to the User Search Base</li> <li>Goto Section 3 add your domain account username and password</li> <li>Click 'Authenticate with Active Directory'</li> </ol>"},{"location":"kbs/000020172/#security-tips-and-best-practices","title":"Security tips and Best Practices","text":"<p>WARNING: Once enabled all users in the Search base will be able to log into Rancher.</p> <ol> <li>Once auth is configured in Rancher change the relaxed default setting from 'Allow any valid Users' to login to 'only allow members of Cluster, Projects' to login. Access must now be specified instead of allowing any User onto the cluster.</li> <li>Under 'Global, Security, Roles' It is best to drop 'New User Default' setting from 'User' to 'User Base' which provide less privleges to new users and must increased as required not as a default.</li> <li>The bind account is critical for ongoing authentication so locking the account will break functionality.</li> <li>If this account gets locked or the password changes your AD authentication will be broken. Setting the account and the password not to expire and removing lockout policies prevent disruption.</li> <li>Remove interactive logon abilites as this account doesn't need to logon to a server and control it</li> </ol>"},{"location":"kbs/000020172/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020173/","title":"How to change Rancher 2.x server-url","text":"<p>This document (000020173) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020173/#situation","title":"Situation","text":""},{"location":"kbs/000020173/#task","title":"Task","text":"<p>Changing the server URL on Rancher 2.x.</p>"},{"location":"kbs/000020173/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>rancher-single-tool for Single Server Rancher Installations</li> <li>cluster-agent-tool for both HA and Single Server Rancher Installtions</li> </ul>"},{"location":"kbs/000020173/#resolution","title":"Resolution","text":""},{"location":"kbs/000020173/#single-server-installation","title":"Single Server Installation","text":"<p>During this tutorial it is recommended to use the rancher-single-tool for Rancher single server installations. It isn't required but it makes the process much easier. As a result this guide will be based on using that tool.</p> <ol> <li>Download the rancher-single-tool to the node that is running your rancher server container.</li> </ol> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh\nwget https://github.com/rancherlabs/support-tools/raw/master/rancher-single-tool/rancher-single-tool.sh\n</code></pre> <ol> <li>Backup your Rancher installation.</li> </ol> <pre><code>bash rancher-single-tool.sh -t'backup'\n</code></pre> <ol> <li> <p>Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url.</p> </li> <li> <p>Now we need to upgrade your Rancher container to reflect new certs. This is required in most cases with the exception of already using a wildcard that also encompasses the new server-url.</p> </li> </ol> <p>a. To generate a new self signed certificate for your new URL use the following upgrade command. Follow the prompts to finish the upgrade.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -s'newhostname.company.com'\n</code></pre> <p>b. To generate a new Let's Encrypt certificate you will need to change the Rancher server options to reflect this. You could do this with the following command.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -r'--acme-domain newhostname.company.com'\n</code></pre> <p>c. If you were using certificates signed by a recognized CA before and just need to replace them, you should modify the docker options to reflect this change. Keep in mind that if you just replaced the cert files on the host path and the filenames didn't change, you can just restart the docker container. However if the filenames did change, I'm providing the example below of how you would do upgrade the container to see this change.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem'\n</code></pre> <p>d. If you were using certificates signed by a private CA or you want to use your own self signed certifiactes (certificates not created by rancher-single-tool option -s). Below is an example of how you would do that. The same rule applies from option c. If the filenames have not changed you don't need to upgrade, you can just restart the container.</p> <pre><code>bash rancher-single-tool.sh -t'upgrade' -d'-d -p 443:443 -p 80:80 --restart=unless-stopped --volume=/etc/rancherssl/certs/cert.pem:/etc/rancher/ssl/cert.pem --volume=/etc/rancherssl/certs/key.pem:/etc/rancher/ssl/key.pem --volume=/etc/rancherssl/certs/ca.pem:/etc/rancher/ssl/cacerts.pem'\n</code></pre> <ol> <li>Once your Rancher container is backup and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool</li> </ol>"},{"location":"kbs/000020173/#ha-installation","title":"HA Installation","text":"<ol> <li> <p>Ensure that you have current etcd backups for your local rancher cluster.</p> </li> <li> <p>Login to the Rancher web interface, navigate to the Global view by clicking the dropdown in the top left corner of the screen and selecting \"Global\". Then click \"settings\" in the middle of the top bar. From the settings page, change the server-url to match your new server url.</p> </li> <li> <p>Log into a box where you have helm and kubectl installed. You will need your local Rancher cluster kubeconfig, ensure that it is set to the default config by either placing it in ~/.kube/config or by setting your KUBECONFIG environment variable.</p> </li> <li> <p>Check current helm chart options:</p> </li> </ol> <pre><code>helm get values rancher -n cattle-system\nhostname: rancher.company.com\nrancherImageTag: v2.3.5\n</code></pre> <ol> <li>Craft an upgrade command based on the values provided in the previous step and then modify the hostname to match the new server hostname/url.</li> </ol> <pre><code>helm upgrade rancher-stable/rancher --name rancher --namespace cattle-system --set hostname=newrancher.company.com --set rancherImageTag=v2.3.5\n</code></pre> <ol> <li>Run the upgrade command then wait for rollout to complete.</li> </ol> <pre><code>kubectl -n cattle-system  rollout status deploy/rancher\n</code></pre> <ol> <li>Once your Rancher deployment is back up and running you need to login to a single controlplane node for each of the downstream clusters and run the cluster-agent-tool. You also need to login to one of the controlplane nodes of your local Rancher cluster and run the cluster-agent-tool. Please see https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool</li> </ol>"},{"location":"kbs/000020173/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020174/","title":"How to setup Nodelocal DNS cache on Rancher 2.x","text":"<p>This document (000020174) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020174/#situation","title":"Situation","text":""},{"location":"kbs/000020174/#why-use-nodelocal-dns-cache","title":"Why use Nodelocal DNS cache?","text":"<p>Like many applications in a containerised architecture, CoreDNS or kube-dns runs in a distributed fashion. In certain circumstances, DNS reliability and latency can be impacted with this approach. The causes of this relate notably to conntrack race conditions or exhaustion, cloud provider limits, and the unreliable nature of the UDP protocol.</p> <p>A number of workarounds exist, however long term mitigation of these and other issues has resulted in a redesign of the Kubernetes DNS architecture, and the result being the Nodelocal DNS cache project.</p>"},{"location":"kbs/000020174/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster of v1.15 or greater created by Rancher v2.x or RKE</li> <li>A Linux cluster, Windows is currently not supported</li> <li>Access to the cluster</li> </ul>"},{"location":"kbs/000020174/#resolution","title":"Resolution","text":""},{"location":"kbs/000020174/#installing","title":"Installing","text":"<p>There are two installation approaches, both approaches should be non-invasive, pods that are currently running will not be modified. The DNS configuration will take effect for pods started after the install is complete.</p>"},{"location":"kbs/000020174/#rke1-using-a-rancher-version-after-v24x-or-rke-version-after-v110","title":"RKE1: Using a Rancher version after v2.4.x, or RKE version after v1.1.0","text":"<p>Update the cluster using 'Edit as YAML' in the Rancher UI. With RKE, edit the cluster.yaml file instead.</p> <p>Note: Updating the cluster using the below will create the <code>node-local-dns</code> Daemonset, and restart the <code>kubelet</code> container on each node.</p> <p>As in the documentation, update or add the <code>dns.nodelocal.ip_address</code> field using the following as an example:</p> <pre><code>  dns:\n[..]\nnodelocal:\nip_address: \"169.254.20.10\"\n</code></pre> <p>New pods created after the change will configure the node-local-dns link-local address as the nameserver in <code>/etc/resolv.conf</code>.</p> <p>Note: No further action is needed to use node-local-dns (as in the option A/B below), the changes to <code>/etc/resolv.conf</code> will take effect for pods started from this point onwards.</p>"},{"location":"kbs/000020174/#rke1-using-a-rancher-version-before-v24x-or-rke-version-before-v110","title":"RKE1: Using a Rancher version before v2.4.x, or RKE version before v1.1.0","text":"<p>Installing the YAML manifest by navigating to the cluster, and clicking the <code>Launch kubectl</code> button in the Rancher UI. This command can also be run from a terminal where a kubeconfig for the cluster is currently configured.</p> <p>Environment variables are replaced before applying the manifest, one assumption is that the cluster service discovery domain name is <code>cluster.local</code> (default), adjust the command if needed.</p> <pre><code>curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml \\\n| sed -e 's/__PILLAR__DNS__DOMAIN__/cluster.local/g' \\\n| sed -e \"s/__PILLAR__DNS__SERVER__/$(kubectl get service --namespace kube-system kube-dns -o jsonpath='{.spec.clusterIP}')/g\" \\\n| sed -e 's/__PILLAR__LOCAL__DNS__/169.254.20.10/g' \\\n| kubectl apply -f -\n</code></pre> <p>Ensure the <code>node-local-dns</code> pods start successfully, a pod should start on each control plane and worker node.</p> <pre><code>kubectl get -n kube-system pod -l k8s-app=node-local-dns\n</code></pre> <p>When deploying the YAML manifest there are two options to configure the cluster to use the new node-local-dns configuration, please choose from option A or B below.</p>"},{"location":"kbs/000020174/#option-a-configure-the-kubelet","title":"Option A - Configure the Kubelet","text":"<p>By default, the Kubelet will configure the <code>/etc/resolv.conf</code> of pods with the <code>kube-dns</code> Service ClusterIP as the nameserver. Configuring all new pods to query node-local-dns will require updating the Kubelet arguments.</p> <p>Note: Updating the arguments using the below will restart the <code>kubelet</code> container on each node.</p> <ul> <li> <p>If the cluster was provisioned by Rancher, edit the cluster in the UI and click on <code>Edit as YAML</code>.</p> </li> <li> <p>If the cluster was provisioned by RKE, edit the cluster.yml file directly.</p> </li> </ul> <p>Update the <code>kubelet</code> service with the <code>cluster-dns</code> argument and IP Address. Click save, or run an <code>rke up</code> to put this change into effect.</p> <pre><code>services:\nkubelet:\nextra_args:\ncluster-dns: \"169.254.20.10\"\n</code></pre> <p>New pods created after the change will configure the node-local-dns link-local address as the nameserver in <code>/etc/resolv.conf</code>.</p>"},{"location":"kbs/000020174/#option-b-configure-workloads","title":"Option B - Configure Workloads","text":"<p>Alternatively, node-local-dns can be configured on a per-workload basis by updating the workload with a <code>dnsConfig</code> and <code>dnsConfig</code> .</p> <ul> <li> <p>If using the Rancher UI, edit the workload, navigate to Show advanced options &gt; Networking &gt; DNS Nameservers and add <code>169.254.20.10</code>. Additionally, adjust the DNS Policy to <code>None</code>.</p> </li> <li> <p>If configuring by YAML, patch in the following to the pod spec to adjust the <code>dnsPolicy</code> and <code>dnsConfig</code>:</p> </li> </ul> <pre><code>    spec:\ndnsPolicy: \"None\"\ndnsConfig:\nnameservers:\n- 169.254.20.10\n</code></pre>"},{"location":"kbs/000020174/#rke2-using-any-rke2-kubernetes-version","title":"RKE2: Using any RKE2 Kubernetes version","text":"<p>Update the default HelmChart for CoreDNS, the nodelocal.enabled: true value will install node-local-dns in the cluster. Please see the documentation here for more details.</p>"},{"location":"kbs/000020174/#testing","title":"Testing","text":"<p>Once installed, start a new pod to test DNS queries.</p> <pre><code>kubectl run --restart=Never --rm -it --image=tutum/dnsutils dns-test -- dig google.com\n</code></pre> <p>Unless Option B was used to install node-local-dns, you should expect to see <code>169.254.20.10</code> as the server, and a successful answer to the query.</p> <p>To verify a pod or container is using node-local-dns by checking the <code>/etc/resolv.conf</code> file, for example:</p> <pre><code>kubectl exec -it &lt;pod name&gt; -- grep nameserver /etc/resolv.conf\nnameserver 169.254.20.10\n</code></pre>"},{"location":"kbs/000020174/#removing-nodelocal-dns-cache","title":"Removing Nodelocal DNS cache","text":"<p>To remove from a cluster, the reverse steps are needed.</p> <p>Note: Pods created with the node-local-dns nameserver in <code>/etc/resolv.conf</code> will need to be started again to use the kube-dns service as a nameserver again.</p>"},{"location":"kbs/000020174/#using-a-rancher-version-after-v24x-or-rke-version-after-v110","title":"Using a Rancher version after v2.4.x, or RKE version after v1.1.0","text":"<p>Remove the <code>dns.nodelocal</code> configuration from the cluster YAML</p>"},{"location":"kbs/000020174/#using-a-rancher-version-before-v24x-or-rke-version-before-v110","title":"Using a Rancher version before v2.4.x, or RKE version before v1.1.0","text":"<ol> <li> <p>Remove the Kubelet configuration (Option A), or remove the dnsConfig from workloads (Option B).</p> </li> <li> <p>If Option A was taken, delete any pods in workloads that were started since the Kubelet configuration change so that they are started with the kube-dns ClusterIP again.</p> </li> <li> <p>Remove the node-local-dns objects with the following command:</p> </li> </ol> <pre><code>curl -sL https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml | kubectl delete -f -\n</code></pre> <p>Note: it is important to perform these steps in order, and only complete step 3 once the pods using node-local-dns have been started with the kube-dns ClusterIP configured in <code>/etc/resolv.conf</code> again.</p>"},{"location":"kbs/000020174/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020174/#troubleshooting","title":"Troubleshooting","text":"<p>Node-local-dns will perform external lookups on behalf of pods, this lookup occurs from the node-local-dns DaemonSet pod running on the same node as the pod.</p> <p>For internal lookups, CoreDNS will be used, node-local-dns will cache successful queries (30s), and negative queries (5s) by default. For an architecture overview please see the diagram here.</p> <p>In no specific order, the following can help understand a DNS issue further.</p>"},{"location":"kbs/000020174/#check-all-kube-dns-and-node-local-dns-objects","title":"Check all kube-dns and node-local-dns objects","text":"<p>Ensure there are no obvious issues with scheduling CoreDNS and node-local-dns pods in the cluster.</p> <pre><code>kubectl get all -n kube-system -l k8s-app=node-local-dns\nkubectl get all -n kube-system -l k8s-app=kube-dns\n</code></pre> <p>All node-local-dns and kube-dns pods should be ready and running, the kube-dns Service should exist. Check the events if needed to locate any warning or failed event messages.</p> <pre><code>kubectl describe ds -n kube-system -l k8s-app=node-local-dns\nkubectl describe rs -n kube-system -l k8s-app=kube-dns\n</code></pre>"},{"location":"kbs/000020174/#check-the-logs-and-configmap-of-kube-dns-and-node-local-dns-pods","title":"Check the logs and ConfigMap of kube-dns and node-local-dns pods","text":"<pre><code>kubectl logs -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=node-local-dns\n</code></pre> <pre><code>kubectl get configmap -n kube-system coredns -o yaml\nkubectl get configmap -n kube-system node-local-dns -o yaml\n</code></pre>"},{"location":"kbs/000020174/#enable-logging-and-perform-a-dns-test","title":"Enable logging and perform a DNS test","text":"<p>Note, query logging can increase the log output from CoreDNS, enabling this temporarily while investigating is suggested.</p> <ul> <li> <p>Enable query logging to understand the pattern from workloads</p> </li> <li> <p>Run a DaemonSet to perform queries from a pod running on each node in the cluster</p> </li> </ul>"},{"location":"kbs/000020174/#ask-questions-to-further-eliminate-the-issue","title":"Ask questions to further eliminate the issue","text":"<ul> <li>Is it only DNS that is affected, or is all connectivity affected?</li> <li>Are internal, external or all DNS queries failing?</li> <li>Are all nodes and workloads experiencing the issue, or a specific node or workload? * Nodes use the upstream DNS configured in <code>/etc/resolv.conf</code>, queries failing from a node could indicate the issue is with upstream DNS</li> <li>What is the error reported by applications? * If logs are aggregated, queries can be performed on the logs to identify timelines and impact</li> <li>Is the issue intermittent or constantly occuring? * If the issue is intermittent, configure monitoring or a loop to identify when the issue occurs, when it does - are internal, external or all queries affected?</li> </ul>"},{"location":"kbs/000020174/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020175/","title":"How to setup HAProxy for Rancher v2.x","text":"<p>This document (000020175) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020175/#situation","title":"Situation","text":""},{"location":"kbs/000020175/#task","title":"Task","text":"<p>Setup HAProxy as a frontend load balancer for Rancher v2.x.</p>"},{"location":"kbs/000020175/#overview","title":"Overview","text":""},{"location":"kbs/000020175/#install-haproxy","title":"Install HAProxy","text":""},{"location":"kbs/000020175/#ubuntu","title":"Ubuntu","text":"<pre><code>apt update\napt install -y haproxy\nsystemctl enable haproxy\nsystemctl start haproxy\n</code></pre>"},{"location":"kbs/000020175/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>yum update\nyum install haproxy -y\nsystemctl enable haproxy\nsystemctl start haproxy\n</code></pre>"},{"location":"kbs/000020175/#example-haproxy-config","title":"Example HAProxy Config","text":""},{"location":"kbs/000020175/#option-a-full-ssl","title":"Option A - Full SSL","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/</li> <li>Verify Rancher URL works when connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping\n</code></pre> <ul> <li>Copy cert and key into a single file called /etc/haproxy/cert.pem</li> <li>Add frontend to /etc/haproxy/haproxy.cfg:</li> </ul> <pre><code>frontend www-http\nbind *:80\nreqadd X-Forwarded-Proto:\\ http\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443 ssl crt /etc/haproxy/cert.pem\nreqadd X-Forwarded-Proto:\\ https\ndefault_backend rancher-https\n</code></pre> <ul> <li>Add backends to /etc/haproxy/haproxy.cfg:</li> </ul> <pre><code>backend rancher-http\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:80 check weight 1 maxconn 1024\nserver rancher02 192.168.1.104:80 check weight 1 maxconn 1024\nserver rancher03 192.168.1.105:80 check weight 1 maxconn 1024\n</code></pre> <pre><code>backend rancher-https\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:443 check weight 1 maxconn 1024 ssl verify none\nserver rancher02 192.168.1.104:443 check weight 1 maxconn 1024 ssl verify none\nserver rancher03 192.168.1.105:443 check weight 1 maxconn 1024 ssl verify none\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"kbs/000020175/#option-b-external-tls-termination","title":"Option B - External TLS Termination","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/options/chart-options/#external-tls-termination</li> <li>Verify Rancher URL works went connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl --header \"Host: rancher.example.com\" http://192.168.1.103/ping\n</code></pre> <ul> <li>Copy cert and key into a single file called /etc/haproxy/cert.pem</li> <li>Create frontends:</li> </ul> <pre><code>frontend www-http\nbind *:80\nreqadd X-Forwarded-Proto:\\ http\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443 ssl crt /etc/haproxy/cert.pem\nreqadd X-Forwarded-Proto:\\ https\ndefault_backend rancher-http\n</code></pre> <ul> <li>Create backends:</li> </ul> <pre><code>backend rancher-http\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:80 check weight 1 maxconn 1024\nserver rancher02 192.168.1.104:80 check weight 1 maxconn 1024\nserver rancher03 192.168.1.105:80 check weight 1 maxconn 1024\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"kbs/000020175/#option-c-tcp-pass-through","title":"Option C - TCP pass-through","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/</li> <li>Verify Rancher URL works when connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping\n</code></pre> <ul> <li>NOTE: The default gateway for all 3 Rancher nodes must be the load balancer. Doc: https://www.haproxy.com/blog/howto-transparent-proxying-and-binding-with-haproxy-and-aloha-load-balancer/</li> <li>Create frontends:</li> </ul> <pre><code>frontend www-http\nbind *:80\nmode tcp\noption tcplog\ntcp-request inspect-delay 5s\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443\nmode tcp\noption tcplog\ntcp-request inspect-delay 5s\ndefault_backend rancher-https\n</code></pre> <ul> <li>Create backends:</li> </ul> <pre><code>backend rancher-http\nmode tcp\nbalance roundrobin\nsource 0.0.0.0 usesrc client\nserver rancher01 192.168.1.103:80\nserver rancher02 192.168.1.104:80\nserver rancher03 192.168.1.105:80\n</code></pre> <pre><code>backend rancher-https\nmode tcp\nbalance roundrobin\nsource 0.0.0.0 usesrc client\nserver rancher01 192.168.1.103:443\nserver rancher02 192.168.1.104:443\nserver rancher03 192.168.1.105:443\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"kbs/000020175/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Add the following to /etc/haproxy/haproxy.cfg before the frontend section.</li> </ul> <pre><code>listen stats\nbind :9000\nmode http\nstats enable\nstats hide-version\nstats realm Haproxy\\ Statistics\nstats uri /\nstats auth admin:admin\n</code></pre> <ul> <li>Go to http://load01.example.com:9000/</li> <li>Username/Password: admin/admin</li> <li>If there are firewall rules blocking port 9000, use ssh tunneling to proxy the connection:</li> </ul> <pre><code>ssh -f -N -L 9000:127.0.0.1:9000 root@192.168.1.101\n</code></pre> <ul> <li>Go to http://localhost:9000/</li> </ul>"},{"location":"kbs/000020175/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020176/","title":"How to collect a trace and heap from nginx ingress","text":"<p>This document (000020176) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020176/#situation","title":"Situation","text":""},{"location":"kbs/000020176/#task","title":"Task","text":"<p>When troubleshooting an ingress-nginx issue, collecting the trace and heap dump from ingress-nginx Pods may be requested. This can assist with understanding issues like excessive memory consumption.</p>"},{"location":"kbs/000020176/#pre-requisites","title":"Pre-requisites","text":"<p>Access to the node(s) where the ingress-nginx Pods are experiencing the issue, or access to the node on <code>10254/TCP</code> from a workstation.</p> <p>To collect the output, the below commands use <code>curl</code>, you may need to install the package. If needed, <code>wget</code> could be used instead.</p> <p>The <code>date</code> command is used to provide a consistent timestamp for the files, this could be changed or removed if the <code>date</code> command on the node doesn't support these flags.</p> <p>The issue should be occurring at the time for the collection to be useful when investigating.</p>"},{"location":"kbs/000020176/#steps","title":"Steps","text":"<p>SSH to the node(s), use the following commands to collect the trace and heap dump. If the issue is intermittent or fluctuating, repeat the commands as necessary to capture the collection when the issue is ocurring.</p>"},{"location":"kbs/000020176/#heap","title":"Heap","text":"<pre><code>curl -s http://localhost:10254/debug/pprof/trace?seconds=5 --output /tmp/nginx-trace.$(date -u --iso-8601=seconds)\n</code></pre>"},{"location":"kbs/000020176/#trace","title":"Trace","text":"<pre><code>curl -s http://localhost:10254/debug/pprof/heap --output /tmp/nginx-heap.$(date -u --iso-8601=seconds)\n</code></pre> <p>Note: if accessing the node on <code>10254/TCP</code> instead, be sure to update <code>localhost</code> with the IP Address of the node.</p> <p>If the files are too large to upload to the ticket, please request or use the provided temporary upload location.</p>"},{"location":"kbs/000020176/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020177/","title":"How to generate a HAR file","text":"<p>This document (000020177) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020177/#situation","title":"Situation","text":""},{"location":"kbs/000020177/#task","title":"Task","text":"<p>When troubleshooting an issue that is reproducible in a browser, it is sometimes necessary to have additional information about the requests and responses. You may be requested to generate a HAR file recording to capture this and attach this to a ticket for analysis.</p> <p>Please note, the information collected in a HAR file can contain sensitive data like content, headers, and cookies. This is not always the case, and some information is transient only. However, please check and santise the information as necessary before uploading.</p>"},{"location":"kbs/000020177/#pre-requisites","title":"Pre-requisites","text":"<p>A browser that can reproduce the issue, we've covered Chrome and Firefox in this article.</p> <p>The issue should be occuring or reproducible at the time of the collection to contain an example of the issue.</p>"},{"location":"kbs/000020177/#steps","title":"Steps","text":"<p>Open your browser ready to reproduce the issue.</p>"},{"location":"kbs/000020177/#chrome","title":"Chrome","text":"<ul> <li>From the menu, select View &gt; Developer &gt; Developer Tools</li> <li>From the pane, click on the Network tab</li> <li>Locate the Preserve log setting in the upper left and ensure it is checked</li> <li>Locate the record button, it should be a red circle to indicate that it is currently recording, if it is grey, click it once to start recording</li> <li>Follow any steps needed to reproduce the issue during the recording</li> <li>Once the issue has occurred, right click in the pane and select Save as HAR with Content</li> </ul> <p>Firefox</p> <ul> <li>From the menu, select Tools &gt; Web Developer &gt; Network</li> <li>The recording to start automatically with any further navigation in the browser</li> <li>Follow any steps needed to reproduce the issue with the network pane open</li> <li>Once the issue has occurred, right click in the pane and select Save all as HAR</li> </ul>"},{"location":"kbs/000020177/#upload-the-har-file","title":"Upload the HAR file","text":"<p>Generally, HAR files are small in size, however if the file are too large to upload directly to the ticket, please request or use the provided temporary upload location.</p>"},{"location":"kbs/000020177/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020178/","title":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x","text":"<p>This document (000020178) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020178/#situation","title":"Situation","text":""},{"location":"kbs/000020178/#how-to-conduct-cis-hardening-benchmark-scanning-for-rancher-v23x_1","title":"How to conduct CIS hardening benchmark scanning for Rancher v2.3.x","text":"<p>CIS Benchmarks are best practices for the secure configuration of a target system. Available for more than 140 technologies, CIS Benchmarks are developed through a unique consensus-based process comprised of cybersecurity professionals and subject matter experts around the world. CIS Benchmarks are the only consensus-based, best-practice security configuration guides both developed and accepted by government, business, industry, and academia.</p> <p>This script is based on <code>CIS Benchmark Rancher Self-Assessment Guide v2.3</code> https://rancher.com/docs/rancher/v2.x/en/security/benchmark-2.3, which was derived from <code>CIS Kubernetes</code> <code>Benchmark v1.4.1</code>.</p>"},{"location":"kbs/000020178/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher version 2.3.x</li> <li>Kubernetes version 1.15</li> <li><code>jq</code>, <code>grep</code>, <code>awk</code> and <code>kubectl</code> installed on target node</li> </ul>"},{"location":"kbs/000020178/#steps","title":"Steps","text":"<ol> <li>Clone the script into the target node <code>git clone https://github.com/nickngch/rancher-hardening.git</code></li> <li>Access the folder <code>cd rancher-hardening</code></li> <li>Execute the script based on the node's role</li> <li>For Control Plane - <code>sudo bash ./master.sh 2.3 cp</code></li> <li>For Control Plane + ETCD - <code>sudo bash ./master.sh 2.3 all</code></li> <li>For ETCD - <code>sudo bash ./master.sh 2.3 etcd</code></li> <li>For worker node - <code>sudo ./worker.sh 2.3</code></li> </ol>"},{"location":"kbs/000020178/#limitation","title":"Limitation","text":"<ul> <li>Section 1.6 and 1.7 in master node require manual verification.</li> </ul>"},{"location":"kbs/000020178/#further-reading","title":"Further reading","text":"<p>https://www.cisecurity.org/cis-benchmarks/cis-benchmarks-faq/</p>"},{"location":"kbs/000020178/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020179/","title":"How to conduct performance testing with Clusterloader2","text":"<p>This document (000020179) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020179/#situation","title":"Situation","text":""},{"location":"kbs/000020179/#how-to-conduct-performance-testing-with-clusterloader2_1","title":"How to conduct performance testing with Clusterloader2","text":"<p>Clusterloader is an opensource performance testing tool to measure the performance metrics of your Kubernetes cluster.</p>"},{"location":"kbs/000020179/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Linux or Mac machine that has Golang and kubectl installed</li> <li>SSH key of the Kubernetes master node</li> <li>Kubeconfig file of the target cluster</li> </ul>"},{"location":"kbs/000020179/#steps","title":"Steps","text":"<ol> <li>Create a folder named k8s.io under <code>~/go/src/</code>:</li> </ol> <pre><code>mkdir ~/go/src/k8s.io\n</code></pre> <ol> <li>Clone the perf-test under k8s.io folder:</li> </ol> <pre><code>cd ~/go/src/k8s.io &amp;&amp; git clone https://github.com/galal-hussein/perf-tests.git\n</code></pre> <ol> <li>Navigate to the clusterloader2 directory:</li> </ol> <pre><code>cd ~/go/src/k8s.io/perf-tests/clusterloader2\n</code></pre> <ol> <li>Edit the testconfig according to the environment:</li> </ol> <pre><code>vim testing/load/config.yaml\n</code></pre> <ol> <li>Execute the clusterloader2 with appropriate options:</li> </ol> <pre><code>KUBE_SSH_USER=&lt;SSH USERNAME&gt; LOCAL_SSH_KEY=&lt;SSH KEY PATH&gt; go run cmd/clusterloader.go --nodes 3 --mastername=&lt;MASTER NODE NAME&gt; --kubeconfig=&lt;KUBECONFIG FILE PATH&gt; --provider=local --masterip=&lt;MASTER NODE IP ADDRESS&gt; --testconfig=testing/&lt;TESTING SUBJECT&gt;/config.yaml --report-dir=/tmp/reports 2&gt;&amp;1 | tee /tmp/tmp.log\n</code></pre> <ol> <li>The results of the testing will be stored in the <code>/tmp/reports</code> directory.</li> </ol>"},{"location":"kbs/000020179/#faq","title":"FAQ","text":"<pre><code>Errors: [config reading error: decoding failed: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal number -4611686018427388 into Go struct field Phase.Steps.Phases.ReplicasPerNamespace of type int32]\"\n</code></pre> <ul> <li>Change the value of <code>{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}}</code> to match the number of the nodes in the config.yaml file</li> </ul> <pre><code>level=warning msg=\"Got errors during step execution: [measurement call APIResponsiveness - APIResponsiveness error: unexpected response: \\\"# HELP aggregator_openapi_v2_regeneration_count [ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason.\n</code></pre> <ul> <li>Comment out the APIResponsiveness section in config.yaml:</li> </ul> <pre><code>measurements:\n- Identifier: APIResponsiveness\nMethod: APIResponsiveness\nParams:\naction: reset\n</code></pre>"},{"location":"kbs/000020179/#further-reading","title":"Further reading","text":"<p>https://github.com/kubernetes/perf-tests</p>"},{"location":"kbs/000020179/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020180/","title":"How do I edit my cluster using RKE Templates?","text":"<p>This document (000020180) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020180/#situation","title":"Situation","text":""},{"location":"kbs/000020180/#question","title":"Question","text":"<p>After converting / managing a cluster using RKE Templates, when trying to make changes under \"Edit Cluster\" the 'Edit' button is gone and removed features such as the Kubernetes version dropdown menu. Where did this go?</p>"},{"location":"kbs/000020180/#pre-requisites","title":"Pre-requisites","text":"<p>Kubernetes clusters managed by the RKE Template feature</p>"},{"location":"kbs/000020180/#answer","title":"Answer","text":"<p>If your Kubernetes cluster now has an RKE Template attached, you now need to make changes to your cluster in the RKE Template section</p> <p>Navigate to Global --&gt; Tools ---&gt; RKE Tempates</p> <p>Click the three-dot menu to make a new revision</p> <p>Here is where you will make changes to the cluster configuration and save it as a new version. However it won't take effect immediately.</p> <p>After saving the revision, navigate back to your cluster, click Edit. Under \"Cluster Options\", there will be a drop down menu to select which version of your template you want to use. Select your new version and Save.</p>"},{"location":"kbs/000020180/#further-reading","title":"Further Reading","text":"<ul> <li>https://rancher.com/docs/rancher/v2.x/en/admin-settings/rke-templates/</li> </ul>"},{"location":"kbs/000020180/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020181/","title":"How to configure Okta Auth with Rancher HA","text":"<p>This document (000020181) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020181/#situation","title":"Situation","text":""},{"location":"kbs/000020181/#issue","title":"Issue","text":"<p>When configuring Okta Authentication using the Rancher Official Documentation in a Rancher HA environment you encounter 501 errors when trying to verify and enable the configuration.</p>"},{"location":"kbs/000020181/#cause","title":"Cause","text":"<p>For Rancher to fully enable Okta Authenication it requires a succesful test of your configuration to verify the information is correct. When the test request is sent from one of your Rancher Servers to Okta the returned verification is routed through a Load Balancer to a different Rancher Server in the cluster. As the recipient has not yet been configured to service Okta Authentication it will return a 501 for the request and the Rancher Server that acted as a requester will fail to enable as it could not complete the verification.</p>"},{"location":"kbs/000020181/#resolution","title":"Resolution","text":""},{"location":"kbs/000020181/#assumptions","title":"Assumptions","text":"<p>You have appropriately configured Okta Authentication according to the Rancher Official Documentation.</p>"},{"location":"kbs/000020181/#steps-to-resolve","title":"Steps to Resolve","text":"<ol> <li>Using the Nodes Tab in your Rancher Management Cluster cordon off the nodes you are not currently connected to, this will force traffic to be returned to the Requester.</li> <li>Run the test and enable procedure for Okta Configuration from Rancher and verify you can now login successfully.</li> <li>Uncordon the other Nodes and the settings will be synced across the cluster automatically.</li> <li>Verify the cluster is working as expected by logging in using an Okta sign-in.</li> </ol> <p>(Optional) To verify the settings have been synced to all nodes in the cluster you can cordon off all but another Node, not the one you used to configure, and attempt logging in using Okta. This process can be repeated for each node.</p>"},{"location":"kbs/000020181/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020182/","title":"Rancher pre v1.6.22 \"Hosts stuck Reconnecting\" Rancher server logs show 'Cursor returned more than one result'","text":"<p>This document (000020182) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020182/#situation","title":"Situation","text":""},{"location":"kbs/000020182/#issue","title":"Issue","text":"<p>Hosts getting stuck in either state Reconnecting or Finishing-Reconnect and Rancher server logs include errors like the following: <code>2019-02-26 12:05:55,265 ERROR [51e1303d-21b2-409f-ba2e-7542e8de4941:9663402] [healthcheckInstanceHostMap:445975] [healthcheckinstancehostmap.remove] [] [ecutorService-3] [c.p.e.p.i.DefaultProcessInstanceImpl] Unknown exception org.jooq.exception.InvalidResultException: Cursor returned more than one result</code></p>"},{"location":"kbs/000020182/#pre-requisites","title":"Pre-requisites","text":"<p>Rancher version lower than 1.6.22</p>"},{"location":"kbs/000020182/#workaround","title":"Workaround","text":"<ol> <li>In the Rancher MySQL database, find all the duplicates by checking column 3 for entries with more than a count of 1 in the return from the following query:</li> </ol> <pre><code>select host_id,healthcheck_instance_id,count(*) from healthcheck_instance_host_map where removed is null group by host_id,healthcheck_instance_id order by 3;\n</code></pre> <ol> <li>For each healthcheck_instance_id in any row with more than 1 in column 3, run the following command:</li> </ol> <pre><code>update healthcheck_instance_host_map set state='removed', removed=now(), remove_time=now() where healthcheck_instance_id='&lt;INSERT_HEALTHCHECK_ID&gt;';\n</code></pre> <ol> <li>Wait and watch the hosts view. The hosts should all finish reconnecting and instances should update.</li> </ol>"},{"location":"kbs/000020182/#resolution","title":"Resolution","text":"<p>Upgrade to 1.6.22+ or 2.x</p>"},{"location":"kbs/000020182/#further-reading","title":"Further reading","text":"<p>https://github.com/rancher/rancher/issues/15284</p>"},{"location":"kbs/000020182/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020183/","title":"Admins cannot edit or see node templates created by another user in Rancher v2.0.0-v2.3.2.","text":"<p>This document (000020183) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020183/#situation","title":"Situation","text":""},{"location":"kbs/000020183/#issue","title":"Issue","text":"<p>Admins cannot edit or see node templates created by another user in Rancher v2.0.0 to v2.3.2. As a result, they are also unable to edit a cluster with a user that did not create it, even if that user is an admin.</p>"},{"location":"kbs/000020183/#workaround","title":"Workaround","text":""},{"location":"kbs/000020183/#change-node-template-owner","title":"Change node template owner","text":"<p>This script will change your node template owner in Rancher 2.x. You can run this script as a Docker image or directly as a bash script. You'll need the cluster ID and the user ID you want to change the ownership to.</p> <ol> <li>To obtain the cluster ID in the Rancher user interface, Navigate to Global &gt; \"Your Cluster Name\", then grab the cluster ID from your address bar. I have listed an example of the URL and a cluster ID derrived from the URL below.</li> </ol> <p>- Example URL: <code>https://&lt;RANCHER URL&gt;/c/c-48x9z/monitoring</code>     - Derived cluster ID from above URL: c-48x9z</p> <ol> <li> <p>Now we need the user ID of the user to become the new node template owner, navigate to Global &gt; Users to find the ID.</p> </li> <li> <p>To run the script using a docker image, make sure your $KUBECONFIG is set to the full path of your Rancher local cluster kube config then run the following command.</p> </li> </ol> <pre><code>docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -c &lt;cluster-id&gt; -n &lt;user-id&gt;\n</code></pre> <ol> <li>To run the script directly, just download the change-nodetemplate-owner.sh script, make sure your $KUBECONFIG or ~/.kube/config is pointing to the correct Rancher local cluster then run the following command:</li> </ol> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh\nbash change-nodetemplate-owner.sh -c &lt;cluster-id&gt; -n &lt;user-id&gt;\n</code></pre>"},{"location":"kbs/000020183/#assign-a-node-template-to-a-clusters-node-pool","title":"Assign a node template to a cluster's node pool","text":"<p>Assign a node template to a cluster's node pool. This is useful for situations where the original owner of a cluster has been deleted which also deletes their node templates. To use this task successfully it is recommended that you create a new node template in the UI before using it. Make sure the node template matches the original ones as closely as possible. You will be shown options to choose from and prompted for confirmation.</p> <p>Run script with docker image:</p> <pre><code>docker run -ti -v $KUBECONFIG:/root/.kube/config patrick0057/change-nodetemplate-owner -t changenodetemplate -c &lt;cluster-id&gt;\n</code></pre> <p>Run script from bash command line:</p> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/change-nodetemplate-owner/change-nodetemplate-owner.sh\nbash change-nodetemplate-owner.sh -t changenodetemplate -c &lt;cluster-id&gt;\n</code></pre>"},{"location":"kbs/000020183/#resolution","title":"Resolution","text":"<p>Upgrade to Rancher v2.3.3 or newer to receive the fix to this issue. More information on this bug can be found at the GitHub issue #12186.</p>"},{"location":"kbs/000020183/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020184/","title":"When will my cluster certificates expire?","text":"<p>This document (000020184) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020184/#situation","title":"Situation","text":""},{"location":"kbs/000020184/#question","title":"Question","text":"<p>I have a cluster which is displaying the message \"This cluster has certs that are expiring or have expired\". Is there any way to determine when the certs will expire?</p>"},{"location":"kbs/000020184/#pre-requisites","title":"Pre-requisites","text":"<p>Kubernetes clusters provisioned via the RKE, or Rancher launched Kubernetes clusters</p>"},{"location":"kbs/000020184/#answer","title":"Answer","text":"<p>Dates on when particular certificates will expire are located in the Rancher API.</p> <p>To view them, from the \"Clusters\" page, click the three-dot menu on your cluster and \"View in API\" Scroll down or search for the section called \"certificatesExpiration\" There you can see the expiration date for each kubernetes component</p> <p>FYI, as of Rancher 2.3.5, there is a known issue that old, removed nodes still appear in this list. This is cosmetic and won't affect your running cluster: https://github.com/rancher/rancher/issues/24333</p>"},{"location":"kbs/000020184/#further-reading","title":"Further Reading","text":"<ul> <li>https://rancher.com/blog/2019/kubernetes-certificate-expiry-and-rotation-in-rancher-kubernetes-clusters</li> </ul>"},{"location":"kbs/000020184/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020185/","title":"Why does a cluster or node show requested memory with a milli (m) unit?","text":"<p>This document (000020185) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020185/#situation","title":"Situation","text":""},{"location":"kbs/000020185/#question","title":"Question","text":"<p>Why does a cluster or node show requested memory with a milli (m) unit?</p>"},{"location":"kbs/000020185/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020185/#answer","title":"Answer","text":"<p>CPU resources in Kubernetes are measured in millicpus, or 1/1000th of a CPU core, and 1 CPU Core = 1000m. The API will change any request for a decimal point into millicpus. For example 0.1 is converted into 100m. One hyperthread is considered one core, or 1000m.</p> <p>Memory resources in Kubernetes are mesured in bytes, and can be expressed as an integer with one of these suffixes: E, P, T, G, M, K - decimal suffixes, or Ei, Pi, Ti, Gi, Mi, Ki - binary suffixes (more commonly used for memory), or omit the suffix altogether. Lowercase \"m\" notation is not a recommended suffix for memory.</p> <p>The \"m\" notation for memory might indicate a misconfiguration, where CPU units are recommended to use that suffix (example: 200m), and Memory units are recommended to use \"Mi\" (example: 128Mi).</p>"},{"location":"kbs/000020185/#further-reading","title":"Further Reading","text":"<p>Explaination of Kubernetes CPU resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu ) Explaination of Kubernetes memory resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory )</p>"},{"location":"kbs/000020185/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020186/","title":"How To Update CoreDNS's Resolver Policy","text":"<p>This document (000020186) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020186/#situation","title":"Situation","text":""},{"location":"kbs/000020186/#task","title":"Task","text":"<p>This article outlines how to change CoreDNS's forward resolver policy.</p>"},{"location":"kbs/000020186/#pre-requisites","title":"Pre-requisites","text":"<p>A custom cluster provisioned by Rancher or RKE with CoreDNS.</p> <p>The CoreDNS docs explain the various configurations. In this case, we are concerned with the policy. Which defaults to <code>random</code>.</p>"},{"location":"kbs/000020186/#resolution","title":"Resolution","text":"<p>To change the policy to <code>sequential</code>, edit your clusters yaml. For RKE provisioned clusters this will be your <code>cluster.yaml</code> and for Rancher provisioned custom clusters this will be found by editing the cluster.</p> <p>For RKE add the following to the end of the file, but for Rancher provisioned clusters, nest this in the <code>rancher_kubernetes_engine_config</code> section.</p> <pre><code>addons: |-\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: coredns\nnamespace: kube-system\ndata:\nCorefile: |\n.:53 {\nerrors\nhealth\nready\nkubernetes cluster.local in-addr.arpa ip6.arpa {\npods insecure\nfallthrough in-addr.arpa ip6.arpa\n}\nprometheus :9153\nforward . \"/etc/resolv.conf\" {\npolicy sequential\n}\ncache 30\nloop\nreload\nloadbalance\n}\n</code></pre> <p>The lines of note here are the following, which are changed from just <code>forward . \"/etc/resolv.conf\"</code>.</p> <pre><code>          forward . \"/etc/resolv.conf\" {\n            policy sequential\n          }\n</code></pre> <p>At this point you should be able to just hit save in Rancher or run <code>rke up</code> and the change will be pushed to the cluster.</p>"},{"location":"kbs/000020186/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020187/","title":"How to update your etcd space alerts for better etcd monitoring","text":"<p>This document (000020187) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020187/#situation","title":"Situation","text":""},{"location":"kbs/000020187/#task","title":"Task","text":"<p>An alert you may have seen is <code>Database usage close to the quota 500M</code> in your Rancher 2.x cluster.</p> <p>This is a default etcd alert built into Rancher, you can find more info on the default alerts here: Rancher v2.x Default Alerts</p> <p>Upon further examination of the alert, you see the description of</p> <p>A warning alert is triggered when the size of etcd exceeds 500M.</p> <p>This alert is somewhat misleading as the default etcd size is 2GB. This alert is also at a severity level of Warning. Below, we suggest configuring your etcd alerts to better utilize the alert thresholds and to avoid the default alert constantly going off.</p>"},{"location":"kbs/000020187/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> </ul>"},{"location":"kbs/000020187/#resolution","title":"Resolution","text":"<p>You can access the alerts by going to <code>Tools -&gt; Alerts</code> at the cluster level. From here, clone the default alert three times. Once the 3 clones are created, disable the default alert so you always retain a clean reference alert.</p> <p>Alert for 1GB usage - INFO</p> <p>Alert for 1.5GB usage - WARNING</p> <p>Alert for 1.75GB usage - CRITICAL</p>"},{"location":"kbs/000020187/#if-you-are-running-out-of-space-please-reference-the-following-two-links-there-will-be-an-upcoming-kb-article-walking-through-the-resize-process-for-etcd","title":"If you are running out of space, please reference the following two links. There will be an upcoming KB article walking through the resize process for etcd.","text":"<p>Rancher v2.x etcd options</p> <p>etcd space quota documentation</p>"},{"location":"kbs/000020187/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020188/","title":"The RancherOS log collector script","text":"<p>This document (000020188) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020188/#situation","title":"Situation","text":""},{"location":"kbs/000020188/#rancheros-log-collection","title":"RancherOS log collection","text":"<p>Host logs and information can be collected from a node running RancherOS using the RancherOS log collector script.</p> <p>The script needs to be downloaded and run directly on the host using sudo, as follows:</p> <pre><code>wget https://raw.githubusercontent.com/rancher/os/master/scripts/tools/collect_rancheros_info.sh | sudo sh\n</code></pre> <p>The output will be written to <code>/tmp</code> to a tar file named <code>rancheros_export_&lt;datetime&gt;.tar</code></p>"},{"location":"kbs/000020188/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020189/","title":"How to test websocket connections to Rancher v2.x","text":"<p>This document (000020189) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020189/#situation","title":"Situation","text":""},{"location":"kbs/000020189/#task","title":"Task","text":"<p>Rancher depends heavily on websocket support for UI and CLI features within Rancher as well as managing and interacting with downstream clusters. This article provides a quick test to determine if websocket connections are working from a potential downstream node or client to the Rancher server cluster.</p>"},{"location":"kbs/000020189/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster.</li> </ul>"},{"location":"kbs/000020189/#executing-the-test","title":"Executing the test","text":"<p>First you will need to create an API token to authenticate against Rancher. Start by logging into the Rancher UI. Once logged in, navigate to the API &amp; Keys section by clicking the user icon in the top right of the pane, then click on the API &amp; Keys menu item. Generate a new key by clicking the Add Key button, providing a name for the token and clicking Create. Copy the bearer token to a safe location.</p> <p>In a Linux shell from the desired test node execute the following, substituting the bearer token and fully qualified domain name of your Rancher endpoint with these environmental variables:</p> <pre><code>export TOKEN=&lt;your token here&gt;\nexport FQDN=&lt;your Rancher fully qualified domain name here&gt;\n</code></pre> <p>Next execute the test using the following command:</p> <pre><code>curl -s -i -N \\\n--http1.1 \\\n-H \"Connection: Upgrade\" \\\n-H \"Upgrade: websocket\" \\\n-H \"Sec-WebSocket-Key: SGVsbG8sIHdvcmxkIQ==\" \\\n-H \"Sec-WebSocket-Version: 13\" \\\n-H \"Authorization: Bearer $TOKEN\" \\\n-H \"Host: $FQDN\" \\\n-k https://$FQDN/v3/subscribe\n</code></pre> <p>If websockets work this will successfully connect to the Rancher server and print a steady stream of json output reflecting configuration items being sent from the server. In the event of a failed connection this should print a meaningful error you can act upon to get websockets working between your client and Rancher server.</p> <p>The below is an example of the output from the test upon a successfully established websocket:</p> <pre><code>HTTP/1.1 101 Switching Protocols\nDate: Tue, 21 Jan 2020 04:54:05 GMT\nConnection: upgrade\nServer: openresty/1.15.8.1\nUpgrade: websocket\nSec-WebSocket-Accept: qGEgH3En71di5rrssAZTmtRTyFk=\n\n{\"name\":\"resource.change\",\"data\":{\"baseType\":\"listenConfig\",\"created\":\"2020-01-04T22:34:26Z\",\"createdTS\":1578177266000,\"creatorId\":null,\"enabled\":true,\"generatedCerts\":{\"local/10.42.0.7\":\"*CERT_CONTENTS_REDACTED*\"},\"id\":\"cli-config\",\"keySize\":0,\"knownIps\":[\"10.42.0.7\",\"10.42.0.8\"],\"labels\":{\"cattle.io/creator\":\"norman\"},\"links\":{\"remove\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"self\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\",\"update\":\"https://yourdomain.example.com/v3/listenConfigs/cli-config\"},\"mode\":\"https\",\"tos\":\"auto\",\"type\":\"listenConfig\",\"uuid\":\"511129ca-aa2c-4d16-a8e5-2d77cb171d61\",\"version\":0}\n</code></pre>"},{"location":"kbs/000020189/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020190/","title":"The Rancher v1.6 log collector script","text":"<p>This document (000020190) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020190/#situation","title":"Situation","text":""},{"location":"kbs/000020190/#rancher-v16-log-collection","title":"Rancher v1.6 log collection","text":"<p>Logs can be collected from a node within a Rancher v1.6 cluster using the Rancher v1.6 log collector script.</p> <p>The script needs to be downloaded and run directly on the host using the root user or using sudo, as follows:</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v1.6/logs-collector/rancher16_logs_collector.sh | sudo bash -s\n</code></pre> <p>The output will be written to <code>/tmp</code> to a gziped tar file named <code>&lt;hostname&gt;-&lt;datetime&gt;.tar.gz</code></p>"},{"location":"kbs/000020190/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020191/","title":"The Rancher v2.x Linux log collector script","text":"<p>This document (000020191) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020191/#situation","title":"Situation","text":""},{"location":"kbs/000020191/#rancher-v2x-linux-log-collector","title":"Rancher v2.x Linux log collector","text":"<p>Logs can be collected from a Linux node using the Rancher v2.x log collector script.</p> <p>Note: This script is intended to collect logs from Rancher Kubernetes Engine (RKE) CLI provisioned clusters, K3s clusters, RKE2 clusters, Rancher provisioned Custom, and Node Driver clusters.</p> <p>This script may not collect all necessary information when run on nodes in Hosted Kubernetes Provider clusters.</p> <p>The script needs to be downloaded and run directly on the node, using the root user or sudo.</p> <p>Output will be written to <code>/tmp</code> as a tar.gz archive named <code>&lt;hostname&gt;-&lt;date&gt;.tar.gz</code>, the default output directory can be changed with the <code>-d</code> flag.</p>"},{"location":"kbs/000020191/#download-and-run-the-script","title":"Download and run the script","text":"<ul> <li>Download the script as: <code>rancher2_logs_collector.sh</code></li> </ul> <p>Using <code>wget</code>:</p> <pre><code>wget https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh\n</code></pre> <p>Using <code>curl</code>:</p> <pre><code>curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh\n</code></pre> <ul> <li>Run the script:</li> </ul> <pre><code>sudo bash rancher2_logs_collector.sh\n</code></pre>"},{"location":"kbs/000020191/#optional-download-and-run-the-script-in-one-command","title":"Optional: Download and run the script in one command","text":"<pre><code>curl -Ls rnch.io/rancher2_logs | sudo bash\n</code></pre> <p>Note: This command requires curl to be installed, and internet access from the node.</p>"},{"location":"kbs/000020191/#options","title":"Options","text":"<p>The available flags that can be passed to the script can be found in the Rancher v2.x log collector script README.</p>"},{"location":"kbs/000020191/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020192/","title":"The Rancher v2.x systems summary script","text":"<p>This document (000020192) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020192/#situation","title":"Situation","text":""},{"location":"kbs/000020192/#rancher-v2x-systems-summary","title":"Rancher v2.x systems summary","text":"<p>Understanding your cluster/node distribution on an on-going basis assists Rancher in sending you any prescriptive advisories related to scale and performance.</p> <p>System information can be collected from a Rancher v2.x server node using the Rancher v2.x systems summary script.</p> <p>The script needs to be downloaded and run directly on a host running a Rancher server container, either as a single node install or a Rancher Pod as part of a High Availability install. The script needs to be run by a user with access to the Docker socket or using sudo, as follows:</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sudo bash -s\n</code></pre>"},{"location":"kbs/000020192/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020193/","title":"What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?","text":"<p>This document (000020193) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020193/#situation","title":"Situation","text":""},{"location":"kbs/000020193/#question","title":"Question","text":"<p>What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?</p>"},{"location":"kbs/000020193/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.0.x - v2.3.x. Note, Kubernetes upgrades will be changing in v2.4.x, see Further Reading below.</li> </ul> <p>OR</p> <ul> <li>RKE CLI v0.2.x+</li> </ul>"},{"location":"kbs/000020193/#answer","title":"Answer","text":"<p>Rancher, either through the UI or API, can be used to upgrade a Kubernetes cluster that was provisioned using the \"Custom\" option or on cloud infrastructure such as AWS EC2 or Azure. This can be accomplished by editing the cluster and selecting the desired Kubernetes version. Clusters provisioned with the RKE CLI can also be upgraded by editing the kubernetes_version key in the cluster YAML file. This will trigger an update of all the Kubernetes components in the order listed below:</p>"},{"location":"kbs/000020193/#etcd-plane","title":"Etcd plane","text":"<p>Each etcd container is updated, one node at a time. If the etcd version has not changed between versions of Kubernetes, no action is taken. The process consists of:</p> <ol> <li>Downloading etcd image</li> <li>Stopping and renaming old etcd container (backend datastore is preserved on host)</li> <li>Creating and starting new etcd container</li> <li>Running etcd health check</li> <li>Removing old etcd container</li> </ol> <p>For RKE CLI provisioned clusters, the etcd-rolling-snapshot container is also upgraded if a new version is available.</p>"},{"location":"kbs/000020193/#control-plane","title":"Control plane","text":"<p>Every Kubernetes update will require the control plane components to be updated. All control plane nodes are updated in parallel. The process consists of:</p> <ol> <li>Downloading hyperkube image, which is used by all control plane components.</li> <li>Stopping and renaming old kube-apiserver container</li> <li>Creating and starting new kube-apiserver container</li> <li>Running kube-apiserver health check</li> <li>Removing old kube-apiserver container</li> <li>Stopping and renaming old kube-controller-manager container</li> <li>Creating and starting new kube-controller-manager container</li> <li>Running kube-controller-manager health check</li> <li>Removing old kube-controller-manager container</li> <li>Stopping and renaming old kube-scheduler container</li> <li>Creating and starting new kube-scheduler container</li> <li>Running kube-scheduler health check</li> <li>Removing old kube-scheduler container</li> </ol>"},{"location":"kbs/000020193/#worker-plane","title":"Worker plane","text":"<p>Every Kubernetes update will require the worker components to be updated. These components run on all nodes, including the control plane and etcd. Nodes are updating in parallel. The process consists of:</p> <ol> <li>Downloading hyperkube image (if not already present)</li> <li>Stopping and renaming old kubelet container</li> <li>Creating and starting new kubelet container</li> <li>Running kubelet health check</li> <li>Removing old kubelet container</li> <li>Stopping and renaming old kube-proxy container</li> <li>Creating and starting new kube-proxy container</li> <li>Running kube-proxy health check</li> <li>Removing old kube-proxy container</li> </ol>"},{"location":"kbs/000020193/#addons-user-workloads","title":"Addons &amp; user workloads","text":"<p>Once Kubernetes etcd, control plane, and worker components have been updated, the latest manifests for addons are applied. This includes, but is not limited to KubeDNS/CoreDNS, Nginx Ingress, Metrics Server, and CNI plugin (Calico, Weave, Flannel, Canal). Depending on the manifest deltas and the upgrade strategy defined in the manifest, pods and their corresponding containers may or may not be removed and recreated. Please be aware that some of these addons are critical for your cluster to operator correctly and you may experience brief outages if these workloads are restarted. For example, when KubeDNS/CoreDNS is restarted, you could have issues resolving hostname to IP addresses. When the Nginx Ingress is restarted, layer 7 http/https traffic from outside your cluster to your workloads may get interrupted. When your CNI plugin is restarted on each node, the workloads running on the node may temporarily not be able to reach workloads running on other nodes. The best way to minimize outages or disruptions is to make sure you have proper fault tolerance in your cluster.</p> <p>The kubelet automatically destroys and recreates all user workload pods when the spec hash value is changed. This value will change for a pod if the Kubernetes upgrade involves any field changes in the pod manifest, such as a new field or the removal of a deprecated field. As a best practice, it's best to assume all your pods and containers will be destroyed and recreated during a Kubernetes upgrade. This is more likely to happen for major/minor releases and less likely for patch releases.</p>"},{"location":"kbs/000020193/#further-reading","title":"Further Reading","text":"<p>Upgrade refactor in v2.4: https://github.com/rancher/rancher/issues/23038</p> <p>Kubeadm upgrades: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p>"},{"location":"kbs/000020193/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020194/","title":"What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?","text":"<p>This document (000020194) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020194/#situation","title":"Situation","text":""},{"location":"kbs/000020194/#question","title":"Question","text":"<p>What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?</p>"},{"location":"kbs/000020194/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x HA deployed using Helm.</li> </ul>"},{"location":"kbs/000020194/#answer","title":"Answer","text":"<p>The bulk of the Rancher HA installation and upgrade are performed by using Helm. The core piece of the Rancher Helm Chart is the Rancher deployment. Please note the following characteristics of this Helm Chart:</p> <ul> <li>Deployment is set to a replica of 3. This means Kubernetes will attempt to run and maintain three rancher pods.</li> <li>Deployment is set to do a rolling update with a max surge of 25% and max unavailability of 25%. This means:</li> <li>During an upgrade, pods are updated in chunks, not all at once.</li> <li>During an update, no more than 4 pods will be running at once</li> <li>During an update, no fewer than 2 pods will be available at once</li> <li>Deployment has an anti-affinity for the node's hostname. This means Kubernetes will attempt to place each pod on a separate host. For three pods and three hosts, that means one pod on each host.</li> </ul> <p>Rancher will also apply two other important manifests to the Rancher HA cluster as well as all managed clusters. These are described below:</p>"},{"location":"kbs/000020194/#cattle-cluster-agent-deployment","title":"cattle-cluster-agent deployment","text":"<ul> <li>Deployment is set to a replica of 1</li> <li>Deployment is set to do a rolling update with a max surge of 25% and a max unavailability of 25%. See Rancher's deployment description above for the behavior of these settings.</li> </ul>"},{"location":"kbs/000020194/#cattle-node-agent-daemonset","title":"cattle-node-agent daemonset","text":"<ul> <li>Daemonset will deploy one agent per node</li> <li>Daemonset is set to a rolling update with max unavailable of 1 pod. That means during an update, one pod is updated at a time.</li> </ul> <p>Given the information above on how the manifests are defined, below is the expected sequence of events during a Rancher upgrade:</p>"},{"location":"kbs/000020194/#rancher-ha-cluster","title":"Rancher HA cluster","text":"<ol> <li>A new rancher pod is created</li> <li>An old rancher pod is terminated</li> <li>A new second rancher pod is created</li> <li>A second old rancher pod is terminated</li> <li>A new third rancher pod is created</li> <li>A third old rancher pod is terminated</li> <li>The latest versions of the cattle-cluster-agent and cattle-node-agent manifests are updated and deployed on the cluster. These deployments are triggered in parallel and will result in a new cattle-cluster-agent and new cattle-node-agents running on the cluster.</li> </ol>"},{"location":"kbs/000020194/#downstream-clusters","title":"Downstream clusters","text":"<p>Once Rancher is upgraded, Rancher will check each cluster it manages to make sure the cattle-cluster-agent and cattle-node-agents are up to date. If the cluster is not in a \"Provisioning\" state, meaning another cluster update is in progress, it will deploy the latest cattle-cluster-agent and cattle-node-agent manifests into the cluster. All managed clusters are updated in parallel and not sequentially.</p> <p>Other workloads running in the cluster should not be impacted.</p>"},{"location":"kbs/000020194/#further-reading","title":"Further Reading","text":"<p>Kubernetes deployments - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</p> <p>Kubernetes daemonsets - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</p>"},{"location":"kbs/000020194/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020195/","title":"How to rollback the Kubernetes version of a Rancher v2.x provisioned cluster","text":"<p>This document (000020195) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020195/#situation","title":"Situation","text":""},{"location":"kbs/000020195/#task","title":"Task","text":"<p>This article details how to rollback the Kubernetes version of a Rancher v2.x provisioned cluster.</p>"},{"location":"kbs/000020195/#important-note","title":"Important Note:","text":"<p>A Kubernetes Cluster Rollback will most definitely cause downtime in the cluster, as you are restoring a snapshot from before the upgrade and the cluster will have to reconcile state.</p>"},{"location":"kbs/000020195/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>In order to rollback your Kubernetes cluster version upgrade, you need to have first taken an etcd snapshot from before the upgrade. You should keep the reference to the snapshot name that was created as your \"pre-upgrade\" snapshot. In my cluster which has cluster ID: <code>c-q8st7</code> , my snapshot name was <code>c-q8st7-ml-qdxdh</code>. Our example upgrade is from <code>v1.14.9-rancher1-2</code> to <code>v1.15.7-rancher1-1</code>.</li> </ul>"},{"location":"kbs/000020195/#rollback-operation","title":"Rollback operation","text":"<p>In order to rollback, you must:</p> <ol> <li>Edit Cluster</li> <li>Edit as YAML</li> <li>Set <code>kubernetes_version</code> back to <code>v1.14.9-rancher1-2</code> (or whatever your desired restore version is)</li> <li>Find the <code>restore</code> key in the YAML.</li> </ol> <p>You will need to update the following configuration:</p> <pre><code>rancher_kubernetes_engine_config:\nrestore:\nrestore: false\n</code></pre> <p>You'll want to closely model the following:</p> <pre><code>rancher_kubernetes_engine_config:\nrestore:\nrestore: true\nsnapshot_name: \"c-q8st7:c-q8st7-ml-qdxdh\"\n</code></pre> <p>Note the <code>snapshot_name</code> has the cluster ID prefixed to it with a <code>:</code> .</p> <ol> <li>Finally, you can save the cluster, and observe the snapshot restore + K8s version rollback occur.</li> </ol>"},{"location":"kbs/000020195/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020196/","title":"HTTP 401 \"clusterID does not match\" error using cluster-scoped Rancher API token in Rancher v2.x","text":"<p>This document (000020196) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020196/#situation","title":"Situation","text":""},{"location":"kbs/000020196/#issue","title":"Issue","text":"<p>When attempting to perform operations against the Rancher v2.x API, with a cluster-scoped API token, you receive a HTTP 401 response code with a body of the following format:</p> <pre><code>{\n\"type\":\"error\",\n\"status\":\"401\",\n\"message\":\"clusterID does not match\"\n}\n</code></pre>"},{"location":"kbs/000020196/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>A cluster-scoped Rancher API token</li> </ul>"},{"location":"kbs/000020196/#root-cause","title":"Root cause","text":"<p>The primary purpose of cluster-scoped API tokens is to permit access to the Kubernetes API for a specific cluster via Rancher, i.e. via the endpoint <code>https://&lt;rancher_url&gt;/k8s/clusters/&lt;cluster_id&gt;</code> for the matching cluster. Cluster-scoped tokens can be used to interact directly with the Kubernetes API of clusters configured with an Authorized Cluster Endpoint.</p> <p>In addition, a cluster-scoped token also works for resources under the Rancher v3 API endpoint for that cluster, at <code>https://&lt;rancher_url&gt;/v3/clusters/&lt;cluster_id&gt;</code>.</p> <p>The token is not valid for the other available API endpoints, nor for other clusters. Attempts to perform API operations on other clusters or endpoints with a cluster-scoped token will result in the HTTP 401 <code>\"clusterID does not match\"</code> error.</p>"},{"location":"kbs/000020196/#resolution","title":"Resolution","text":"<p>Only use a cluster-scoped API token where you wish to restrict usage of the token to the Kubernetes API for that cluster, or the Rancher v3 cluster endpoint. To permit access to other API endpoints, or to use a token for API access to multiple clusters, create a Rancher API token that is not cluster-scoped.</p>"},{"location":"kbs/000020196/#further-reading","title":"Further reading","text":"<p>You can read more on the Rancher v2.x API within the API documentation.</p>"},{"location":"kbs/000020196/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020197/","title":"Provisioning of Kubernetes clusters in Rancher v2.x, prior to v2.3.3, using nodes in an infrastructure provider, does not respect NO_PROXY entries in CIDR format","text":"<p>This document (000020197) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020197/#situation","title":"Situation","text":""},{"location":"kbs/000020197/#issue","title":"Issue","text":"<p>Attempting to provision a Kubernetes cluster with the vSphere node-driver, in a Rancher v2.x environment, prior to v2.3.3, using a HTTP proxy configuration results in an error of the following format:</p> <pre><code>Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&amp;token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect\n</code></pre> <p>In addition, the Rancher logs show an error message of the following format:</p> <pre><code>...\n2019/12/06 10:23:51 [INFO] [node-controller-docker-machine] (vsphere-all1) Waiting for VMware Tools to come online...\n2019/12/06 10:25:49 [INFO] [node-controller-docker-machine] (vsphere-all1) Provisioning certs and ssh keys...\n2019/12/06 10:27:35 http: TLS handshake error from 127.0.0.1:41746: EOF\n2019/12/06 10:28:03 [INFO] [node-controller-docker-machine] The default lines below are for a sh/bash shell, you can specify the shell you're using, with the --shell flag.\n2019/12/06 10:28:03 [INFO] [node-controller-docker-machine]\n2019/12/06 10:28:04 [INFO] Generating and uploading node config vsphere-all1\n2019/12/06 10:28:04 [ERROR] NodeController c-f6xbs/m-fsl6t [node-controller] failed with : Error creating machine: Error in driver during machine creation: Put https://172.16.2.13:443/guestFile?id=1600&amp;token=528090dd-cf9d-3973-b08b-d1782fd80bd21600: Unable to connect\n...\n</code></pre>"},{"location":"kbs/000020197/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, prior to Rancher v2.3.3.</li> <li>A HTTP Proxy configured on Rancher, per the documentation for a single node or High Availability (HA) install of Rancher, in which the vSphere datacenter ESXi hosts are not reachable via the proxy.</li> <li>A Rancher provisioned Kubernetes cluster, using the the vSphere node-driver.</li> <li>The IP Range containing the ESXi hosts within the vSphere datacenter configured in CIDR notation within the Rancher NO_PROXY configuration.</li> </ul>"},{"location":"kbs/000020197/#root-cause","title":"Root cause","text":"<p>This issue was caused by the Go version used to build the docker-machine driver that provides the Rancher node driver capabilities, including the vSphere node driver.</p> <p>Support for NO_PROXY entries in CIDR notation was introduced in Go v1.10.x; however, the docker-machine version in Rancher v2.x, prior to v2.3.3, was built using an earlier version of Go.</p> <p>As a result, NO_PROXY entries in CIDR notation did not take effect during cluster provisioning via node drivers, even though these same NO_PROXY entries were observed by the Rancher server itself, built with a later version of Go.</p>"},{"location":"kbs/000020197/#workaround","title":"Workaround","text":"<p>To workaround this issue in Rancher v2.x versions before v2.3.3, you should ensure that the vSphere server address, and all ESXi hosts within the vSphere datacenter in which you are provisioning the cluster, are listed as individual IPs within the Rancher NO_PROXY configuration.</p>"},{"location":"kbs/000020197/#resolution","title":"Resolution","text":"<p>This issue was tracked in Rancher GitHub issue #21674 and a fix, bumping the Go version of the docker-machine driver to v1.12.9, was released in Rancher v2.3.3. Users can therefore upgrade to Rancher v2.3.3, or above, to resolve this issue.</p>"},{"location":"kbs/000020197/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020198/","title":"Launching kubectl for cluster within Rancher UI fails in a cluster after following the CIS Benchmark Hardening Guide for Kubernetes","text":"<p>This document (000020198) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020198/#situation","title":"Situation","text":""},{"location":"kbs/000020198/#issue","title":"Issue","text":"<p>Attempting to launch kubectl in the Rancher v2.x UI, for a cluster upon which the Rancher CIS Hardening Guide has been applied, results in a <code>Closed Code: 1006</code> message. Further, using the browser developer tools to inspect requests when opening this page reveals the API request to initiate the connection (https:///v3/clusters/?shell=true) receiving a HTTP 403 response.</p>"},{"location":"kbs/000020198/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE CLI or Rancher v2.x launched Kubernetes cluster, with the Rancher v2.1.x, v2.2.x or v2.3.x CIS Hardening Guide applied.</li> </ul>"},{"location":"kbs/000020198/#root-cause","title":"Root cause","text":"<p>This behaviour is caused by CIS Control 1.1.12, which specifies that the DenyEscalatingExec Admission Controller should be enabled on the Kubernetes API Server.</p> <p>The terminal for the Rancher UI is provided by exec'ing into a cattle-node-agent Pod, whilst Pods within this DaemonSet run in Privileged mode. As a result the exec to open the terminal session is denied by the DenyEscalatingExec Admission Controller.</p>"},{"location":"kbs/000020198/#workaround","title":"Workaround","text":"<p>You can workaround the issue by removing <code>DenyEscalatingExec</code> from the list of <code>enable-admission-plugins</code> in <code>extra_args</code> for the <code>kube-api</code> service.</p>"},{"location":"kbs/000020198/#resolution","title":"Resolution","text":"<p>This issue is tracked in the Rancher GitHub issue #19439.</p>"},{"location":"kbs/000020198/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020199/","title":"How to increase the log level of Kubernetes components in an RKE CLI or Rancher provisioned Kubernetes cluster","text":"<p>This document (000020199) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020199/#situation","title":"Situation","text":""},{"location":"kbs/000020199/#task","title":"Task","text":"<p>When troubleshooting an issue with an RKE CLI or Rancher provisioned Kubernetes cluster, it may be helpful to increase the verbosity of logging on one or more of the Kubernetes components, above the default level. This article details the process of increasing logging on both those components that use the Kubernetes hyperkube image (kubelet, kube-apiserver, kube-controller-manager, kube-scheduler, kube-proxy) as well as the etcd component.</p>"},{"location":"kbs/000020199/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the RKE CLI or Rancher v2.x</li> </ul>"},{"location":"kbs/000020199/#resolution","title":"Resolution","text":""},{"location":"kbs/000020199/#kubernetes-api-server-controller-manager-scheduler-kube-proxy-and-kubelet","title":"Kubernetes API Server, Controller Manager, Scheduler, Kube Proxy and Kubelet","text":"<p>The Kubernetes core components, which run using the Kubernetes hyperkube image, will log ERROR, WARNING and INFO messages. The verbosity of the INFO level log output is controlled by the <code>--v</code> flag, which is set to an integer from 0 to 9. In an RKE CLI or Rancher launched Kubernetes cluster, the <code>--v</code> flag is configured to <code>2</code> by default. At this level, the components will log <code>useful steady state information about the service and important log messages that may correlate to significant changes in the system.</code></p> <p>In order to troubleshoot an issue, it may be useful to increase the verbosity flag to one of the following:</p> Verbosity Description --v=3 Extended information about changes. --v=4 Debug level verbosity. --v=6 Display requested resources. --v=7 Display HTTP request headers. --v=8 Display HTTP request contents. --v=9 Display HTTP request contents without truncation of contents."},{"location":"kbs/000020199/#update-the-v-flag-in-an-rke-cli-launched-cluster","title":"Update the <code>--v</code> flag in an RKE CLI launched cluster","text":"<ol> <li>First set the <code>--v</code> flag for the desired components within the <code>cluster.yml</code>. For each of the services you wish to change the verbosity on, you should add an extra_args option with <code>v: \"&lt;value&gt;\"</code> in the services block, per the example below. The appropriate name for each service within this block can be found within the RKE documentation. N.B. Please see the separate section below for updating the log verbosity of the etcd component</li> </ol> <pre><code>     services:\nkube-api:\nextra_args:\nv: '9'\n</code></pre> <ol> <li>Having set the flag in the cluster.yml, run <code>rke up --config cluster.yml</code> to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020199/#update-the-v-flag-in-a-rancher-launched-cluster","title":"Update the <code>--v</code> flag in a Rancher launched cluster","text":"<p>Navigate to the cluster within the Rancher UI and click <code>Edit Cluster</code>, then <code>Edit as YAML</code>. For each of the services you wish to change the verbosity on, you should add an extra_args option with <code>v: \"&lt;value&gt;\"</code> in the services block of the cluster, per the example below.</p> <p>N.B. Please see the separate section below for updating the log verbosity of the etcd component.</p> <pre><code>  services:\nkube-api:\nextra_args:\nv: '9'\n</code></pre> <p>The appropriate name for each service within this block can be found within the RKE documentation.</p> <p>Having set the verbosity flag, click <code>Save</code> at the bottom of the page, to update the cluster.</p>"},{"location":"kbs/000020199/#etcd","title":"etcd","text":"<p>The etcd component is configured to log at an INFO level by default, in an RKE CLI or Rancher launched Kubernetes cluster, but this can be set to DEBUG level by setting the <code>--debug=true</code> flag.</p>"},{"location":"kbs/000020199/#update-etcd-verbosity-in-an-rke-cli-launched-cluster","title":"Update etcd verbosity in an RKE CLI launched cluster","text":"<ol> <li>First set the <code>--debug=true</code> flag, within the <code>cluster.yml</code> cluster configuration file, under <code>extra_args</code> for the etcd service, per the following example:</li> </ol> <pre><code>     services:\netcd:\nextra_args:\ndebug: 'true'\n</code></pre> <ol> <li>Having set the flag in the cluster.yml, run <code>rke up --config cluster.yml</code> to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020199/#update-etcd-verbosity-in-a-rancher-launched-cluster","title":"Update etcd verbosity in a Rancher launched cluster","text":"<p>Navigate to the cluster within the Rancher UI and click <code>Edit Cluster</code>, then <code>Edit as YAML</code>. Set the <code>--debug=true</code> flag under <code>extra_args</code>, for the etcd service, per the following example:</p> <pre><code>  services:\netcd:\nextra_args:\ndebug: 'true'\n</code></pre> <p>Having set the debug flag, click <code>Save</code> at the bottom of the page, to update the cluster.</p>"},{"location":"kbs/000020199/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020200/","title":"In Rancher v2.x, prior to v2.3, nodes in Rancher provisioned clusters deleted via Kubernetes, instead of via Rancher, remain present in Rancher in an 'unavailable' state","text":"<p>This document (000020200) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020200/#situation","title":"Situation","text":""},{"location":"kbs/000020200/#issue","title":"Issue","text":"<p>In Rancher v2.x, prior to v2.3, if a node is deleted via Kubernetes, rather than Rancher itself - i.e. via <code>kubectl delete node</code> or another process connecting to the Kubernetes API, such as the use of the cluster-autoscaler - the node will be removed from the Kubernetes cluster, but still be present according to Rancher, remaining in an 'unavailable' state.</p> <p>Kubernetes scheduling and workloads will perform as expected for the removal of the node, as the node is correctly removed from the Kubernetes cluster. However, the view in Rancher will continue to show the node as 'unavailable' until it is manually deleted from within Rancher too.</p>"},{"location":"kbs/000020200/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned Kubernetes cluster, prior to v2.3, using either custom nodes or nodes hosted in an infrastructure provider.</li> </ul>"},{"location":"kbs/000020200/#workaround","title":"Workaround","text":"<p>To remove nodes, in a Rancher v2.x provisioned cluster, that have been deleted in Kubernetes, and are no longer present in the output of <code>kubectl get nodes</code>, but remain in Rancher in an 'unavailable' state, you can delete these from within the node list for the cluster within the Rancher UI.</p>"},{"location":"kbs/000020200/#resolution","title":"Resolution","text":"<p>This was tracked in Rancher GitHub issue #14184 and has been resolved since the release of Rancher v2.3. Where a node is deleted via Kubernetes, in Rancher v2.3 and above, this is detected by Rancher and the cluster is reconciled by Rancher to reflect the removal.</p>"},{"location":"kbs/000020200/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020201/","title":"\"log unreadable. It is excluded and would be examined next time.\" warning messages, for kubelet and kube-proxy, in rancher-logging-fluentd Pod logs of worker nodes","text":"<p>This document (000020201) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020201/#situation","title":"Situation","text":""},{"location":"kbs/000020201/#issue","title":"Issue","text":"<p>In a Rancher v2.x provisioned Kubernetes cluster, with Rancher Cluster Logging configured, the <code>rancher-logging-fluentd</code> Pod logs on Linux worker role only nodes show warning messages of the following format:</p> <pre><code>2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kubelet_5c47838dd4af749a7a0d1c457b04a6d7b905e680157718063c8e5d9eb61268fa.log unreadable. It is excluded and would be examined next time.\n2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kube-proxy_d2beb2e667eefbd6d95355082af4bc61c367fc4c220d9f1d165d15a8c8be2ab1.log unreadable. It is excluded and would be examined next time.\n</code></pre> <p>The output of <code>ls /var/lib/rancher/rke/log/</code> on affected workers shows that these files referenced in the warning log messages are broken symlinks. In addition, <code>docker ps</code> output shows the container ID for the currently running <code>kubelet</code> and <code>kube-proxy</code> containers does not match the IDs in these filenames.</p>"},{"location":"kbs/000020201/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider.</li> <li>Rancher Cluster Logging configured on the cluster, with the 'Include System Log' option set.</li> <li>Worker role only nodes in the cluster.</li> </ul>"},{"location":"kbs/000020201/#root-cause","title":"Root cause","text":"<p>These warning level messages in the <code>rancher-logging-fluentd</code> Pod are the result of the issue tracked in Rancher GitHub issue #22549.</p> <p>The container log symlinks in <code>/var/lib/rancher/rke/log/</code> for cluster component containers ( <code>kubelet</code>, <code>kube-proxy</code>, <code>nginx-proxy</code>) on worker nodes in Rancher launched clusters are not cleaned up when these components are re-created, i.e. due to a Kubernetes version upgrade, or other configuration update for these components.</p> <p>As a result these broken symlinks persist and cause the <code>log unreadable</code> warning messages when the <code>rancher-logging-fluentd</code> Pod attempts to parse files in the <code>/var/lib/rancher/rke/log/</code> directory.</p> <p>This warning message itself is harmless and can be ignored.</p>"},{"location":"kbs/000020201/#resolution","title":"Resolution","text":"<p>The request to handle automatic clean-up of these log symlinks on worker nodes, in Rancher provisioned clusters, is tracked in Rancher GitHub issue #22549.</p>"},{"location":"kbs/000020201/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020202/","title":"Many rancher-agent containers running on Rancher v2.x provisioned Kubernetes cluster, where stopped containers are regularly deleted on hosts","text":"<p>This document (000020202) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020202/#situation","title":"Situation","text":""},{"location":"kbs/000020202/#issue","title":"Issue","text":"<p>On a Rancher v2.x provisioned cluster, a host shows a large number of containers running the <code>rancher-agent</code> image, per the following output of <code>docker ps | grep rancher-agent</code>:</p> <pre><code>$ docker ps | grep rancher-agent\n...\naeffe9725521        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   About a minute ago   Up About a minute                       sleepy_hopper\n130120f49b71        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   6 minutes ago        Up 6 minutes                            stoic_hypatia\n498b923d9b6e        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   11 minutes ago        Up 11 minutes                            laughing_elbakyan\n3453865e5f70        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   16 minutes ago        Up 16 minutes                            wonderful_gagarin\nf925209cd16a        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   21 minutes ago       Up 21 minutes                           silly_shannon\n7d7fb5d4bf04        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   26 minutes ago       Up 26 minutes                           gifted_elgamal\n...\n</code></pre> <p>A <code>docker inspect &lt;container_id&gt;</code> for these containers, shows the Path and Args are of the following format:</p> <pre><code>\"Path\": \"run.sh\",\n\"Args\": [\n\"--server\",\n\"https://167.172.96.240\",\n\"--token\",\n\"gwrp7zlnwvsnzh2nhbvwcgdw45ccv6cq9pztzdd92j6xlv69xxhvnp\",\n\"--ca-checksum\",\n\"bbc8c7ca05c87a7140154554fa1a516178852f2710538c57718f4c874c29533c\",\n\"--no-register\",\n\"--only-write-certs\"\n],\n</code></pre>"},{"location":"kbs/000020202/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider.</li> <li>Repeated deletion of stopped containers on hosts in the cluster, e.g. use of <code>docker system prune</code>, either manually or as part of an automated process such as a cronjob.</li> </ul>"},{"location":"kbs/000020202/#root-cause","title":"Root cause","text":"<p>This behaviour is a result of the issue tracked in Rancher GitHub issue #15364.</p> <p>The <code>share-mnt</code> container is created on a Rancher provisioned Kubernetes cluster, and exits upon completion, but is not removed such that it can be invoked again.</p> <p>Meanwhile, the Rancher <code>node-agent</code> Pod on a host will spawn a new <code>share-mnt</code> container, if the <code>share-mnt</code> is removed. Upon starting, the <code>share-mnt</code> process spawns a <code>rancher-agent</code> container to write certificates. This agent container will run indefinitely until the <code>node-agent</code> is triggered to reconnect to the Rancher server or the <code>node-agent</code> process is restarted.</p> <p>As a result, where the <code>share-mnt</code> container on a host is removed repeatedly, either manually or by an automated process, this will result in multiple running <code>rancher-agent</code> containers.</p>"},{"location":"kbs/000020202/#workaround","title":"Workaround","text":"<p>To trigger automatic removal of the <code>rancher-agent</code> containers, the <code>node-agent</code> container on the host can be restarted. Identifying the running agent container with <code>docker ps | grep k8s_agent_cattle-node</code> restart the container with <code>docker restart &lt;container_id&gt;</code>.</p> <p>In addition, you can prevent further creation of multiple <code>rancher-agent</code> container instances by removing whichever process is triggering the deletion of stopped containers.</p>"},{"location":"kbs/000020202/#resolution","title":"Resolution","text":"<p>An enhancement request, to prevent the creation of multiple long-running <code>rancher-agent</code> containers, in the event of repeated deletion of the <code>share-mnt</code> container, is tracked in Rancher GitHub issue #15364.</p>"},{"location":"kbs/000020202/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020203/","title":"Is it safe to update the Docker bridge IP range on hosts in an RKE or Rancher v2.x launched Kubernetes cluster?","text":"<p>This document (000020203) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020203/#situation","title":"Situation","text":""},{"location":"kbs/000020203/#question","title":"Question","text":"<p>The <code>docker0</code> bridge network has a default IP range of <code>172.17.0.0/16</code> (with an additional <code>docker-sys</code> bridge for system-docker using <code>172.18.0.0/16</code> by default on RancherOS). These ranges will be routed to these interfaces, per the below example of the <code>route</code> output. If the range(s) overlap with the internal IP space usage in your own network, the host will not be able to route packets to other hosts in your network that lie within these ranges. As a result you may wish to change the bridge range(s) to enable successful routing to hosts within these.</p> <pre><code>$ route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n...\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n...\n</code></pre>"},{"location":"kbs/000020203/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x, or Rancher v2.x</li> </ul>"},{"location":"kbs/000020203/#answer","title":"Answer","text":"<p>Updating the <code>docker0</code> bridge IP range (and <code>docker-sys</code> bridge IP range in RancherOS) is possible in an RKE or Rancher v2.x provisioned Kubernetes cluster, where no cluster containers are in fact running attached to the Docker bridge network. The only impact of the change should be some downtime, as you will be required to restart the Docker daemon for the change to take effect.</p> <p>On RancherOS the bridge IP range ( <code>bip</code>) can be updated for docker and system-docker per the RancherOS documentation on <code>Configuring Docker or System Docker</code>. You will need to reboot the host for the change to take effect after updating the settings.</p> <p>For other operating systems, where Docker is installed from the upstream Docker repositories, you should update the <code>bip</code> configuration in <code>/etc/docker/daemon.json</code> per the dockerd documentation.</p> <p>On CentOS 7, RHEL 7 and SLES 12 you should also check the configuration in /etc/sysconfig/docker to ensure <code>--bip</code> has not been configured there.</p>"},{"location":"kbs/000020203/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020204/","title":"Is it safe to disable inter-container connectivity (icc) on the Docker daemon in an RKE or Rancher v2.x launched Kubernetes cluster?","text":"<p>This document (000020204) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020204/#situation","title":"Situation","text":""},{"location":"kbs/000020204/#question","title":"Question","text":"<p>The Docker daemon provides a configuration option <code>icc</code> which permits a user to disable inter-container connectivity (icc) on the Docker bridge network. Is is safe to disable this Docker daemon option in an RKE or Rancher v2.x launched Kubernetes cluster?</p>"},{"location":"kbs/000020204/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This article is only applicable to Kubernetes cluster launched by RKE v0.1.x, v0.2.x and v0.3.x or Rancher v2.x</li> </ul>"},{"location":"kbs/000020204/#answer","title":"Answer","text":"<p>Setting <code>icc</code> to false in the docker daemon.json configuration, or as an argument to to dockerd, is possible but is unnecessary in an RKE or Rancher v2.x provisioned Kubernetes cluster, as containers are not run attached to the Docker bridge network. Therefore, whilst this step is often included in standard 'hardening Docker daemon' guides, it is not relevant to operating an RKE or Rancher launched Kubernetes cluster.</p>"},{"location":"kbs/000020204/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020205/","title":"Users assigned the Project Owner or Member role on a project are able to create namespaces on any project, in the same cluster, to which they have access","text":"<p>This document (000020205) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020205/#situation","title":"Situation","text":""},{"location":"kbs/000020205/#issue","title":"Issue","text":"<p>A user assigned the Project Owner or Member role on one project is able to create namespaces on any project, in the same cluster, to which they have access.</p> <p>For example, if a user has been granted the Project Member role on a Project named Dev in a cluster, and the Read-only role on a project named Test in that cluster, they will be able to create namespaces on both the Dev and Test projects.</p>"},{"location":"kbs/000020205/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.x</li> <li>A user granted the Project Member or Owner role on one project, and access e.g. the Read-only role, on another project</li> </ul>"},{"location":"kbs/000020205/#explanation","title":"Explanation","text":"<p>Per the caveat explanation in the Rancher v2.x documentation:</p> <p>Users assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole, meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the owner or member role for a project can create namespaces in other projects they\u2019re assigned to, even with only the Read Only role assigned.</p>"},{"location":"kbs/000020205/#further-reading","title":"Further Reading","text":"<p>Read more on Cluster and Project Roles in the Rancher v2.x. documentation.</p>"},{"location":"kbs/000020205/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020206/","title":"How does session management work in the Rancher v1.6 UI?","text":"<p>This document (000020206) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020206/#situation","title":"Situation","text":""},{"location":"kbs/000020206/#question","title":"Question","text":"<p>This article looks at how session management, and expiry, functions in the Rancher v1.6 UI.</p>"},{"location":"kbs/000020206/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This article is applicable to Rancher v1.6 instances</li> </ul>"},{"location":"kbs/000020206/#answer","title":"Answer","text":"<p>The Rancher user interface uses a token for session management. The token is originally obtained by the client by calling the <code>/v2-beta/token</code> API. This API is triggered by the end-user entering their username and password and clicking the \"Log In\" button. Below is an example request:</p> <p>URL: https://rancher.example.com/v2-beta/token</p> <p>Method: POST</p> <p>Request body (formatted for readability):</p> <pre><code>{\n\"code\":\"admin:&lt;password here&gt;\",\n\"authProvider\":\"localauthconfig\"\n}\n</code></pre> <p>Upon successful authentication, the server will generate a random 40 character token that is associated with the authenticated user. This token is provided back to the user interface in the <code>jwt</code> field in the JSON response. The token is valid for 16 hours from the time of creation. This expiration is enforced by the server. Below is a sample response (formatted for readability):</p> <pre><code>{\n\"id\":null,\n\"type\":\"token\",\n\"links\":{},\n\"baseType\":\"token\",\n\"actionLinks\":{},\n\"accountId\":\"1a1\",\n\"authProvider\":\"localAuthConfig\",\n\"code\":null,\n\"enabled\":true,\n\"jwt\":\"V1dMyPArix5nN1jxiA6DdzsqdZitDJhZuBR3vZNr\",\n\"originalLogin\":null,\n\"redirectUrl\":null,\n\"security\":true,\n\"user\":\"admin\",\n\"userIdentity\":\n{\n\"externalId\":\"1a1\",\n\"profilePicture\":null,\n\"name\":\"admin\",\n\"externalIdType\":\"rancher_id\",\n\"profileUrl\":null,\n\"login\":\"admin\",\n\"role\":null,\n\"projectId\":null,\n\"user\":false,\n\"all\":null,\n\"id\":\"rancher_id:1a1\"\n},\n\"userType\":\"admin\"\n}\n</code></pre> <p>The user interface stores the token in a cookie called <code>token</code> and will send this cookie to all subsequent API requests to the server. In addition to a token, the server also sends a CSRF (Cross-Site Request Forgery) cookie which must be sent back on each request. This ensures the request came from the client and not a third party or malicious script. Below is a sequence diagram that demonstrates how a token is created and used.</p> <p></p> <p>Upon session expiration, the user interface will redirect the user back to the login page.</p> <p>Note, the session token expiration duration is not currently configurable. There is an enhancement request on GitHub to add this functionality, tracked in https://github.com/rancher/rancher/issues/16467</p>"},{"location":"kbs/000020206/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020207/","title":"Node labels and taints reset on reboot with AWS cloudprovider in Kubernetes lower than v1.12.0","text":"<p>This document (000020207) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020207/#situation","title":"Situation","text":""},{"location":"kbs/000020207/#issue","title":"Issue","text":"<p>In a Kubernetes cluster, running on AWS EC2 instances, with the AWS cloudprovider configured, labels and taints for a node are reset when the EC2 instance is rebooted.</p>"},{"location":"kbs/000020207/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes version lower than v1.12.0</li> <li>Cluster running on AWS EC2 instances, with the AWS cloudprovider configured</li> </ul>"},{"location":"kbs/000020207/#root-cause","title":"Root Cause","text":"<p>This behaviour is caused by the AWS cloudprovider in Kubernetes versions prior to v1.12.0, in which a stopped EC2 instance is deleted from the Kubernetes cluster, and then re-created when started again. As a result of this deletion and re-creation labels and taints on the node are lost during the reboot. Details of the issue and fix can be found in Kubernetes Pull Request #66835.</p>"},{"location":"kbs/000020207/#resolution","title":"Resolution","text":"<p>In order to resolve this issue, the cluster should be upgraded to Kubernetes version v1.12.0 or above.</p> <p>For clusters provisioned via the RKE CLI, users can upgrade the cluster to a Kubernetes version of v1.12.0 or higher with RKE v0.1.10 or above.</p> <p>For clusters provisioned via Rancher, users can upgrade the cluster to a Kubernetes version of v1.12.6 or higher with Rancher v2.1.7 or above.</p>"},{"location":"kbs/000020207/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020208/","title":"CronJobs fail to run in a Kubernetes v1.14 cluster, with more than 500 Job resources: \"expected type *batchv1.JobList, got type *internalversion.List\"","text":"<p>This document (000020208) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020208/#situation","title":"Situation","text":""},{"location":"kbs/000020208/#issue","title":"Issue","text":"<p>In a Kubernetes v1.14 cluster, CronJobs fail to run when there are more than 500 Job resources in the cluster. The Kubernetes controller manager logs show errors of the format <code>{\"log\":\"E0818 18:25:50.081946 1 cronjob_controller.go:117] expected type *batchv1.JobList, got type *internalversion.List\\n\",\"stream\":\"stderr\",\"time\":\"2019-08-18T18:25:50.082127727Z\"}</code>.</p>"},{"location":"kbs/000020208/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster, running Kubernetes v1.14, from v1.14.0 - v1.14.6</li> <li>More than 500 Job resources in the cluster</li> </ul>"},{"location":"kbs/000020208/#workaround","title":"Workaround","text":"<p>To mitigate the issue you should ensure that there are fewer than 500 Job resources in the cluster, you can view all Jobs with <code>kubectl get jobs --all-namespaces -o wide</code>.</p> <p>You should aim to delete completed Jobs to reduce the total number below 500. You can also check and adjust the configured job history limits for CronJobs to reduce the number of Jobs maintained for completed CronJobs per the Kubernetes documentation.</p>"},{"location":"kbs/000020208/#resolution","title":"Resolution","text":"<p>The issue was tracked in Kubernetes GitHub Issue #77465 and a fix was released in Kubernetes v1.14.7.</p> <p>A Kubernetes v1.14 patch release of v1.14.7 or above, including this fix, is available in Rancher v2.2, starting with v2.2.9 (v1.14.8), and v2.3, starting with v2.3.0 (v1.14.7). Similarly the fix is available via the RKE CLI in v0.2, starting with v0.2.9 (v1.14.8), and v0.3, starting with v0.3.0 (v1.14.7).</p>"},{"location":"kbs/000020208/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020209/","title":"How to configure iptables on RancherOS","text":"<p>This document (000020209) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020209/#situation","title":"Situation","text":""},{"location":"kbs/000020209/#task","title":"Task","text":"<p>How to configure firewall rules using iptables on RancherOS</p>"},{"location":"kbs/000020209/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A RancherOS v1.5.x host</li> </ul>"},{"location":"kbs/000020209/#resolution","title":"Resolution","text":"<p>The runcmd option in cloud-config can be used to run commands, such as iptables rules, to set firewall rules on a RancherOS host. For example the following can be used to disable SSH access on port 22.</p> <pre><code>#cloud-config\nruncmd:\n- \"iptables -A INPUT -p tcp --destination-port 22 -j DROP\"\n</code></pre> <p>The above snipet can be placed in /var/lib/rancher/conf/cloud-config.d/xxx.yaml, or added to the initial config while installing RancherOS. It will be executed every time RancherOS is booted.</p> <p>You can use the following iptables command to view the status of the rules:</p> <pre><code>$ iptables -t filter -nv -L INPUT\nChain INPUT (policy ACCEPT 321 packets, 41200 bytes)\npkts bytes target     prot opt in     out     source               destination\n    9     523 DROP       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:22\n</code></pre>"},{"location":"kbs/000020209/#further-reading","title":"Further reading","text":"<p>More information on running command on boot can be found here.</p>"},{"location":"kbs/000020209/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020210/","title":"RKE errors connecting to the Docker socket whilst updating clusters with the Aqua Enforcer deployed","text":"<p>This document (000020210) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020210/#situation","title":"Situation","text":""},{"location":"kbs/000020210/#issue","title":"Issue","text":"<p>During invocations of <code>rke up</code> via the RKE CLI or whilst modifying Rancher provisioned Kubernetes clusters, the process fails upon attempted creation of a Kubernetes component container with an error of the following format:</p> <pre><code>2019-04-30T15:19:17.9826528Z time=\"2019-04-30T15:19:17Z\" level=fatal msg=\"[etcd] Failed to bring up Etcd Plane: Failed to create [etcd] container on host [rancher.example.com]: Failed to create [etcd] container on host [rancher.example.com]: error during connect: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create?name=etcd: EOF\n</code></pre>"},{"location":"kbs/000020210/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned via the RKE CLI or Rancher</li> <li>The Aqua Enforcer workload deployed in the cluster, with AQUA_RUNC_INTERCEPTION environment variable set to 0</li> </ul>"},{"location":"kbs/000020210/#root-cause","title":"Root cause","text":"<p>The issue is caused by Aqua Enforcer's use of the Docker socket to perform runtime enforcement operations preventing RKE from successfully connecting to the Docker socket upon some requests.</p>"},{"location":"kbs/000020210/#resolution","title":"Resolution","text":"<p>To resolve this issue set the AQUA_RUNC_INTERCEPTION environment variable on the Aqua Enforcer daemonset to 1. With this setting the Aqua Enforcer will interact directly with runC to perform runtime enforcement operations, and not with the Docker daemon via the Docker socket. This is the default behaviour in new versions of the Aqua Enforcer, as it brings stability and performance benefits. More information on this setting can be found at https://docs.aquasec.com/docs/40-ga#section-new-aqua-enforcer-architecture-for-enhanced-stability-and-performance</p>"},{"location":"kbs/000020210/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020211/","title":"Editing Rancher launched Kubernetes cluster in infrastructure provider restricted to creating user","text":"<p>This document (000020211) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020211/#situation","title":"Situation","text":""},{"location":"kbs/000020211/#issue","title":"Issue","text":"<p>When attempting to edit a Rancher launched Kubernetes cluster, hosted on nodes in an infrastructure provider neither the <code>Cluster Options</code> nor <code>Node Pools</code> sections are available and configurable in the edit cluster view, if logged in as a different user to the cluster creator.</p>"},{"location":"kbs/000020211/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider</li> <li>Access to the Rancher UI as a user different to the cluster creator</li> </ul>"},{"location":"kbs/000020211/#root-cause","title":"Root cause","text":"<p>Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider.</p> <p>Node templates are user-scoped and, as a result, where <code>userA</code> creates a node template in Rancher it is not accessible by <code>userB</code>. This prevents Rancher launched Kubernetes clusters, provisioned on nodes in an infrastructure provider by <code>userA</code> from being edited by other users, as only <code>userA</code> has access to the node template configuration.</p>"},{"location":"kbs/000020211/#resolution","title":"Resolution","text":"<p>An enhancement request to enable users, other than the cluster creator, to edit Rancher launched Kubernetes clusters is tracked in Rancher GitHub Issue #12038.</p> <p>Where it is necessary for another user to edit the cluster, i.e. the original user who created the cluster has left the business, it is possible to re-associate the node template with a different user. If you encounter this situation, please open a ticket with Rancher Support for assistance.</p>"},{"location":"kbs/000020211/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020212/","title":"Blank provider listed for cluster when logged in as user who did not create cluster in Rancher v2.x","text":"<p>This document (000020212) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020212/#situation","title":"Situation","text":""},{"location":"kbs/000020212/#issue","title":"Issue","text":"<p>When viewing a Rancher launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider, as a user other than the cluster creator, the infrastructure provider name is blank.</p> <p></p>"},{"location":"kbs/000020212/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider</li> <li>Access to the Rancher UI as a user different to the cluster creator</li> </ul>"},{"location":"kbs/000020212/#root-cause","title":"Root cause","text":"<p>Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider.</p> <p>Node templates are user-scoped and, as a result, where <code>userA</code> creates a node template in Rancher it is not accessible by <code>userB</code>.</p> <p>Meanwhile, the Rancher v2.x UI determines the provider for a Rancher launched Kubernetes cluster by mapping <code>nodes</code> to <code>node templates</code> to <code>node drivers</code>.</p> <p>The user-scoping of node templates therefore prevents users, other than the creator, from viewing the provider of the cluster.</p>"},{"location":"kbs/000020212/#resolution","title":"Resolution","text":"<p>An enhancement request to enable all users to view the cluster provider is tracked in Rancher GitHub Issue #12038.</p>"},{"location":"kbs/000020212/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020213/","title":"How to recover after deleting the Calico CRDs from a cluster","text":"<p>This document (000020213) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020213/#situation","title":"Situation","text":""},{"location":"kbs/000020213/#issue","title":"Issue","text":"<p>Calico uses a number of Custom Resource Definitions (CRDs) in order to store configuration data in Custom Resources. In the event that these CRDs are accidentally deleted from a cluster by a user, the configuration data in these Custom Resources will be deleted, preventing successful programming of pod networking. This article documents how to recreate the CRDs and ensure the configuration data is also re-populated.</p>"},{"location":"kbs/000020213/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes v1.8.x - v1.16.x cluster provisioned by the RKE CLI or Rancher v2.x, running with the Canal or Calico network providers</li> </ul>"},{"location":"kbs/000020213/#resolution","title":"Resolution","text":""},{"location":"kbs/000020213/#1-re-create-the-crds","title":"1. Re-create the CRDs","text":"<p>The first step is to re-create the CRDs. The definitions are dependent upon the Kubernetes version running in the cluster, as well as whether the cluster is running the Canal or Calico network provider. Please refer to the matching network provider and Kubernetes version combination below:</p> <p>Canal Network Provider and Kubernetes version 1.8.x - 1.12.x</p> <p>Download the canal-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-8-to-1-12.yaml</code></p> <p>Canal Network Provider and Kubernetes version 1.13.x - 1.14.x</p> <p>Download the canal-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-13-to-1-14.yaml</code></p> <p>Canal Network Provider and Kubernetes version 1.15.x</p> <p>Download the canal-calico-crds-k8s-1-15.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-15.yaml</code></p> <p>Canal Network Provider and Kubernetes version 1.16.x</p> <p>Download the canal-calico-crds-k8s-1-16.yaml file and apply this to the cluster: <code>kubectl apply -f canal-calico-crds-k8s-1-16.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.8.x - 1.12.x</p> <p>Download the calico-calico-crds-k8s-1-8-to-1-12.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-8-to-1-12.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.13.x - 1.14.x</p> <p>Download the calico-calico-crds-k8s-1-13-to-1-14.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-13-to-1-14.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.15.x</p> <p>Download the calico-calico-crds-k8s-1-15.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-15.yaml</code></p> <p>Calico Network Provider and Kubernetes version 1.16.x</p> <p>Download the calico-calico-crds-k8s-1-16.yaml file and apply this to the cluster: <code>kubectl apply -f calico-calico-crds-k8s-1-16.yaml</code></p>"},{"location":"kbs/000020213/#2-delete-a-network-pod-to-trigger-re-creation-of-the-calico-custom-resources","title":"2. Delete a network pod to trigger re-creation of the Calico custom resources","text":"<p>Delete a network provider pod from a single node in the cluster, per the network provider specific instructions below. This will trigger creation of a new pod on that node, and the initialization of this will create the Calico custom resources containing Calico configuration. After this cluster networking should be fully restored.</p> <p>Canal Network Provider</p> <p>Delete one of the <code>canal</code> pods within the <code>kube-system</code> namespace.</p> <p>Calico Network Provider</p> <p>Delete one of the <code>calico-node</code> pods within the <code>kube-system</code> namespace.</p>"},{"location":"kbs/000020213/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020214/","title":"'Error: snapshot missing hash but --skip-hash-check=false' when performing `rke etcd snapshort-restore` with .zip extension included in snapshot name","text":"<p>This document (000020214) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020214/#situation","title":"Situation","text":""},{"location":"kbs/000020214/#issue","title":"Issue","text":"<p>When performing an etcd snapshot restore via RKE, including the <code>.zip</code> file extension in the snapshot name parameter, i.e. <code>rke etcd snapshot-restore --name snapshot.zip</code> and a snapshot filename of <code>snapshot.zip</code>, the restoration fails with an error of the following format:</p> <pre><code>FATA[0020] [etcd] Failed to restore etcd snapshot: Failed to run etcd restore container, exit status is: 128, container logs: Error: snapshot missing hash but --skip-hash-check=false\n</code></pre>"},{"location":"kbs/000020214/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This issue is applicable to RKE CLI v0.2.x, starting with v0.2.5, v0.3.x and v1.0.x</li> </ul>"},{"location":"kbs/000020214/#resolution","title":"Resolution","text":"<p>This issue is caused by the incorrect inclusion of the <code>.zip</code> file extension to the snapshot name parameter.</p> <p>The snapshot name parameter ( <code>--name</code>) should contain the snapshot name, excluding the file extension.</p> <p>In the example of a snapshot filename of <code>snapshot.zip</code> the correct name parameter is therefore just <code>snapshot</code>, i.e. <code>rke etcd snapshot-restore --name snapshot</code>.</p>"},{"location":"kbs/000020214/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020215/","title":"How to configure the Docker bridge IP range on RancherOS v1.5","text":"<p>This document (000020215) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020215/#situation","title":"Situation","text":""},{"location":"kbs/000020215/#task","title":"Task","text":"<p>In RancherOS v1.5.x the <code>docker0</code> bridge network has a default IP range of <code>172.17.0.0/16</code> and the <code>docker-sys</code> bridge for system-docker has a default range of <code>172.18.0.0/16</code>. This article details how to update these ranges.</p>"},{"location":"kbs/000020215/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A RancherOS v1.5.x host</li> </ul>"},{"location":"kbs/000020215/#resolution","title":"Resolution","text":"<p>The <code>docker0</code> bridge range is set by the <code>rancher.docker.bip</code> argument, whilst the <code>docker-sys</code> bridge is set by <code>rancher.system_docker.bip</code> argument.</p> <p>These can be configured in the cloud-config as follows:</p> <pre><code>rancher:\ndocker:\nbip: 192.168.0.0/16\nsystem_docker:\nbip: 172.19.0.0/16\n</code></pre> <p>These can also be customized after the host has started with the <code>ros config</code> command, and will take effect after a reboot:</p> <pre><code>ros config set rancher.docker.bip 192.168.0.0/16\nros config set rancher.system_docker.bip 172.19.0.0/16\n</code></pre>"},{"location":"kbs/000020215/#further-reading","title":"Further reading","text":"<p>You can read more on configuring Docker or System Docker within the RancherOS documentation.</p>"},{"location":"kbs/000020215/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020216/","title":"subPath does not work with hostPath volumes in Rancher v2.x or RKE CLI launched Kubernetes clusters","text":"<p>This document (000020216) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020216/#situation","title":"Situation","text":""},{"location":"kbs/000020216/#issue","title":"Issue","text":"<p>Attempting to use the <code>subPath</code> option with a <code>hostPath</code> volume in a Rancher Kubernetes Engine (RKE) CLI, or Rancher v2.x, launched Kubernetes fails. The particular failure type depends upon the value of the <code>type</code> field specified on the <code>hostPath</code> volume.</p>"},{"location":"kbs/000020216/#type-undefined","title":"<code>type</code> undefined","text":"<p>If no <code>type</code> is specified (defined as <code>Anything: do not check the target path</code> within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format <code>Error: lstat /site-data: no such file or directory</code></p> <pre><code>spec:\ncontainers:\n- name: nginx\nimage: nginx:latest\nvolumeMounts:\n- mountPath: /volume/nginx\nname: site-data\nsubPath: nginx\nvolumes:\n- name: site-data\nhostPath:\npath: /site-data\n</code></pre>"},{"location":"kbs/000020216/#type-directory","title":"<code>type: Directory</code>","text":"<p>If the <code>type</code> is specified as <code>Directory</code> (defined as <code>An existing directory</code> within the Rancher UI), per the example spec below, then the Pod will fail to start and the Pod events will show an error of the format <code>MountVolume.SetUp failed for volume \"site-data\" : hostPath type check failed: /site-data is not a directory</code></p> <pre><code>spec:\ncontainers:\n- name: nginx\nimage: nginx:latest\nvolumeMounts:\n- mountPath: /volume/nginx\nname: site-data\nsubPath: nginx\nvolumes:\n- name: site-data\nhostPath:\npath: /site-data\ntype: Directory\n</code></pre>"},{"location":"kbs/000020216/#type-directoryorcreate","title":"<code>type: DirectoryOrCreate</code>","text":"<p>If the <code>type</code> is specified as <code>DirectoryOrCreate</code> (defined as <code>A directory, or create if it does not exist</code> within the Rancher UI), per the example spec below, then the Pod will start successfully; however, an empty local volume will be mounted in the container at the <code>mountPath</code>, rather than this being bind-mounted to the path on the host as expected.</p> <pre><code>spec:\ncontainers:\n- name: nginx\nimage: nginx:latest\nvolumeMounts:\n- mountPath: /volume/nginx\nname: site-data\nsubPath: nginx\nvolumes:\n- name: site-data\nhostPath:\npath: /site-data\ntype: DirectoryOrCreate\n</code></pre>"},{"location":"kbs/000020216/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Any RKE CLI v0.1.x or v0.2.x, or Rancher v2.x launched Kubernetes cluster (except those launched in hosted Kubernetes providers)</li> </ul>"},{"location":"kbs/000020216/#root-cause","title":"Root Cause","text":"<p>This behavior is a result of the containerized kubelet process in RKE and Rancher launched Kubernetes clusters, and is tracked in GitHub issue https://github.com/rancher/rancher/issues/14836</p> <p>Pending resolution of this issue, you should avoid the use of the <code>subPath</code> option on <code>hostPath</code> volumes, to prevent encountering this.</p>"},{"location":"kbs/000020216/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020217/","title":"How to install or upgrade to a specific Rancher v2.x version","text":"<p>This document (000020217) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020217/#situation","title":"Situation","text":""},{"location":"kbs/000020217/#issue","title":"Issue","text":"<p>By default the installation and upgrade documentation references the installation of, or upgrade to, the most recently released latest or stable tagged version of Rancher. This article details how to install a specific version, both in a single node and high availability installation.</p> <p>For details on the difference between the latest and stable releases please see the documentation on 'Choosing a Version'.</p> <p>N.B. We strongly recommend you only run product releases tagged \u201cStable\u201d in your production and any other business-critical environments. Any product release with the \u201cLatest\u201d tag should only be used for testing the latest releases.\"</p>"},{"location":"kbs/000020217/#resolution","title":"Resolution","text":""},{"location":"kbs/000020217/#single-node-install","title":"Single Node Install","text":"<p>To install or upgrade to a specific Rancher version in a single node install, you can specify the exact version number of the image to run, <code>rancher/rancher:vX.X.X</code>, i.e.:</p> <pre><code>docker run -d --restart=unless-stopped \\\n-p 80:80 -p 443:443 \\\nrancher/rancher:v2.2.2\n</code></pre>"},{"location":"kbs/000020217/#high-availability-ha-install","title":"High Availability (HA) Install","text":"<p>To install or upgrade to a specific version in a High Availability install, you can specify the <code>--version X.X.X</code> parameter when running the <code>helm install</code> or <code>helm upgrade</code> command, i.e.:</p> <pre><code>helm install rancher-stable/rancher \\\n--name rancher \\\n--namespace cattle-system \\\n--set hostname=rancher.my.org \\\n--version 2.2.2\n</code></pre>"},{"location":"kbs/000020217/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020218/","title":"Why are namespaces created via the kubectl CLI not assigned to a project?","text":"<p>This document (000020218) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020218/#situation","title":"Situation","text":""},{"location":"kbs/000020218/#question","title":"Question","text":"<p>When a user creates a Kubernetes namespace via the Rancher UI, API or CLI the namespace is created within a specified Rancher project in the cluster; however, when a user creates a namespace via the kubectl CLI ( <code>kubectl create ns &lt;namespace&gt;</code>) it is created outside of any project, why is this?</p>"},{"location":"kbs/000020218/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed via Rancher v2.x</li> </ul>"},{"location":"kbs/000020218/#answer","title":"Answer","text":"<p>It is expected behaviour that namespaces created via the kubectl CLI are created outside of a Project. Projects are Rancher abstractions and do not exist natively within the Kubernetes, as a result when you create a namespace via the kubectl CLI (or by otherwise POST'ing directly to the kube-apiserver) it is not associated with any project in Rancher.</p> <p>If you wish to create namespaces within a Rancher Project with a command-line tool, then you should use the Rancher CLI ( https://rancher.com/docs/rancher/v2.x/en/cli/ ).</p> <p>Where a namespace has been created outside a project via kubectl, a cluster admin can move the namespace into a project, within the 'Projects/Namespaces' view for the cluster.</p>"},{"location":"kbs/000020218/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020219/","title":"'SIGSEGV: segmentation violation' in prometheus container of the prometheus-project-monitoring-0 Pod when enabling Project monitoring on the System Project","text":"<p>This document (000020219) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020219/#situation","title":"Situation","text":""},{"location":"kbs/000020219/#issue","title":"Issue","text":"<p>Enabling project monitoring in a Rancher v2.2 cluster, in which cluster monitoring is enabled, fails with the Prometheus Pod in a CrashLoopBackOff.</p> <p>The <code>prometheus</code> container in the <code>prometheus-project-monitoring</code> StatefulSet fails with an error of the following format:</p> <pre><code>panic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x669c12]\ngoroutine 437 [running]:\nnet/http.(*Client).deadline(0x0, 0xc005381070, 0x40bb8f, 0xc0055e3600)\n/usr/local/go/src/net/http/client.go:187 +0x22\nnet/http.(*Client).do(0x0, 0xc005cdaa00, 0x0, 0x0, 0x0)\n/usr/local/go/src/net/http/client.go:527 +0xab\nnet/http.(*Client).Do(0x0, 0xc005cdaa00, 0x23, 0xc002802230, 0x9)\n/usr/local/go/src/net/http/client.go:509 +0x35\ngithub.com/prometheus/prometheus/scrape.(*targetScraper).scrape(0xc0060fa960, 0x1fd4a60, 0xc00010ec60, 0x1fb2760, 0xc0002eb110, 0x0, 0x0, 0x0, 0x0)\n/app/scrape/scrape.go:471 +0x111\ngithub.com/prometheus/prometheus/scrape.(*scrapeLoop).run(0xc00616a100, 0xdf8475800, 0x2540be400, 0x0)\n/app/scrape/scrape.go:813 +0x487\ncreated by github.com/prometheus/prometheus/scrape.(*scrapePool).sync\n/app/scrape/scrape.go:336 +0x45d\n</code></pre>"},{"location":"kbs/000020219/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>Cluster monitoring enabled and Project monitoring enabled on the System project</li> </ul>"},{"location":"kbs/000020219/#resolution","title":"Resolution","text":"<p>Project monitoring is not compatible with the Rancher System project and should not be enabled in the System project. Starting with Rancher v2.3.0 monitoring of the System project is performed by cluster monitoring, when this is enabled, and the UI prevents enabling of project monitoring on the System project.</p>"},{"location":"kbs/000020219/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020220/","title":"Is it possible to migrate a Rancher launched Kubernetes cluster between Rancher instances?","text":"<p>This document (000020220) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020220/#situation","title":"Situation","text":""},{"location":"kbs/000020220/#question","title":"Question","text":"<p>Is it possible to migrate a Rancher launched Kubernetes cluster from one Rancher server instance to another, e.g. to launch a custom cluster using one Rancher server, and then at a later time, to migrate this to be managed instead via a different Rancher instance?</p>"},{"location":"kbs/000020220/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched and managed by Rancher v2.x</li> </ul>"},{"location":"kbs/000020220/#answer","title":"Answer","text":"<p>No, it is not possible to migrate a cluster between Rancher server instances. A feature request for this is tracked in GitHub Issue #16471.</p> <p>Currently, if you launch a Kubernetes cluster in one Rancher instance, then later attempt to use the imported cluster feature to import this cluster into another Rancher instance, you will lose any ability to add or remove nodes from the cluster, perform etcd backups or disaster recovery, or to edit any of the cluster configuration. We would therefore strongly recommend against this. Instead, we recommend performing regular Rancher server backups, so that you can recover the Rancher server cluster in a disaster recovery scenario, ensuring successful on-going management of downstream clusters launched by the server.</p>"},{"location":"kbs/000020220/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020221/","title":"Is it possible to update the Cluster (Pod) CIDR or Service CIDR for an RKE CLI or Rancher launched cluster post-provisioning?","text":"<p>This document (000020221) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020221/#situation","title":"Situation","text":""},{"location":"kbs/000020221/#question","title":"Question","text":"<p>The Cluster (Pod) CIDR (default 10.42.0.0/16) and Service CIDR (default 10.43.0.0/16) ranges for a cluster can be specified in the cluster configuration YAML when launching a Kubernetes cluster via both the RKE CLI and Rancher v2.x. Is it possible to change these values after the cluster has been provisioned?</p>"},{"location":"kbs/000020221/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched by the RKE CLI, or Rancher v2.x</li> </ul>"},{"location":"kbs/000020221/#answer","title":"Answer","text":"<p>Updating either the Cluster (Pod) CIDR or Service CIDR after the cluster has been provisioned is not supported and you should be careful to set these as required when first configuring the cluster.</p>"},{"location":"kbs/000020221/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020222/","title":"\"fatal: could not read Username for 'http://host:port': No such device or address\" error for pipeline Publish Catalog Template step in Rancher v2.2 when Git URL contains a port","text":"<p>This document (000020222) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020222/#situation","title":"Situation","text":""},{"location":"kbs/000020222/#issue","title":"Issue","text":"<p>If a pipeline is configured in Rancher v2.2 with a Publish Catalog Template step, in which the specified Git URL contains a port, the step will fail to execute with an error of the format <code>fatal: could not read Username for 'http://host:port': No such device or address\"</code>.</p>"},{"location":"kbs/000020222/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>A pipeline configured with a Publish Catalog Template step, in which the Git URL contains a port</li> </ul>"},{"location":"kbs/000020222/#resolution","title":"Resolution","text":"<p>A patch was developed for this issue, and is available in Rancher v2.3.0 and above.</p>"},{"location":"kbs/000020222/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020223/","title":"Groups assigned Project roles do not display under the Edit Project view in the Rancher v2.2.4 UI","text":"<p>This document (000020223) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020223/#situation","title":"Situation","text":""},{"location":"kbs/000020223/#issue","title":"Issue","text":"<p>Despite being able to successfully assign user groups a project role, the groups are not listed under <code>Members</code> in the <code>Edit Project</code> view in the Rancher v2.2.4 UI.</p> <p>After adding a new group to the project as follows:</p> <p></p> <p>The newly added group does not display in the list:</p> <p></p> <p>However, querying the <code>projectRoleTemplateBindings</code> for the project via the Rancher API confirms it has been successfully added:</p> <pre><code>{\n\n\"annotations\": {\n\"lifecycle.cattle.io/create.cluster-prtb-sync_c-r5gcn\": \"true\",\n\"lifecycle.cattle.io/create.mgmt-auth-prtb-controller\": \"true\"\n},\n\"baseType\": \"projectRoleTemplateBinding\",\n\"created\": \"2019-09-19T10:19:08Z\",\n\"createdTS\": 1568888348000,\n\"creatorId\": \"user-2qtnq\",\n\"groupId\": null,\n\"groupPrincipalId\": \"freeipa_group://cn=developers,cn=groups,cn=accounts,dc=ipa,dc=example,dc=com\",\n\"id\": \"p-nbhcs:prtb-nq2c9\",\n\"labels\": {\n\"cattle.io/creator\": \"norman\"\n},\n\"links\": {\n\"remove\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\",\n\"self\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\",\n\"update\": \"\u2026/v3/projectRoleTemplateBindings/p-nbhcs:prtb-nq2c9\"\n},\n\"name\": \"prtb-nq2c9\",\n\"namespaceId\": null,\n\"projectId\": \"c-r5gcn:p-nbhcs\",\n\"roleTemplateId\": \"project-member\",\n\"type\": \"projectRoleTemplateBinding\",\n\"userId\": null,\n\"userPrincipalId\": null,\n\"uuid\": \"ead04e5d-dac6-11e9-9a87-0242ac110002\"\n\n}\n</code></pre>"},{"location":"kbs/000020223/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This issue only affects Rancher v2.2.4</li> </ul>"},{"location":"kbs/000020223/#root-cause","title":"Root Cause","text":"<p>This issue is caused by a UI bug ( GitHub Issue #20760) in Rancher v2.2.4 that was patched in Rancher v2.2.5.</p>"},{"location":"kbs/000020223/#resolution","title":"Resolution","text":"<p>Users should upgrade Rancher to v2.2.5 or above to resolve the issue.</p>"},{"location":"kbs/000020223/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020224/","title":"Cluster Logging (log forwarding) fails to deploy with restricted PodSecurityPolicy in Rancher v2.2","text":"<p>This document (000020224) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020224/#situation","title":"Situation","text":""},{"location":"kbs/000020224/#issue","title":"Issue","text":"<p>Attempting to enable Cluster Logging (log forwarding) in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the cluster, fails with the <code>rancher-logging-fluentd</code> and <code>rancher-logging-log-aggregator</code> Deployments failing to create Pods.</p> <p>The <code>rancher-logging-fluentd</code> Deployment fails to validate against the restricted PSP, with an error of the following format in events:</p> <pre><code>Error creating: pods \"rancher-logging-fluentd-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[1]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[2]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[3]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[4]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[5]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[6]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.volumes[10]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used]\n</code></pre> <p>The <code>rancher-logging-log-aggregator</code> Deployment fails to validate against the restricted PSP, with and error of the following format in events:</p> <pre><code>Error creating: pods \"rancher-logging-log-aggregator-\" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: \"hostPath\": hostPath volumes are not allowed to be used spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed]\n</code></pre>"},{"location":"kbs/000020224/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>Cluster Logging enabled</li> <li>PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level</li> </ul>"},{"location":"kbs/000020224/#root-cause","title":"Root Cause","text":"<p>The logging system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, cluster logging will fail to deploy successfully.</p>"},{"location":"kbs/000020224/#workaround","title":"Workaround","text":"<p>To workaround this issue, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with <code>kubectl --config &lt;cluster kubeconfig&gt; apply -f &lt;file&gt;</code>.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: default-psp-role\nnamespace: cattle-logging\nrules:\n- apiGroups:\n- extensions\nresourceNames:\n- default-psp\nresources:\n- podsecuritypolicies\nverbs:\n- use\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: default-psp-rolebinding\nnamespace: cattle-logging\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: default-psp-role\nsubjects:\n- kind: Group\nname: system:serviceaccounts:cattle-logging\napiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"kbs/000020224/#resolution","title":"Resolution","text":"<p>An update to ensure the logging system-charts are compatible with the restricted PodSecurityPolicy and is available in Rancher v2.3.0 and above.</p>"},{"location":"kbs/000020224/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020225/","title":"How to migrate from CentOS packaged to upstream Docker","text":"<p>This document (000020225) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020225/#situation","title":"Situation","text":""},{"location":"kbs/000020225/#task","title":"Task","text":"<p>This article will describe the process by which you can migrate a CentOS node in a Rancher cluster from running the CentOS packaged Docker package to the upstream package from Docker.</p> <p>In order to perform this migration you will be required to first uninstall the CentOS packaged Docker, before installing the upstream version. This process is destructive, and will remove all container state from the host. As a result the process outlined below, will guide you through first removing the node from the Rancher cluster, before conducting the package migration, then finally re-adding the node to the cluster.</p>"},{"location":"kbs/000020225/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched with the Rancher Kubernetes Engine (RKE) CLI, v0.1.x or v0.2.x, or a Rancher v2.x launched Kubernetes cluster on custom nodes</li> <li>Nodes running CentOS 7.x, with Docker installed from the CentOS extras repository.</li> </ul>"},{"location":"kbs/000020225/#resolution","title":"Resolution","text":""},{"location":"kbs/000020225/#cluster-launched-by-the-rke-cli","title":"Cluster launched by the RKE CLI","text":""},{"location":"kbs/000020225/#create-a-backup","title":"Create a Backup","text":"<p>As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the RKE documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster.</p>"},{"location":"kbs/000020225/#perform-migration-on-each-cluster-node-in-turn","title":"Perform migration on each cluster node in turn","text":"<ol> <li>Check if you should first add an additional node to the cluster, to replace the node during its migration:</li> </ol> <p>Controlplane or etcd nodes     In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, or add the role(s) to an existing node, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node to the cluster configuration YAML and run <code>rke up</code> to provision this.</p> <p>Worker nodes     If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node to the cluster configuration YAML and run <code>rke up</code> to provision this.</p> <ol> <li> <p>Remove the node which you are migrating from the cluster, to do so remove the node from the cluster configuration YAML and then run <code>rke up</code> to reconcile the cluster.</p> </li> <li> <p>Once the <code>rke up</code> invocation in step 2. completes successfully, run the Extended Rancher 2 cleanup script on the node that you are migrating, to clean up Rancher state.</p> </li> <li>Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section <code>Uninstall old versions</code> here.</li> <li>Add the node back to the cluster configuration YAML and run <code>rke up</code> to provision it.</li> </ol>"},{"location":"kbs/000020225/#custom-cluster-launched-by-rancher","title":"Custom cluster launched by Rancher","text":""},{"location":"kbs/000020225/#create-a-backup_1","title":"Create a Backup","text":"<p>As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the Rancher documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster, if S3 backups are not configured for the cluster.</p>"},{"location":"kbs/000020225/#perform-migration-on-each-cluster-node-in-turn_1","title":"Perform migration on each cluster node in turn","text":"<ol> <li>Check if you should first add an additional node to the cluster, to replace the node during its migration:</li> </ol> <p>Controlplane or etcd nodes     In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the replacement node.</p> <p>Worker nodes     If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node by running the Rancher agent command from the 'Edit Cluster' view, with the worker role, on the replacement node.</p> <ol> <li> <p>Remove the node which you are migrating from the cluster, to do so delete it from the node list for the cluster within Rancher.</p> </li> <li> <p>Once the cluster reconciliation triggered by step 2. is complete, and the cluster no longer shows as updating within Rancher, run the Extended Rancher 2 cleanup script on the node that you are migrating to clean up Rancher state.</p> </li> <li>Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS from the section <code>Uninstall old versions</code> here.</li> <li>Add the node back by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the node.</li> </ol>"},{"location":"kbs/000020225/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020226/","title":"How to rotate certificates for clusters launched by RKE v0.1.x or Rancher v2.0.x and v2.1.x","text":"<p>This document (000020226) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020226/#situation","title":"Situation","text":""},{"location":"kbs/000020226/#task","title":"Task","text":"<p>Kubernetes clusters use multiple certificates to provide both encryption of traffic to the Kubernetes components as well as authentication of these requests. These certificates are auto-generated for clusters launched by Rancher and also clusters launched by the Rancher Kubernetes Engine (RKE) CLI.</p> <p>In Rancher v2.0.x and v2.1.x, the auto-generated certificates for Rancher-launched Kubernetes clusters have a validity period of one year, meaning these certificates will expire one year after the cluster is provisioned. The same applies to Kubernetes clusters provisioned by v0.1.x of the Rancher Kubernetes Engine (RKE) CLI.</p> <p>If you created a Rancher-launched or RKE-provisioned Kubernetes cluster about 1 year ago, and have not already rotated the certificates, you need to rotate the certificates. If no action is taken, then when the certificates expire, the cluster will go into an error state and the Kubernetes API for the cluster will become unavailable. Rancher recommends that you rotate the certificates before they expire to avoid an unexpected service interruption. The rotation is a one time operation, and the newly-generated certificates will be valid for the next 10 years.</p>"},{"location":"kbs/000020226/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched by RKE CLI v0.1.x, or Rancher v2.0.x and v2.1.x</li> </ul>"},{"location":"kbs/000020226/#resolution","title":"Resolution","text":"<p>Full details on who to rotate the certificates for the both RKE and Rancher launched clusters can be found in the Rancher blog post \"Manual Rotation of Certificates in Rancher Kubernetes Clusters\".</p>"},{"location":"kbs/000020226/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020227/","title":"Cluster and Project monitoring fail to deploy with restricted PodSecurityPolicy in Rancher v2.2","text":"<p>This document (000020227) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020227/#situation","title":"Situation","text":""},{"location":"kbs/000020227/#issue","title":"Issue","text":"<p>Attempting to enable Cluster or Project monitoring, in a Rancher v2.2 cluster, where the restricted PodSecurityPolicy (PSP) is configured on the Cluster or Project, fails with the Grafana and Prometheus Pods in a CrashLoopBackOff.</p> <p>The <code>grafana-proxy</code> container in the <code>grafana-cluster-monitoring</code>/ <code>grafana-project-monitoring</code> Deployment and the <code>promtheus-proxy</code> container in the <code>prometheus-cluster-monitoring</code>/ <code>prometheus-project-monitoring</code> StatefulSet fail with an error of the following format:</p> <pre><code>2019/09/20 11:54:17 [warn] 1#1: duplicate MIME type \"text/html\" in /var/run/nginx.conf:46\nnginx: [warn] duplicate MIME type \"text/html\" in /var/run/nginx.conf:46\n2019/09/20 11:54:17 [emerg] 1#1: chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted)\nnginx: [emerg] chown(\"/tmp/nginx\", 100) failed (1: Operation not permitted)\n</code></pre> <p>The <code>grafana</code> container in the <code>grafana-cluster-monitoring</code>/ <code>grafana-project-monitoring</code> Deployment fails to start with an error of the following format shown in the events for the Pod:</p> <pre><code>Error: failed to start container \"grafana\": Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"rootfs_linux.go:58: mounting \\\\\\\"/var/lib/kubelet/pods/6c9bbb63-db9a-11e9-932e-2af11a72a258/volume-subpaths/grafana-static-contents/grafana/1\\\\\\\" to rootfs \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged\\\\\\\" at \\\\\\\"/var/lib/docker/overlay2/bec56dbc35983bd46debc3b8f1e7d88227556db353356695647d44a09a686eb2/merged/usr/share/grafana/public/app/plugins/datasource/prometheus/plugin.json\\\\\\\" caused \\\\\\\"not a directory\\\\\\\"\\\"\": unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type\n</code></pre>"},{"location":"kbs/000020227/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>Cluster or Project monitoring enabled</li> <li>PodSecurityPolicy enabled in the cluster and the restricted PSP configured at the cluster level, or on the Project for which monitoring is enabled</li> </ul>"},{"location":"kbs/000020227/#root-cause","title":"Root Cause","text":"<p>The monitoring system-charts in Rancher v2.2 are not compatible with the restricted PodSecurityPolicy. As a result where the restricted PSP is configured, monitoring will fail to deploy successfully.</p>"},{"location":"kbs/000020227/#workaround","title":"Workaround","text":"<p>To workaround the impact to cluster monitoring, the following Role and RoleBinding can be applied to the cluster, by copying these to a file and applying with <code>kubectl --config &lt;cluster kubeconfig&gt; apply -f &lt;file&gt;</code>.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: default-psp-role\nnamespace: cattle-prometheus\nrules:\n- apiGroups:\n- extensions\nresourceNames:\n- default-psp\nresources:\n- podsecuritypolicies\nverbs:\n- use\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: default-psp-rolebinding\nnamespace: cattle-prometheus\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: default-psp-role\nsubjects:\n- kind: Group\nname: system:serviceaccounts:cattle-prometheus\napiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"kbs/000020227/#resolution","title":"Resolution","text":"<p>An update to ensure the monitoring system-charts are compatible with the restricted PodSecurityPolicy is available in Rancher v2.3.0 and above.</p>"},{"location":"kbs/000020227/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020228/","title":"'[object Object]' error attempting to edit Load Balancing or Service Discovery resource YAMLs via the UI in Rancher v2.1.7 - v2.2.3","text":"<p>This document (000020228) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020228/#situation","title":"Situation","text":""},{"location":"kbs/000020228/#issue","title":"Issue","text":"<p>Selecting <code>View/Edit YAML</code> on Load Balancing (Ingress) or Service Discovery (Service) resources in the Rancher UI for Rancher v2.1.7 - v2.2.3 fails to display the resource YAML and displays an error <code>[object Object]</code> per the screenshot below.</p> <p></p>"},{"location":"kbs/000020228/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This issue is only applicable to Rancher v2.1.x, starting with v2.1.7, and Rancher v2.2.x, from Rancher v2.2.0 to Rancher v2.2.3</li> </ul>"},{"location":"kbs/000020228/#root-cause","title":"Root Cause","text":"<p>Issue was caused by an incorrect content-length header returned by the Rancher API for gzipped responses ( GitHub Issue #19723) and was patched in Rancher v2.2.4.</p>"},{"location":"kbs/000020228/#workaround","title":"Workaround","text":"<p>In affected versions of Rancher it is still possible to edit the YAML of Load Balancing (Ingress) or Service Discovery (Service) resources by using the <code>kubectl</code> CLI.</p>"},{"location":"kbs/000020228/#resolution","title":"Resolution","text":"<p>Users should upgrade to Rancher v2.2.4 or above to resolve the issue.</p>"},{"location":"kbs/000020228/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020229/","title":"No monitoring for etcd in the local Rancher cluster with cluster monitoring in Rancher v2.2.x","text":"<p>This document (000020229) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020229/#situation","title":"Situation","text":""},{"location":"kbs/000020229/#issue","title":"Issue","text":"<p>When enabling cluster monitoring in Rancher v2.2.x, upon the local cluster running the Rancher server itself, no monitoring metrics are available for the etcd instances within the cluster.</p>"},{"location":"kbs/000020229/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE provisioned Rancher HA cluster</li> <li>Rancher v2.2.x</li> <li>Cluster monitoring enabled on the local Rancher cluster</li> </ul>"},{"location":"kbs/000020229/#root-cause","title":"Root cause","text":"<p>Cluster monitoring was introduced in Rancher v2.2.0 and when enabled for a Rancher launched Kubernetes clusters, i.e. a custom cluster or using node templates for an infrastructure provider, will include etcd monitoring metrics. The local cluster running Rancher, whilst provisioned via RKE, is an imported cluster from the Rancher perspective. As a result etcd monitoring metrics are not collected and displayed for this cluster.</p>"},{"location":"kbs/000020229/#resolution","title":"Resolution","text":"<p>An enhancement request to include etcd monitoring metric collection, for the local Rancher cluster, within Rancher cluster monitoring is tracked in Rancher Issue #18619.</p>"},{"location":"kbs/000020229/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020230/","title":"How to collect and share Rancher logs with Support","text":"<p>This document (000020230) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020230/#situation","title":"Situation","text":""},{"location":"kbs/000020230/#issue","title":"Issue","text":"<p>When corresponding with Rancher Support to troubleshoot an issue, a common request is to retrieve environment and log data to assist with investigation.</p> <p>To standardise and simplify this data collection, product-specific scripts exist to retrieve the information, per the details below.</p> <p>Once logs are collected from each required node, per the direction of Rancher Support, this output can be uploaded to the Support case.</p>"},{"location":"kbs/000020230/#resolution","title":"Resolution","text":""},{"location":"kbs/000020230/#rancher-v2x-linux-log-collector","title":"Rancher v2.x Linux log collector","text":"<p>Logs can be collected from a Linux node within a Rancher v2.x cluster using the Linux log collector script as detailed here.</p>"},{"location":"kbs/000020230/#rancher-v2x-windows-log-collector","title":"Rancher v2.x Windows log collector","text":"<p>Logs can be collected from a Windows node within a Rancher v2.x cluster using the Windows log collector script as detailed here.</p>"},{"location":"kbs/000020230/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020231/","title":"Network ingress traffic from 192.168.0.0/16 always SNAT'd in Kubernetes clusters with canal network provider","text":"<p>This document (000020231) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020231/#situation","title":"Situation","text":""},{"location":"kbs/000020231/#issue","title":"Issue","text":"<p>Network ingress traffic to a Kubernetes cluster with the canal network provider, from IP addresses in the range <code>192.168.0.0/16</code>, is always SNAT'd, even in instances where this is not desired.</p> <p>For example on NodePort services configured with <code>externalTrafficPolicy: Local</code> the source IP should be preserved without SNAT, per the Kubernetes documentation. With this issue the source IP is SNAT'd even in instances of NodePort services configured with <code>externalTrafficPolicy: Local</code>.</p>"},{"location":"kbs/000020231/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned via the RKE CLI or Rancher, using the canal network provider</li> </ul>"},{"location":"kbs/000020231/#root-cause","title":"Root cause","text":"<p>When a cluster is provisioned with the canal network provider selected, Flannel is used for networking and Calico for network policy enforcement, and IP address management is therefore managed by Flannel.</p> <p>The <code>calico-node</code> container in the canal pod is still configured with an (un-used) IP pool, which defaults to <code>192.168.0.0/16</code>. By default Calico programs iptables rules in the <code>cali-nat-outgoing</code> chain of the <code>nat</code> table on cluster nodes to perform SNAT on traffic from this IP pool. The purpose of these rules is to masquerade egress traffic from pods where Calico is used for networking (and not just network policy). As a result in a canal network provider cluster, where the <code>calico-node</code> container is present for network policy enforcement, these rules are programed and any ingress traffic from the range <code>192.168.0.0/16</code> will match and be SNAT'd.</p>"},{"location":"kbs/000020231/#resolution","title":"Resolution","text":"<p>The permanent solution to prevent this issue is to update the RKE deployment templates for the canal daemonset, to set the environment variable <code>CALICO_IPV4POOL_NAT_OUTGOING</code> to <code>0</code> for the <code>calico-node</code> container. This will prevent programming of the problematic <code>cali-nat-outgoing</code> iptables rules and is tracked in Rancher Issue #20500.</p> <p>In order to workaround the issue in existing clusters, the Calico ippool configuration can be edited to disable outgoing nat, which removes programming of the <code>cali-nat-outgoing</code> iptables rules. To implement this workaround run kubectl against the affected to edit the <code>default-ipv4-ippool</code> object: <code>kubectl edit ippools default-ipv4-ippool</code>. Edit the line <code>natOutgoing: true</code> to set <code>natOutgoing: false</code> and save the change. Calico will detect the configuration update and remove the <code>cali-nat-outgoing</code> iptables rules.</p>"},{"location":"kbs/000020231/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020232/","title":"Is it possible to perform etcd snapshots to an s3 endpoint with a certificate signed by a custom CA?","text":"<p>This document (000020232) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020232/#situation","title":"Situation","text":""},{"location":"kbs/000020232/#question","title":"Question","text":"<p>Is it possible to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom certificate authority (CA)?</p>"},{"location":"kbs/000020232/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes clusters provisioned via the RKE CLI v0.2.x, or Rancher launched Kubernetes clusters via Rancher v2.2.x.</li> </ul>"},{"location":"kbs/000020232/#answer","title":"Answer","text":""},{"location":"kbs/000020232/#rancher-v220-v224-and-rke-cli-v020-v024","title":"Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4","text":"<p>In Rancher v2.2.0 - v2.2.4 and RKE CLI v0.2.0 - v0.2.4 it is not possible to configure etcd snapshots to use a custom CA. Attempts to perform etcd snapshots to an S3 endpoint with a certificate signed by a custom CA will fail with an error similar to the following:</p> <pre><code>FATA[0002] Failed to take one-time snapshot, exit code [1]: time=\"2019-04-29T08:37:15Z\" level=fatal msg=\"faield to set s3 server: failed to check s3 bucket:rke, err:Get https://s3.example.com/rke/?location=: x509: certificate signed by unknown authority\"\n</code></pre>"},{"location":"kbs/000020232/#rancher-v225","title":"Rancher v2.2.5+","text":"<p>In Rancher v2.2.5 and above it is possible to specify a custom CA for the S3 endpoint within the S3 backup options. Expanding 'Show advanced options' under the 'Edit Cluster' view, a 'Custom CA Certificate' field is shown when the s3 backup target is selected, enabling you to enter the certificate or upload this from file.</p>"},{"location":"kbs/000020232/#rke-v025","title":"RKE v0.2.5+","text":"<p>With the RKE CLI v0.2.5 and above it also possible to specify a custom CA for the S3 endpoint within the S3 backup options. To do you specify the certificate via the <code>custom_ca</code> field in the <code>s3backupconfig</code> block of the cluster configuration YAML. The cert should be provided as string, with newlines replaced with \\n, per the example below:</p> <pre><code>services:\netcd:\nbackup_config:\ninterval_hours: 12\nretention: 6\ns3backupconfig:\naccess_key: S3_ACCESS_KEY\nsecret_key: S3_SECRET_KEY\nbucket_name: s3-bucket-name\nregion: \"\"\nendpoint: s3.amazonaws.com\ncustom_ca: \"-----BEGIN CERTIFICATE-----\\nMIIDazCCAlOgAwIBAgIUMoCmUpa4u2UJWqNIkizFbpeJkwowDQYJKoZIhvcNAQEL\\nBQAwRTELMAkGA1UEBhMCQVUxEzARBgNVBAgMClNvbWUtU3RhdGUxITAfBgNVBAoM\\nGEludGVybmV0IFdpZGdpdHMgUHR5IEx0ZDAeFw0xOTA5MTgwOTI4NDBaFw0yMjA3\\nMDgwOTI4NDBaMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEw\\nHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQwggEiMA0GCSqGSIb3DQEB\\nAQUAA4IBDwAwggEKAoIBAQDIW8aN2vszkiNAqykYvqivZgWPRqEukPSAZz39Qtyx\\nkv2wl3B29chBzw5+vjG6veaUnWufOpGeiwglL2PEBOMI0a62zmmm3ttyJDy1lY+A\\ncuxZ1+hveWjWrA2B2bN69/wdkQTQu6ZLoguk+8mRFBZ7ghu6YTZQfczBsHlDxUpA\\n77qQunE4RmcQzOBHoWmMkSSxSGMBsVIj2rRihtVqpgbrMr3/LtCqzqsF+UcroJPC\\nIIBd8bSFlcgkWLnJdqlSa8s1PUodcKD3q6mbMZPDudraszuRgLyC5pIylGQOk+XF\\nMjf2I8zkkAV4QtfSpgBpNXbZEZ3a6CPhveDZqoZN4rxTAgMBAAGjUzBRMB0GA1Ud\\nDgQWBBTD/EagPfxclAlfViV5kKLq0YwBYzAfBgNVHSMEGDAWgBTD/EagPfxclAlf\\nViV5kKLq0YwBYzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQB0\\nyJ6vjtmuvBEKuNgWwIJLh2CqZubUL+lUQGi1NhdFzkXj7+fLeLjqsmbi2Xj/qQ5n\\nooI/p4MeHfYrUqqS7nqTBIsRZQZDZcKUYTZWzDRBdQZtxvEsB1WUq5+nsCQqVuZO\\n+ICsXQFL45xDKaWOoRMH8z9JksYf2CSKeRWViAFElC/IDwf8d5mtufe17h5vlyPR\\nLaIMJ37vyAosN6h8icztVHRzfcIjp1KLqwaGfaOrNSCv8zja9YsD6kbYL64lKND4\\nHiOJy3oSjjjTNdnXjIO44Ngo7L4TWF1CshFlsRF3a5/Jw+NmsEV46Vq41YcuRX9E\\n5JYZWzGRsPDeG4vrzWrV\\n-----END CERTIFICATE-----\"\n</code></pre>"},{"location":"kbs/000020232/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020233/","title":"Upgrade to Rancher v2.2.4 fails for instances managing OpenStack CloudProvider enabled clusters with a Loadbalancer config: 'cannot unmarshal number into Go value of type string'","text":"<p>This document (000020233) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020233/#situation","title":"Situation","text":""},{"location":"kbs/000020233/#issue","title":"Issue","text":"<p>Upon attempting to upgrade to Rancher v2.2.4, where the Rancher instance manages an, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config, the Rancher server fails to start. Logs for the Rancher pods show error messages of the format:</p> <pre><code>E0606 07:39:20.296926       8 reflector.go:134] github.com/rancher/norman/controller/generic_controller.go:175: Failed to list *v3.Cluster: json: cannot unmarshal number into Go value of type string\n</code></pre>"},{"location":"kbs/000020233/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Upgrading Rancher to v2.2.4</li> <li>A Rancher launched, OpenStack Cloud Provider enabled, Kubernetes cluster with a Loadbalancer config.</li> </ul>"},{"location":"kbs/000020233/#root-cause","title":"Root cause","text":"<p>In order to resolve Rancher/14577, the <code>monitor-delay</code> and <code>monitor-timeout</code> parameters for OpenStack cluster loadbalancer healthchecks were set from an integer type to a string, in Rancher v2.2.4.</p> <p>As the default in the Rancher API framework had configured these values to 0, upon upgrade to Rancher v2.2.4 an error occurs attempting to unmarshal these integer values of 0 to a string type. If these had been manually set to a non-zero integer value, resulting in kubelet failures in the OpenStack cluster itself previously, these will now result in failure of the Rancher pods themselves.</p>"},{"location":"kbs/000020233/#resolution","title":"Resolution","text":"<p>You can apply a one time fix, to workaround this issue, by manually editing the <code>monitor-delay</code> and <code>monitor-timeout</code> values of the <code>cluster</code> Custom Resource of affected clusters, via <code>kubectl</code> run against the Rancher management cluster.</p> <p>Using your RKE generated kube config, perform the following operations:</p> <ol> <li> <p>Identify affected clusters by running <code>kubectl get clusters</code> and checking for those with a <code>spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer</code> definition.</p> </li> <li> <p>For affected clusters run <code>kubectl edit &lt;cluster name&gt;</code>, where <code>&lt;cluster name&gt;</code> is the <code>metadata.name</code> value for the cluster and update the <code>spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-delay</code> and <code>spec.rancherKubernetesEngineConfig.cloudProvider.openstackCloudProvider.loadBalancer.monitor-timeout</code> fields to a quoted string. Example: if it was 30, change it to \"30s\", if it was 0, change it to \"\".</p> </li> </ol>"},{"location":"kbs/000020233/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020234/","title":"Rancher 1.6.x - Rancher Server Tuning","text":"<p>This document (000020234) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020234/#situation","title":"Situation","text":"<p>Rancher Server Tuning</p> <p>Refer PDF attachment</p>"},{"location":"kbs/000020234/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020235/","title":"Rancher 1.6.x - Production Environment Sizing guide","text":"<p>This document (000020235) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020235/#situation","title":"Situation","text":"<p>Production Environment Sizing Guide</p> <p>Refer PDF attachment</p>"},{"location":"kbs/000020235/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020236/","title":"Rancher 1.6.x - MySQL Config Tuning","text":"<p>This document (000020236) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020236/#situation","title":"Situation","text":"<p>MySQL Config Tuning Guide</p> <p>Refer PDF attachment</p>"},{"location":"kbs/000020236/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020237/","title":"Rancher 2.1.x - CIS Kubernetes v1.3.0 Benchmark Self Assessment","text":"<p>This document (000020237) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020237/#situation","title":"Situation","text":""},{"location":"kbs/000020237/#rancher-v21x","title":"Rancher v2.1.x","text":"<p>Version 1.0.0 - Nov 2018</p> <p>Authors</p> <ul> <li>Jason Greathouse</li> </ul> <p>Overview</p> <p>The following document scores an RKE cluster provisioned according to the Rancher 2.1.x hardening guide against the CIS 1.3.0 Kubernetes benchmark. This document is to be used by Rancher operators, security teams, auditors and decision makers.</p> <p>Download document here:</p> <p>https://releases.rancher.com/documents/security/latest/Rancher_Benchmark_Assessment.pdf</p>"},{"location":"kbs/000020237/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020238/","title":"Rancher 2.1.x - Rancher Hardening Guide","text":"<p>This document (000020238) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020238/#situation","title":"Situation","text":""},{"location":"kbs/000020238/#rancher-v21x","title":"Rancher v2.1.x","text":"<p>Version: 0.1.0 - November 26th 2018</p> <p>Overview</p> <p>This document provides prescriptive guidance for hardening a production installation of Rancher v2.1.x. It outlines the configurations and controls required to address CIS-Kubernetes benchmark controls.</p> <ul> <li>Rancher CIS-Kubernetes self assessment using RKE</li> </ul> <p>This document has been created by the Engineering team at Rancher Labs.</p> <p>Profile Definitions</p> <p>The following profile definitions agree with the CIS Benchmarks for Kubernetes.</p> <p>Level 1</p> <p>Items in this profile intend to:</p> <ul> <li>offer practical advice appropriate for the environment;</li> <li>deliver an obvious security benefit; and</li> <li>not alter the functionality or utility of the environment beyond an acceptable margin</li> </ul> <p>Level 2</p> <p>Items in this profile extend the \u201cLevel 1\u201d profile and exhibit one or more of the following characteristics:</p> <ul> <li>are intended for use in environments or use cases where security is paramount</li> <li>act as a defense in depth measure</li> <li>may negatively impact the utility or performance of the technology</li> </ul> <p>Authors</p> <ul> <li>Jason Greathouse</li> <li>Bill Maxwell</li> </ul> <p>Download document here:</p> <p>https://releases.rancher.com/documents/security/latest/Rancher_Hardening_Guide.pdf</p>"},{"location":"kbs/000020238/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020239/","title":"How to change the log level for Rancher v2.x","text":"<p>This document (000020239) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020239/#situation","title":"Situation","text":""},{"location":"kbs/000020239/#task","title":"Task","text":"<p>By default the Rancher v2.x server log level is set to <code>info</code>; however, when investigating an issue it may be helpful to increase the log verbosity to <code>debug</code>. This article details how to control the log verbosity on Rancher v2.x containers.</p>"},{"location":"kbs/000020239/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster.</li> </ul>"},{"location":"kbs/000020239/#resolution","title":"Resolution","text":"<p>The log verbosity is set within a running Rancher server container by use of the <code>loglevel</code> command:</p> <pre><code>loglevel --set &lt;verbosity&gt;\n</code></pre> <p>Using <code>kubectl</code> with your cluster's context, you can update the log level of all your Rancher server containers by running the following:</p> <pre><code>kubectl -n cattle-system get pods -l app=rancher --no-headers -o custom-columns=name:.metadata.name | while read rancherpod; do kubectl -n cattle-system exec $rancherpod -c rancher -- loglevel --set debug; done\n</code></pre> <p>where verbosity is one of <code>error</code>, <code>info</code> or <code>debug</code>.</p> <p>Instructions on how to run this command in either a single node or High Availability installation of Rancher can be found within the Rancher documentation under the \"Logging\" troubleshooting guide.</p> <p>If the log level is increased to <code>debug</code> for troubleshooting purposes, you should be sure to reduce to <code>info</code> after the necessary logs have been captured, in order to reduce disk usage and minimise noise when reading the logs.</p>"},{"location":"kbs/000020239/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020241/","title":"Istio-init container fails to start when SElinux is enabled on RHEL 7.x","text":"<p>This document (000020241) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020241/#environment","title":"Environment","text":"<p>OS: RHEL 7.x</p> <p>ISTIO Chart Version: v1.8.300</p> <p>Rancher Version : v2.5.7</p>"},{"location":"kbs/000020241/#situation","title":"Situation","text":"<p>Starting a workload on Istio-enabled namespace fails as the istio-init container failed to start.</p> <p>The istio-init container shows below error;</p> <pre><code>The error is:\nnat\n-N ISTIO_INBOUND\n-N ISTIO_REDIRECT\n-N ISTIO_IN_REDIRECT\n-N ISTIO_OUTPUT\n-A ISTIO_INBOUND -p tcp --dport 15008 -j RETURN\n-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001\n-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006\n-A PREROUTING -p tcp -j ISTIO_INBOUND\n-A ISTIO_INBOUND -p tcp --dport 22 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15090 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15021 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15020 -j RETURN\n-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT\n-A OUTPUT -p tcp -j ISTIO_OUTPUT\n-A ISTIO_OUTPUT -o lo -s 127.0.0.6/32 -j RETURN\n-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT\n-A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT\n-A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN\n-A ISTIO_OUTPUT -j ISTIO_REDIRECT\nCOMMIT\n\niptables-restore --noflush /tmp/iptables-rules-1618985143596701894.txt019825926\niptables-restore: line 25 failed\niptables-save\n</code></pre>"},{"location":"kbs/000020241/#resolution","title":"Resolution","text":"<p>Execute the modprobe in the below order to fix this without a reboot.</p> <pre><code>modprobe br_netfilter\nmodprobe nf_nat_redirect\nmodprobe xt_REDIRECT\nmodprobe xt_owner\n</code></pre> <p>To load the modules automatically during boot, create a file inside /etc/modules-load.d/ as shown below.</p> <pre><code>cat &gt;/etc/modules-load.d/istio-iptables.conf &lt;&lt;EOF\nbr_netfilter\nnf_nat_redirect\nxt_REDIRECT\nxt_owner\nEOF\n</code></pre>"},{"location":"kbs/000020241/#cause","title":"Cause","text":"<p>The issue is caused by SELinux which prevents the istio-init to load kernel modules that are needed for the iptables rules.</p>"},{"location":"kbs/000020241/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020243/","title":"Can I receive an application backup of my SUSE Rancher Hosted environment if I decide not to renew at the end of my term?","text":"<p>This document (000020243) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020243/#situation","title":"Situation","text":""},{"location":"kbs/000020243/#resolution","title":"Resolution","text":"<p>Yes, the SUSE can provide an application backup of your SUSE Rancher Hosted environment. You can follow the Rancher documentation to restore your environment into a Rancher server running in your datacenter or cloud account.</p> <p>To request a backup of your SUSE Rancher Hosted environment, open a support ticket on our support portal.</p>"},{"location":"kbs/000020243/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020265/","title":"Node become NotReady state because of unhealthy PLEG","text":"<p>This document (000020265) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020265/#situation","title":"Situation","text":"<p>The rancher UI displays the following:</p> <p>\"PLEG is not healthy: pleg was last seen active 18h50m17.324752357s ago; threshold is 3m0s \"</p> <p>Kubelet log shows;</p> <pre><code>docker logs kubelet --tail=20 -f\n</code></pre> <p>Output:-</p> <pre><code>E0511 09:12:59.037051 10851 remote_runtime.go:312] ListContainers with filter &amp;ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)\n\nE0511 09:12:59.037105 10851 kuberuntime_container.go:382] getKubeletContainers failed: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)\n\nE0511 09:12:59.037123 10851 generic.go:205] GenericPLEG: Unable to retrieve pods: rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (17182828 vs. 16777216)\n</code></pre>"},{"location":"kbs/000020265/#resolution","title":"Resolution","text":"<p>The workaround here is to prune stopped or dead containers to reduce the message size.</p> <pre><code>docker system prune\n</code></pre> <p>The Paketo Buildpacks fixed this issue already.</p> <p>Please refer\u00a0GitHub issue #80\u00a0for more details.</p>"},{"location":"kbs/000020265/#cause","title":"Cause","text":"<p>The labels set on the containers build by Paketo Buildpacks\u00a0are causing a huge message size.</p> <p>This results in the default gRPC message buffer size of 16Mb overflowing in kubelet, causing PLEG to fail.</p>"},{"location":"kbs/000020265/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020274/","title":"Rancher Support Migration - Frequently Asked Questions","text":"<p>This document (000020274) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020274/#environment","title":"Environment","text":"<p>Dear SUSE Rancher Customer,</p> <p>This page is specific to customer users\u00a0who are being migrated from the Rancher Support system on Zendesk to the SUSE Customer Center (SCC). \u00a0This page is intended to answer some of the Frequently Asked Questions our team has received about the migration process. \u00a0We hope this gives you and your team some additional insight into what is changing and what is not changing.</p>"},{"location":"kbs/000020274/#will-rancher-support-be-available-during-the-maintenance-window-for-this-migration","title":"Will Rancher Support be available during the maintenance window for this migration?","text":"<p>Yes. \u00a0During this migration, our support leadership team will be monitoring an email address for any customers that are in need of immediate assistance. \u00a0If you find yourself in need of assistance please do not hesitate to email us at RancherSupportNow@suse.com.\u00a0 From here our team will assess the issue and work with you for resolution. For customers with accounts already in SCC, during the migration, you may continue to interact with our team through SCC. \u00a0For any customers that do not have SCC accounts (please don't create one until you are advised to do so), please use the\u00a0RancherSupportNow@suse.com\u00a0to interact with our team during this migration, especially to log Sev1 cases. \u00a0Once the migration is complete, SCC will be the new system moving forward and how our teams interact.</p>"},{"location":"kbs/000020274/#what-will-happen-with-my-existing-tickets-in-zendesk-after-this-migration","title":"What will happen with my existing tickets in Zendesk after this migration?","text":"<p>Ensuring the historical context of your account is important to us. \u00a0During the migration, our team will be working to ensure that all of your historical (closed) tickets are migrated. \u00a0Additionally, we will work to ensure that each of the tickets that we are currently working on is migrated. \u00a0During the migration, you may see a new ticket (in the new customer center) that informs you that your previous Zendsk ticket has been migrated to a new SalesForce ticket. \u00a0This new ticket will contain the history of that open/working ticket.</p>"},{"location":"kbs/000020274/#what-will-happen-to-our-existing-user-accounts-in-zendesk-after-the-migration","title":"What will happen to our existing user accounts in Zendesk after the migration?","text":"<p>Because we are migrating to a new CRM, each user will need to register for a new account (directions for registration will be sent in future communications). Each of the users that is currently active in Zendesk will be receiving the same messaging that contains the instructions on how they may create a new account in the new system. In the new system, the creation of new accounts is now able to be done by users on the account that have administrator permissions. \u00a0Each of these users will be able to invite users and set the level of permissions for those users.</p> <p>If a user in your team either doesn't receive information on how to create a new user account or if you simply want to add a new user that did not exist in Zendesk, your account administrators will be able to take control of the account invites. \u00a0The process of emailing us with Register in the subject will not work, so an administrator will need to be contacted to create the accounts moving forward.</p>"},{"location":"kbs/000020274/#how-will-i-access-the-knowledgebase-articles-found-at-supportranchercom","title":"How will I access the Knowledgebase articles found at\u00a0support.rancher.com?","text":"<p>During this migration period, we have set all the support documents to be viable by anyone accessing the site, this site no longer requires an agent login. \u00a0Our team is in the process of migrating these articles to\u00a0suse.com.\u00a0Once the migration of articles is complete, they will be accessible at\u00a0https://www.suse.com/support/kb/\u00a0Once our team has cloned all of the articles we will place a redirection on the support.rancher.com page informing you that all of the documents have been moved.</p>"},{"location":"kbs/000020274/#may-i-continue-to-open-and-update-tickets-via-email","title":"May I continue to open and update tickets via email?","text":"<p>No, post-migration, it won\u2019t be possible to create a new ticket (support case) by email. \u00a0After the migration, this email address\u00a0support@rancher.com\u00a0will not create new support cases. \u00a0It will be monitored by our team and there will be an automated response to emails sent to this address on how to create a new support case.\u00a0 While we will continue to monitor that mailbox and offer help on best effort, we request that you create a new support case via the SCC portal. \u00a0Creating a ticket through the portal will ensure that our engineers are aware and able to assist more quickly.</p>"},{"location":"kbs/000020274/#what-will-happen-if-i-already-have-a-support-contract-with-suse-separate-from-my-rancher-account","title":"What will happen if I already have a support contract with SUSE, separate from my Rancher account?","text":"<p>For customers that have a service account with both, we are working to ensure that the entitlements are maintained. \u00a0If you already have an account where you are accessing your SUSE entitlements there is no need to create a new account, the (SUSE) Rancher entitlements should be added to your existing account with no issue. After the migration, if you find this not to be the case please email your Customer Success Manager or your Services Delivery Manager. \u00a0If you have users that you want to</p>"},{"location":"kbs/000020274/#how-will-i-know-if-the-migration-is-successful-or-failed","title":"How will I know if the migration is successful or failed?","text":"<p>Leading up to the migration and on the day of the migration process our team will be sending out messages, when appropriate, to keep you apprised of the process. \u00a0If the migration is successful, our final message will let you know that and will include steps for how you and your team may set up your new accounts in the new customer center. If the migration is unsuccessful, our team will let you know as well. \u00a0 If the migration is unsuccessful, nothing will change with your current support and you will continue to use Zendesk. \u00a0We will then continue to work on another try to migrate</p>"},{"location":"kbs/000020274/#what-if-i-have-issues-with-my-scc-account-after-the-migration-occurs","title":"What if I have issues with my SCC account after the migration occurs?","text":"<p>If there are issues you should first check\u00a0https://suse.okta.com/help/login.\u00a0\u00a0\u00a0If you are having a password or username issue please visit\u00a0https://suse.okta.com/, select \u201cNeed Help signing in?\u201d, and complete the steps that work best for you. \u00a0If neither of these resolves your problem and you are still unable to log in our team will be available to assist with any other issues. Please email your Customer Success Manager and/or\u00a0RancherSupportNow@suse.com.</p>"},{"location":"kbs/000020274/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020277/","title":"Is SUSE Rancher Hosted data encrypted at rest?","text":"<p>This document (000020277) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020277/#environment","title":"Environment","text":"<p>Hosted Rancher</p>"},{"location":"kbs/000020277/#resolution","title":"Resolution","text":"<p>Yes, the disk volumes of the virtual machines used by SUSE Rancher Hosted are encrypted as well as the disks used by the backend databases.</p>"},{"location":"kbs/000020277/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020296/","title":"[Rancher] How do I change severity level on an open case?","text":"<p>This document (000020296) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020296/#situation","title":"Situation","text":"<p>I have reviewed the severity levels listed below and I would now like to know\u00a0how I can change the severity level of an open case myself.</p> Severity Description Sev 1 (Critical) The solution is in production or is mission-critical to your business. It is inoperable, and the situation is resulting in a total disruption of work. There is no workaround available. Sev 2 (High) Operations are severely restricted, but work can continue. Core functionalities can operate in a restricted fashion despite important features being unavailable. A workaround is available that ensures no immediate business impact. Sev3 (Medium) The solution does not work as designed, resulting in a minor loss of usage. It may also be a significant software defect that impacts the Customer when performing some actions and has no workaround. Sev 4 (Low) There is no loss of service, and this may be a request for documentation, general information, product enhancement request, Software defects with workarounds or medium or low functionality impact, etc."},{"location":"kbs/000020296/#resolution","title":"Resolution","text":"<p>During the creation of a support case, you may select the severity of the support case via the drop-down menu. The creation of a Sev1 or a Sev2 support case will trigger our escalation workflows. If during the life of your support case the initial severity is no longer accurate, you may post a new comment with the string\u00a0bump_to_sev1\u00a0or\u00a0bump_to_sev2\u00a0to escalate a ticket to higher severity. This string can be stand-alone as a comment or can be a part of a sentence. \u00a0There is no need for any special characters (such as \" \" or * *).</p> <p>During the life of the case, you can also recategorize it to sev3 or sev4 by using\u00a0bump_to_sev3\u00a0or\u00a0bump_to_sev4. Using these\u00a0bump_to_\u00a0strings for sev1 and sev2 will trigger our escalation workflows, where sev3 and sev4 will update the severity with no escalation workflows.</p>"},{"location":"kbs/000020296/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020302/","title":"How to add custom labels to Alerts in Monitoring v2","text":"<p>This document (000020302) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020302/#environment","title":"Environment","text":"<p>Rancher Version: v2.5.8</p> <p>Monitoring Chart Version: 14.5.100</p>"},{"location":"kbs/000020302/#situation","title":"Situation","text":"<p>Alerts in Monitoring v2 contain standard labels. But in some cases, users want to inject custom labels like Kubernetes cluster names to easily identify the environment or need the labels for further Alert routing.</p>"},{"location":"kbs/000020302/#resolution","title":"Resolution","text":"<p>Use\u00a0defaultRules.additionalRuleLabels\u00a0 in the Monitoring Apps's YAML spec to inject custom labels.</p> <p>To inject cluster name, open the Monitoring App in Apps&amp; Marketplace from Cluster explorer.</p> <p>Under \"Edit as YAML\", add the custom label as below.</p> <pre><code>defaultRules:\n\u00a0 additionalRuleLabels:\n\u00a0 \u00a0 cluster: \"My_Test_cluster\"\n</code></pre> <p>Then click on \"Deploy\" or \"Upgrade\" if App is already installed.</p> <p>If your receiver is webhook, then the alerts will have the custom labels as shown in the below example alert.</p> <pre><code>...\n...\nstatus\":\"firing\",\n\"labels\":{\n\u00a0 \"alertname\":\"NodeClockNotSynchronising\",\n\u00a0 \"cluster\":\"My_Test_cluster\", &lt;&lt;&lt;------\n\u00a0 \"container\":\"node-exporter\",\n\u00a0 \"endpoint\":\"metrics\",\n\u00a0 \"instance\":\"192.168.110.157:9796\",\n\u00a0 \"job\":\"node-exporter\",\n\u00a0 \"namespace\":\"cattle-monitoring-system\",\n\u00a0 \"pod\":\"rancher-monitoring-prometheus-node-exporter-lg2g6\",\n\u00a0 \"prometheus\":\"cattle-monitoring-system/rancher-monitoring-prometheus\",\n\u00a0 \"service\":\"rancher-monitoring-prometheus-node-exporter\",\n...\n...\n</code></pre>"},{"location":"kbs/000020302/#additional-information","title":"Additional Information","text":"<p>GitHub issue #3325 is opened to add additional labels via UI.</p>"},{"location":"kbs/000020302/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020306/","title":"What Desginated Name (DN) fields are used by Kubernetes when generating custom certificates from a provided CA Cert or PKI tool?","text":"<p>This document (000020306) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020306/#environment","title":"Environment","text":"<p>RKE Kubernetes cluster, where the user decides to generate custom certificates from an external, non-self-signed Root Cerficiate Authority (CA).</p>"},{"location":"kbs/000020306/#situation","title":"Situation","text":"<p>Upon issuing the following command,</p> <pre><code>rke up\n</code></pre> <p>a user might experience the error message below.</p> <pre><code>INFO[0092] [authz] Creating rke-job-deployer ServiceAccount FATA[0119] Failed to apply the ServiceAccount needed for job execution: clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"kube-admin\" cannot create resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\n</code></pre>"},{"location":"kbs/000020306/#resolution","title":"Resolution","text":"<p>While working with your own PKI toolsets, and providing a custom CA (Certificate Authority),</p> <p>check the Organization (O) fields for the generated certificates Common Names (CN) and their signing requests (CSR).\u00a0 The examples here are directly from Kubernetes the Hard Way, and outline a manual process for certificate generation.\u00a0Where ${Instance} is mentioned, this is a variable for the name of the node that represents each kubelet, with every node having its own certificate.</p> Kubernetes Component Certificates Common Name (CN) Organization (O) CA Root kubernetes Kubernetes Admin admin system:masters Kubelet system:node:${Instance} system:nodes Controller Manager system:kube-controller-manager system:kube-controller-manager Kube Proxy system:kube-proxy system:node-proxier Scheduler system:kube-schedluer system:kube-scheduler API Server kubernetes Kubernetes Service Account Key Pair service-accounts Kubernetes"},{"location":"kbs/000020306/#cause","title":"Cause","text":"<p>If the proper Organization fields are not set, Kubernetes cannot assign compatible permissions to the different underlying components.\u00a0 With self-signed certificates these fields are generated automatically.\u00a0 With custom certificates, it's important to verfiy these fields are correct, as they may have been set by external tooling or automation.</p> <p>Sometimes a Kubernetes administrator may leave all the fields of their certificates or CSRs blank or with predefined values, it can be easy to overlook the O field for a specific CN, an the proper kubernetes permissions will not get applied.</p>"},{"location":"kbs/000020306/#additional-information","title":"Additional Information","text":"<p>Kubernetes the Hard Way, Certificate Authority -\u00a0https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md</p>"},{"location":"kbs/000020306/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020328/","title":"containerd.io 1.4.4 bug advisory","text":"<p>This document (000020328) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020328/#environment","title":"Environment","text":"<p>Recently our team has been made aware of PLEG health issues caused by a recent containerd.io package version.</p> <p>This can manifest in a few ways, we usually see all Docker commands failing, but in recent cases, the Docker commands with a specific security flag fail and hang. We have isolated this to containerd.io package version 1.4.4-x, and has been logged in\u00a0this GitHub issue.</p>"},{"location":"kbs/000020328/#situation","title":"Situation","text":""},{"location":"kbs/000020328/#update-as-of-25may21","title":"******Update as of 25MAY21 ********","text":"<p>Our team is now aware that this issue is resolved in rc95 or higher. Please note that the issue was resolved in rc94, but there was a CVE tied to that rc version. Our recommendation for anyone experiencing this issue to move to rc95 or higher to resolve the containerd issues as well as avoiding the known CVE found in rc94. Even if you are not experiencing issues tied to this advisory, we would recommend you move to rc95.</p>"},{"location":"kbs/000020328/#end-update-as-of-25may21","title":"******End Update as of 25MAY21 ********","text":""},{"location":"kbs/000020328/#how-do-i-know-if-i-am-impacted","title":"How do I know if I am impacted?","text":"<p>Customers running RKE could be impacted by this. This is impacting customers running Docker 19.03 and 20.10 where\u00a0containerd.io\u00a0is using 1.4.4 -x. \u00a0Nodes running\u00a0containerd.io 1.4.4 may experience containers hanging on initialization after a certain number of containers with\u00a0<code>no-new-privileges</code>\u00a0are started.</p> <p>Often this has come as a result of upgrading Docker with Rancher 2.5.6. The symptoms include PLEG timeout errors in the Rancher UI, CoreDNS pods failing to start, and\u00a0<code>docker inspect</code>\u00a0commands to hang on certain containers.</p> <p>As the issue relates to the specific runc version (1.0.0-rc93) bundled with containerd.io, the following can be a basic test to identify if the node is running the affected runc build:</p> <pre><code>runc --version | grep -q 1.0.0-rc93 &amp;&amp; echo \"AFFECTED\" || echo \"NOT AFFECTED\"\n</code></pre>"},{"location":"kbs/000020328/#resolution","title":"Resolution","text":""},{"location":"kbs/000020328/#is-there-a-workaround","title":"Is there a workaround?","text":""},{"location":"kbs/000020328/#update-as-of-25may21_1","title":"******Update as of 25MAY21 ********","text":"<p>The below workaround should not be used any longer. \u00a0With the release of rc95 (mentioned above) any customers experiencing this issue should upgrade to rc95 as the resolution is found there.</p>"},{"location":"kbs/000020328/#end-update-as-of-25may21_1","title":"******End Update as of 25MAY21 ********","text":"<p>Yes, currently our team recommends that you take the following step:</p> <p>Downgrade or install the containerd.io package to a\u00a01.4.3-x version.\u00a0 There is no need to modify privileges on CoreDNS pods, once downgraded to 1.4.3 you should pin that version to not auto-update. Please ensure your team is aware of\u00a0CVE-2021-21334\u00a0in 1.4.3-x.</p> <p>As examples of downgrading the containerd.io package on affected nodes:</p> <p>Ubuntu:</p> <pre><code>apt install containerd.io=1.4.3-1\n</code></pre> <p>EL:</p> <pre><code>yum downgrade containerd.io-1.4.3-3.1.el7\n</code></pre> <p>As needed, drain and cordon the node, followed by restarting the Docker daemon.</p> <p>For the most accurate steps, we recommend you consult the documentation for your OS on downgrading and version pinning for the specific package manager and Linux distribution.</p> <p>For customers who have not upgraded their Rancher clusters to 2.5.6+, we recommend that you hold off on upgrading until this is resolved upstream. If you need to upgrade to Rancher 2.5.6+, you should be safe to upgrade to Rancher when using the above process to install and pin the containerd.io package to a 1.4.3-x version.</p> <p>In the meantime, if you have any questions, please reach out to your Customer Success Manager or Rancher Support via a Support Ticket.</p>"},{"location":"kbs/000020328/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020350/","title":"Streaming server stopped unexpectedly: listen tcp x.x.x.x:0: bind: cannot assign requested address","text":"<p>This document (000020350) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020350/#environment","title":"Environment","text":"<p>Rancher v2.5.7</p> <p>Kubernetes v1.20.4-rancher1</p>"},{"location":"kbs/000020350/#situation","title":"Situation","text":"<p>Adding a new node to an existing cluster stuck in \u201cregistering\u201d.</p> <p>Kubelet pod is in\u00a0<code>Restarting</code>\u00a0state on the new node.</p> <pre><code>$ docker ps -a |grep kubelet\n</code></pre> <pre><code>66bd40b36e76 rancher/hyperkube:v1.20.4-rancher1 \"/opt/rke-tools/entr\u2026\" 7 minutes ago Restarting (255) 32 seconds ago kubelet\n</code></pre> <p>The kubelet is failing with below error.</p> <pre><code>$ docker logs kubelet\n</code></pre> <pre><code>\"2021-07-26T11:13:48.270162766Z F0726 11:13:48.270086   40730 docker_service.go:415] Streaming server stopped unexpectedly: listen tcp 27.0.0.1:0: bind: cannot assign requested address\"\n</code></pre>"},{"location":"kbs/000020350/#resolution","title":"Resolution","text":"<p>Update\u00a0<code>/etc/hosts</code>\u00a0file with the correct entry.</p> <pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n</code></pre> <p>After updating the file, the kubelet will restart itself and pick up the modified /etc/host file to bind to the loopback IP.</p>"},{"location":"kbs/000020350/#cause","title":"Cause","text":"<p>File /etc/hosts in the new node had below incorrect entry.</p> <pre><code>27.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n</code></pre>"},{"location":"kbs/000020350/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020373/","title":"How to drain node from local node using docker command","text":"<p>This document (000020373) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020373/#environment","title":"Environment","text":"<p>RKE1</p> <p>Rancher v2.4.x , v2.5.x</p>"},{"location":"kbs/000020373/#situation","title":"Situation","text":"<p>Need a way to cordon or drain node when OS patching is automated, and the automation doesn't have access to Rancher API.</p>"},{"location":"kbs/000020373/#resolution","title":"Resolution","text":"<p>Integrate the below command in the automation to drain the node in the OS patching workflow.</p> <pre><code>docker exec kubelet bash -c 'kubectl --kubeconfig &lt;(kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\") drain $(hostname -s) --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s'\n</code></pre> <p>Please note that the below flags need to be changed according to your requirements.</p> <pre><code>--delete-local-data=true\n--force=true\n--grace-period=60\n--ignore-daemonsets=true\n--timeout=120s\n</code></pre>"},{"location":"kbs/000020373/#cause","title":"Cause","text":"<p>Draining operation using kubeconfig file \" /etc/kubernetes/ssl/kubecfg-kube-node.yaml\" will result in errors like below since the \" system:node\" role is not authorized to access needed API groups.</p> <pre><code>cannot delete daemonsets.apps \"nginx-ingress-controller\" is forbidden: User \"system:node\" cannot get resource \"daemonsets\" in API group \"apps\" in the namespace \"ingress-nginx\"\n</code></pre> <p>But this kubeconfig file can access the config map \"full-cluster-state\" in namespace \" kube-system\"\u00a0 contains the kubeconfig file, which has the privilege to do the drain operation.</p>"},{"location":"kbs/000020373/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020415/","title":"Updating roles in Rancher UI fails with certificate error","text":"<p>This document (000020415) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020415/#environment","title":"Environment","text":"<p>Rancher v2.5.x</p>"},{"location":"kbs/000020415/#situation","title":"Situation","text":"<p>Editing roles in Rancher UI fails with the below error.</p> <pre><code>Internal error occurred: failed calling webhook \"rancherauth.cattle.io\": Post \"https://rancher-webhook.cattle-system.svc:443/v1/webhook/validation?timeout=10s\": x509: certificate has expired or is not yet valid: current time 2021-10-25T07:43:50Z is after 2021-10-06T20:20:47Z\n</code></pre>"},{"location":"kbs/000020415/#resolution","title":"Resolution","text":"<ul> <li>Set kubectl context to Rancher management cluster.</li> <li>Take the backup of existing secret</li> </ul> <pre><code>kubectl get secret -n cattle-system cattle-webhook-tls -o yaml &gt; cattle-webhook-tls.yaml\n</code></pre> <ul> <li><code>Delete the secret that contains expired certificate <pre><code>\n</code></pre> kubectl delete secret -n cattle-system cattle-webhook-tls <pre><code>- Delete the rancher webhook Pod to regenerate the expired certificate.\n</code></pre> kubectl delete pod -n cattle-system -l app=rancher-webhook</code></li> </ul>"},{"location":"kbs/000020415/#cause","title":"Cause","text":"<p>This issue is caused by the expired certificate of the rancher webhook.</p>"},{"location":"kbs/000020415/#additional-information","title":"Additional Information","text":"<p>The issue is tracked in GitHub issue\u00a035068</p>"},{"location":"kbs/000020415/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020416/","title":"Downstream clusters flapping between available and unavailable state","text":"<p>This document (000020416) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020416/#environment","title":"Environment","text":"<p>Rancher version: v2.5.6</p> <p>Management cluster K8S version: 1.21.5</p>"},{"location":"kbs/000020416/#situation","title":"Situation","text":"<p>After upgrading the Kubernetes version of the Rancher management cluster, the downstream cluster status in the WebUI flaps between the available and unavailable states.</p> <p>Rancher Pod logs show errors like the below;</p> <pre><code>Failed to connect to peer wss://x.x.x.x/v3/connect [local ID=y.y.y.y]: websocket: bad handshake\n</code></pre>"},{"location":"kbs/000020416/#resolution","title":"Resolution","text":"<p>Upgrade Rancher to v2.6.x</p> <p>A workaround until Rancher upgarde is to reduce the Rancher deployment replicas to one.</p>"},{"location":"kbs/000020416/#cause","title":"Cause","text":"<p>Rancher is storing the service account token from the initial Pod, and then trying to reuse that on subsequent requests even though that pod has been deleted.</p> <p>As of Kubernetes version v1.21, service account tokens are pod-specific, and are invalidated when the pod is deleted, which is why Rancher is unable to use it and thus unable to reach other Rancher replica instances via web-socket.</p>"},{"location":"kbs/000020416/#additional-information","title":"Additional Information","text":"<p>The issue is tracked in the GitHub issue\u00a026082</p>"},{"location":"kbs/000020416/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020430/","title":"[Rancher] What is the difference between \"stable\" and \"latest\" release tags?","text":"<p>This document (000020430) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020430/#resolution","title":"Resolution","text":"<p>For the Rancher portfolio of products, we make available \u201clatest\u201d tagged releases for our community to test-drive a new release and provide us feedback. These \u201clatest\u201d tagged releases, whilst covered by Rancher Support Services, are not meant for production use cases. Customers are recommended to use the \u201cstable\u201d tagged releases for their own production use cases.</p>"},{"location":"kbs/000020430/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020430/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020431/","title":"[Rancher] What does 'Developer Support\" mean?","text":"<p>This document (000020431) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020431/#resolution","title":"Resolution","text":"<p>Any mention of \"Developer Support\" is basically a reference to this:</p> <p>Rancher Support is part of Rancher Engineering organization and has access to Engineers who developed Rancher product features. These Engineers are able to assist Rancher Support should some deep troubleshooting investigation be needed.</p>"},{"location":"kbs/000020431/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020432/","title":"Rancher Support FAQ","text":"<p>This document (000020432) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020432/#resolution","title":"Resolution","text":"## Quick Links- SUSE Rancher Product Lifecycle Information- SUSE Rancher Product Support Matrices- SUSE Rancher Support Advisories- SUSE Rancher Hosted FAQ- SUSE Customer Center\u00a0(SCC) (Support Portal)\u00a0- SCC sign-in help- Rancher 2.x Linux log collector script- Rancher 2.x Windows log collector script- Rancher 2.x Systems summary script"},{"location":"kbs/000020432/#severity-levels-designations-and-escalations","title":"Severity Levels, Designations, and Escalations","text":"<ul> <li>Could you help us understand\u00a0how to determine the severity level for my case, what the response times are? (SUSE Support blog post)</li> <li>How do I change the severity level on an open case?</li> <li>Could you illustrate the severity levels with subject lines of sample support cases?</li> </ul>"},{"location":"kbs/000020432/#product-releases-and-support-phases","title":"Product Releases and Support Phases","text":"<ul> <li>What is the difference between \"stable\" and \"latest\" release tags?</li> <li>What does 'Developer Support\" mean?</li> <li>What is \"Full Support\" vs \"Limited Support\"?</li> <li>Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version?</li> </ul>"},{"location":"kbs/000020432/#request-for-assistance","title":"Request for Assistance","text":"<ul> <li>Can Rancher Support validate our planned upgrade?</li> <li>We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?</li> <li>How much in advance do we need to notify Rancher Support on upgrades?</li> <li>Can Rancher Support join me as I do my upgrade?</li> <li>We need help validating our deployment design and operational readiness, can Rancher Support help?</li> </ul>"},{"location":"kbs/000020432/#rancher-and-rancheros","title":"Rancher and RancherOS","text":"<ul> <li>Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?</li> <li>Could you help us understand the development and support status of RancherOS for 2020 and beyond?</li> <li>Does Rancher support migration from single node installation to high availability installation?</li> <li>Does Rancher Support cover single node installations?</li> </ul>"},{"location":"kbs/000020432/#kubernetes","title":"Kubernetes","text":"<ul> <li>How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?</li> <li>I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?</li> <li>As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?</li> <li>Is Rancher Support only for RKE-provisioned clusters?</li> <li>Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?</li> </ul>"},{"location":"kbs/000020432/#docker-and-os","title":"Docker and OS","text":"<ul> <li>What does Rancher support for Docker on Ubuntu cover?</li> <li>Will Rancher support us should our deployment be on Red Hat Atomic?</li> <li>What does Rancher support for Docker on RHEL cover?</li> <li>What does Rancher support for Docker on CentOS cover?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL?</li> <li>What does Rancher support for Docker on Windows Server cover?</li> <li>What does Rancher support for Docker on Oracle Linux cover?</li> </ul>"},{"location":"kbs/000020432/#security","title":"Security","text":"<ul> <li>How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?</li> <li>How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?</li> </ul>"},{"location":"kbs/000020432/#certified-integrations","title":"Certified Integrations","text":"<ul> <li>What are the certified integrations with persistent volume plugins covered by Rancher Support?</li> <li>What are the certified integrations with storage class provisioners covered by Rancher Support?</li> <li>What are the certified integrations with authentication providers covered by Rancher Support?</li> <li>Could you clarify what you generally mean by a \"certified integration\" to another software system or service?</li> </ul>"},{"location":"kbs/000020432/#included-open-source-software-components","title":"Included Open Source Software Components","text":"<ul> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?</li> <li>Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?</li> <li>What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?</li> <li>Will Rancher fix problems root-caused to be in nginx?</li> <li>Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?</li> <li>We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?</li> </ul>"},{"location":"kbs/000020432/#third-party-software-components","title":"Third-party Software Components","text":"<ul> <li>Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software?</li> <li>Does Rancher include any 3rd party, commercial software components?</li> </ul>"},{"location":"kbs/000020432/#support-matrix","title":"Support Matrix","text":"<ul> <li>We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA?</li> <li>We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then?</li> <li>I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then?</li> </ul>"},{"location":"kbs/000020432/#node-drivers-and-infrastructure","title":"Node Drivers and Infrastructure","text":"<ul> <li>My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack?</li> <li>Does it matter what hardware my hosts are on? Are virtualized servers supported?</li> <li>I see node drivers tagged as \"Built-in\". What does that mean?</li> <li>I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?</li> </ul>"},{"location":"kbs/000020432/#kubernetes-cloud-providers","title":"Kubernetes Cloud Providers","text":"<ul> <li>What are the Kubernetes cloud providers supported by Rancher?</li> </ul>"},{"location":"kbs/000020432/#requests-for-enhancements-rfes","title":"Requests for Enhancements (RFEs)","text":"<ul> <li>I filed an RFE as a support case. What should I expect on how it will be followed up on?</li> </ul>"},{"location":"kbs/000020432/#support-for-all-other-rancher-software-projects","title":"Support for all other Rancher software projects","text":"<ul> <li>What about support for Harvester, Longhorn, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider?</li> <li>Does Rancher Support cover RKE (standalone CLI)?</li> <li>How about support for ancillary projects, such as an API client, from Rancher Labs?</li> </ul>"},{"location":"kbs/000020432/#customizations","title":"Customizations","text":"<ul> <li>We have a few customizations with our on- premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations?</li> <li>Is there a way we can get our custom work be included into Rancher Support?</li> </ul>"},{"location":"kbs/000020432/#localization","title":"Localization","text":"<ul> <li>What language(s) is Rancher Support service offered in?</li> <li>Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support?</li> </ul>"},{"location":"kbs/000020432/#licensing-and-usage","title":"Licensing and Usage","text":"<ul> <li>Does my support subscription to Rancher include support for Longhorn?</li> <li>Does my support subscription to Rancher include support for RancherOS?</li> <li>Can we have a mix of unsupported and supported nodes at our choice/discretion?</li> <li>Can we request support for the management/upstream cluster and certain downstream clusters and not others?</li> <li>How does Rancher track our license usage?</li> <li>Is Rancher Support only for production environments?</li> <li>I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well?</li> <li>Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue?</li> </ul>"},{"location":"kbs/000020432/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020434/","title":"[Rancher] I filed an RFE as a support case. What should I expect on how it will be followed up on?","text":"<p>This document (000020434) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020434/#resolution","title":"Resolution","text":"<p>For any RFEs received via a support case, Rancher Support will update the case with a GitHub link to the enhancement request.</p> <p>Rancher Support will try to understand your request first and then qualify the request with information on business impact, time sensitivity, and criticality. It is not uncommon at all for there to be follow-up questions from Rancher Support on your request. This information will be most helpful to advocate for the request with Rancher Product Management.</p> <p>Unless there is a pressing urgency that has been understood and acknowledged, it could take up to a few weeks to triage the RFE (from the product backlog) through our release planning and identify the earliest release vehicle the item could be considered for (or committed to).</p> <p>As there is progress and/or should there be specific questions from Engineering, Rancher Support will keep you updated via this case. As necessary, Rancher Support will also recommend and schedule/facilitate a direct web conference session for you with our product management to go over the RFE.</p> <p>For committed RFEs, Rancher Support will update you as the item gets closer to its general availability via a new Rancher release or keep you informed should there be any delays or proposed changes in the previously understood scope.</p>"},{"location":"kbs/000020434/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020435/","title":"[Rancher] What is \"Full Support\" vs \"Limited Support\"?","text":"<p>This document (000020435) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020435/#resolution","title":"Resolution","text":"<p>\"Full Support\" is the support phase when a product is between its GA (General Availability) and End of Maintenance (EOM) milestones. Support entails general troubleshooting of a specific issue to isolate potential causes. Issue resolution is pursued through one or more of the following:</p> <ul> <li>Applying configuration changes</li> <li>Upgrade recommendation to an existing newer version of the product</li> <li>Code-level maintenance in the form of product updates; typically, results in a maintenance release, which is a newer version of the product that was not existing at the time the issue was encountered</li> </ul> <p>\"Limited Support\" is the support phase when a product is between its EOM and End of Life (EOL) milestones. During this phase, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner in the form of:</p> <ul> <li>General troubleshooting of a specific issue to isolate potential causes</li> <li>Issue resolution is limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product</li> </ul>"},{"location":"kbs/000020435/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020436/","title":"[Rancher] Will Rancher fix the specific product version that my deployment is on, should a bug be reported in that version?","text":"<p>This document (000020436) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020436/#resolution","title":"Resolution","text":"<p>As our standard policy, any bug will only be fixed and delivered as a new maintenance release within the minor version of a Rancher product. Customer will be advised to upgrade to this new maintenance release that contains the fix.</p> <p>Table below illustrates this with an example:</p> v2.1.3 Version that the Customer deployment is currently on. This is the product version that the bug has been found and acknowledged in, stemming from the support issue reported by\u00a0Customer. v2.1.9 Version that is the latest maintenance release of Rancher v2.1 at the time the bug was found and acknowledged. v2.1.c Version \"c\" is where the bug is likely to get fixed. Here, \"c\" = 10 or greater. This is the soonest maintenance release of Rancher 2.1 that the Customer will need to upgrade to, to realize\u00a0the bug fix. <p>Should the bug fix be deemed as necessary for other minor versions of the same Rancher product, Rancher shall deliver the fix in an approach similar to the sequence in the above table. For a scenario where v2.0.14 and v2.2.2 are the latest maintenance releases of Rancher v2.0 and v2.2, and the same bug fix needs to be made available in these two minor versions, it is shall only be delivered in a release that is v2.0.15 or later and v2.2.3 or later, respectively.</p> <p>In extraordinary situations, as an exception to our standard policy and as feasible, Rancher can provide a patch fix on a specific Customer-requested version.</p>"},{"location":"kbs/000020436/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020437/","title":"[Rancher] Can Rancher Support validate our planned upgrade?","text":"<p>This document (000020437) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020437/#resolution","title":"Resolution","text":"<p>Rancher Support recommends Customers to notify them ahead of planned maintenance events for upgrade and migration. The notification can be made as a support case. At a minimum, this case should have information on:</p> <ul> <li>Current Rancher version</li> <li>Target Rancher version</li> <li>Reason for upgrade</li> <li>Upgrade date and maintenance time window</li> </ul> <p>But it would be best if Customer could provide the requested information in a document (please ask for the \" Upgrade Notification RFI Template\" in the support case). Based on the information provided, Rancher Support can provide any applicable advisories to the Customer for the planned event. Rancher Support may request Customer to run specific information gathering scripts. Data collected from these scripts will be used by Rancher Support to understand the Customer deployment and validate the upgrade path that is being considered.</p>"},{"location":"kbs/000020437/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020437/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020438/","title":"[Rancher] We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?","text":"<p>This document (000020438) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020438/#resolution","title":"Resolution","text":"<p>Rancher Support will not join the Customer team remotely for the duration of the event. However, they will remain on standby, on high alert, to respond per SLA should there be an issue.</p> <p>Rancher Support recommends Customer to notify in advance, via a support ticket, all necessary information about such events. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the event.</p>"},{"location":"kbs/000020438/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020439/","title":"[Rancher] How much in advance do we need to notify Rancher Support on upgrades?","text":"<p>This document (000020439) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020439/#resolution","title":"Resolution","text":"<p>On all regular, planned upgrades, Rancher Support requests Customer to notify as early as possible. Notifications that are one (1) calendar week in advance provide reasonable time for Rancher Support to follow up with any applicable advisories.</p>"},{"location":"kbs/000020439/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020440/","title":"[Rancher] Can Rancher Support join me as I do my upgrade?","text":"<p>This document (000020440) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020440/#resolution","title":"Resolution","text":"<p>Generally speaking, Rancher Support will not join a Customer for the purpose of performing upgrades of their environment. However, Customers are encouraged to notify Rancher Support of planned maintenance events for upgrade and migration. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the maintenance window. Additionally, such notifications provide for an opportunity to validate the planned upgrade proactively rather than react to issues that could have been avoided.</p>"},{"location":"kbs/000020440/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020441/","title":"[Rancher] We need help validating our deployment design and operational readiness, can Rancher Support help?","text":"<p>This document (000020441) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020441/#resolution","title":"Resolution","text":"<p>Validating deployment design and operational readiness is an activity for SUSE Rancher Premium Support and/or Consulting teams. Requests coming in as a support case for this activity shall be routed to the leadership team of Rancher Premium Support and Consulting, for follow-up with the Customer.</p> <p>Topics such as the following, but not limited to, will be handled in a similar manner:</p> <ul> <li>Best Practice Guidance</li> <li>Deployment Review</li> <li>Design Assistance</li> <li>Co-development</li> <li>Migration Assistance</li> <li>Performance Tuning</li> <li>Security Assessment</li> <li>Solution Consulting</li> <li>Topical Training</li> <li>Technology Recommendation</li> </ul> <p>Typically, these are topics that are not specific to incidents and issue troubleshooting. They are more requests with strategic drivers that usually require the help and guidance of SUSE Rancher Premium Support and/or Consulting teams.</p>"},{"location":"kbs/000020441/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020441/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020442/","title":"[Rancher] Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?","text":"<p>This document (000020442) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020442/#resolution","title":"Resolution","text":"<p>Whilst technically possible, running other workloads or microservices in the same Kubernetes cluster that Rancher is installed on invalidates the Rancher Support SLA. So it is something that is recommended against.</p> <p>Any technical support that is offered for tickets stemming from this scenario shall only be on a best-effort basis.</p> <p>Note:</p> <p>Run Rancher on a Separate Cluster is also one of the top items called out in our docs page on Tips for Running Rancher.</p>"},{"location":"kbs/000020442/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020443/","title":"[Rancher] Could you help us understand the development and support status of RancherOS for 2020 and beyond?","text":"<p>This document (000020443) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020443/#resolution","title":"Resolution","text":""},{"location":"kbs/000020443/#development-status","title":"Development Status","text":""},{"location":"kbs/000020443/#edit-nov-2021-rancheros-1x-is-past-its-eol-milestone-and-no-longer-maintained","title":"EDIT (NOV 2021): RancherOS 1.x is past its EOL milestone and no longer maintained.","text":"<p>RancherOS 1.x is currently in a maintain-only-as-essential mode. That is to say, it is no longer being actively maintained at a code level other than addressing critical or security fixes. There are two significant reasons behind this product decision:</p> <p>1. Docker. The current industry requirements for a container runtime is very much evolving. Container runtimes like containerd and CRIO are now being actively considered as the default choice. RancherOS 1.x, which was specifically designed around using Docker engine only, unfortunately does not lend itself, in its current design, to this new evolving requirement.</p> <p>2. ISV Support. RancherOS was specifically designed as a minimalistic OS to support purpose-built containerized applications. It was not designed to be used as a general-purpose OS (such as CentOS or Ubuntu). As such, most ISVs have not certified their software to run on RancherOS, nor does RancherOS even contain the necessary components for many of these applications to run.</p>"},{"location":"kbs/000020443/#support-status","title":"Support Status","text":""},{"location":"kbs/000020443/#rancheros-1x-is-no-longer-commercially-supported","title":"RancherOS 1.x is no longer commercially supported.","text":"<p>Please refer this FAQ: Does my support subscription to Rancher include support for RancherOS?</p> <p>Any assistance from Rancher Support on RancherOS topics, filed as support cases, is not SLA- bound.</p>"},{"location":"kbs/000020443/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020444/","title":"[Rancher] Does my support subscription to Rancher include support for RancherOS?","text":"<p>This document (000020444) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020444/#resolution","title":"Resolution","text":"<p>No. RancherOS is an independent product. A separate commercial support subscription is\u00a0needed for RancherOS support.\u00a0 But that is no longer available for purchase as RancherOS 1.x\u00a0is past its End-of-Life (EOL) and End-of-Sale (EOS)\u00a0milestone dates.</p> <p>Also, refer Could you help us understand the development and support status of RancherOS for 2020 and beyond?</p>"},{"location":"kbs/000020444/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020445/","title":"[Rancher] Does Rancher support migration from single node installation to high availability installation?","text":"<p>This document (000020445) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020445/#resolution","title":"Resolution","text":"<p>There is currently no validated path that is officially covered by Rancher Support, for any Customer starting with a single node installation, and wishing to migrate to a high availability installation at a later time. Whilst there is a Rancher blog post\u00a0that talks about one possible migration path, it is not covered by Rancher Support.</p> <p>Where scale and performance criteria are well understood to be critical, Customers are recommended to set up Rancher in a high availability configuration, right from the outset.</p>"},{"location":"kbs/000020445/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020446/","title":"[Rancher] Does Rancher Support cover single node installations?","text":"<p>This document (000020446) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020446/#resolution","title":"Resolution","text":"<p>Rancher Support can cover single node installations only for support tickets that are NOT related to (a) scale and performance and (b) recovery of data and software (embedded etcd).</p> <p>Customer environments in need of scale and performance and meeting recovery criteria should be set up as high availability installations.</p> <p>Refer this Rancher docs page for single node installation versus high availability installation. Rancher recommends high-availability installs in production environments, where the Customer's user base requires 24\u20447 access to running applications.</p>"},{"location":"kbs/000020446/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020447/","title":"[Rancher] How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?","text":"<p>This document (000020447) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020447/#resolution","title":"Resolution","text":"<p>Kubernetes versions are expressed as x.y.z, where x is the major version, y is the minor version, and z is the patch version, following Semantic Versioning terminology.</p> <p>Where an upstream version x.y.z has been posted as supported in the support matrix, under Rancher Kubernetes, z is the highest patch version for that minor version (y) of Kubernetes, that has been tested and validated for the specific Rancher product version.</p> <p>In the case of Rancher v2.6.2 , the following are listed as the supported k8s upstream versions for Rancher Kubernetes:</p> <ul> <li>v1.21.5</li> <li>v1.20.11</li> <li>v1.19.15</li> <li>v1.18.20</li> </ul> <p>That is to say, the following are the k8s versions that are supported in Rancher v2.6.2:</p> <ul> <li>v1.21.x (v1.21.0-v1.21.5)</li> <li>v1.20.x (v1.20.0-v1.20.11)</li> <li>v1.19.x (v1.19.0-v1.19.15)</li> <li>v1.18.x (v1.18.0-v1.18.20)</li> </ul> <p>Note:</p> <p>As described in this\u00a0Rancher docs page, the RKE metadata feature\u2014available as of v2.3.0\u2014allows users to provision clusters with new versions of Kubernetes as soon as they are released, without upgrading Rancher. For a specific version of Rancher, if a k8s patch version ( z+i) that is higher than what is listed in the support matrix is available via a metadata refresh, then that patch version z+i is considered supported for that version of Rancher.</p> <p>Also, see Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?</p>"},{"location":"kbs/000020447/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020448/","title":"[Rancher] Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?","text":"<p>This document (000020448) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020448/#resolution","title":"Resolution","text":"<p>Rancher supports the most recent three minor releases of kubernetes that are still active. Additionally, Rancher may also support one (or more) recent upstream version to drop off its active maintenance status.\u00a0 As an example, below are the highest k8s minor versions supported by the respective Rancher 2.x product versions (as of January 2022):</p> <ul> <li>Rancher v2.4.18 =&gt; k8s 1.15, 1.16, 1.17, 1.18</li> <li>Rancher v2.5.14 =&gt; k8s 1.17, 1.18, 1.19, 1.20</li> <li>Rancher v2.6.3 =&gt; k8s 1.18, 1.19, 1.20, 1.21</li> </ul> <p>Arithmetic progression, if any, in the sequence above is merely coincidental and should not be used to extrapolate the k8s versions that a future version of Rancher such as v2.7 would support. Also, for the most up-to-date information, please visit the All Supported Versions page.</p> <p>Generally speaking, the following should help understand the Rancher approach to supporting k8s versions:</p> <p>Rancher Labs strives to certify the latest GA release of k8s roughly in a month's timeframe from its availability. For example, k8s v1.16 became generally available in September 2019. The Rancher roadmap consideration would then be to certify and support k8s v1.16 in a release vehicle targeted for no later than October 2019.</p> <p>The ability to certify a new GA release of k8s, per above, could however be impacted by any unplanned- for CVEs that Rancher Labs needs to react to. This turned out to be the case for Rancher v2.3.1 that shipped on 16 Oct 2019.</p> <p>The focus of v2.3.1 shifted to addressing on priority a new k8s CVE (CVE-2019-11253) announced by upstream kubernetes. And, hence the support for k8s v1.16 got moved to the release vehicle after v2.3.1. And, when v1.16 is supported in that release, v1.13 shall be dropped in our support matrix from that version forward.</p> <p>This is to also keep up with the k8s version maintenance policy that you can see here: https://kubernetes.io/docs/setup/release/version-skew-policy/#supported-versions</p> <p>Specifically,</p> <p>\"The Kubernetes project maintains release branches for the most recent three minor releases.</p> <p>Minor releases occur approximately every 3 months, so each minor release branch is maintained for approximately 9 months.\"</p> <p>Also, see\u00a0How should I understand the kubernetes patch version mentioned in the support matrix under Rancher Kubernetes for a specific Rancher product version?</p>"},{"location":"kbs/000020448/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020449/","title":"[Rancher] I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?","text":"<p>This document (000020449) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020449/#resolution","title":"Resolution","text":"<p>Rancher Support is comprehensive for RKE-provisioned clusters. In imported clusters, Rancher Support applies only to the extent of troubleshooting and root-causing. For issues in (legacy) clusters under consideration for import into Rancher, Customer is advised to take it up with the party that is the provider of support for such clusters. Post-import, Rancher Support for such clusters is per response to this question, Is Rancher Support only for RKE-provisioned clusters?</p>"},{"location":"kbs/000020449/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020450/","title":"[Rancher] Is Rancher Support only for RKE-provisioned clusters?","text":"<p>This document (000020450) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020450/#resolution","title":"Resolution","text":"Update:In November 2019, k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here . With the general availability of k3s, Rancher Support extends to k3s clusters. <p>Comprehensive Rancher Support, inclusive of Kubernetes and Docker, applies only to RKE-provisioned clusters for Rancher releases before 2.6.5. \u00a0As of the 2.6.6 release of Rancher, k3s, RKE2 are fully supported as well What this means is the following:</p> <p>In the RKE-provisioned clusters, Rancher can provide patch fixes as needed at the levels of Kubernetes and Docker.</p> <p>For clusters that are brought under management of the Rancher control plane, as imported clusters, Rancher Support applies only to those Kubernetes versions published in the support matrix and to the extent of making sure Rancher control plane functionality works as published in Rancher docs. Issues root- caused to be inside these clusters will need to be taken up by Customer with the provider of support for these clusters.</p>"},{"location":"kbs/000020450/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020451/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?","text":"<p>This document (000020451) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020451/#resolution","title":"Resolution","text":"<p>Yes, should there be a critical need, Rancher can provide patch fixes to address issues root-caused in RKE-provisioned Kubernetes clusters. As a first option, Rancher will investigate if the fix is already available in a later version of Kubernetes. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later Kubernetes version that has the fix. Or validate and certify one of its existing versions to work with the later Kubernetes version that has the fix.</p> <p>Further, Rancher can submit a PR for the fix to Kubernetes for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Kubernetes.</p>"},{"location":"kbs/000020451/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020452/","title":"[Rancher] As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?","text":"<p>This document (000020452) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020452/#resolution","title":"Resolution","text":"<p>It is recommended to move to the k8s versions listed in the matrix for a Rancher version but not doing that should not break things. You could leave the cluster (for example, k8s v1.11) as is, for a move to say Rancher v2.2.6 from v2.2.2. It should be ok. Rancher Support will continue to help should there be a need.</p> <p>That said, should there be an issue on that cluster that requires a fix in k8s v1.11 we may not be able to do that and would at that point require an upgrade to a higher supported k8s version.</p>"},{"location":"kbs/000020452/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020453/","title":"[Rancher] What are the Kubernetes cloud providers supported by Rancher?","text":"<p>This document (000020453) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020453/#resolution","title":"Resolution","text":"<p>Rancher Support is currently limited to Kubernetes cloud providers for Amazon and Azure. When adding a cluster, Amazon and Azure are the only two cloud providers that are currently surfaced up in the Rancher UX.</p> <p>For all other cloud providers, directly editing the yaml file via the custom option is the only way to pass configuration information. In this scenario, Rancher Support expects Customer to manage and troubleshoot the syntactic correctness of the yaml file, per guidance\u00a0provided by Kubernetes.</p>"},{"location":"kbs/000020453/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020454/","title":"[Rancher] What about support for Harvester, Longhorn, Rancher Desktop, rio, k3s, k3os, Submariner, and Terraform Rancher2 Provider?","text":"<p>This document (000020454) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020454/#resolution","title":"Resolution","text":"Note:In December 2021, Harvester graduated from being just a community project led by SUSE Rancher to a product that is now commercially supported by SUSE Rancher with a subscription to an add-on plan.\u00a0 View release notes for Harvester v1.0.0 here.In June 2020, Longhorn graduated from being just a community project led by Rancher Labs to a product that is now commercially supported by Rancher Labs with a subscription to an add-on plan. View release notes for Longhorn v1.0.0 here.In June 2020, Terraform Provider for Rancher v2 graduated from a community project that was supported on a best-effort basis to being fully supported as part of an active subscription to a SUSE Rancher Support plan. Support for this project does not require any additional subscriptions. Visit the project repo here.In November 2019, k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here. <p>Projects Rancher Desktop, Kubewarden, Hypper, Epinio, and Opni are some of the new open-source software projects led by SUSE Rancher. These projects (and some legacy ones from Rancher Labs such as rio, k3os, and Submariner) are not yet available for commercial support from SUSE Rancher. For any bugs or questions, users are encouraged to post their issues here:</p> Project GitHub Location Rancher Desktop https://github.com/rancher-sandbox/rancher-desktop/issues Kubewarden https://github.com/kubewarden Hypper https://github.com/rancher-sandbox/hypper/issues Epinio https://github.com/epinio/epinio/issues Opni https://github.com/rancher/opni/issues rio https://github.com/rancher/rio/issues k3os https://github.com/rancher/k3os/issues Submariner https://github.com/submariner-io/submariner/issues"},{"location":"kbs/000020454/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020454/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020456/","title":"[Rancher] How about support for ancillary projects, such as an API client, from Rancher Labs?","text":"<p>This document (000020456) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020456/#resolution","title":"Resolution","text":"<p>Projects such as the Rancher Python Client are maintained by SUSE Rancher on a best effort basis only.\u00a0 Any Rancher user is welcome to use them. However, these projects are not covered by Rancher SLA. For any bugs or questions, users are encouraged to post their issues here:</p> Project GitHub Location Rancher ClientPython https://github.com/rancher/client-python/issues"},{"location":"kbs/000020456/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020457/","title":"[Rancher] Does Rancher Support cover RKE (standalone CLI)?","text":"<p>This document (000020457) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020457/#resolution","title":"Resolution","text":"<p>Yes, support for RKE is implicit and covered under an active Rancher subscription to one of our support plans.\u00a0T ~~here is currently no separate product SKU that offers commercial support separately and only for RKE.~~</p>"},{"location":"kbs/000020457/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020458/","title":"[Rancher] We have a few customizations with our on-premise deployment. For example, we run a forked version of the RKE OpenStack driver with new capabilities. Will Rancher Support cover such customizations?","text":"<p>This document (000020458) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020458/#resolution","title":"Resolution","text":"<p>Customizations are defined to include any changes to the original source code, including but not limited to changes to the User Interface, the addition or modification of adapters such as for authentication, VM or server provisioning, deploying the software (e.g., the management server) on an operating system or Docker versions that are not certified by SUSE Rancher, and altering the scripts and byte code included with the product. Customizations to this software may have unintended consequences and cause issues that are not present with the original, unmodified software. As a result, it is our\u00a0policy that any bugs, defects, or other issues that are present in areas of the product that the Customer has altered must be reproduced by the Customer on an unmodified system prior to the submission of a support case or bug report. Additionally, the Customer is required to state all customizations present on the system when submitting a support case.</p>"},{"location":"kbs/000020458/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020459/","title":"[Rancher] Is there a way we can get our custom work be included into Rancher Support?","text":"<p>This document (000020459) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020459/#resolution","title":"Resolution","text":"<p>By definition, something custom is needed because it solves a unique (or specific or snowflake) requirement for one particular team. A product is meant to solve for 1000\u2019s (and more) of teams.</p> <p>To take on support for custom work is not merely about Rancher Support services looking into tickets and issues reported in them. Besides the cost of the initial vetting and validation effort on the custom work, it has ongoing Engineering and QA impact. Rancher has to keep testing (as QA) the custom work across every next release to make sure that nothing is broken. Maintain knowledge of what the custom work is about and keep maintaining (as Engineering) that piece of code for its entire life cycle including potential improvements as needed.</p> <p>This is expensive, distracting, and misaligned with the objectives of any product team. Hence, custom work will not be considered for inclusion in product/support. And, any issues in such custom work are considered out of scope of Rancher Support.</p> <p>That said, here are scenarios and options that could be considered, if what started as custom work by one team is believed to be valuable for many teams and a customer is interested in exploring its productization (and support) in Rancher:</p> <ul> <li>Rancher is 100% open source. Users are welcome to make contributions / PRs on the public Rancher repos on GitHub. Contributions will be evaluated for inclusion in product, based on merit and alignment with the Rancher roadmap. Customers are welcome to optionally advocate for their contribution via their Rancher Customer Success Manager or Account Executive.</li> <li>To accelerate and secure commitment toward productizing a specific work or feature, Customer can explore the possibility of a Non-Recurring Engineering (NRE) engagement with Rancher on a commercial basis. In this scenario, Customer is recommended to request a conversation with Rancher Product Management, via their Rancher Customer Success Manager or Account Executive. Any NRE work shall be pursued by Rancher only if it is determined by Rancher Product Management as being viable and in alignment with the product roadmap and its strategic goals.</li> </ul> <p>Lastly, most Rancher customers that have such custom software have their own dev teams that support and maintain them across the versions of all vendor and open-source software, Rancher included, that they use for their overall solution.</p>"},{"location":"kbs/000020459/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020460/","title":"[Rancher] What language(s) is Rancher Support service offered in?","text":"<p>This document (000020460) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020460/#resolution","title":"Resolution","text":"<p>English is the official language in which support is delivered to global customers, with the exception of China, where it is in Chinese. If it is deemed helpful and necessary, SUSE Rancher Support will engage colleagues, from our\u00a0Premium Support, Consulting, and Customer Success teams, who are fluent in specific local languages for assistance on a case.</p>"},{"location":"kbs/000020460/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020461/","title":"[Rancher] Besides English, we see many languages listed in the Rancher UI dropdown. Are they all covered by Rancher Support?","text":"<p>This document (000020461) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020461/#resolution","title":"Resolution","text":"<p>No. English and Chinese (simplified+traditional) are the only languages currently covered by Rancher Support. Translations for all the other languages are maintained by Rancher Community users.</p> <p>Please visit https://translate.rancher.com/\u00a0if you would like to help translate Rancher into other languages.</p>"},{"location":"kbs/000020461/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020462/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?","text":"<p>This document (000020462) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020462/#resolution","title":"Resolution","text":"<p>Yes. Prometheus and Grafana are the components natively used and supported by Rancher v2.2+ for monitoring and dashboards. Any issues root-caused in one of these two projects, as an included component of Rancher v2.2+, will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"kbs/000020462/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020463/","title":"[Rancher] Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x?","text":"<p>This document (000020463) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020463/#resolution","title":"Resolution","text":"<p>Correct. To be more accurate, Rancher Support is only for the Prometheus/Grafana components that are natively embedded in Rancher 2.x for the functionality of monitoring and dashboards.</p> <p>In the exceptional scenario of any legacy support subscriptions that are limited to RKE-only (without Rancher 2.x), it does not include support of Prometheus/Grafana that are set up externally to monitor the RKE clusters.</p> <p>Refer this article for more details: Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?</p>"},{"location":"kbs/000020463/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020464/","title":"[Rancher] Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?","text":"<p>This document (000020464) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020464/#resolution","title":"Resolution","text":"<p>Support for Prometheus/Grafana is only for what is embedded in Rancher 2.x and natively used by it for the functionalities of monitoring and dashboards.</p> <p>Refer Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?</p> <p>Where Prometheus/Grafana has been enabled from the chart in the \"Library\" catalog, refer Are the applications underlying the charts in the \"Library\" Catalog covered by Rancher Support?</p> <p>Prometheus/Grafana installed by all other means that did not originate from Rancher fall outside the scope of Rancher Support and are not covered by the terms of service of a Rancher Support subscription.</p>"},{"location":"kbs/000020464/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020465/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?","text":"<p>This document (000020465) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020465/#resolution","title":"Resolution","text":"<p>Yes. Jenkins is the open-source component used and supported by Rancher 2.1+ as the native build engine for running pipelines. Any issues root-caused in Jenkins will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"kbs/000020465/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020466/","title":"[Rancher] What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?","text":"<p>This document (000020466) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020466/#resolution","title":"Resolution","text":"<p>Rancher supports its certified integrations with log aggregation systems such as Elasticsearch, Splunk, Kafka, Syslog, and Fluentd. What this means is the following:</p> <ul> <li>Rancher will help Customer troubleshoot the root cause for any issue related to one of these logging services.</li> <li>For issues root-caused to be in the integration to one of these services, Rancher will provide a fix to resolve such issues.</li> <li>For issues root-caused to be inside one of these logging services, Rancher will investigate if the fix is already available in a later version of the logging service. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later version that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the logging service, Rancher will advise Customer to contact the logging service vendor directly for issue resolution.</li> </ul>"},{"location":"kbs/000020466/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020467/","title":"[Rancher] Will Rancher fix problems root-caused to be in nginx?","text":"<p>This document (000020467) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020467/#resolution","title":"Resolution","text":"<p>Rancher will do one of the following, to help Customer resolve the issue root-caused to be in nginx:</p> <ul> <li>Should the issue have been resolved in a later version of nginx, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of nginx.</li> <li>If the issue is unresolved in any acceptable nginx product versions, Rancher will advise Customer to contact nginx directly for issue resolution. Should a new version or patch be provided by nginx, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>Note:</p> <p>Inclusion of nginx as a certified component in the Rancher support matrix, is based on Rancher's own testing and validation of nginx with its default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine tune the settings for scale and performance.</p>"},{"location":"kbs/000020467/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020468/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?","text":"<p>This document (000020468) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020468/#resolution","title":"Resolution","text":"<p>No. However, Rancher will do one of the following, to help Customer resolve the issue root-caused to be in an add-on such as Weave, Cisco ACI, Cilium, and Calico:</p> <ul> <li>Should the issue have been resolved in a later version of the add-on component, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of the add-on.</li> <li>If the issue is unresolved in any acceptable versions of the add-on, Rancher will advise Customer to contact the add-on vendor directly for issue resolution. Should a new version or patch be provided by the add-on vendor, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>Note:</p> <p>Any inclusion of such add-ons as a certified component in the Rancher support matrix, is based on Rancher's own testing and validation of these add-ons with their default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine-tune the settings for scale and performance.</p>"},{"location":"kbs/000020468/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020470/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?","text":"<p>This document (000020470) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020470/#resolution","title":"Resolution","text":"<p>Yes. Any issues root-caused in one of these two projects will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"kbs/000020470/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020471/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?","text":"<p>This document (000020471) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020471/#resolution","title":"Resolution","text":"<p>Yes. Istio is the open-source component used and supported by Rancher 2.3+ for service mesh functionality. Any issues root-caused in Istio will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"kbs/000020471/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020472/","title":"[Rancher] We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?","text":"<p>This document (000020472) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020472/#resolution","title":"Resolution","text":"<p>Fluentd is the open-source component used and supported by Rancher 2.x for the native log forwarder functionality. To the extent of this functionality, any issues root-caused in Fluentd will be supported fully, like how any Rancher product issue would be.</p> <p>In the context of Fluentd as a log aggregation system, Rancher Support is limited to ensuring its certified integration with Fluentd works as intended.</p>"},{"location":"kbs/000020472/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020473/","title":"[Rancher] Does Rancher include any 3rd party, commercial software components?","text":"<p>This document (000020473) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020473/#resolution","title":"Resolution","text":"<p>No. There are no commercial software components included in Rancher. Rancher products are 100% open source and only include components that are also open source and with the right kind of open source license such as, but not limited to, Apache 2.0.</p>"},{"location":"kbs/000020473/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020474/","title":"[Rancher] Is the Rancher software itself completely free (outside of the choice to add support or not add support)? Meaning if we choose no support, we can still run the same full features software?","text":"<p>This document (000020474) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020474/#resolution","title":"Resolution","text":"<p>Yes. Rancher products are 100% open source and free to use for anyone. There are no hidden product features that are unlocked by signing up for a Rancher Support subscription.</p> <p>Also, see Does Rancher include any 3rd party, commercial software components?</p>"},{"location":"kbs/000020474/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020475/","title":"[Rancher] How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?","text":"<p>This document (000020475) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020475/#resolution","title":"Resolution","text":"<p>\"How are you going to do the support for production systems, particularly for financial institutions that have to comply with PCI-DSS standards and that will not allow external access to the systems?\"</p> <p>Not just for customer systems that need to comply with restrictions but the following is applicable to any customer system that is covered by Rancher Support:</p> <p>A Rancher Support Engineer will never work on an issue on any customer system directly and with unmonitored access.</p> <p>Any troubleshooting will be done via a combination of the following:</p> <ul> <li>Log collection</li> <li>Over a screen share session and with the customer on a jump box</li> </ul> <p>Should there be very high-security scenarios where troubleshooting via the above is still not possible, Rancher Support can still help troubleshoot issues but help can only be provided in a second-hand manner. That is to say, any response can only be provided to the extent of the sanitized information shared by the Customer, with Rancher Support, and in a back-and-forth request-response transaction that may not be very efficient.</p> <p>Where such scenarios are well known and access-based support is still sought, Customer is requested to inquire with their Rancher Account Executive or Customer Success Manager for other commercial models of engagement such as Premium Support\u00a0or via SUSE RGS.</p>"},{"location":"kbs/000020475/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020476/","title":"[Rancher] How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?","text":"<p>This document (000020476) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020476/#environment","title":"Environment","text":"<ul> <li>Rancher</li> <li>RKE</li> <li>RKE2</li> <li>K3s</li> <li>Harvester</li> <li>Longhorn</li> <li>NeuVector</li> </ul>"},{"location":"kbs/000020476/#resolution","title":"Resolution","text":"<p>For industry-reported vulnerabilities in Rancher, RKE, RKE2, K3s, Harvester, Longhorn, NeuVector and upstream vulnerabilities in Kubernetes, Docker, and containerd, SUSE Rancher strives to adhere to industry standards and best practices. Due to the nature of upstream dependencies inherent to open-source software, the final delivery of patch releases may vary in timeline. We will prioritize our efforts and coordinate with upstream organizations and third-party entities according to the following guidelines:</p> <ul> <li>Critical: Immediate engagement to remediate the issue in code, and/or coordinate with upstream and/or third-party entities to deliver the remediation in the shortest timeline available. This includes creating an emergency release patch version when an existing one is not readily available.</li> <li>High: Prioritized engagement to align the delivery of the remediation with our next available release cycle. Emergency releases should only be needed unless the timing is such that the next available security release cycle is not in a reasonable timeline.</li> </ul>"},{"location":"kbs/000020476/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020477/","title":"[Rancher] We need to run Antivirus on our cluster nodes. Would that impact the terms of service of our Rancher Support SLA?","text":"<p>This document (000020477) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020477/#resolution","title":"Resolution","text":"<p>Rancher Support SLA is based on our QA validation and certification on default OS kernels for operating systems listed in the product support matrices.</p> <p>Antivirus software adds an unknown variable to the existing complexity of Kubernetes. Most of them have not yet kept up with newer technologies such as Kubernetes and have not reached a CNCF certified status. In environments where antivirus software had been enabled, Rancher Support has seen issues stemming from interfering actions from such software. As an example, there have been incidents where the antivirus software had pruned files in the Docker filesystem incorrectly, causing the Docker mounts to go corrupt.</p> <p>Issues resulting from third-party tools, such as antivirus and intrusion detection software, interfering with Docker or other necessary system calls are deemed resolved should disabling such tools restore functionality.</p> <p>Lastly, all certified configurations, as published in the product support matrices, are based on the default settings of individual components. Where a customer environment has deviated from certified configurations, Rancher Labs reserves the right to recommend the customer to revert to a certified configuration to resolve the reported issue.</p>"},{"location":"kbs/000020477/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020478/","title":"[Rancher] We use components that are not listed in the Rancher support matrix. Would Rancher Support be not valid then?","text":"<p>This document (000020478) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020478/#resolution","title":"Resolution","text":"<p>For open source components not listed in the Rancher support matrix page, support is limited to troubleshooting for root cause up to Rancher\u2019s drivers and interfaces to those components.</p> <p>Root causes that are identified to be beyond this limit will need to be pursued by Company with the maintainers and providers of commercial support for those components.</p> <p>For ensuring best support and clarity on supportability, Company is recommended to publish to Rancher a list of components that are critical to its deployment but not explicitly called out in the support matrix.</p>"},{"location":"kbs/000020478/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020479/","title":"[Rancher] I see many included components listed in the Support Matrix. I plan to change the default configurations of one or more of these components. Would Rancher Support be not valid then?","text":"<p>This document (000020479) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020479/#resolution","title":"Resolution","text":"<p>Certified configurations in the Rancher support matrix page\u00a0are based on the default settings of individual components.</p> <p>Where Customer has deviated from certified configurations, Rancher Support reserves the right to recommend the Company to revert to a certified configuration to resolve the reported issue.</p>"},{"location":"kbs/000020479/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020480/","title":"[Rancher] What are the certified integrations with persistent volume plugins covered by Rancher Support?","text":"<p>This document (000020480) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020480/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Amazon EBS Disk Azure Disk</li> <li>Azure Filesystem</li> <li>Google Persistent Disk</li> <li>Longhorn</li> <li>Local Node Disk</li> <li>Local Node Path</li> <li>NFS Share</li> <li>VMware vSphere Volume</li> </ul> <p>Note:</p> <p>For any other persistent volume plugins from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.</p>"},{"location":"kbs/000020480/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020481/","title":"[Rancher] What are the certified integrations with storage class provisioners covered by Rancher Support?","text":"<p>This document (000020481) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020481/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Amazon EBS Disk</li> <li>Azure Disk</li> <li>Azure File</li> <li>Google Persistent Disk</li> <li>VMware vSphere Volume</li> <li>Longhorn</li> <li>Local</li> </ul> <p>Note:</p> <p>For any other storage class provisioners from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.</p>"},{"location":"kbs/000020481/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020482/","title":"[Rancher] What are the certified integrations with authentication providers covered by Rancher Support?","text":"<p>This document (000020482) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020482/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Active Directory</li> <li>Azure AD</li> <li>GitHub</li> <li>Google (supported only from Rancher v2.3 or higher)</li> <li>PingIdentity (SAML)</li> <li>Keycloak (SAML)</li> <li>AD FS (SAML)</li> <li>Okta (SAML)</li> <li>FreeIPA (LDAP)</li> <li>OpenLDAP (LDAP)</li> </ul>"},{"location":"kbs/000020482/#rancher-support-does-not-cover","title":"Rancher Support does not cover:","text":"<ul> <li>Switching between these external authentication providers.</li> <li>Migration from authentication scheme of one provider to another that would typically require maintaining existing user settings/preferences, access levels and privileges, user and group authorizations.</li> </ul>"},{"location":"kbs/000020482/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020483/","title":"[Rancher] Could you clarify what you generally mean by a \"certified integration\" to another software system or service?","text":"<p>This document (000020483) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020483/#resolution","title":"Resolution","text":"<p>Rancher supports its certified integrations with various software systems and services for functionality ranging from User Authentication to Infrastructure. What this means is the following:</p> <ul> <li>Rancher has validated the integration for one or more of its product versions to work as designed with such systems and services.</li> <li>Rancher Support will help Customer troubleshoot the root cause for any issue related to one of these integrations.</li> <li>For issues root-caused to be in the integration component to one of these systems or services, Rancher will provide a fix to resolve such issues.</li> <li>For issues root-caused to be inside one of these systems or services, Rancher will investigate if the fix is already available in a later version of the system or service. If it is, Rancher will provide a newer version of its own product that is validated and certified to work with the later version of the system that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the system or service, Rancher will advise Customer to directly contact the vendor providing support for the system or service for issue resolution.</li> </ul>"},{"location":"kbs/000020483/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020484/","title":"[Rancher] What does Rancher support for Docker on Ubuntu cover?","text":"<p>This document (000020484) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020484/#situation","title":"Situation","text":"<p>Note:</p> <p>Only upstream Docker has been validated and certified for Rancher Support. Ubuntu's own distribution of Docker is not covered by the Rancher support matrix.</p> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Ubuntu, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Ubuntu and Rancher that has been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> <li>If a code fix is not possible, Rancher will advise Customer to contact Canonical directly for issue resolution. Should a new version or patch be provided by Canonical, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul>"},{"location":"kbs/000020484/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020485/","title":"[Rancher] Will Rancher support us should our deployment be on Red Hat Atomic?","text":"<p>This document (000020485) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020485/#resolution","title":"Resolution","text":"<p>Rancher has not validated and certified Red Hat Atomic for inclusion in its Support Matrix as one of the supported operating systems. Following conditions shall apply to Customers procuring Rancher Support for running a Rancher deployment on Red Hat Atomic:</p> <ul> <li>Rancher Support is predicated on Customer running a Red Hat distribution of Docker that has been validated and certified for Rancher Support on a comparable, certified version of RHEL OS.</li> <li>Rancher Support will rely on Customer to reproduce the issue on a comparable, certified version of RHEL OS running a fully supported configuration.</li> <li>In the event of Customer encountering an issue whilst using Red Hat Atomic, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the Red Hat Atomic OS will need to be taken up by the Customer with Red Hat. Troubleshooting by Rancher Support shall be limited to running scripts and commands, log analysis, and screen share sessions with the Customer. It shall not extend to a setting up a Rancher deployment on Red Hat Atomic by Rancher Support to reproduce the issue.</li> </ul>"},{"location":"kbs/000020485/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020486/","title":"[Rancher] What does Rancher support for Docker on RHEL cover?","text":"<p>This document (000020486) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020486/#resolution","title":"Resolution","text":"<p>Note:</p> <p>Both, RHEL's own distribution of Docker and upstream Docker (Docker CE) on RHEL, have been validated and certified for Rancher Support.</p> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis).</p> <p>Where the root cause has been identified as an issue with RHEL's own distribution of Docker, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of RHEL's own distribution of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version RHEL's own distribution of Docker.</li> <li>If no fix is available yet for the issue, Rancher will advise the Customer to contact Red Hat directly for issue resolution. Should a new version or patch be provided by Red Hat, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>Where the root cause has been identified as an issue with upstream Docker (or Docker CE) on RHEL, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of RHEL and Rancher that have been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> </ul>"},{"location":"kbs/000020486/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020487/","title":"[Rancher] What does Rancher support for Docker on CentOS cover?","text":"<p>This document (000020487) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020487/#resolution","title":"Resolution","text":"<p>Note:</p> <p>Only upstream Docker has been validated and certified for Rancher Support.</p> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on CentOS, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of CentOS and Rancher that has been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however, the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> </ul>"},{"location":"kbs/000020487/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020488/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Docker on RHEL?","text":"<p>This document (000020488) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020488/#resolution","title":"Resolution","text":"<p>No. In this case, Customer needs to contact Red Hat.</p>"},{"location":"kbs/000020488/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020489/","title":"[Rancher] What does Rancher support for Docker on Windows Server cover?","text":"<p>This document (000020489) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020489/#resolution","title":"Resolution","text":"<p>Note:</p> <ul> <li>Only Docker Enterprise (Docker EE) has been validated and certified for Rancher Support.</li> <li>Support for Windows Server is available only for Rancher v2.3 and higher.</li> <li>Windows clusters are supported for Kubernetes 1.15+ on Windows Server, versions 1809 and 1903. Windows clusters can only be created from new clusters and are supported only with the flannel network provider. You will not need to do any specific scheduling to ensure your Windows workloads are scheduled onto Windows nodes. When creating a Windows cluster, Rancher automatically adds taints to the required Linux nodes to prevent any Windows workloads to be scheduled. If you are trying to schedule Linux workloads into the cluster, you will need to add specific tolerations and node scheduling in order to have them deployed on the Linux nodes.</li> </ul> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker Enterprise on Windows Server, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker Enterprise, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Windows Server and Rancher that have been certified for this later version of Docker Enterprise.</li> <li>If no fix is available yet for the issue, Rancher will advise the Customer to contact Microsoft directly for issue resolution. Should a new version or patch be provided by Microsoft Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul>"},{"location":"kbs/000020489/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020490/","title":"[Rancher] What does Rancher support for Docker on Oracle Linux cover?","text":"<p>This document (000020490) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020490/#resolution","title":"Resolution","text":"<p>Note:</p> <ul> <li>Only upstream Docker has been validated and certified for Rancher Support.</li> <li>Support for Oracle Linux is available only from Rancher v2.3.2.</li> <li>Some restrictive firewall rules will need to be turned off for a supported baseline configuration of Oracle Linux. ~~Refer this docs page for more details.~~</li> </ul> <p>Rancher support in this scenario will start with issue troubleshooting and RCA (root cause analysis). Where the root cause has been identified as an issue with Docker on Oracle Linux, Rancher will do one of the following, to help Customer resolve the issue:</p> <ul> <li>Should the issue have been resolved in a later version of Docker, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Oracle Linux and Rancher that has been certified for this later version of Docker.</li> <li>If no fix is available yet for the issue, however the situation is critical that an urgent solution is needed, Rancher shall fork Docker with a fix, as possible. Further, Rancher will submit a PR for the fix to Docker for consideration to be accepted into upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Docker.</li> </ul>"},{"location":"kbs/000020490/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020491/","title":"[Rancher] My clusters run on OpenStack. I do not see OpenStack listed in your Support Matrix. Does my Rancher Support cover clusters that are run on OpenStack?","text":"<p>This document (000020491) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020491/#resolution","title":"Resolution","text":"<p>OpenStack is not listed under supported node drivers in the Rancher support matrix. What this means is that currently the OpenStack node driver is not officially supported and because of this, there isn't a supported way to use Rancher as the cluster self-provisioning platform on an OpenStack-based infrastructure.</p> <p>This is not to say that clusters cannot be run in a Rancher-supported way on OpenStack-based infrastructure. To do that, the option of custom clusters will need to be used.</p> <p>In the event of a Customer encountering an issue in such custom clusters, Rancher Support will troubleshoot up to the point of root-causing the issue. Issues root-caused to be in the OpenStack platform will need to be taken up by the Customer with the maintainer and provider of support for the OpenStack platform.</p> <p>Also, troubleshooting by Rancher Support is currently limited to log analysis and screen share sessions with the Customer and does not extend to an OpenStack setup by Rancher Support to recreate the issue.</p>"},{"location":"kbs/000020491/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020492/","title":"[Rancher] Does it matter what hardware my hosts are on? Are virtualized servers supported?","text":"<p>This document (000020492) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020492/#resolution","title":"Resolution","text":"<p>Rancher Support for Rancher 2.x covers running Rancher and Kubernetes clusters on supported OSes on a 64-bit x86 architecture host.</p> <p>Refer the support matrix\u00a0page\u00a0for supported OS by product version.</p> <p>Refer this\u00a0docs page for installation requirements and this\u00a0docs page for node requirements for user clusters.</p> <p>These host nodes could be bare-metal or a virtual server running on any Type-1 hypervisor or a cloud server on AWS, Azure, Digital Ocean, Google, and Linode.</p> <p>With the exception of KVM, hosts running on Type-2 hypervisors, such as VirtualBox, VMware Fusion or Parallels, are not in scope of Rancher Support. For use cases, where a Rancher Customer is SLA-bound to their downstream users, Rancher Support does not recommend running clusters and workloads on these hosts. Any assistance provided by Rancher Support in this scenario is not bound by the Rancher Support SLA. It shall be limited to being on a best effort basis only and not include troubleshooting issues related to the setup and configuration of the virtual infrastructure.</p> <p>Refer this\u00a0page on wikipedia for what is a Type-1 and Type-2 hypervisor. Hosts on an ARM64 architecture are not covered by Rancher Support SLA.</p>"},{"location":"kbs/000020492/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020493/","title":"[Rancher] I see node drivers tagged as \"Built-in\". What does that mean?","text":"<p>This document (000020493) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020493/#resolution","title":"Resolution","text":"<p>Built-in drivers are those that are included in a Rancher product distribution.</p>"},{"location":"kbs/000020493/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020494/","title":"[Rancher] I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?","text":"<p>This document (000020494) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020494/#resolution","title":"Resolution","text":"<p>Active drivers are those drivers that have been turned on. Only four Built-in node drivers are surfaced up in UI, as Active, in a default Rancher installation. It is only these five Built-in, Active node drivers that are validated and certified in any Rancher 2.x product version and consequently, published in the Rancher support matrix. These five Built-in, Active node drivers that are covered by Rancher Support are for:</p> <ul> <li>Digital Ocean</li> <li>AWS</li> <li>Azure</li> <li>vSphere - 6.5, 6.7, 7.0 update 2a</li> <li>Linode (supported only from Rancher v2.3 or higher)</li> </ul>"},{"location":"kbs/000020494/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020495/","title":"[Rancher] Does my support subscription to Rancher include support for Longhorn?","text":"<p>This document (000020495) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020495/#resolution","title":"Resolution","text":"<p>No, a subscription to Rancher does not include support for Longhorn. With the general availability of Longhorn in June 2020, a separate commercial subscription to an add-on plan is needed to receive SLA-based support for Longhorn.</p>"},{"location":"kbs/000020495/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020496/","title":"[Rancher] Can we have a mix of unsupported and supported nodes at our choice/discretion?","text":"<p>This document (000020496) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020496/#resolution","title":"Resolution","text":"<p>No. Within a supported Rancher environment, categorizing cluster nodes as \"supported'\" and \"unsupported\" at a team's own choice/discretion is not allowed.</p> <p>Also, see Can we request support for the management/upstream cluster and certain downstream clusters and not others?</p>"},{"location":"kbs/000020496/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020497/","title":"[Rancher] Can we request support for the management/upstream cluster and certain downstream clusters and not others?","text":"<p>This document (000020497) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020497/#resolution","title":"Resolution","text":"<p>No. Mixing the management of \"supported'\" and \"unsupported\" clusters from a single Rancher server instance in this manner is not a supported configuration, so we would suggest that you run a separate Rancher server instance to manage clusters you do not want to be covered by Rancher Support. The other option you have here would be to take advantage of our blended Platinum and Standard plan, to cover less critical clusters with the Standard plan, and you can reach out to your Account Executive if you wish to look at the options with this.</p>"},{"location":"kbs/000020497/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020498/","title":"[Rancher] How does Rancher track our license usage?","text":"<p>This document (000020498) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020498/#resolution","title":"Resolution","text":"<p>Currently, Rancher relies on customers to report their usage and procure additional licenses if their usage has increased. From time to time, as part of support calls, a Rancher Support Engineer may request customers to run a simple command-line script on the Rancher Server that will generate high-level usage information:</p> <pre><code>wget -O - https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sh - &gt; rancher_stats.txt\n</code></pre> <p>This is both to keep track of our customer's usage for license compliance as well as to offer advisories should customers be approaching any thresholds related to scale and performance.</p>"},{"location":"kbs/000020498/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020499/","title":"[Rancher] Is Rancher Support only for production environments?","text":"<p>This document (000020499) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020499/#resolution","title":"Resolution","text":"<p>No. Rancher Support can be procured for any Rancher installation/environment for which the customer wishes to get SLA-based assistance. In addition to production environments, it is very common for orgs to procure support subscriptions to cover other environments, such as their Dev/Test, Staging, Demo, Perf, PreProd, and Production Sandbox, for which they have their own SLAs to meet with their downstream customer users.</p>"},{"location":"kbs/000020499/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020500/","title":"[Rancher] I procured a support subscription to 1 Rancher Management Server and a 20-node starter pack for my Production environment. Does my support subscription cover issues in my Dev/Test and Staging environments as well?","text":"<p>This document (000020500) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020500/#resolution","title":"Resolution","text":"<p>No. In examples such as this one, the customer is required to procure license(s) for any additional Rancher Management Server installations for which support has been sought. In this example, whilst Rancher Support will help customers only on a best-effort basis, for their issues in Dev/Test and Staging, they will recommend a conversation for the customer with their Rancher Account Executive for procuring additional licenses at the earliest.</p>"},{"location":"kbs/000020500/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020501/","title":"[Rancher] Our usage has exceeded the number of nodes for which we purchased a Rancher Support subscription. We now have an issue. Can we continue to get assistance from Rancher Support to troubleshoot and resolve the issue?","text":"<p>This document (000020501) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020501/#resolution","title":"Resolution","text":"<p>We understand that this can happen, as you have your own customer adoption and growth. Yes, for sure, Rancher Support will help you on the specific issue. However, to avoid any future disruption, it will be required that the Rancher Support subscription is upgraded to the necessary quantity, at the earliest and within a reasonable period of time.</p>"},{"location":"kbs/000020501/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020502/","title":"[Rancher] Could you illustrate the severity levels with subject lines of sample support cases?","text":"<p>This document (000020502) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020502/#resolution","title":"Resolution","text":"<p>Here are\u00a0some sample subject lines by severity level:</p> Severity Level Subject line of sample support cases Severity 1 1. Production cluster is down2. Apps in production cluster throwing '502 Bad Gateway' Error3. Enabled enhanced cluster monitoring and the cluster sporadically goes offline4. etcd not healthy in production rancher server5. Kubernetes ingress endpoint went unresponsive Severity 2 1. etcd restore failed for RKE-deployed cluster2. Microservices client encounters UnknownHostException in Production3. Network traffic routed over IPsec is super slow4. LDAP is intermittently failing to log in users in one of our environments5. DNS intermittent timeouts Severity 3 1. Webhook notifier not working as expected2. Need assistance with creating custom global roles to prevent cluster creation3. Problem adding new nodes to clusters4. Ingress timeout issue in Rancher-deployed cluster5. Bug: Unable to add AD groups to Rancher ACLs Severity 4 1. Ability to change UID and GID in docker containers on container creation.2. Feature Request: Show audit logs content in Rancher UI3. FYI: Scheduled upgrade XX/XX/XXXX XX:XXPM, request on-call assistance\u00a0as needed4. How can I disable the creation of the Prometheus-Operator CRDs on cluster creation5. Understand the impact of changing IPs of worker nodes in our k8s cluster"},{"location":"kbs/000020502/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020502/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020526/","title":"Security vulnerability: log4j remote code execution aka log4shell CVE-2021-44228","text":"<p>This document (000020526) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020526/#environment","title":"Environment","text":"<p>All products</p>"},{"location":"kbs/000020526/#situation","title":"Situation","text":"<p>A 0-day exploit in the log4j Java logging framework was found by Chen Zhaojun of Alibaba Cloud Security Team, which allowed remote attackers able to inject strings into log4j based Java logging to execute code by</p> <p>exploiting the default enabled JNDI bindings. This is possible without any preconditions, making it critical.</p>"},{"location":"kbs/000020526/#resolution","title":"Resolution","text":"<p>SUSE considers log4j versions 2.0 and newer as affected, log4j 1.2.x does not have the same critical vulnerability and is not considered affected by this CVE.</p> <p>SUSE Linux Enterprise products do not ship log4j 2.x.</p> <p>SUSE Manager does not ship log4j 2.x.</p> <p>SUSE Enterprise Storage does not ship log4j 2.x.</p> <p>SUSE Openstack Cloud embeds log4j2 in the \"storm\" component, which will receive updates.</p> <p>SUSE NeuVector product does not ship log4j 2.x.</p> <p>SUSE Rancher is not affected by this vulnerability. The Helm chart for Istio 1.5, provided by Rancher and which is currently deprecated, includes Zipkin and is vulnerable to Log4j. Customers are advised to upgrade to the recent Istio version provided in Cluster Explorer, which does not uses Zipkin and is not affect to the vulnerability.</p> <p>Please refer to the upstream guidance from log4j on fixing and mitigation measures if you deploy your Java Application stacks.</p>"},{"location":"kbs/000020526/#status","title":"Status","text":"<p>Security Alert</p>"},{"location":"kbs/000020526/#additional-information","title":"Additional Information","text":"<p>Additional information can be found here:</p> <ul> <li>https://suse.com/security/cve/CVE-2021-44228.html</li> <li>https://www.suse.com/c/suse-statement-on-log4j-log4shell-cve-2021-44228-vulnerability/</li> <li>https://logging.apache.org/log4j/2.x/security.html</li> </ul> <p>Note in regards to SUSE Manager Server:</p> <p>The CVE-search will use meta-data within a patch to display the needed information. As there is no patch needed (as SUSE is not effected), the CVE-search for CVE-2021-44228 will return a \"not found\".</p>"},{"location":"kbs/000020526/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020535/","title":"Security vulnerability: Trojan Source, invisible source code vulnerabilities. (CVE-2021-42574)","text":"<p>This document (000020535) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020535/#environment","title":"Environment","text":"<p>All products</p>"},{"location":"kbs/000020535/#situation","title":"Situation","text":"<p>CVE-2021-42574 ('Trojan Source') refers to vulnerabilities that can come about through the use of bi-directional unicode text in contexts where it is not properly displayed. \u00a0Various source-code viewers and editors currently do not show content which is \"visually hidden by unicode\". \u00a0These may include editors and pagers such as vi, emacs and less as well as the web interfaces of tools that display source code.</p> <p>The failure to display such things as bidirectional control characters can lead to a situation in which source code when compiled or interpreted behaves in ways that someone seeing the displayed text would not expect.</p> <p>This is not a compiler issue, but future compiler versions will also have options or features to display warnings in cases where such special unicode characters are used.</p>"},{"location":"kbs/000020535/#resolution","title":"Resolution","text":"<p>Even where this does not affect SUSE products directly, SUSE is currently taking action to harden the supply chain for SUSE products in order to detect any such unicode sequences in code that could have harmful effects.</p>"},{"location":"kbs/000020535/#cause","title":"Cause","text":"<p>Unicode supports both left-to-right and right-to-left languages, and it makes use of invisible codepoints called \"bidirectional override\"\u00a0to aid writing left-to-right words inside a right-to-left sentence. It is common to find these inside a sentence of another language to embed a word with a different text direction. \u00a0Researchers discovered that these codepoints could be misused to manipulate how source code is displayed in some editors and code review tools, fooling a reviewer into approving code that behaves in unexpected ways (potentially maliciously).</p>"},{"location":"kbs/000020535/#status","title":"Status","text":"<p>Security Alert</p>"},{"location":"kbs/000020535/#additional-information","title":"Additional Information","text":"<p>Additional information can be found at:</p> <p>-\u00a0https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-42574</p>"},{"location":"kbs/000020535/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020536/","title":"[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20","text":"<p>This document (000020536) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020536/#resolution","title":"Resolution","text":"Note:This\u00a0Rancher Labs operational advisory below was originally sent in December 2020.\u00a0 It has been published here to continue SUSE Rancher customer conversations around this topic via support cases and for sharing any relevant updates around it.For an update on this topic, please see\u00a0[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24. <p>Dear Rancher Customer,</p> <p>This is an operational advisory from Rancher Support related to the deprecation of dockershim in Kubernetes v1.20</p> <p>As announced on the official Kubernetes blog, the dockershim, which enables the use of the Docker Daemon as the container runtime in Kubernetes, will be deprecated with the upcoming Kubernetes v1.20 release.</p>"},{"location":"kbs/000020536/#what-is-dockershim","title":"What is dockershim?","text":"<p>The dockershim is built into Kubernetes to provide a Container Runtime Interface (CRI) compliant layer between the kubelet and the Docker Daemon. The shim is necessary because Docker Daemon is not CRI-compliant.</p>"},{"location":"kbs/000020536/#what-does-deprecation-of-dockershim-in-kubernetes-v120-mean","title":"What does deprecation of dockershim in Kubernetes v1.20 mean?","text":"<p>The dockershim will only be deprecated in Kubernetes v1.20, and will not yet be removed from the kubelet. As a result, no immediate action needs to be taken and Kubernetes clusters can continue to operate with the Docker Daemon container runtime in Kubernetes v1.20. The only change at this time will be a deprecation warning printed in the kubelet logs when running on Docker.</p>"},{"location":"kbs/000020536/#what-are-ranchers-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","title":"What are Rancher's plans to ensure on-going container runtime support in future Kubernetes releases?","text":"<p>We are working on our roadmap to ensure that all Rancher provisioned clusters will continue to operate on a CRI-compliant runtime.</p> <p>For existing RKE customers, users will continue to get Kubernetes updates until the shim is officially removed. The removal is currently targeted for late 2021 and will be supported with patches during the 12-month upstream maintenance window. Before the end of maintenance, we fully expect an upgrade path from RKE to RKE2.</p> <p>Looking forward, containerd is already the default runtime in both K3s and RKE2, so any removal of dockershim will have zero impact on future releases. As with RKE, organizations currently using K3s with the Docker runtime will continue to get Kubernetes updates until the shim is officially removed. The dockershim deprecation schedule is tracked by the upstream Kubernetes community in Kubernetes Enhancement Proposal (KEP) 1985,\u00a0Rancher will continue to keep you updated with related news on the roadmap from our product management, as well as information related to migration off of Docker.</p> <p>Thanks,</p> <p>Rancher Support Team</p>"},{"location":"kbs/000020536/#additional-information","title":"Additional Information","text":"<ul> <li>[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020536/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020538/","title":"[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24","text":"<p>This document (000020538) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020538/#resolution","title":"Resolution","text":"Note: This is a follow-up to the Rancher Operational Advisory that was sent on this topic in December 2020. <p>In the latest announcement\u00a0from the Kubernetes blog, it has been notified that dockershim removal has been planned in Kubernetes v1.24,\u00a0slated for release around April 2022.</p>"},{"location":"kbs/000020538/#what-are-suses-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","title":"What are SUSE's plans to ensure on-going container runtime support in future Kubernetes releases?","text":"<p>Starting with Kubernetes v1.21, RKE added support for CRI plugin cri-dockerd, see here\u00a0instructions to enable. All RKE clusters will need to leverage this CRI plugin before upgrading to Kubernetes v1.24. Future updates beyond Kubernetes v1.24 RKE will rely on the cri-dockerd shim.</p> <p>For more information on the dockershim removal schedule, you can check the upstream Kubernetes Enhancement Proposal (KEP) 2221.</p> <p>K3s and RKE2 are not impacted by the removal of dockershim and use the CRI plugin containerd. We expect to deliver a migration path from RKE to RKE2 in the future.</p> <p>SUSE will continue to keep you updated with related news on the roadmap from our product management</p>"},{"location":"kbs/000020538/#additional-information","title":"Additional Information","text":"<ul> <li>[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020538/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020543/","title":"[Rancher] Support Advisories","text":"<p>This document (000020543) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020543/#resolution","title":"Resolution","text":"<ul> <li>[Rancher] Operational Advisory, 20220405:\u00a0Rancher Kubernetes Distributions and Etcd 3.5 Updates</li> <li>[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5</li> <li>[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24</li> <li>[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20</li> <li>[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits</li> </ul>"},{"location":"kbs/000020543/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020543/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020556/","title":"Rancher Hosted Prime FAQ","text":"<p>This document (000020556) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020556/#resolution","title":"Resolution","text":""},{"location":"kbs/000020556/#general","title":"General","text":"<ul> <li>What is Rancher Hosted Prime?</li> <li>What do I need to provide to get started on Rancher\u00a0Hosted Prime?</li> <li>Do you have a whitepaper available for Rancher\u00a0Hosted Prime?</li> <li>Can I move from self-managed Rancher to Rancher Hosted Prime?</li> <li>What if I no longer want Rancher Hosted Prime to manage my downstream clusters?</li> <li>Is it possible to have alpha, beta, or release candidate (RC) versions on Rancher Hosted Prime?</li> <li>Can Rancher Hosted Prime manage my on-premise clusters running on VMWare or bare metal servers?</li> <li>Is there any limit on the number of downstream clusters or nodes Rancher Hosted Prime can manage?</li> <li>Do I have access to the Rancher Hosted Prime \"local\" cluster in the management UI?</li> <li>Where is Rancher Hosted Prime hosted?</li> <li>Can I have more than one\u00a0Rancher Hosted Prime environment?</li> <li>What type of cluster is Rancher Hosted Prime running on?</li> <li>Does Rancher Hosted Prime provide downstream clusters?</li> <li>Can I move an existing cluster to Rancher Hosted Prime?</li> <li>How is Rancher Hosted Prime different from the open-source Rancher I can download for free?</li> </ul>"},{"location":"kbs/000020556/#maintenance-operations","title":"Maintenance &amp; Operations","text":"<ul> <li>Who creates user accounts in Rancher Hosted Prime?</li> <li>How is my Rancher Hosted Prime environment monitored?</li> <li>How often is maintenance performed on Rancher Hosted Prime?</li> <li>Can the admin password be reset if I\u2019m locked out of my Rancher Hosted Prime?</li> <li>Who upgrades Kubernetes on my\u00a0Rancher Hosted Prime downstream clusters?</li> <li>Does Rancher Hosted Prime offer a support SLA?</li> <li>How often are backups taken and retained on Rancher Hosted Prime?</li> </ul>"},{"location":"kbs/000020556/#upgrades-uptime","title":"Upgrades &amp; Uptime","text":"<ul> <li>What can be expected during a Rancher Hosted Prime upgrade?</li> <li>How often is Rancher Hosted Prime upgraded?</li> <li>How is uptime measured for my Rancher Hosted Prime?</li> <li>Does Rancher Hosted Prime offer an uptime SLA?</li> </ul>"},{"location":"kbs/000020556/#network-security-logging","title":"Network, Security, &amp; Logging","text":"<ul> <li>What are the networking requirements for using Rancher Hosted Prime?</li> <li>Can I use my own SSL/TLS certificates with Rancher Hosted Prime?</li> <li>How to connect your Rancher Hosted Prime network to your AWS transit gateway?</li> <li>How to make a VPN connection to your Rancher Hosted Prime network?</li> <li>Can I integrate Rancher Hosted\u00a0Prime with my Active Directory, SAML, or LDAP-based directory service?</li> <li>Does Rancher Hosted Prime support multi-factor authentication (MFA)?</li> <li>Are API audit logs enabled in Rancher Hosted Prime?</li> <li>Can I get a copy of the Rancher API audit logs?</li> <li>What information is stored in Rancher Hosted Prime and where is it stored?</li> <li>Do SUSE employees have a login account for my Rancher Hosted Prime environment?</li> <li>Do SUSE employees have the credentials to my \u201cadmin\u201d account?</li> <li>Is Rancher Hosted Prime data encrypted at rest?</li> </ul>"},{"location":"kbs/000020556/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020557/","title":"Do you have a whitepaper available for SUSE Rancher Hosted?","text":"<p>This document (000020557) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020557/#resolution","title":"Resolution","text":"<p>Yes, there is a SUSE Rancher Hosted architecture whitepaper that can be downloaded on SUSE's website. It can be found here - https://more.suse.com/fy21-global-web-landing-page-hosted-rancher-technical-guide</p>"},{"location":"kbs/000020557/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020558/","title":"What can be expected during a SUSE Rancher Hosted upgrade?","text":"<p>This document (000020558) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020558/#resolution","title":"Resolution","text":"<p>There is minimal impact or disruption during a SUSE Rancher Hosted upgrade. During the one-hour maintenance window you can expect a brief (usually 1-2 minutes) when your Rancher control plane UI/API is inaccessible. This does not impact the workloads on your managed downstream clusters, only your ability to make changes to these clusters. Immediately following the Rancher control plane upgrade, the cluster and node agents running in your managed downstream clusters will be upgraded and restarted. This also only takes a few minutes (unless you have very large clusters or very poor network speeds) and during this time Rancher will be unable to manage the cluster, but again the workloads running on the clusters should operate normally.</p>"},{"location":"kbs/000020558/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020559/","title":"What if I no longer want SUSE Rancher Hosted to manage my downstream clusters?","text":"<p>This document (000020559) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020559/#resolution","title":"Resolution","text":"<p>If you decide you no longer want to continue using the SUSE Rancher Hosted service, we can provide a one-time backup of your Rancher control plane which you can use to restore into a self-managed Rancher management server. Information on the restore process can be found in our documentation. To request a backup file, you can submit a support case on our support portal. After restoring Rancher, you will need to reconfigure your downstream clusters to point to the new server URL of your self-managed Rancher management server.</p>"},{"location":"kbs/000020559/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020561/","title":"How to connect your SUSE Rancher Hosted network to your AWS transit gateway?","text":"<p>This document (000020561) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020561/#resolution","title":"Resolution","text":"<p>The following steps can be taken to connect your SUSE Rancher Hosted network to an AWS transit gateway running in your AWS account.</p> <ol> <li>Make sure you have provided the SUSE Rancher Hosted team with a CIDR that does not overlap with your existing infrastructure. If not, your SUSE Rancher Hosted environment may need to be redeployed with the new CIDR. The CIDR must be a /25 block or larger. Using a /24 is normally preferred.</li> <li>If you haven't already, create a transit gateway in your AWS account. See Create a transit gateway.</li> <li>In the AWS console, go to Resource Access Manager (RAM) service.</li> <li>In RAM, click the orange button in the top right corner labeled \"Create a resource share\".</li> <li>For the name, use something descriptive that includes both your company name and \"SUSE Rancher Hosted\". For example, \"Widget Corp transit gateway for SUSE Rancher Hosted\". For resource type, select Transit Gateways. Select the transit gateway you want to share. In Principals, check Allow external accounts and enter the AWS account number provided by the SUSE Rancher Hosted team. Click the orange \"Create resource share\" in the bottom right corner.</li> <li>Let the SUSE Rancher Hosted team know you have created the share. We will accept the share and make a request to attach the transit gateway to your SUSE Rancher Hosted VPC.</li> <li>Accept the request to attach your transit gateway to the SUSE Rancher Hosted VPC. To do this, go to the VPC service, click \"Transit Gateway Attachments\" in the navigation pane, select the transit gateway attachment, choose Actions -&gt; Accept.</li> <li>Provide the SUSE Rancher Hosted team with a list of CIDRs you want to be routed through the transit gateway.</li> </ol> <p>See also Transit gateways and Transit gateway sharing considerations for more information.</p>"},{"location":"kbs/000020561/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020562/","title":"How to make a VPN connection to your SUSE Rancher Hosted network?","text":"<p>This document (000020562) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020562/#resolution","title":"Resolution","text":"<p>It's normally preferred to connect SUSE Rancher Hosted with your network using VPC peering through an AWS transit gateway. This is the most cost-effective, secure, and manageable solution. However, if this is not an option, a VPN connection can be established between your corporate network and SUSE Rancher Hosted through an IPSec VPN tunnel. The following steps are required to set this up:</p> <ol> <li>Provide the SUSE Rancher Hosted team with the following information about your VPN device:</li> <li>Public IP address for your VPN endpoint</li> <li>Routing option: a) static (no BGP support) or b) dynamic (BGP support)</li> <li>BGP ASN (only if dynamic routing)</li> <li>VPN device make and model used on-premise that we'll be connecting to.</li> <li>The SUSE Rancher Hosted team will configure the VPN connection and provide configuration information based on the VPN device</li> <li>Customer will configure their VPN device to connect to SUSE Rancher Hosted's network.</li> </ol> <p>See also AWS Site-to-Site VPN User Guide.</p>"},{"location":"kbs/000020562/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020563/","title":"Can I get a copy of the Rancher API audit logs?","text":"<p>This document (000020563) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020563/#resolution","title":"Resolution","text":"<p>Yes, if you provide a log flow configuration to your logging solution, such as AWS CloudWatch, Elasticsearch, Splunk, etc. we can stream Rancher API audit logs to you.</p>"},{"location":"kbs/000020563/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020592/","title":"Rancher on Windows RKE 1 to RKE 2","text":"<p>This document (000020592) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020592/#environment","title":"Environment","text":"<p>Any customer running Rancher on Windows on RKE1 is impacted by this change. Rancher has been aware of a shifting trend in the cloud native ecosystem to move toward container runtimes with a smaller surface area than Docker</p>"},{"location":"kbs/000020592/#situation","title":"Situation","text":"<p>Rancher foresaw this trend and has built two Kubernetes distributions on the open source containerd container runtime, K3s and RKE 2.\u00a0 RKE 2 is built on K3s and is fully conformant Kubernetes distribution that focuses on security and compliance.</p> <p>The future of Windows containers on Rancher is only found on RKE 2</p>"},{"location":"kbs/000020592/#resolution","title":"Resolution","text":"<p>Customers currently running on RKE 1 will need to move to RKE 2</p> <p>Because of the change in approach to provisioning Windows-specific clusters in RKE 1 to free-form mixed-OS clusters RKE 2, there is no direct migration path for Windows containers on RKE 1 to RKE 2.</p> <p>Rancher Labs recommends as part of customers testing workloads on the Windows containers on RKE 2 technical preview that they begin planning to refactor their Windows workloads on RKE2 using Fleet. Fleet is a GitOps solution from Rancher, now integrated directly into Rancher, with support for Windows containers.</p> <p>Windows containers on RKE 2 remains in technical preview as of Rancher 2.6.3, the current release of Rancher. Rancher 2.6.4 will be released in March bringing the Windows container experience on RKE 2 to general availability (GA). Windows containers on RKE 2 will match and then exceed the Windows containers on RKE 1 feature set, reaching even greater feature parity between Windows and Linux containers on Rancher. The Windows on Rancher team develops in the open, with full transparency into their development processes. Rancher Labs anticipates Windows containers on RKE 2 reaching general availability (GA) with official support alongside the Rancher Cluster Provisioning v2 in early March 2022.</p>"},{"location":"kbs/000020592/#additional-information","title":"Additional Information","text":"<p>Additional guidance and a guide for transitioning the most common workloads from RKE 1 to RKE 2 will be forthcoming. Customers seeking additional assistance in migrating between RKE 1 and RKE 2 should consult with SUSE Global Services. Customers with additional questions or concerns regarding this transition should contact the Windows on Rancher team:</p> <ul> <li> <p>By filing issues in the Windows on Rancher GitHub</p> </li> <li> <p>By joining the Rancher Users Slack and posting in #windows</p> </li> <li> <p>Reaching out to their Customer Success Manager</p> </li> </ul> <p>See our blog post here https://community.suse.com/posts/the-future-of-windows-containers-on-rancher?agree=true</p>"},{"location":"kbs/000020592/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020630/","title":"[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits","text":"<p>This document (000020630) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020630/#resolution","title":"Resolution","text":"Note: Below is the Rancher Customer email advisory sent in Nov 2020 on the topic of Docker Hub rate limits. <p>Dear Rancher Customer,</p> <p>As\u00a0announced by Docker Inc, the rate-limiting by Docker Hub is expected to progressively take effect beginning Nov 2.</p> <p>We have been engaged in direct conversations with many of you on the possible impact of this\u00a0rate-limiting and steps toward managing that.\u00a0 This operational advisory is a summary of those conversations.</p>"},{"location":"kbs/000020630/#do-the-rate-limits-apply-to-rancher-images-that-we-pull-anonymously","title":"Do the rate limits apply to Rancher images that we pull anonymously?","text":"<p>No.\u00a0 To ensure customers pulling Rancher resources are not affected by this, Rancher Labs has partnered with Docker, Inc. so that\u00a0pulls from the Rancher namespace on Docker Hub are exempt from these limits.</p> <p>If you run into any rate-limiting issues with images hosted in the Rancher namespace, please let us know.</p>"},{"location":"kbs/000020630/#what-about-images-that-are-outside-the-rancher-namespace-that-we-pull-anonymously","title":"What about images that are outside the Rancher namespace that we pull anonymously?","text":"<p>Yes. Rate limits do apply to the images that are outside of the Rancher namespace.</p>"},{"location":"kbs/000020630/#what-can-we-do-about-the-limits-on-images-outside-the-rancher-namespace","title":"What can we do about the limits on images outside the Rancher namespace?","text":"<p>We can introduce you to the right contact at Docker Inc should you be interested in procuring an exemption for your org based on something like an IP range.\u00a0 This is to derisk being limited on image pulls outside of the Rancher namespace.</p>"},{"location":"kbs/000020630/#what-other-practical-options-can-we-pursue","title":"What other practical options can we pursue?","text":"<p>Other options to mitigate this issue are:</p> <ul> <li>Moving from anonymous pulls to authenticated pulls on Docker Hub</li> <li>Copying resources to a private registry</li> </ul> <p>The viability of these options depends on the specific context of your environment.</p>"},{"location":"kbs/000020630/#are-there-any-plans-to-host-rancher-images-elsewhere-to-help-alleviate-the-potential-issues-caused-by-this","title":"Are there any plans to host Rancher images elsewhere to help alleviate the potential issues caused by this?","text":"<p>We are looking into alternates to Docker Hub. There have been recent offerings announced by\u00a0AWS and\u00a0GitHub. We are exploring them as well as other options and should have an update on this from our product management in the near future.</p> <p>Thanks,</p> <p>Rancher Support Team</p>"},{"location":"kbs/000020630/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020630/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020631/","title":"[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5","text":"<p>This document (000020631) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020631/#resolution","title":"Resolution","text":"Note:This Rancher Customer advisory below was originally sent by email to subscribed customer users on March 30, 2022. <p>Dear SUSE Rancher user,</p> <p>We have been sharing important SUSE Rancher product lifecycle dates on our\u00a0Product Support Lifecycle page.</p> <p>To help you plan for any necessary upgrades for your deployments, we would like to bring to your attention information on some Rancher Manager product versions that are approaching (or have reached) their End of Maintenance (EOM)\u00a0and\u00a0End of Life (EOL)\u00a0milestones.</p> <p>EOM Dates</p> Rancher Version EOM Date v2.4.x July 30, 2021 v2.5.x January 5, 2022 <p>EOL Dates</p> Rancher Version EOL Date v2.4.x March 31, 2022 v2.5.x October 5, 2022"},{"location":"kbs/000020631/#what-does-the-above-mean","title":"What does the above mean?","text":"<p>After a product release reaches its EOM date, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner until it reaches EOL, in the form of:</p> <ul> <li>General troubleshooting of a specific issue to isolate potential causes</li> <li>Upgrade recommendation to an existing newer version of product</li> <li>Issue resolution limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product</li> </ul> <p>Once a product release reaches its EOL date, a Rancher user may continue to use the product within the terms of the product licensing agreement. However, Support Plan SLAs from SUSE Rancher do not apply to product versions that are past their EOL dates.</p> <p>Please review in detail the following resources to understand changes and prepare for your upgrade.</p> <ul> <li>Rancher Support Matrix</li> <li>Rancher 2.5.0 release notes</li> <li>Rancher 2.6.0 release notes</li> <li>Rancher Support Upgrade checklist</li> <li>Rancher Support FAQs</li> </ul> <p>In addition, please note that with the upcoming release of Kubernetes 1.24, scheduled for April 19th, dockershim removal from Kubernetes will be final. Please see the\u00a0Kubernetes blog and our\u00a0Rancher Support Advisory for more information.</p> <p>If you have any questions on this\u00a0advisory or would like assistance validating your upgrade path, simply contact your Customer Success Manager or open a new support case via the\u00a0SCC portal referencing this advisory.</p> <p>Thanks,</p> <p>SUSE Rancher Support Team</p>"},{"location":"kbs/000020631/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020631/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020632/","title":"[Rancher] Operational Advisory, 20220405: Rancher Kubernetes Distributions and Etcd 3.5 Updates","text":"<p>This document (000020632) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020632/#environment","title":"Environment","text":"<p>The etcd maintainers have recommended against the use of etcd 3.5.0-3.5.2 for new production workloads, due to a recently discovered bug that may cause data loss when etcd is killed under high load. Their published\u00a0advisory2\u00a0provides recommendations of how to avoid triggering the issue, but as of today, there is no official fix/resolution to the existing\u00a0issue4.</p>"},{"location":"kbs/000020632/#situation","title":"Situation","text":"<p>Who does this affect?</p> <ul> <li>Users running Rancher 2.6.4+ who have deployed Rancher on a\u00a0single node as a Docker installation.\u00a0Reminder: This installation method is not recommended for any production environment and is only recommended for development/sandbox testing. If you are running Rancher on a managed Kubernetes cluster, then you will have to refer to your Kubernetes service provider to determine if you are affected by this advisory.</li> <li>Users running Kubernetes 1.22+ and 1.23+ of any of the Rancher Kubernetes Distributions (RKE, RKE2, K3s) and using etcd as your datastore. The default datastore for RKE and RKE2 is etcd. This applies to standalone Kubernetes clusters as well as any downstream clusters provisioned by Rancher.</li> </ul>"},{"location":"kbs/000020632/#resolution","title":"Resolution","text":"<p>What should you do?</p> <ul> <li>Stop deploying into production any new Kubernetes clusters using Rancher Kubernetes distributions versions 1.22/1.23 until a proper fix is provided by the etcd maintainers and included into the affected distribution.</li> <li>Update your etcd configuration to enable the\u00a0experimental-initial-corrupt-checkoption. This flag will be turned on by default in etcd v3.6, but does not by itself fix the problem; it can only detect and repair the issue if it does occur.</li> </ul> <p>Note: each distribution has its own recommendation on how to enable this option; see below for more details. - Avoid terminating etcd unexpectedly (using\u00a0kill \u20139, etc)   - For RKE1 clusters, avoid stopping/killing the etcd containers adhoc without properly cordoning/draining nodes and taking backups   - For RKE2 clusters,     - Avoid sending SIGKILL to the etcd or rke2 process.     - Avoid using the killall script (rke2-killall.sh) to stop RKE2 on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services.   - For k3s clusters,     - Avoid sending SIGKILL to the k3s process.     - Avoid using the killall script (k3s-killall.sh) to stop K3s on servers hosting production workloads. The killall script is meant to clean up hosts prior to uninstallation or reconfiguration and should not be used as a substitute for properly cordoning/draining a node and stopping services. - Ensure nodes are not under significant memory pressure that may cause the Linux kernel to terminate the etcd process.</p> <p>Ensure that nodes are not terminated unexpectedly. Avoid force-terminating VMs, unexpected power loss, etc.</p> <p>How do I enable the recommended flag in etccd?</p> <p>For Users provisioning RKE/k3s/RKE2 clusters through Rancher</p> <p>Provisioned RKE Clusters</p> <p>If you are running 1.22 or 1.23, upgrade to the following respective versions to enable the recommended\u00a0experimental-initial-corrupt-check\u00a0flag in etcd.</p> <ul> <li>RKE 1.22 -\u00a0v1.22.7-rancher1-2</li> <li>RKE 1.23 (Experimental) -\u00a0v1.23.4-rancher1-2</li> </ul> <p>Provisioned K3s/RKE2 Clusters (Tech Preview)</p> <p>Provisioned k3s/RKE2 clusters are still in tech preview, so we do not recommend running production workloads on these clusters. If you have provisioned clusters, you can enable the recommended\u00a0experimental-initial-corrupt-check\u00a0flag by editing the cluster as YAML. If you have an imported k3s/RKE2 cluster, review the standalone Kubernetes distribution section.</p> <ol> <li>From the \u201cCluster Management\u201d page, click the vertical three-dots on the right-hand side for the cluster you want to edit.</li> <li>From the menu, select \u201cEdit YAML\u201d.</li> <li>Edit the\u00a0spec.rkeConfig.machineGlobalConfig.etcd-arg\u00a0section of the YAML to add in an etcd argument. Note: Your YAML may be slightly different from the example below.</li> </ol> <p>Example:</p> <p>spec:</p> <p>cloudCredentialSecretName: cattle-global-data:cc-xxxxx</p> <p>kubernetesVersion: v1.22.7+rke2r2</p> <p>localClusterAuthEndpoint: {}</p> <p>rkeConfig:</p> <p>chartValues:</p> <p>rke2-calico: {}</p> <p>etcd:</p> <p>snapshotRetention: 5</p> <p>snapshotScheduleCron: 0 */5 * * *</p> <p>machineGlobalConfig:</p> <p>cni: calico</p> <p>etcd-arg: [\"experimental-initial-corrupt-check=true\"]</p> <ul> <li>Click \u201cSave\u201d at the bottom. Rancher will update the configuration and restart the necessary services.</li> </ul> <p>For Users running standalone Kubernetes distributions</p> <p>RKE Clusters</p> <p>As of RKE v1.3.8, the default version of Kubernetes was set to 1.22.x. In order to not use the default Kubernetes version, please set the\u00a0kubernetes_version\u00a0to other available versions in your\u00a0cluster.yml\u00a0file for any new deployments through RKE.</p> <ul> <li>If you already have an existing RKE1 cluster using an affected version, you can set\u00a0experimental-initial-corrupt-check: true\u00a0in\u00a0extra_args\u00a0for etcd.</li> </ul> <p>RKE v1.3.9 was released where the default version of Kubernetes is still set to 1.22.x, but the default version (1.22-rancher1-2) has the recommended flag enabled by default.</p> <p>RKE2/k3s Clusters</p> <p>The flag only needs to be added if you are using HA with embedded etcd. Single-server clusters with sqlite, or clusters using HA with an external SQL datastore are not affected. The flag only needs to be enabled on servers, as agents do not run etcd.</p> <p>Customization of etcd was introduced with 1.22.4 and 1.23.0, so if you are running a lower version of 1.22.x, then you will need to upgrade to at least 1.22.4 in order to customize the etcd configuration.</p> <p>RKE2 Clusters</p> <ol> <li>Create or edit the config file at\u00a0/etc/rancher/rke2/config.yaml.</li> <li>Add the following line to the end of the file:</li> </ol> <p>etcd-arg: [\"experimental-initial-corrupt-check=true\"]</p> <ol> <li>Save the config file and then run\u00a0systemctl restart rke2-server\u00a0to apply the change.</li> </ol> <p>K3S Clusters</p> <ol> <li>Create or edit the config file at\u00a0/etc/rancher/k3s/config.yaml.</li> <li>Add the following line to the end of the file:</li> </ol> <p>etcd-arg: [\"experimental-initial-corrupt-check=true\"]</p> <ol> <li>Save the config file and then run\u00a0systemctl restart k3s\u00a0to apply the change.</li> </ol>"},{"location":"kbs/000020632/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020632/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020657/","title":"Exclude cattle-system namespace from Dynatrace monitoring","text":"<p>This document (000020657) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020657/#environment","title":"Environment","text":"<p>The cattle-cluster-agent pod is stuck in a CrashLoopBackOff, logging the following messages:</p> <pre><code>usage: agent &lt;absolute path of static Go application&gt; [Go application arguments]\n</code></pre>"},{"location":"kbs/000020657/#situation","title":"Situation","text":"<p>We have observed messages from downstream clusters where the cattle-cluster-agent on any Rancher version can contain no logging outputs, with messages only relating to environment variables.</p> <p>Running kubectl exec to access a shell on the pod and running /usr/bin/run.sh also produces the error.</p>"},{"location":"kbs/000020657/#resolution","title":"Resolution","text":"<p>It is recommend to exclude the cattle-system namespace from being monitored by Dynatrace. The below documentation is specific for Dynatrace, however other solutions could have similar options.</p> <p>Option 3:\u00a0https://www.dynatrace.com/support/help/setup-and-configuration/setup-on-container-platforms/kubernetes/get-started-with-kubernetes-monitoring/dto-config-options-k8s#annotate</p>"},{"location":"kbs/000020657/#cause","title":"Cause","text":"<p>Solutions like Dynatrace can potentially inject sidecar containers alongside cattle-cluster-agent/cattle-node-agent containers, causing the agent to exit unpredictably..</p>"},{"location":"kbs/000020657/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020658/","title":"Removing the global system-default-registry value","text":"<p>This document (000020658) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020658/#situation","title":"Situation","text":"<p>When removing the global system-default-registry setting in order to use a cluster-level private registry with authentication, the order in which you make these changes is important. If you remove the global system-default-registry setting first, this could cause downstream clusters to go into an error state showing the following error in the Rancher UI:</p> <pre><code>Cluster must have at least one etcd plane host: failed to connect to the following etcd host(s) [IP Address]\n</code></pre>"},{"location":"kbs/000020658/#resolution","title":"Resolution","text":"<p>Set the private registry at the cluster-level first and wait for the downstream cluster to finish updating before removing the value from the global system-default registry.</p> <p>If the value for the global system-default-registry was removed first and the downstream cluster is in an error state, the following should resolve the issue:</p> <ul> <li>From the Rancher (local) management cluster edit each of the control plane/etcd nodes in the downstream cluster with the following command:</li> </ul> <pre><code>kubectl edit node.management.cattle.io -n &lt;clusterID&gt; &lt;machineID&gt;\n</code></pre> <ul> <li>Locate the nodePlan field, to find the image field and append the proper registry to the start of the value</li> <li>Update the cluster-level private registry</li> </ul>"},{"location":"kbs/000020658/#cause","title":"Cause","text":"<p>If the global system-default-registry value is removed, the downstream cluster will begin updating and may not be able to pull the rancher/rancher-agent image causing the downstream cluster to not be able to communicate with the local Rancher cluster. Therefore, the downstream cluster should have the proper private registry set at the cluster-level before the global system-default-registry value is removed.</p>"},{"location":"kbs/000020658/#additional-information","title":"Additional Information","text":"<p>Global registry: https://rancher.com/docs/rancher/v2.6/en/admin-settings/config-private-registry/</p> <p>Cluster-level registry: https://rancher.com/docs/rancher/v2.6/en/cluster-admin/editing-clusters/rke-config-reference/#private-registries</p>"},{"location":"kbs/000020658/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020672/","title":"How to set multiple rules in the Rancher2 Terraform Provider Role Template resource","text":"<p>This document (000020672) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020672/#situation","title":"Situation","text":"<p>Defining multiple rules within a single role template.</p>"},{"location":"kbs/000020672/#resolution","title":"Resolution","text":"<p>To define multiple rules for a single role template, set multiple rule blocks as per the following example:</p> <pre><code>resource \"rancher2_role_template\" \"foo\" {\n  name = \"foo\"\n  context = \"project\"\n  default_role = false\n  description = \"Terraform role template acceptance test\"\n  rules {\n    api_groups = [\"*\"]\n    resources = [\"secrets\"]\n    verbs = [\"create\"]\n  }\n  rules {\n    api_groups = [\"*\"]\n    resources = [\"namespaces\"]\n    verbs = [\"create\"]\n  }\n}\n</code></pre>"},{"location":"kbs/000020672/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020682/","title":"Azure AD API Removal","text":"<p>This document (000020682) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020682/#situation","title":"Situation","text":""},{"location":"kbs/000020682/#summary-of-changes","title":"Summary of Changes","text":"<p>Microsoft is ending support of the existing AzureAD Graph API before 2023. Accordingly, Rancher has updated our AzureAD auth provider to use the new Microsoft Graph API to access users and groups in Active Directory.</p>"},{"location":"kbs/000020682/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>Old</p> <ul> <li>ADAL is the authentication library we use to get access tokens to the deprecated Azure AD Graph API.</li> </ul> <p>New</p> <ul> <li>MSAL is the new authentication library we will instead use to get access tokens to the new Microsoft Graph API.</li> </ul>"},{"location":"kbs/000020682/#actions-required-of-users","title":"Actions Required of Users","text":"<ul> <li>New users of v2.6.x and v2.7.x will use the new Microsoft Graph API when they register Rancher with Azure AD. There will be no need for a transition.</li> <li>Existing users who have Azure AD as the auth provider will see an informational notification/banner that will urge them to upgrade Rancher's auth provider before the end of 2022. Beforehand, their app in Azure will need to have the necessary permissions for Rancher to be able to work with Users and Groups in AD. To upgrade, the UI will have a button to instruct the backend to use the new authentication/authorization flow without requiring Rancher admins to reconfigure the existing auth provider.</li> <li>AD admins must add the necessary Microsoft Graph permissions to their apps. Specifically, User.Read.All and Group.Read.All - both must be Application (not Delegated) permissions.</li> </ul>"},{"location":"kbs/000020682/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>When you choose to upgrade the existing Azure AD auth provider configuration in Rancher, please keep in mind that all users' access tokens to the deprecated Azure AD Graph API will be deleted, since Rancher won't need them anymore because it won't be communicating with it.</p> <p>Instead, Rancher will store in a secret only one access token to the new Microsoft Graph API - that of the service principal associated with the App registration in Azure AD. This token is refreshed once an hour (not in the background, but when its use triggers a refresh).</p> <p>Additional migration instructions can be found at these links:</p> <p>For Rancher 2.6.x</p> <p>For Rancher 2.7.x</p>"},{"location":"kbs/000020682/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020683/","title":"vSphere 6.7 EOL","text":"<p>This document (000020683) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020683/#situation","title":"Situation","text":""},{"location":"kbs/000020683/#summary-of-changes","title":"Summary of Changes","text":"<p>VMWare vSphere 6.7 will enter end of life (EoL) status on October 15, 2022. The original date was November 15, 2021, however, VMWare opted to extend their end of general support (EoGS) date based on customer requests at the time. As far as Rancher is concerned, vSphere 6.7 will reach EoL in the Rancher Support Matrix on the stated VMWare vSphere 6.7 EoGS date of October 15, 2022.</p> <p>VMWare Blog Announcing vSphere 6.7 EoGS https://blogs.vmware.com/vsphere/2020/06/announcing-extension-of-vsphere-6-7-general-support-period.html</p> <p>VMWare Product Lifecycle Matrix https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/support/product-lifecycle-matrix.pdf</p>"},{"location":"kbs/000020683/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>vSphere 7, released in early 2020, introduces native support and integrations for Kubernetes. This includes new functionality utilized in the Rancher vSphere CSI and CPI charts.</p>"},{"location":"kbs/000020683/#actions-required-of-users","title":"Actions Required of Users","text":"<p>End-users must upgrade to vSphere 7.0 by October 15, 2022, to stay in compliance with the Rancher Support Matrix.</p>"},{"location":"kbs/000020683/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>vSphere 7.0 has been supported since Rancher 2.4.9 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-4-9/, 2.5.2 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-5-2/, and v2.6.0 https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/rancher-v2-6-0/.</p> <p>Customers have had ample time to migrate from vSphere 6.7 to 7.0. There will not be any extensions of the Rancher EoL date for vSphere 6.7.</p> <p>The original VMWare vSphere EoTG (End of Technical Guidance) date of November 15, 2023 still applies for vSphere 6.7. Rancher Support should expect customers to potentially remain on vSphere 6.7 beyond the end of general support (EoGS) period. However, once the EoGS date is reached, Rancher and its products will no longer be validated on 6.7 and customers should expect a best effort from Support and Engineering for issues involving vSphere 6.7 environments</p>"},{"location":"kbs/000020683/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020684/","title":"Windows RKE1 EOL","text":"<p>This document (000020684) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020684/#situation","title":"Situation","text":""},{"location":"kbs/000020684/#summary-of-changes","title":"Summary of Changes","text":"<p>RKE1 Windows will move to EOL status on September 1, 2022, due to the deprecation of Docker EE support by Microsoft. Docker EE is the only supported container runtime for Windows nodes in RKE1.</p> <p>The impact on customers and internal development/QA efforts after September 1, 2022, is summarized below:</p> <p>- Rancher users should expect that they are unable to provision new RKE1 Windows clusters.</p> <p>- Rancher users should expect to have no available upgrade path for existing RKE1 Windows clusters.</p> <p>- Customers who are unable to move to RKE2 Windows would need to purchase a support contract for Mirantis Container Runtime, previously known as Docker Engine - Enterprise.</p> <p>- Rancher Support is unable to provide full-stack support for workloads deployed on the Mirantis Container Runtime.</p> <p>- We expect cloud providers to completely remove images that contain Docker EE for Windows Server.</p> <p>- Microsoft has indicated to us in an unofficial capacity that the current and only installation method for Docker EE on Windows Server, which is through the PSGallery, will be removed as part of the EOL of Docker EE.</p>"},{"location":"kbs/000020684/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>RKE2 Windows is the only path forward for Windows support in Rancher.</p> <p>RKE1 Windows Clusters: Each Linux node in an RKE1 Windows cluster, regardless of the role assigned to it, will have have a default taint that prevents workloads to be scheduled on it unless the workload has a toleration configured. This is a major design feature for RKE1 Windows clusters which were designed to only run Windows workloads.</p> <p>RKE2 Hybrid Clusters: Based on feedback and requests for hybrid workload support, RKE2 Windows was designed to support both Linux and Windows workloads by default. RKE2 scheduling relies on node selectors. This is a marked change from RKE1 as taints and tolerations were not incorporated into RKE2. Node selectors were a critical part of RKE1 Windows clusters, which makes for an easy migration of your workloads.</p>"},{"location":"kbs/000020684/#actions-required-of-users","title":"Actions Required of Users","text":"<p>Moving forward, Rancher customers will need to upgrade to Rancher v2.6.5+ (to have GA of RKE2 Windows provisioning available) and migrate their container workloads to run on RKE2 Hybrid clusters, which are built on the containerd runtime. These steps will be required for them to stay in compliance with the Rancher Support Matrix.</p>"},{"location":"kbs/000020684/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>It should be assumed that all existing functionality required for creating new RKE1 Windows clusters or upgrading existing RKE1 Windows clusters will cease functioning on this date. Rancher will be unable to publish any future versions of RKE1 with Windows Support for Docker EE. RKE2 is the only option for Windows customers who wish to stay in a supported configuration.</p> <p>Windows Server 2019 LTSC and Windows Server 2022 LTSC are the only supported versions of Windows for RKE2.</p>"},{"location":"kbs/000020684/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020699/","title":"How to recreate rancher-webhook-tls secret if incorrectly deleted","text":"<p>This document (000020699) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020699/#environment","title":"Environment","text":"<p>Rancher 2.5.8 or higher, incorrectly deleted\u00a0rancher-webhook-tls secret instead of\u00a0cattle-webhook-tls secret</p>"},{"location":"kbs/000020699/#situation","title":"Situation","text":"<p>The rancher-webhook-tls is expired on the local rancher cluster.</p> <p>After following the documentation to renew the certificate, the rancher-webhook pods cannot start.</p> <p>https://rancher.com/docs/rancher/v2.6/en/troubleshooting/expired-webhook-certificates</p>"},{"location":"kbs/000020699/#resolution","title":"Resolution","text":"<p>Trigger recreation of the rancher-webhook-tls secret:</p> <p>1. Remove\u00a0rancher.cattle.io\u00a0validating and mutating webhooks, as well as the webhook-service:</p> <pre><code>kubectl delete mutatingwebhookconfigurations\u00a0rancher.cattle.io\nkubectl delete validatingwebhookconfigurations\u00a0rancher.cattle.io\nkubectl -n cattle-system delete service webhook-service\n</code></pre> <p>2. Navigate to Apps &amp; Marketplace in the local cluster Explorer, Installed Apps, and perform an 'upgrade'</p> <p>of\u00a0rancher-webhook to trigger the recreation of deleted resources and a new rancher-webhook-tls</p> <p>certificate secret.</p>"},{"location":"kbs/000020699/#cause","title":"Cause","text":"<p>Unintentionally deletion of\u00a0_rancher-webhook-tls_secretinstead of_cattle-webhook-tls_secret</p>"},{"location":"kbs/000020699/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020710/","title":"After Rancher 2.6.x upgrade, HTTP 403 Errors in Rancher UI","text":"<p>This document (000020710) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020710/#environment","title":"Environment","text":"<p>Several features of Rancher UI don't work and return HTTP 403 for some users after Rancher upgrade from 2.6.x :</p> <p>- Shell execution</p> <p>- Yaml editing</p>"},{"location":"kbs/000020710/#situation","title":"Situation","text":"<p>For some users, several Rancher features are not working and returning HTTP 403 (Forbidden)</p> <p>Rancher Trace log:</p> <pre><code>User-system-serviceaccount-cattle-impersonation-system-cattle-impersonation-u-vnds56pccy-cannot-impersonate-resource-users-in-API-group-at-the-cluster-scope-due-to-missing-clusterrolebinding\n</code></pre>"},{"location":"kbs/000020710/#resolution","title":"Resolution","text":"<p>1. Check RBAC Clusterroles and Clusterrolebindings of the affected user</p> <pre><code>## Clusterroles of the user\n$ kubectl get clusterrole | grep u-b3l74guter\n\n## Clusterrolebindings of the  user\n$ kubectl get clusterrolebinding | grep u-b3l74guter\n</code></pre> <p>2. From the previous output, the expected Clusterrole cattle-impersonation-u-xxxxxxxx is present, but the Clusterrolebinding is absent.</p> <p>3. Delete the cattle-impersonation-user-xxxx Clusterrole of the user</p> <pre><code>$ kubectl delete clusterrole\u00a0cattle-impersonation-u-b3l74guter\n</code></pre> <p>4. Trigger the recreation of the Clusterrole and Clusterrolebinding by browsing to a Rancher feature.</p> <p>e.g: open a Monitoring link in the cluster</p> <p>This action triggered the recreation of the Clusterrole and Clusterrolebinding</p>"},{"location":"kbs/000020710/#additional-information","title":"Additional Information","text":"<p>https://github.com/rancher/rancher/issues/33912</p>"},{"location":"kbs/000020710/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020712/","title":"Is it possible use mTLS for rancher agent connectivity?","text":"<p>This document (000020712) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020712/#resolution","title":"Resolution","text":"<p>It is not possible to configure mTLS authentication for the rancher-agent connectivity; however, the connection to Rancher is secured via TLS and the agents use a token to authenticate themselves to Rancher.</p>"},{"location":"kbs/000020712/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020713/","title":"Firewalld block rancher cluster dns: Weave CNI does not work with Firewalld on RHEL 8 based OSs","text":"<p>This document (000020713) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020713/#environment","title":"Environment","text":"<p>Issue:</p> <p>Firewalld service block Rancher cluster DNS on Rhel8.</p> <p>Steps to reproduce:</p> <p>1. Install and setup RKE on RHEL 8</p> <p>2. CoreDNS is deployed as part of the RKE setup</p> <p>3. Start firewalld service on RHEL8 nodes.</p> <p>After starting firewalld service, k8s pod logs return connection error:</p> <p>ent-041273.voicelab.local. A: read udp 172.21.0.19:58953-&gt;1.10.64.26:53: i/o timeout --------------</p>"},{"location":"kbs/000020713/#situation","title":"Situation","text":"<p>The Internal Kubernetes DNS server (coredns) is blocked. Once firewalld is stopped, the Kubernetes DNS works well.</p> <p>Firewalld block these ports that are required:</p> <p>-\u00a02379-2380/tcp</p> <p>-\u00a04789/udp</p> <p>-\u00a05000/tcp</p> <p>-\u00a06443/tcp</p> <p>-\u00a06783/tcp</p> <p>-\u00a06783-6784/udp</p> <p>-\u00a09100/tcp</p> <p>-\u00a010250/tcp</p> <p>-\u00a010257/tcp</p> <p>-\u00a010259/tcp</p>"},{"location":"kbs/000020713/#resolution","title":"Resolution","text":"<p>Stop firewalld on RHEL8 nodes, it is a requirement as described in Rancher requirements:</p> <p>https://rancher.com/docs/rancher/v2.6/en/installation/requirements/#operating-systems-and-container-runtime-requirements</p>"},{"location":"kbs/000020713/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020721/","title":"How to test Rancher RC/Alpha versions","text":"<p>This document (000020721) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020721/#environment","title":"Environment","text":"<p>Rancher RC versions</p>"},{"location":"kbs/000020721/#situation","title":"Situation","text":"<p>By default, Helm only returns the final releases. This article provides details on how to install Rancher RC versions.</p>"},{"location":"kbs/000020721/#resolution","title":"Resolution","text":"<p>First, you need to add the Rancher 'latest' helm repository.</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n</code></pre> <p>Then you can list the current repositories on your configuration machine with</p> <pre><code>$ helm repo ls\nNAME            URL\ncoredns         https://coredns.github.io/helm\nrancher-charts  https://charts.rancher.io\nrancher-stable  https://releases.rancher.com/server-charts/stable\nrancher-latest  https://releases.rancher.com/server-charts/latest\n</code></pre> <p>You can list the final releases with</p> <pre><code>$ helm search repo \"rancher-latest\" --versions\nNAME                    CHART VERSION   APP VERSION DESCRIPTION\nrancher-latest/rancher  2.6.6           v2.6.6      Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.5           v2.6.5      Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.4           v2.6.4      Install Rancher Server to manage Kubernetes clu...\n[...]\nrancher-latest/rancher  2.0.4           v2.0.4      Install Rancher Server to manage Kubernetes clu...\n</code></pre> <p>To list the RC versions, you can use the --devel argument.</p> <pre><code>helm search repo \"rancher-latest\" --versions --devel\nNAME                    CHART VERSION       APP VERSION         DESCRIPTION\nrancher-latest/rancher  2.6.7-rc7           v2.6.7-rc7          Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.7-rc6           v2.6.7-rc6          Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.7-rc5           v2.6.7-rc5          Install Rancher Server to manage Kubernetes clu...\n[...]\nrancher-latest/rancher  2.0.4               v2.0.4              Install Rancher Server to manage Kubernetes clu...\n</code></pre> <p>The Rancher installation command line becomes</p> <pre><code>helm install rancher &lt;rancher-latest-repo&gt;\n  --devel\n  --version 2.6.7-rc1\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set replicas=3\n</code></pre>"},{"location":"kbs/000020721/#additional-information","title":"Additional Information","text":"<p>https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart</p>"},{"location":"kbs/000020721/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020727/","title":"Rancher upgrade FAQ","text":"<p>This document (000020727) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020727/#situation","title":"Situation","text":"<p>As we near the end of maintenance, end of life, and end of support for Rancher 2.5, we felt it pertinent to provide a one-stop FAQ page as customers begin to plan their upgrades.</p> <p>All upgrades should go from the latest to the latest. For instance, if you are on 2.5.3, you should go to 2.5.latest and then to 2.6.latest. The stop on 2.5.latest should be about a week to ensure you can catch any issues before moving forward.</p> <p>After the upgrade to 2.5.latest, you should then upgrade the underlying Kubernetes version to the latest supported version of the Rancher release. And again, the new Kubernetes version should be tested and verified for around one week. From there, progress to the next Rancher upgrade, test, and then Kubernetes.</p> <p>Many customers ask why we recommend one week. The 1-week recommendation is because this is often enough time for your clusters and app to be thoroughly tested and any issues flagged. We have seen customers who do less of a testing phase and only find issues when their cluster is being fully used and under normal \"strain.\"</p> <p>Here are some useful links for upgrades \u2014 see link (1) below for our team's general best practices around upgrade paths.</p> <p>During the course of your upgrade, see link (2) for how you and your team can bump the severity of this case if there is an impacting event during your upgrade. Doing so will notify our on-call engineer, who will engage as quickly as possible.</p> <p>Please review link (3) to verify that the Rancher and Kubernetes versions remain inline. It is best to ensure that any testing is done in a lower environment first so that you and your team can be more aware of any issues that the version jumps could have on your environment and applications.</p> <p>Lastly, for our team to better understand the updated environment, would you please run our system support script (4). This script will give our team a better understanding of the upgraded environment and the ability to call out items that may be out of our best practices.</p> <p>(1) https://www.suse.com/support/kb/doc/?id=000020061</p> <p>(2) https://www.suse.com/support/kb/doc/?id=000020296</p> <p>(3) https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/</p> <p>(4) https://www.suse.com/support/kb/doc/?id=000020192</p>"},{"location":"kbs/000020727/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020728/","title":"Collecte de journaux Linux Rancher v2.x","text":"<p>This document (000020728) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020728/#situation","title":"Situation","text":""},{"location":"kbs/000020728/#collecte-de-journaux-linux-rancher-v2x_1","title":"Collecte de journaux Linux Rancher v2.x","text":"<p>Les journaux peuvent \u00eatre collect\u00e9s \u00e0 partir d'un n\u0153ud Linux dans un cluster Rancher v2.x \u00e0 l'aide du script de collecte de journaux Rancher v2.x.</p> <p>Important:Ce script ne peut \u00eatre utilis\u00e9 que pour collecter des journaux \u00e0 partir de clusters provisionn\u00e9s par l'interface de ligne de commande Rancher Kubernetes Engine (RKE) , de clusters K3s, de clusters personnalis\u00e9s\u00a0provisionn\u00e9s par Rancher et de clusters provisionn\u00e9s par Rancher \u00e0 l'aide d'un pilote de n\u0153ud.</p> <p>Ce script n'est pas adapt\u00e9 \u00e0 la collecte de journaux \u00e0 partir de clusters de fournisseurs Kubernetes h\u00e9berg\u00e9s.</p> <p>Le script doit \u00eatre t\u00e9l\u00e9charg\u00e9 et ex\u00e9cut\u00e9 directement sur l'h\u00f4te en utilisant l'utilisateur root ou en utilisant sudo, comme suit:</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s\n</code></pre> <p>Par d\u00e9faut, la sortie sera \u00e9crite dans /tmpdans un fichier tar gzipp\u00e9 nomm\u00e9 -.tar.gz"},{"location":"kbs/000020728/#choix","title":"Choix","text":"<p>Les indicateurs disponibles pouvant \u00eatre transmis au script se trouvent dans le script de collecteur de journaux Rancher v2.x README</p>"},{"location":"kbs/000020728/#disclaimer","title":"Disclaimer","text":"<p>Cette base de connaissances de support technique fournit un outil pr\u00e9cieux aux clients SUSE et autres parties int\u00e9ress\u00e9es par nos produits et solutions pour obtenir des informations, des id\u00e9es et apprendre r\u00e9ciproquement. Les documents sont fournis \u00e0 des fins d'information, personnelles ou non commerciales au sein de votre organisation et sont pr\u00e9sent\u00e9s \u00abEN L'\u00c9TAT \u00bb SANS GARANTIE D'AUCUNE SORTE.</p>"},{"location":"kbs/000020731/","title":"Tuning for nodes with a high number of CPUs allocated","text":"<p>This document (000020731) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020731/#environment","title":"Environment","text":"<p>An RKE cluster built by Rancher, or the RKE CLI</p>"},{"location":"kbs/000020731/#situation","title":"Situation","text":"<p>Some components in a Kubernetes cluster apply a linear scaling mechanism, often based on the number of CPU cores allocated.</p> <p>For nodes that have a high number CPU cores allocated the defaults can create a steep scaling curve and can introduce issues.</p> <p>Two components provided with RKE that scale in this way are kube-proxy and ingress-nginx. However, additional workloads (like nginx) may be deployed to the cluster and also need consideration.</p> <p>Adjusting the scaling for these components can avoid these issues.</p>"},{"location":"kbs/000020731/#resolution","title":"Resolution","text":""},{"location":"kbs/000020731/#kube-proxy","title":"kube-proxy","text":"<p>As explained in the Kubernetes GitHub issue here, the default scaling of the conntrack-max setting allocates 32K of memory per CPU core.</p> <p>This can manifest in the below events in OS logs:</p> <pre><code>kernel: nf_conntrack: falling back to vmalloc.\n</code></pre> <p>This static default can present issues with contiguous memory being allocated for the conntrack table, or reach unnecessary levels of space allocated. When observed frequently, this has been associated with network instability.</p> <p>As a starting point, the suggestion is to halve this amount for a cluster with affected nodes, this can be done by editing the cluster as YAML, or the cluster.yml file when using the RKE CLI.</p> <pre><code>kubeproxy:\n  extra_args:\n    conntrack-max-per-core: '16384'\n</code></pre>"},{"location":"kbs/000020731/#ingress-nginx","title":"ingress-nginx","text":"<p>A common configuration of nginx is to set the worker_processes to auto. When set, nginx will scale the worker_processes to the number of CPU cores on the node. This can result in high numbers of PIDs and consume open files with the threads consumed (number of cores * 32\u00a0(default thread_pool size)).</p> <p>\\* http://nginx.org/en/docs/ngx_core_module.html#worker_processes</p> <p>\\* http://nginx.org/en/docs/ngx_core_module.html#thread_pool</p> <p>This can be adjusted by editing the cluster as YAML, or the cluster.yml file when using the RKE CLI. An example of 8 worker_processes is used below. For a nodes that may process a high amount of ingress traffic, you may wish to use a higher number.</p> <pre><code>ingress:\n  provider: nginx\n  options:\n    worker-processes: \"8\"\n</code></pre>"},{"location":"kbs/000020731/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020733/","title":"Reduce Memory and CPU footprint of Prometheus Monitoring Operator","text":"<p>This document (000020733) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020733/#environment","title":"Environment","text":"<p>rancher-monitoring:\u00a0100.1.3+up19.0.3</p>"},{"location":"kbs/000020733/#situation","title":"Situation","text":"<p>On a fresh install of rancher-monitoring, the Prometheus monitoring operator consumes high CPU and Memory resources without any custom Prometheus CRDs configured. You would notice the following error messages on the Prometheus Operator Pod very frequently.</p> <pre><code>$ kubectl logs -n cattle-monitoring-system rancher-monitoring-operator-784c69bc54-dvndg -f\nlevel=info ts=2022-07-26T09:34:55.46606005Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\nlevel=info ts=2022-07-26T09:34:55.612269913Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:55.694485011Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:55.92009322Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\nlevel=info ts=2022-07-26T09:34:59.042606472Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:59.043983987Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\n</code></pre>"},{"location":"kbs/000020733/#resolution","title":"Resolution","text":"<p>Add SecretListWatchSelector to reduce memory and CPU footprint.</p> <pre><code>prometheusOperator:\n   secretFieldSelector: \"type!=helm.sh/release.v1\"\n</code></pre>"},{"location":"kbs/000020733/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020733/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020737/","title":"How to set up Alertmanager configs in Monitoring V2 in Rancher","text":"<p>This document (000020737) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020737/#environment","title":"Environment","text":"<p>Before we get started:</p> <ul> <li>Admin permissions are needed</li> <li>Gather the required information for the alerting you'd like to set up</li> <li>Monitoring will need to be installed</li> </ul> <p>Please note that I will set up Slack alerts in this example</p> <ul> <li>To set up Slack, add and configure the Incoming webhooks app within your Slack environment</li> <li>Copy the Webhook URL and paste it into a blank notepad</li> </ul>"},{"location":"kbs/000020737/#situation","title":"Situation","text":""},{"location":"kbs/000020737/#resolution","title":"Resolution","text":"<ul> <li> <p>Create an Opaque Secret in the cattle-monitoring-system namespace</p> </li> <li> <p>Click on Projects/Namespaces</p> </li> <li>Select cattle-monitoring-system</li> <li>Then click on Secrets</li> <li>Then select Create</li> <li>Choose Opaque Secret</li> <li>Specify a key name (Insert name here for the secret)</li> <li>Then paste the Slack Webhook URL under Value</li> <li>Verify that the secret has been created successfully</li> <li> <p>Next, select the Monitoring tab on the left side</p> </li> <li> <p>Select Alerting</p> </li> <li>Then create a new AlertManagerConfig in the same namespace</li> <li>Add a name for this configuration</li> <li>Then select Create</li> <li>Once added, we will need to Edit Config</li> <li>From there, we can add a new Receiver(This is where we can specify which type of notifications to receive)</li> <li>For Slack, select Add Slack</li> <li>Under Secret with Slack Webhook URL, select the Secret name</li> <li>Then under the Key drop-down, we'll see the new, generated webhook secret key</li> <li>Next, specify the channel that the notifications will be sent to<ul> <li>(Optional) specify a proxy if applicable</li> </ul> </li> <li>Then select Create</li> <li>After creation, a Slack receiver will be shown, pointing to the webhook URL secret</li> <li>Next, edit the AlertManagerConfig again and point the base route of the config to the slack receiver</li> <li>Under the drop-down, our receiver will show up</li> <li>Select the Receiver</li> <li> <p>Specify Groupings and Matchers and set different intervals here</p> <ul> <li>For faster testing, change the default interval times to quicker times like 5 seconds, 10 seconds, 1 minute, etc.</li> <li>Then click Save</li> <li>Under status, now we see the resulting config in the Alertmanager section in the Rancher UI</li> <li>Then, if everything has been set up correctly, a notification should be sent to the desired Slack channel</li> </ul> </li> </ul>"},{"location":"kbs/000020737/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020745/","title":"Troubleshooting downstream cluster disconnections and kubectl timeouts in Rancher 2.6.4 and 2.6.5","text":"<p>This document (000020745) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020745/#environment","title":"Environment","text":"<p>Rancher Version 2.6.4 and 2.6.5 with multiple downstream clusters</p>"},{"location":"kbs/000020745/#situation","title":"Situation","text":"<p>Clusters will be unavailable or running kubectl commands from the built-in shell from the UI will timeout.</p> <p>Examples:</p> <pre><code>kubectl get nodes\nUnable to connect to the server: net/http: request canceled (Client.Timeout exceeded while awaiting headers)\nThere are no log errors in rancher or downstream agents\nLog nginx: upstream_status=\"-\" upstream_response_time=\"32.000\" request_uri=\"/k8s/clusters/[cluster]/apis?timeout=32s\"\n</code></pre>"},{"location":"kbs/000020745/#resolution","title":"Resolution","text":"<p>To restart the Rancher Pods:</p> <pre><code>kubectl rollout restart deploy rancher -n cattle-system ; kubectl rollout status deploy rancher -n cattle-system\n</code></pre>"},{"location":"kbs/000020745/#cause","title":"Cause","text":"<p>https://github.com/rancher/rancher/issues/37250</p>"},{"location":"kbs/000020745/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020747/","title":"Prometheus Exporter not able to scrape filesystem metrics from nodes when SELinux is enabled.","text":"<p>This document (000020747) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020747/#environment","title":"Environment","text":"<p>Any Linux distribution with SELinux enabled would have this issue.</p>"},{"location":"kbs/000020747/#situation","title":"Situation","text":"<p>Using Prometheus Graphs, when users are trying to query filesystem metrics from nodes, they would not see any metrics due to SELinux being enabled. SELinux policies reject the query due to insufficient privileges.</p> <p>PromQL Query:</p> <pre><code>node_filesystem_*\n</code></pre> <p>Error on the Prometheus exporter pod:</p> <pre><code>level=error ts=2022-07-21T16:13:08.502Z caller=collector.go:169 msg=\"collector failed\" name=filesystem duration_seconds=0.000837846 err=\"open /host/proc/1/mounts: permission denied\"\n</code></pre>"},{"location":"kbs/000020747/#resolution","title":"Resolution","text":"<p>This is not an issue with the Prometheus node exporter from the rancher monitoring chart but rather an issue from the SELinux side. There are three ways to verify that the SELinux team needs to be involved in fixing this issue.</p> <p>1.) Disable SELinux, and the Prometheus node exporter should be able to start querying the node-level metrics. (NOT RECOMMENDED)</p> <p>2.) If SELinux cannot be disabled, you can modify the helm chart and update the Prometheus-node-exporter section with the below seLinuxOption called spc_t, which gives container super-privileged access. (NOT RECOMMENDED)</p> <pre><code>securityContext:\n    seLinuxOptions:\n      type: spc_t\n</code></pre> <p>3.) If super-privileged access cannot be provided to the container, ask the SELinux team to create necessary policies for processes to access the files that node-exporter pods are looking for. (RECOMMENDED APPROACH)</p>"},{"location":"kbs/000020747/#cause","title":"Cause","text":"<p>We notice this issue with SELinux enabled cluster nodes.</p>"},{"location":"kbs/000020747/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020747/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020750/","title":"Customize Helm Chart values for RKE2 default addons","text":"<p>This document (000020750) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020750/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>RKE2 cluster</p>"},{"location":"kbs/000020750/#situation","title":"Situation","text":"<p>By default, RKE2 installs multiple addons, including CoreDNS, Local-Storage, Nginx-Ingress, etc.:</p> <pre><code>kubectl -n kube-system get addons\n</code></pre> <p>Many of these addons are deployed from a Helm chart and represented within the cluster via a HelmChart custom resource:</p> <pre><code>kubectl -n kube-system get helmchart\n</code></pre> <p>These built-in addons deployed from a Helm chart can be customized with the use of a HelmChartConfig custom resource:</p> <pre><code>kubectl -n kube-system get helmchartconfig\n</code></pre> <p>This is where you can use the Helm chart values to change an addon's default installation.</p>"},{"location":"kbs/000020750/#resolution","title":"Resolution","text":"<p>To edit the values of a Helm chart, you must find the currently installed version. To do this navigate\u00a0to the RKE2 GitHub repository releases page to find the Packaged Component Versions (https://github.com/rancher/rke2/releases/) for the specific RKE release.</p> <p>For example, RKE2 1.23.10+rke2r1 uses ingress-nginx 4.1.0 (https://github.com/rancher/rke2/releases/tag/v1.23.10+rke2r1). Checking the values for this version of the ingress-nginx chart \u00a0within the ingress-nginx GitHub repository\u00a0you can determine the possible values</p> <p>(https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.1.0/charts/ingress-nginx/values.yaml).</p> <p>Thus, to add tolerations to the ingress-nginx controller, you can use the below manifest as an example. All of the values from the chart can be customised via this schema.</p> <pre><code>---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-ingress-nginx\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    controller:\n      tolerations:\n        - key: \"key\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n</code></pre> <p>After creating the HelmChartConfig manifest, you need to apply it via Rancher. To do so:</p> <p>1. Navigate to Cluster Management.</p> <p>2. On the selected cluster, click\u00a0Edit Config.</p> <p>3. Click on the Add-On Config tab and enter the manifest at the bottom in\u00a0Additional Manifest.</p> <p></p>"},{"location":"kbs/000020750/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020767/","title":"Downstream Cluster not Available with Websockets failing","text":"<p>This document (000020767) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020767/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>AKS 1.22+</p>"},{"location":"kbs/000020767/#situation","title":"Situation","text":"<p>After upgrading AKS to version 1.22+ users may experience a situation where the Downstream clusters show as unavailable on Rancher.</p> <p>Testing the Websocket using these instructions will show the following error:</p> <pre><code>Bad Request\n{\"baseType\":\"error\",\"code\":\"ServerError\",\"message\":\"websocket: the client is not using the websocket protocol: 'upgrade' token not found in 'Connection' header\",\"status\":400,\"type\":\"error\"}\n</code></pre>"},{"location":"kbs/000020767/#resolution","title":"Resolution","text":"<p>Update the Kubernetes Ingress NGINX with the tag --set controller.watchIngressWithoutClass=true:</p> <pre><code>helm upgrade --install \\\n  ingress-nginx ingress-nginx/ingress-nginx \\\n  --namespace ingress-nginx \\\n  --set controller.service.type=LoadBalancer \\\n  --version 4.0.18 \\\n  --create-namespace \\\n  --set controller.watchIngressWithoutClass=true\n</code></pre> <p>Alternatively, on Rancher 2.6.7 onward, you can add the class name on the helm install/upgrade steps :</p> <pre><code>--set ingress.ingressClassName=nginx\n</code></pre>"},{"location":"kbs/000020767/#cause","title":"Cause","text":"<p>Kubernetes version 1.22 deprecated versions of the Ingress APIs in favor of the stable <code>networking.k8s.io/v1</code> API. That leads to this scenario, where we update the controller.watchIngressWithoutClass tag.</p>"},{"location":"kbs/000020767/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020771/","title":"How to create seperate ETCD and Controlplane nodes in RKE2","text":"<p>This document (000020771) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020771/#environment","title":"Environment","text":"<p>RKE2 1.21.2 and higher</p>"},{"location":"kbs/000020771/#situation","title":"Situation","text":"<p>As part of a HA set-up, it may be required to run RKE2 with the ETCD database split from the Control plane nodes.</p>"},{"location":"kbs/000020771/#resolution","title":"Resolution","text":"<p>1. On the desired ETCD node create `/etc/rancher/rke2/config.yaml` with the following contents:</p> <pre><code>   disable-apiserver: true\n   disable-controller-manager: true\n   disable-kube-proxy: false\n   disable-scheduler: true\n</code></pre> <p>2. On the etcd node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"\" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` <p>3. On the controlplane node create `/etc/rancher/rke2/config.yaml` with the following contents:</p> <pre><code>   server: https://&lt;ip of the etcd node&gt;:9345\n   token: &lt;token string from /var/lib/rancher/rke2/server/node-token on the etcd node&gt;\n   disable-etcd: true\n   disable-kube-proxy: false\n   etcd-expose-metrics: false\n</code></pre> <p>4. On the controlplane node install rke2 `curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"\" INSTALL_RKE2_TYPE=\"server\" sh -` and start it `systemctl start rke2-server` <p>5. Add agent nodes (https://docs.rke2.io/install/ha/#5-optional-join-agent-nodes).</p>"},{"location":"kbs/000020771/#additional-information","title":"Additional Information","text":"<p>This is only an example to show this configuration in a working state. In a prod environment, you should configure a fixed registration address per the documentation at https://docs.rke2.io/install/ha/#1-configure-the-fixed-registration-address</p>"},{"location":"kbs/000020771/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020786/","title":"Restore fails with metadata.deletionGracePeriodSeconds: Invalid value: 0: field is immutable","text":"<p>This document (000020786) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020786/#environment","title":"Environment","text":"<p>Rancher backup &lt;\u00a0 2.1.3</p>"},{"location":"kbs/000020786/#situation","title":"Situation","text":"<p>Restore of backup fails with error:</p> <pre><code>Error restoring namespaced resources [error restoring rancher-k3s-upgrader of type project.cattle.io/v3, Resource=apps: restoreResource: err updating resource App.project.cattle.io \"rancher-k3s-upgrader\" is invalid: metadata.deletionGracePeriodSeconds: Invalid value: 0: field is immutable]\n</code></pre>"},{"location":"kbs/000020786/#resolution","title":"Resolution","text":"<p>There are two ways to resolve this:</p> <p>1. Upgrade to Rancher-backup 2.1.3 or higher.</p> <p>2. Extract the backup archive, search for resources that have a deletingGracePeriod field. Remove these resources manually and package the archive again. You can either remove just the metadata field or remove the entire resource. It\u2019s a resource that\u2019s supposed to be deleted so will have no negative effects.</p>"},{"location":"kbs/000020786/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020788/","title":"How to clean the orphaned cluster objects from the deleted cluster namespaces.","text":"<p>This document (000020788) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020788/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p>"},{"location":"kbs/000020788/#situation","title":"Situation","text":"<p>In some cases there may be orphaned cluster objects left behind after the in-proper deletion of a downstream cluster in Rancher. These orphaned objects could introduce a condition that causes the leader Rancher pod to enter a CrashLoop state.</p> <p>Examples of errors from the Rancher pod logs.</p> <pre><code>[ERROR] failed to call leader func: namespaces \"c-xxxxx\" not found\nfatal error: concurrent map read and map write\n</code></pre> <pre><code>[ERROR] error syncing \u2018c-xxxx/p-xxxx\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing\n[ERROR] error syncing \u2018c-xxxxx/p-xxxxx\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing\n[ERROR] error syncing \u2018c-xxxxx/p-xxxxx\u2019: handler cluster-registration-token: clusters.management.cattle.io \"c-xxxxx\" not found, requeuing\n</code></pre>"},{"location":"kbs/000020788/#resolution","title":"Resolution","text":"<p>Find the objects under the deleted cluster namespaces and manually delete each objects. Make sure there are no such orphaned objects or namespaces left in the local cluster.</p> <p>1. Set a kubeconfig for the Rancher (local) management cluster to be used with the following steps</p> <p>2. Verify the Active downstream clusters</p> <pre><code>kubectl get clusters.management.cattle.io -o custom-columns=\"ID:.metadata.name,NAME:.spec.displayName,K8S_VERSION:.status.version.gitVersion,CREATED:.metadata.creationTimestamp,DELETED:.metadata.deletionTimestamp,LAST_READY:.status.conditions[?(@.type == 'Ready')].lastUpdateTime,READY:.status.conditions[?(@.type == 'Ready')].status\" --sort-by=.metadata.creationTimestamp\n</code></pre> <p>3. Cross verify with the Rancher pod logs to get the deleted downstream cluster namespace and collect the details. Compare with the active list of clusters versus the cluster namespaces.</p> <pre><code>kubectl logs -n cattle-system -l app=rancher -c rancher\n</code></pre> <pre><code>kubectl get ns -A |grep \"c-\"\n</code></pre> <p>4. If there is a cluster that is stuck deleting, this may not complete. In this case, the finalizer object can be removed from the cluster.management.cattle.io object. Please note the c-xxxxx needs to be replaced with the cluster ID that is stuck deleting.</p> <pre><code>kubectl patch clusters.management.cattle.io &lt;c-xxxxx&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre> <p>5. If there is a namespace for a cluster that no longer exists, get the orphaned object details under the deleted cluster namespace.</p> <pre><code>kubectl api-resources --verbs=list --namespaced -o name  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;c-xxxxx&gt;\n</code></pre> <p>6. Do the cleanup of orphaned objects.</p> <ul> <li>Create the cluster namespace which is deleted, ignore if the cluster namespace is present</li> </ul> <pre><code>kubectl create ns &lt;c-xxxxx&gt;\n</code></pre> <ul> <li>Check the objects detected (in step 5) if desired, each object should have a deletion timestamp if a finalizer is preventing the object from being deleted.</li> </ul> <pre><code>kubectl -n &lt;c-xxxxx&gt; get &lt;resource type&gt; &lt;name of object&gt; -o yaml\n</code></pre> <ul> <li>Remove the finalizer to unblock the deletion of the objects. The command needs to be run for each object.</li> </ul> <pre><code>kubectl -n &lt;c-xxxxx&gt; patch &lt;resource type&gt; &lt;name of object&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre> <ul> <li>Make sure there are no objects left in the namespace.</li> </ul> <pre><code>kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;c-xxxxx&gt;\n</code></pre> <ul> <li>Finally, delete the namespace.</li> </ul> <pre><code>kubectl delete ns &lt;c-xxxxx&gt;\n</code></pre>"},{"location":"kbs/000020788/#cause","title":"Cause","text":"<p>It is important to delete downstream clusters in a process to allow Rancher to delete clusters and clean nodes that are in an Active state.</p> <p>Downstream cluster deletion is ideally performed from the Rancher UI / API, where nodes are available and able to be gracefully removed. For example, where possible do not terminate nodes in the infrastructure before the deletion is completed.</p>"},{"location":"kbs/000020788/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020788/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020792/","title":"How to make a simple terraform API request","text":"<p>This document (000020792) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020792/#environment","title":"Environment","text":"<p>Latest version of Rancher 2.6.X</p> <p>Rancher Terraform Provider 1.24.1+</p> <p>Latest Version of Terraform 1.3.1+</p> <p>User with a token created via the API and proper permissions to the local Rancher cluster.</p> <p>Local directory for terraform plan files ( name.tf) and a local terraform.tfstate file.</p>"},{"location":"kbs/000020792/#situation","title":"Situation","text":"<p>Sometimes it is necessary to create a basic skeleton for beginning a task, like using the Rancher2 Terraform Provider to speak with the Rancher API.</p> <p>This represents a starting point to choose a simple read-only task like \"query the cluster information for the local Rancher cluster\".</p>"},{"location":"kbs/000020792/#resolution","title":"Resolution","text":"<p>Terraform commands are very easy, below are the main options one may typically use.</p> <ul> <li>terraform init -- download needed files like the rancher2 terraform provider</li> <li>terraform plan -- compare the environment to the state file, plan is like a diff of any changes to be made</li> <li>terraform apply -- apply the planned changes</li> <li>terraform refresh -- update the state to match remote systems</li> <li>terraform output -- show output values from the main.tf plan</li> <li>terraform destroy -- clean up anything created by terraform</li> <li>terraform fmt -- spacing is important in HCL, terraform's language, use this command to format all spacing in the current working directory</li> </ul> <p>To get started, create a directory to hold all of the files.\u00a0 Terraform will examine the local file or files, and then populate a local terraform.tfstate data file which represents the most recent refresh of the information from the Rancher API.\u00a0 The files below can be separate or all together in a main.tf file.\u00a0 Separating plan files into individual pieces can make managing a larger project easier.\u00a0 Terraform will take actions required using variables supplied by the user or admin, or computed during the \"apply\" operation.\u00a0 As a typical rule of thumb for any provider, \"data\" sources are read operations while \"resource\" operations are write/create/change.</p> <p>Upon running \"terraform apply\" with the main.tf file below, terraform will contact the Rancher API, authenticate, request the cluster_info for the local Rancher cluster with ID \"local\" and store it into the terraform statefile, as well as output to the screen.\u00a0 The comments explain a potential name for each file, the only requirement that it ends in \"tf\".</p> <pre><code>### tfvars.tf or environment.tf\n\n#  these outline the url speaking to, and the authorization token\n\nvariable \"api_url\" {\n  description = \"rancher api url\"\n  default     = \"https://urlto.rancher-fqdn.com/v3\"\n}\n\nvariable \"token_key\" {\n  description = \"api key to use for tf\"\n  default     = \"token-nameid:jwt-long-hash-string\"\n}\n\n### providers.tf\n\n# use the variables from the earlier section to define the provider\n\nprovider \"rancher2\" {\n  api_url   = var.api_url\n  token_key = var.token_key\n  insecure  = true\n}\n\n### versions.tf\n\n# tell terraform what versions of providers and terraform itself, to expect\n\nterraform {\n  required_providers {\n    rancher2 = {\n      source  = \"rancher/rancher2\"\n      version = \"&gt;= 1.24.1\"\n    }\n  }\n  required_version = \"&gt;= 1.3.1\"\n}\n\n### main.tf\n\n## hard-coded example, read cluster info for local\n## export with 'terraform output cluster_info'\n\ndata \"rancher2_cluster\" \"local\" {\n  name = \"local\"\n}\n\noutput \"cluster_info\" {\n  value     = data.rancher2_cluster.local\n}\n</code></pre>"},{"location":"kbs/000020792/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020792/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020803/","title":"Restore k3s from MySQL dump","text":"<p>This document (000020803) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020803/#environment","title":"Environment","text":"<p>k3s with an external database MySQL (this has been tested using Azure MySQL)</p>"},{"location":"kbs/000020803/#situation","title":"Situation","text":"<p>MySQL DB dump is available, and the cluster token from /var/lib/rancher/k3s/server/token</p>"},{"location":"kbs/000020803/#resolution","title":"Resolution","text":"<ol> <li>In the new MySQL instance create a database</li> <li>Here we set the Character set to latin1 and collation to latin1_swedish_ci, as the original DB</li> <li>we also chose the same name, as it is a new instance</li> <li>Restore the dump, you may use mysql db_name &lt; backup-file.sql</li> <li>on the first node start k3s with:</li> <li>curl -sfL https://get.k3s.io | sh -s - server --token  --datastore-endpoint=\"mysql://:@tcp(.mysql.database.azure.com:3306)/?tls=true\" <li>the token is retrieve from the failed cluster in /var/lib/rancher/k3s/server/token</li> <li>Remove the failed nodes running:</li> <li>k3s kubectl get nodes</li> <li>k3s kubectl delete nodes  <li>Join any additional node using the instructions from k3s documentation</li>"},{"location":"kbs/000020803/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020805/","title":"Rancher upgrade has failed with an error no matches for kind \"Issuer\" in version \"cert-manager.io/v1alpha2\"","text":"<p>This document (000020805) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020805/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p>"},{"location":"kbs/000020805/#situation","title":"Situation","text":"<p>Rancher upgrade is failing due to the deprecated apiVersion for the cert-manager CRD. This affects cert-manager upgrades from an earlier release, for example upgrading cert-manager from 0.12 to 1.7.1, which in turn has the potential to create a deprecated apiVersion within the existing Rancher release manifest.</p> <p>The relevant error message may appear as below and occurs when running the helm upgrade command to upgrade Rancher.</p> <pre><code>Error: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: resource mapping not found for name: \"rancher\" namespace: \"\" from \"\": no matches for kind \"Issuer\" in version \"cert-manager.io/v1alpha2\" ensure CRDs are installed first\n</code></pre>"},{"location":"kbs/000020805/#resolution","title":"Resolution","text":"<p>Follow the below steps to edit the latest Helm v3 config for Rancher, and replace cert-manager.io/v1alpha2 with cert-manager.io/v1.</p> <p>1. Execute the below command and locate the latest version of\u00a0sh.helm.release.v1.rancher.v*</p> <pre><code> kubectl get secrets -n cattle-system\n</code></pre> <p>2. Back up the object, this example assumes sh.helm.release.v1.rancher.v1 is the latest</p> <pre><code>kubectl get secret sh.helm.release.v1.rancher.v1 -n cattle-system -o yaml &gt; helm-rancher-config.yaml\n</code></pre> <p>3. Decode the data.release field and save the output to yaml (jq must be installed before executing the below steps)</p> <pre><code>kubectl get secrets sh.helm.release.v1.rancher.v1 -n cattle-system -o json | jq .data.release | tr -d '\"' | base64 -d | base64 -d | gzip -d &gt; helm-rancher-config-data-decoded.yaml\n</code></pre> <p>4. Change the apiVersion from v1/alpha2 to v1.</p> <pre><code>sed -e 's/cert-manager.io\\/v1alpha2/cert-manager.io\\/v1/' helm-rancher-config-data-decoded.yaml &gt; helm-rancher-config-data-decoded-replaced.yaml\n</code></pre> <p>5. Store the encoded data in a variable to reuse in the next step</p> <pre><code>releaseData=$(cat helm-rancher-config-data-decoded-replaced.yaml | gzip | base64 | base64 | tr -d \"\\n\")\n</code></pre> <p>6. Replace the release data</p> <pre><code>sed 's/^\\(\\s*release\\s*:\\s*\\).*/\\1'$releaseData'/' helm-rancher-config.yaml &gt; helm-rancher-config-final.yaml\n</code></pre> <p>7. Apply the yaml</p> <pre><code>kubectl apply -f helm-rancher-config-final.yaml -n cattle-system\n</code></pre>"},{"location":"kbs/000020805/#cause","title":"Cause","text":"<p>Old CRD's are not deleted properly after the upgrade of cert-manager, this may cause a deprecated apiVersion to be used in the Rancher release manifest.</p>"},{"location":"kbs/000020805/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020805/#additional-information","title":"Additional Information","text":"<p>The correct way of upgrading cert-manager is in the below link</p> <p>https://docs.ranchermanager.rancher.io/getting-started/installation-and-upgrade/resources/upgrade-cert-manager#option-a-upgrade-cert-manager-with-internet-access</p> <p>Below is a snippet of helm get manifest -n cattle-system rancher which uses old CRDs, and thus has deprecated apiVersions.</p> <pre><code>---\n# Source: rancher/templates/issuer-rancher.yaml\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: rancher\n  labels:\n    app: rancher\n    chart: rancher-2.6.6\n    heritage: Helm\n    release: rancher\nspec:\n  ca:\n    secretName: tls-rancher\n</code></pre> <p>As in the above, /v1apha2 is referenced, this version has been deprecated.</p> <p>Command to get the available apiVersion for cert-manager</p> <pre><code>kubectl get --raw /apis/cert-manager.io | jq .\n</code></pre>"},{"location":"kbs/000020805/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020831/","title":"How to troubleshoot Overlay Network Connectivity issues","text":"<p>This document (000020831) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020831/#situation","title":"Situation","text":"<p>pod-to-pod communication not happening</p>"},{"location":"kbs/000020831/#resolution","title":"Resolution","text":"<p>Pod-to-Pod communication should depend on multiple factors. Mainly network communication should be allowed in between the nodes. The following checkpoints help us trace the problem's root cause.</p> <ul> <li> <p>Check ports for their overlay are open between nodes (if they have multiple subnets/VLANs/DCs); testing from just one node to nodes in the other network should be good enough,\u00a0for e.g.,\u00a0`nc -uvz  8472`\u00a0(if they\u2019re using canal, change the port as needed).[https://rancher.com/docs/rancher/v2.6/en/installation/requirements/ports/#commonly-used-ports] <li> <p>Check the DNS from a test pod with suitable\u00a0tools (not busybox, it has nslookup issues),\u00a0`rancherlabs/swiss-army-knife`\u00a0is good for this.\u00a0`dig  @`, do this for all coredns pod IPs. <p>-Use the same test pod to test their upstream nameservers (all 3, over a few retries),\u00a0`dig  \u00a0@` <p>[ https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/dns]</p> <p>Note: \u00a0In an air-gap environment, Swiss-army-knife is not available. You can try a specific busy box image with network tools like busybox image v1.28.</p> <ul> <li>Run the overlay test mentioned in the Rancher documentation to test pod-to-pod communication. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Overlay network test steps test the pod to pod connectivity between the nodes \u00a0:https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/networking#check-if-overlay-network-is-functioning-correctly.</li> </ul> <p>[Note: This overlay test performs the pod-to-pod communication using ICMP protocol, which means you will still see networking issues because TCP communication might be blocked even though the test passes. So you have to test with good network tools like NC and iperf.]</p> <ul> <li>Check the Infra VMS \u00a0knowns issues and overlay network ports are allowed at the switch level.</li> </ul> <p>e.g., In case of Vmware\u00a0vSphere version 6.7u2.</p> <ol> <li>Change the VXLAN port to 8472 (when NSX is not used) or 4789 (when NSX is used)</li> <li>Disable the VXLAN hardware offload feature on the VMXNET3 NIC (which recent Linux driver version enable by default.\u00a0 [https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer\u00a0PR 2766401 , https://github.com/projectcalico/calico/issues/4727\u00a0]</li> </ol>"},{"location":"kbs/000020831/#additional-information","title":"Additional Information","text":"<p>Reference Artiles&amp; Links:</p> <p>https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer\u00a0PR 2766401</p> <p>https://github.com/projectcalico/calico/issues/4727</p>"},{"location":"kbs/000020831/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020834/","title":"How to define additional static pods on an RKE2 cluster node","text":"<p>This document (000020834) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020834/#situation","title":"Situation","text":"<p>Is it possible to define additional static Pods to start on an RKE2 during RKE2 node initialization?</p>"},{"location":"kbs/000020834/#resolution","title":"Resolution","text":"<p>Yes, you can define additional static Pods on an RKE2 host by placing the manifests into the directory /var/lib/rancher/rke2/agent/pod-manifests/ Any Pod manifests within this directory will be created by the kubelet, as static pods, during the RKE2 node agent startup.</p>"},{"location":"kbs/000020834/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020859/","title":"Resolving a fleet-agent that is stuck in the Pending-Upgrade state","text":"<p>This document (000020859) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020859/#situation","title":"Situation","text":"<p>The fleet-agent is stuck in a \"Pending-Upgrade\" state and showing the following error:</p> <pre><code>time=\"2022-07-19T16:12:08Z\" level=info msg=\"preparing upgrade for fleet-agent-c-97hcq\" time=\"2022-07-19T16:12:08Z\" level=info msg=\"getting history for release fleet-agent-c-97hcq\" time=\"2022-07-19T16:12:08Z\" level=error msg=\"error syncing 'cluster-fleet-default-c-97hcq-86145229ab95/fleet-agent-c-97hcq': handler bundle-deploy: another operation (install/upgrade/rollback) is in progress, requeuing\"\n</code></pre>"},{"location":"kbs/000020859/#resolution","title":"Resolution","text":"<p>Run the following command against the cluster where the fleet-agent is running:</p> <pre><code>kubectl get secret -A -l status=pending-upgrade\n</code></pre> <p>It will show the output of a secret that is causing the pending-upgrade state as follows:</p> <pre><code>NAMESPACE             NAME                                        TYPE                 DATA   AGE\ncattle-fleet-system   sh.helm.release.v1.fleet-agent-c-97hcq.v2   helm.sh/release.v1   1      132d\n</code></pre> <p>Based on the above output above, run through the following steps:</p> <p>1. Backup the yaml (and save it to a persistent location) for the fleet secret that is causing the pending-upgrade state:</p> <pre><code>kubectl get secret -n cattle-fleet-system sh.helm.release.v1.fleet-agent-c-97hcq.v2 -oyaml &gt; fleet-agent-c-97hcq.yaml\n</code></pre> <p>2. Delete the secret:</p> <pre><code>kubectl delete secret -n cattle-fleet-system sh.helm.release.v1.fleet-agent-c-97hcq.v2\n</code></pre> <p>3. In the Rancher UI, go to Continuous Delivery and do a \"Force Update\" on the downstream cluster in question</p>"},{"location":"kbs/000020859/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020895/","title":"Adding tolerations to components in the Rancher Logging chart","text":"<p>This document (000020895) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020895/#environment","title":"Environment","text":"<p>- Rancher v2.5.x, v2.6.x or v2.7.x</p> <p>- Rancher Logging Chart</p> <p>- Taints defined on cluster nodes</p>"},{"location":"kbs/000020895/#situation","title":"Situation","text":"<p>When deploying the Rancher logging chart, Pods are not scheduled, where user-added taints\u00a0are present on cluster nodes, and tolerations are not set on the rancher-logging chart. If only some nodes within the cluster are tainted, logs will be missing from those nodes. If all nodes are tainted, no logs will be forwarded and Pods for Deployments within the Rancher Logging will fail to schedule with an error of the following format:</p> <pre><code>Events:\nType Reason Age From Message\n---- ------ ---- ---- -------\nWarning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate\n</code></pre>"},{"location":"kbs/000020895/#resolution","title":"Resolution","text":"<p>If there are user-added\u00a0taints on nodes within the cluster, tolerations for these taints must be added in the rancher-logging chart via the tolerations value, alongside the default\u00a0cattle.io/os=linux\u00a0NoSchedule toleration.</p> <p>For example, if the taint with key=foo, value=bar and effect=NoSchedule is present on nodes within the cluster, the following tolerations should be defined in the values of the rancher-logging Chart:</p> <pre><code>tolerations:\n  - key: cattle.io/os\n    operator: \"Equal\"\n    value: \"linux\"\n    effect: NoSchedule\n\u00a0 - key: foo\n\u00a0   operator: \"Equal\"\n\u00a0   value: \"bar\"\n\u00a0   effect: NoSchedule\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020895/#cause","title":"Cause","text":"<p>Nodes with specific taints will deny the scheduling of any pod if no matching toleration is present on the workload.</p>"},{"location":"kbs/000020895/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020896/","title":"Unable to see Rancher GUI, getting 500, 502, 503","text":"<p>This document (000020896) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020896/#situation","title":"Situation","text":"<p>When attempting to navigate to the Rancher UI, cannot see the UI and instead are receiving a 500, 502,503 error.</p>"},{"location":"kbs/000020896/#cause","title":"Cause","text":"<p>For HTTP, the 5XX errors (500-511) are server-side errors meaning that the service itself is not responding to queries. The most common errors seen when trying to navigate to Rancher and seeing a 5XX error are:</p> <ul> <li>500 Internal Server Error - Generic error that something is wrong with eh server</li> <li>502 Bad Gateway - Upstream server provided an invalid response</li> <li>503 Service Unavailable - The server cannot handle the request</li> </ul> <p>Generally speaking, these errors are seen due to either a problem with the ingress or the Rancher pods themselves.\u00a0Generally speaking, a 5XX error can be corrected by trying to redeploy the Rancher pods, but we should first try to verify what the primary issue is. To begin troubleshooting these errors, check that the Rancher pods are running. If they are all running, check that there are no errors in the Rancher logs. If everything with the Rancher pods seems healthy, check that your ingress controller pods are also running and not posting errors.</p>"},{"location":"kbs/000020896/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020897/","title":"Infrastructure pod /etc/resolv.conf different than kubelet resolv.conf","text":"<p>This document (000020897) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020897/#environment","title":"Environment","text":"<p>RKE1 on an OS running systemd-resolved</p>"},{"location":"kbs/000020897/#situation","title":"Situation","text":"<p>An infrastrucutre pods (one that relies on the node network to communicate rather than the kubernetes overlay network) has a different resolv.conf than kubelet or the node on which it is being hosted.</p>"},{"location":"kbs/000020897/#resolution","title":"Resolution","text":"<p>There are multiple options to correct this issue:</p> <ul> <li>Update the symlink for /etc/resolvd.conf from\u00a0/run/systemd/resolve/stub-resolv.conf to /run/systemd/resolve/resolv.conf</li> <li>Confirm that /run/systemd/resolve/resolv.conf has the correct DNS entries in it</li> <li>Disable systemd-resolved if it is no longer needed</li> </ul> <p>After making any of these changes, you will need to restart the impacted pods</p>"},{"location":"kbs/000020897/#cause","title":"Cause","text":"<p>systemd-resolved is an updated method of managing DNS lookups on modern Linux operating systems. By default, /etc/resolv.conf is symlinked to /run/systemd/resolve/stub-resolv.conf that points only to the loopback DNS stub of 127.0.0.1. Changes to /etc/resolv.conf only modify /run/systemd/resolve/stub-resolv.conf. Because this is a stub meant to lookup entries against the configured nameservers in /run/systemd/resolve/resolv.conf, some system tools that rely on networking outside of the host, such as Docker, are configured to check if systemd-resolved is running and use /run/systemd/resolve/resolv.conf directly if it is. This can lead to a discrepancy between what the majority of the host is using and what Docker containers are using to resolve DNS queries.</p>"},{"location":"kbs/000020897/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020904/","title":"Is it possible to change the CNI for a rke/rke2/k3s cluster or Rancher launched cluster post-provisioning?","text":"<p>This document (000020904) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020904/#environment","title":"Environment","text":"<p>RKE1/RKE2/K3s Clusters</p> <p>Any cluster provisioned by Rancher</p>"},{"location":"kbs/000020904/#situation","title":"Situation","text":"<p>The CNI\u00a0for a cluster can be specified in the cluster configuration\u00a0 when launching a Kubernetes cluster via both the CLI and Rancher v2.x. Is it possible to change the CNI after the cluster has been provisioned?</p>"},{"location":"kbs/000020904/#resolution","title":"Resolution","text":"<p>Changing the cluster CNI after the cluster has been provisioned is not supported and care should be taken to set these as required when first configuring the cluster.</p>"},{"location":"kbs/000020904/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020904/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020907/","title":"Cannot provision new RKE Cluster from Template: Unable to validate S3 backup target configuration","text":"<p>This document (000020907) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020907/#environment","title":"Environment","text":"<ul> <li>Suse Rancher 2.5.9</li> <li>Air-gapped environment</li> <li>1 or more RKE downstream clusters</li> <li>RKE template configured for the downstream clusters</li> </ul>"},{"location":"kbs/000020907/#situation","title":"Situation","text":""},{"location":"kbs/000020907/#the-provisioning-of-an-rke-downstream-cluster-fails-after-adding-extra_args-for-the-kubelet-service-to-a-new-rke-template-and-using-this-newly-created-template-to-provision-the-rke-cluster","title":"The provisioning of an RKE downstream cluster fails after adding extra_args for the kubelet service to a new RKE template and using this newly created template to provision the RKE cluster.","text":"<p>Error:</p> <pre><code>Error message while creating a new RKE cluster from the new RKE Template:\nUnable to validate S3 backup target configuration: Get\u00a0http://169.254.169.254/latest/meta-data/iam/security-credentials/\": dial tcp 169.254.169.254:80: i/o timeout\n</code></pre> <p>Configuration:</p> <pre><code>E.g. extra_args for the kubelet added to the RKE Template:\n\n    kubelet:\n      ...\n      extra_args:\n        ...\n        image-gc-high-threshold: '80'\n        image-gc-low-threshold: '75'\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020907/#resolution","title":"Resolution","text":"<p>Verify if the RKE Template has configurable questions:</p> <ul> <li>Check in\u00a0the RKE Template if there is any setting set as a configurable question for the end-user</li> <li>Go to Cluster Management =&gt; RKE1 Configuration =&gt; RKE Templates</li> <li>Select your desired RKE Template, and click on the 3-dot menu far on the right menu, select \"Clone revision\"</li> <li>Go to the \"Cluster Option Overrides\" section lower down on the cloned template, and uncheck any available options.\u00a0After removing these configurable questions,\u00a0\u00a0the value set in the template is used without user entry.</li> <li>Set a Name for the new Template and click on Save. Select the newly created template and set it as Default Template if this is going to be your default template.</li> </ul>"},{"location":"kbs/000020907/#cause","title":"Cause","text":"<p>Configurable questions on the RKE template would require the end-user to answer those questions to set up a new RKE cluster and won't allow a new RKE cluster to be deployed automatically.</p>"},{"location":"kbs/000020907/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020909/","title":"How to enable fluent-bit debug logging","text":"<p>This document (000020909) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020909/#environment","title":"Environment","text":"<p>Rancher with Rancher-Logging chart v2</p>"},{"location":"kbs/000020909/#situation","title":"Situation","text":"<p>When troubleshooting the fluent-bit log level might need to be increased to debug level.</p>"},{"location":"kbs/000020909/#resolution","title":"Resolution","text":"<p>Fluent-bit debug level logging can be changed by modifying the logLevel spec of the logging CRD</p> <p>The following steps will help enable or disable debug-level logging for fluent-bit:</p> <p>1.\u00a0Edit rancher logging CRD</p> <pre><code>&gt; kubectl edit logging rancher-logging-root\n</code></pre> <p>2. Add logLevel debug to the fluentbit spec</p> <pre><code>spec:\n  controlNamespace: cattle-logging-system\n  fluentbit:\n    image:\n      repository: rancher/mirrored-fluent-fluent-bit\n      tag: 1.9.3\n    logLevel: debug\n</code></pre> <p>3. Confirm log level is set to debug</p> <pre><code>&gt; kubectl get pods -n cattle-logging-system -l app.kubernetes.io/name=fluentbit\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0READY \u00a0 STATUS \u00a0 \u00a0RESTARTS \u00a0 AGE\nrancher-logging-root-fluentbit-gqq8l \u00a01/1 \u00a0 \u00a0 Running \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a019s\n\n&gt; kubectl logs --tail=10 -n cattle-logging-system rancher-logging-root-fluentbit-gqq8l\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1841\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=927\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1840\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1840\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=927\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=913\n</code></pre> <p>Notes:</p> <ul> <li>To reset the logging level to the default info, re-run the same steps and change the logLevel to info.</li> <li>The logLevel value is not exposed via the rancher-logging chart and cannot be configured persistently. As a result, the\u00a0change\u00a0will be overwritten and logLevel reset to the default of info after a rancher-logging chart\u00a0upgrade.</li> </ul> <p>Reference:</p> <ul> <li>https://banzaicloud.com/docs/one-eye/logging-operator/configuration/crds/v1beta1/fluentbit_types/#fluentbitspec-loglevel</li> </ul>"},{"location":"kbs/000020909/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020910/","title":"Downstream clusters in unavailable state after upgrade from Rancher v2.5 at v2.5.16 or above to Rancher v2.6 below v2.6.7","text":"<p>This document (000020910) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020910/#environment","title":"Environment","text":"<p>Rancher v2.5 at or above patch release v2.5.16 upgraded to Rancher v2.6 below patch release v2.6.7</p>"},{"location":"kbs/000020910/#situation","title":"Situation","text":"<p>After the upgrade of a Rancher v2.5 environment running patch release v2.5.16 or above, to Rancher v2.6 below patch release v2.6.7 (e.g. an upgrade from Rancher v2.5.16 to v2.6.6) downstream clusters are in an unavailable state within Rancher.</p> <p>Rancher Pod logs contain error messages of the following format:</p> <pre><code>2022/12/17 08:47:18 [ERROR] error syncing 'c-ayhjd': handler cluster-deploy: cluster context c-ayhjd is unavaiblable, requeuing\n2022/12/17 08:47:25 [ERROR] error syncing '_all_': handler user-controllers-controller: failed to start user controllers for cluster c-ayhjd: ClusterUnavailable 503: cluster not found\n</code></pre>"},{"location":"kbs/000020910/#resolution","title":"Resolution","text":"<p>To resolve the issue it is necessary to set the status.serviceAccountToken field on the cluster.management.cattle.io object for each downstream cluster from the service account token secret for the cluster. This can be done with the following BASH one-liner, with a kubeconfig sourced for the Rancher local cluster:</p> <pre><code>for cluster in $(kubectl get clusters.management.cattle.io --field-selector metadata.name!=local -o custom-columns=NAME:.metadata.name --no-headers); do echo $cluster; kubectl patch -v=9 cluster.management.cattle.io $cluster --type=merge -p \"{\\\"status\\\":{\\\"serviceAccountToken\\\":\\\"`kubectl -n cattle-global-data get secret -o jsonpath=\\\"{.items[?(@.metadata.ownerReferences[0].name==\\\\\"$cluster\\\\\")].data.credential}\\\"|base64 -d`\\\"}}\"; done\n</code></pre> <p>Next, edit the cluster.management.cattle.io resource in the Rancher local cluster, for each downstream cluster, to set the status of the ServiceAccountMigrated condition from True to Unknown. This action is taken to ensure that on upgrade to Rancher v2.6.7+ the secretAccountToken field is again removed and migrated to a secret. With a kubeconfig sourced for the Rancher local cluster, get the cluster IDs for all downstream clusters:</p> <pre><code>kubectl get clusters.management.cattle.io --field-selector metadata.name!=local -o custom-columns=NAME:.metadata.name --no-headers\n</code></pre> <p>One at a time for each cluster ID listed execute `kubectl edit cluster.management.cattle.io ` locate the condition with the type ServiceAccountMigrated in the status.conditions array, and update the status from \"True\" to \"Unknown\" per the following example: <pre><code>[...]\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:57Z\"\n\u00a0 \u00a0 status: \"True\"\n\u00a0 \u00a0 type: Updated\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:51Z\"\n\u00a0 \u00a0 status: \"Unknown\"\n\u00a0 \u00a0 type: ServiceAccountMigrated\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:57Z\"\n\u00a0 \u00a0 status: \"True\"\n\u00a0 \u00a0 type: GlobalAdminsSynced\n\u00a0 - lastUpdateTime: \"2023-01-04T12:17:40Z\"\n\u00a0 [...]\n</code></pre> <p>Finally, take a copy of the service account token secrets and then remove these, as they are no longer used and fresh secrets will be created upon upgrade to Rancher v2.6.7+.</p> <p>With a kubeconfig for the Rancher local cluster sourced, first take a copy of the service account token secret manifests, with tthe following bash one-liner:</p> <pre><code>for secret in `kubectl -n cattle-global-data get secrets -o name | grep \"cluster-serviceaccounttoken-\"`; do kubectl -n cattle-global-data get $secret -o yaml &gt;&gt; cluster-serviceaccounttoken-secrets.yaml; echo \"---\" &gt;&gt; cluster-serviceaccounttoken-secrets.yaml; done\n</code></pre> <p>Then with the Rancher local cluster kubeconfig still sourced, delete the secrets:</p> <pre><code>for secret in `kubectl -n cattle-global-data get secrets -o name | grep \"cluster-serviceaccounttoken-\"`; do kubectl -n cattle-global-data delete $secret; done\n</code></pre>"},{"location":"kbs/000020910/#cause","title":"Cause","text":"<p>In order to address CVE-2021-36782 the service account token used by Rancher to connect to the Kubernetes API Server of a downstream cluster was moved from the status.serviceAccountToken field of the cluster.management.cattle.io resource to a secret referenced by the status.serviceAccountTokenSecret field. This fix was introduced to Rancher v2.6 in patch release v2.6.7 and above; and to Rancher v2.5 in patch release v2.5.16 and above. Rancher versions v2.5.0 - v2.5.15 and v2.6.0 - v2.6.6 inclusive use the status.serviceAccountToken field to store and retrieve the service account token for downstream clusters.</p> <p>As a result, where a Rancher environment is upgraded from Rancher v2.5 at v2.5.16 or above (containing the fix), to Rancher v2.6 below patch release v2.6.7 (which does not contain the fix), the status.serviceAccountToken field will be missing from the cluster.management.cattle.io resource and Rancher will be unable to connect to existing downstream clusters.</p>"},{"location":"kbs/000020910/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020917/","title":"How to fix Azure AD authenication errors when upgrading to new graph endpoint in Rancher v2.6.7+","text":"<p>This document (000020917) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020917/#environment","title":"Environment","text":"<p>If you are using Rancher v2.6.7 and above and wish to set up Azure AD initially, please review our documentation here and enable Azure AD as your primary authentication method.</p> <p>For those using Rancher version(s) v2.6.6 and below, please refer to the following messages under the v2.6.0-v2.6.6 section on our documentation page here.</p>"},{"location":"kbs/000020917/#situation","title":"Situation","text":"<p>Disclaimer:</p> <p>There are a few issues that you may encounter when upgrading to the latest Microsoft Graph endpoint when using Azure AD. This KB Article aims to address these particular errors and provide the best solution for each scenario.</p> <p>If you are experiencing one of the following error messages, please continue to the corresponding numerical value.</p> <ol> <li><code>server error while authenticating: missing required permissions from Microsoft Graph: need Group.Read.All, User.Read.All</code></li> <li><code>\"refusing to set principal on user that is already bound to another user\"</code></li> <li>``` Error:AADSTS9000411: The request is not properly formatted.</li> </ol> <pre><code>4. ```\nError during login \"AADSTS901002: The 'resource' request parameter is not supported\"\n</code></pre>"},{"location":"kbs/000020917/#resolution","title":"Resolution","text":"<ol> <li> <p>If you are receiving this error, you likely have the incorrect type of permissions in the Azure console. When setting up Azure AD, you will need Application Permissions, NOT Delegated Permissions. For more information, please review our documentation here.</p> </li> <li> <p>If you are receiving this error in the Rancher UI, likely, you are using a different user that initially set up Azure AD to make modifications. For example, suppose you are logged in as an Azure AD user and try to disable/re-enable the authorization provider. In that case, it is likely, the local Rancher admin had initially set up the authentication provider and is bound to that admin user. So when trying to re-enable Azure AD as a local user and missing the correct permissions, you'll likely run into this error. Rancher will be aware of this user, and there is a link between the Azure Ad user and the Rancher user. There are a few solutions to this:</p> </li> <li> <p>In the Users &amp; Authentication section in the Rancher UI, as a local admin, you can grant the Azure AD user Configure Authentication and Manage Users permissions. Doing this should allow the Azure AD user to make changes to the authentication provider and should be able to re-enable it.</p> </li> <li> <p>Another way you can fix this issue is by enabling Azure AD with an Azure AD user unknown to Rancher.</p> </li> <li> <p>If you are receiving this Error: AADSTS9000411: The request is not properly formatted. The parameter 'response_type' is duplicated; Rancher is likely trying to send multiple requests simultaneously. You will want to verify that you are logging into Rancher with the correct URL; the URL will be https://rancher_url/dashboard.</p> </li> <li> <p>If you are receiving this error Error during login \"AADSTS901002: The 'resource' request parameter is not supported\" and are running on Rancher 2.6.10 or below, the error is likely due to Conditional access policies set up in your organization. To fix this issue, upgrade to Rancher 2.6.11 or above in the 2.6.x version, or to 2.7.0 or above in the 2.7.x versions. You will also need to change the Azure AD application permissions from user.read.all and group.read.all permissions to <code>directory.read.all.</code></p> </li> </ol>"},{"location":"kbs/000020917/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020946/","title":"Basic troubleshooting steps for the Logging Operator","text":"<p>This document (000020946) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020946/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>Rancher 2.7.x</p> <p>Logging Operator V2</p>"},{"location":"kbs/000020946/#situation","title":"Situation","text":"<p>Logs may not be sent from the operator to the chosen destination (ELK, Splunk, etc)</p>"},{"location":"kbs/000020946/#resolution","title":"Resolution","text":"<p>1. On the Rancher menu, go to Workload, Pods.</p> <p>Check the rancher-logging-root-fluentd-configcheck pod. Its status should be `Completed`.</p> <p>If it is in a different state, check its logs to understand the error on your configuration.</p> <p>You may do so by clicking on the three dots menu and 'View Logs'.</p> <p>2. Check the fluentd logs. They are not written to standard output. Executing on the container rancher-logging-root-fluentd-0 navigate to</p> <p>/fluentd/out and check the log file there:</p> <pre><code>$ kubectl -n cattle-logging-system exec -it rancher-logging-root-fluentd-0 -- sh\n$ cat /fluentd/out/log\n</code></pre> <p>If there are any errors, you can try to fix them or open a case if you need more help.</p> <p>3. If there are no errors in the two first steps, a simple way to check if the logs are being scrapped is to send them to a file. An output YAML would look like this:</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: Output\nmetadata:\n  name: fileoutput\n  namespace: default\nspec:\n  file:\n    path: /tmp/${tag}\n</code></pre> <p>Note the ${tag} on the path, required by Fluentd.</p> <p>In this case, the logs will be available on the /tmp folder of the Pod rancher-logging-root-fluentd-0</p> <p>If logs are not written to the /tmp folder, open a case with us to help you.</p> <p>4. If logs are flowing to the file, you need to check your Outputs.</p> <p>Tweaking the buffer values might be a solution.</p>"},{"location":"kbs/000020946/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020949/","title":"RKE2 cluster provisioning in Rancher with profile: cis-1.6, requires parameter protect-kernel-defaults to true","text":"<p>This document (000020949) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020949/#environment","title":"Environment","text":"<p>Rancher 2.6</p>"},{"location":"kbs/000020949/#situation","title":"Situation","text":"<p>When provisioning a new custom RKE2 cluster with Worker CIS Profile 1.6 from Rancher UI, if\u00a0 the parameter\u00a0 \"protect-kernel-defaults\"\u00a0 is not set to \"true\", the RKE2 server will exit with error:</p> <pre><code>RKE2 server error log\n\n#journalctl -fu rke2-server\nStarting Rancher Kubernetes Engine v2 (server)...\nsh[26475]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service\nsh[26475]: /bin/sh: 1: /usr/bin/systemctl: not found\nrke2[26486]: time=\"2023-01-23T12:11:54Z\" level=fatal msg=\"--protect-kernel-defaults must be true when using --profile=cis-1.6\"\nJsystemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE\n systemd[1]: rke2-server.service: Failed with result 'exit-code'\n</code></pre>"},{"location":"kbs/000020949/#resolution","title":"Resolution","text":""},{"location":"kbs/000020949/#how-to-set-flag-protect-kernel-defaults","title":"How to set flag\u00a0<code>protect-kernel-defaults?</code>","text":"<p>When provisioning the cluster, the \"protect-kernel-default\" can be set in the\u00a0\u00a0Advanced\u00a0section under Cluster Configuration.</p> <ol> <li>Click\u00a0\u2630 &gt; Cluster Management</li> <li>On the\u00a0Clusters\u00a0page, click\u00a0Create</li> <li>Toggle the switch to\u00a0RKE2/K3s</li> <li>Custom</li> <li>Cluster Configuration ==&gt; Advanced</li> <li>Click the checkbox</li> </ol> <pre><code>Raise error if kernel parameters are different than the expected kubelet defaults\n</code></pre>"},{"location":"kbs/000020949/#cause","title":"Cause","text":"<p>When\u00a0 RKE2 starts with the \"profile\" flag set to cis-1.6, \" <code>protect-kernel-defaults\"</code>\u00a0is exposed as a configuration flag for RKE2. This flag has to be set to \"true\" when provisioning the cluster.</p>"},{"location":"kbs/000020949/#additional-information","title":"Additional Information","text":"<p>RKE2 is designed to be \"hardened by default\" and pass the majority of the Kubernetes CIS controls without modification. There are a few notable exceptions to this that require manual intervention to fully pass the CIS Benchmark.</p> <p>CIS Hardening Guide</p> <ul> <li> <p>Host-level requirements</p> </li> <li> <p>RKE2 CIS v1.6 Benchmark - Self-Assessment Guide - Rancher v2.6</p> </li> </ul>"},{"location":"kbs/000020949/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020950/","title":"Failed to handling tunnel request from remote address x.x.x.x:42412 (X-Forwarded-For: x.x.x.x): response 400: cluster not found","text":"<p>This document (000020950) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020950/#environment","title":"Environment","text":"<ul> <li>Rancher v2.x</li> </ul>"},{"location":"kbs/000020950/#situation","title":"Situation","text":"<p>There are a lot of errors observed in the Rancher pods:</p> <pre><code>[ERROR] Failed to handling tunnel request from remote address x.x.x.x:42412 (X-Forwarded-For: x.x.x.x): response 400: cluster not found\n</code></pre>"},{"location":"kbs/000020950/#resolution","title":"Resolution","text":"<p>The error message \" Failed to handling tunnel request from remote address x.x.x.x:42412 (X-Forwarded-For: x.x.x.x): response 400: cluster not found\"\u00a0 indicates that on some hosts\u00a0from now-deleted clusters, there are Rancher agent containers/Pods that are still running and attempting to connect to Rancher.</p> <p>To locate these instances and stop the running containers or hosts to prevent these messages:</p> <p>Suppose your load-balancer is performing Layer-7 load-balancing and setting the X-Forwarded-For header itself. In that case, you could enable use-forwarded-headers on the ingress-nginx controller in the Rancher local cluster ( https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-forwarded-headers).</p> <p>This would pass through the X-Forwarded-For header from the loadbalancer, enabling you to identify the hosts from which these requests originate within the Rancher Pod logs.</p>"},{"location":"kbs/000020950/#cause","title":"Cause","text":"<p>Remaining nodes from deleted clusters trying to connect to Rancher.</p>"},{"location":"kbs/000020950/#additional-information","title":"Additional Information","text":"<p>The reason for seeing\u00a0 IPs in the range 10.42.x.x in the remote address and your load-balancer IPs in the X-Forwarded-For on the logs is that:</p> <ol> <li>the remote address\u00a0is the ingress-nginx Pod IP</li> <li>the X-Forwarded-For address is the load-balancer forwarding the requests to the cluster.</li> </ol>"},{"location":"kbs/000020950/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020951/","title":"Adding missing cAdvisor labels to k8s 1.24+","text":"<p>This document (000020951) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020951/#environment","title":"Environment","text":"<p>RKE1 on k8s 1.24+, Rancher 2.6.8+ or 2.7.0+</p>"},{"location":"kbs/000020951/#situation","title":"Situation","text":"<p>Monitoring V2 dashboards are not displaying CPU/Memory usage.</p>"},{"location":"kbs/000020951/#resolution","title":"Resolution","text":"<p>Rancher Engineering team has put in place a workaround that consists of the following:</p> <ul> <li>Creating a cAdvisor standalone instance</li> <li>Creating ServiceMonitor with some relabeling</li> <li> <p>Disabling\u00a0kubelet.serviceMonitor.cAdvisor in the rancher-monitoring chart</p> </li> <li> <p>Example standalone cAdvisor instance and ServiceMonitor YAML:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\nrules:\n- apiGroups:\n  - policy\n  resourceNames:\n  - cadvisor\n  resources:\n  - podsecuritypolicies\n  verbs:\n  - use\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cadvisor\nsubjects:\n- kind: ServiceAccount\n  name: cadvisor\n  namespace: kube-system\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: cadvisor\n      name: cadvisor\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: \"\"\n      labels:\n        app: cadvisor\n        name: cadvisor\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - args:\n        - --housekeeping_interval=10s\n        - --max_housekeeping_interval=15s\n        - --event_storage_event_limit=default=0\n        - --event_storage_age_limit=default=0\n        - --enable_metrics=app,cpu,disk,diskIO,memory,network,process\n        - --docker_only\n        - --store_container_labels=false\n        - --whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace\n        image: gcr.io/cadvisor/cadvisor:v0.45.0\n        name: cadvisor\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 800m\n            memory: 2000Mi\n          requests:\n            cpu: 400m\n            memory: 400Mi\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n          readOnly: true\n        - mountPath: /var/run\n          name: var-run\n          readOnly: true\n        - mountPath: /sys\n          name: sys\n          readOnly: true\n        - mountPath: /var/lib/docker\n          name: docker\n          readOnly: true\n        - mountPath: /dev/disk\n          name: disk\n          readOnly: true\n      priorityClassName: system-node-critical\n      serviceAccountName: cadvisor\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - key: node-role.kubernetes.io/controlplane\n        value: \"true\"\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/etcd\n        value: \"true\"\n        effect: NoExecute\n      volumes:\n      - hostPath:\n          path: /\n        name: rootfs\n      - hostPath:\n          path: /var/run\n        name: var-run\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /var/lib/docker\n        name: docker\n      - hostPath:\n          path: /dev/disk\n        name: disk\n---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\nspec:\n  allowedHostPaths:\n  - pathPrefix: /\n  - pathPrefix: /var/run\n  - pathPrefix: /sys\n  - pathPrefix: /var/lib/docker\n  - pathPrefix: /dev/disk\n  fsGroup:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  volumes:\n  - '*'\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cadvisor\n  labels:\n    app: cadvisor\n  namespace: kube-system\nspec:\n  selector:\n    app: cadvisor\n  ports:\n  - name: cadvisor\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\nspec:\n  endpoints:\n  - metricRelabelings:\n    - sourceLabels:\n      - container_label_io_kubernetes_pod_name\n      targetLabel: pod\n    - sourceLabels:\n      - container_label_io_kubernetes_container_name\n      targetLabel: container\n    - sourceLabels:\n      - container_label_io_kubernetes_pod_namespace\n      targetLabel: namespace\n    - action: labeldrop\n      regex: container_label_io_kubernetes_pod_name\n    - action: labeldrop\n      regex: container_label_io_kubernetes_container_name\n    - action: labeldrop\n      regex: container_label_io_kubernetes_pod_namespace\n    port: cadvisor\n    relabelings:\n    - sourceLabels:\n      - __meta_kubernetes_pod_node_name\n      targetLabel: node\n    - sourceLabels:\n      - __metrics_path__\n      targetLabel: metrics_path\n      replacement: /metrics/cadvisor\n    - sourceLabels:\n      - job\n      targetLabel: job\n      replacement: kubelet\n  namespaceSelector:\n    matchNames:\n    - kube-system\n  selector:\n    matchLabels:\n      app: cadvisor\n</code></pre> <ol> <li>On the Rancher Monitoring chart, change the kubelet.serviceMonitor.cAdvisor to false</li> </ol> <pre><code>kubelet:\n  serviceMonitor:\n    cAdvisor: false\n</code></pre>"},{"location":"kbs/000020951/#cause","title":"Cause","text":"<p>This is caused by the changes on k8s 1.24, by the removal of the Dockershim</p>"},{"location":"kbs/000020951/#status","title":"Status","text":"<p>Reported to Engineering</p>"},{"location":"kbs/000020951/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020954/","title":"Testing Connectivity Between Kubernetes Pods with Iperf3","text":"<p>This document (000020954) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020954/#situation","title":"Situation","text":"<p>In this article, we will show you how to use iperf3 to test the connectivity between Kubernetes pods in your cluster. We will use a Deployment to ensure that iperf3 pods are running on each node in the cluster.</p>"},{"location":"kbs/000020954/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster</li> <li>The kubectl command-line tool installed on your local machine</li> </ul>"},{"location":"kbs/000020954/#step-1-deploy-iperf3-as-a-deployment","title":"Step 1: Deploy Iperf3 as a Deployment","text":"<p>To deploy iperf3 as a Deployment on your Kubernetes cluster, you will need to create a YAML file with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: iperf3-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: iperf3\n  template:\n    metadata:\n      labels:\n        app: iperf3\n    spec:\n      containers:\n      - name: iperf3\n        image: leodotcloud/swiss-army-knife\n        ports:\n        - containerPort: 5201\n</code></pre> <pre><code>\n</code></pre> <p>This YAML file creates a Deployment named <code>iperf3-deployment</code> with 3 replicas of the iperf3 container.</p> <p>You can then create the Deployment by running the following command:</p> <pre><code>kubectl apply -f iperf3-deployment.yaml\n</code></pre>"},{"location":"kbs/000020954/#step-2-verify-the-deployment","title":"Step 2: Verify the Deployment","text":"<p>Once the Deployment is up and running, you can use the following command to check that iperf3 pods are running on all nodes in the cluster:</p> <pre><code>kubectl get pods\n</code></pre> <p>You should see output similar to the following:</p> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\niperf3-deployment-5b7f6f5c8-5wcw5   1/1     Running   0          36s\niperf3-deployment-5b7f6f5c8-6q4q4   1/1     Running   0          36s\niperf3-deployment-5b7f6f5c8-8wnw7   1/1     Running   0          36s\n</code></pre>"},{"location":"kbs/000020954/#step-3-test-connectivity-between-pods","title":"Step 3: Test Connectivity Between Pods","text":"<p>To test the connectivity between pods, you will need to use the pod's IP addresses. You can use the following command to get the IP addresses of the pods:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>You should see output similar to the following:</p> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE    IP            NODE\niperf3-deployment-5b7f6f5c8-5wcw5   1/1     Running   0          36s   10.1.0.1      node1\niperf3-deployment-5b7f6f5c8-6q4q4   1/1     Running   0          36s   10.1.0.2      node2\niperf3-deployment-5b7f6f5c8-8wnw7   1/1     Running   0          36s   10.1.0.3      node3\n</code></pre> <p>Once you have the IP addresses of the pods you want to test, you can use the iperf3 command to test the connectivity. (Make sure to choose pods running on different nodes)</p> <p>First, choose a pod to run in server mode:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- iperf3 -s -p 12345\n</code></pre> <p>Second, choose a pod to run in client mode:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; --\u00a0iperf3 -c &lt;server pod IP address&gt; -p 12345\n</code></pre> <p>The iperf3 command will output the network performance statistics, including the bandwidth, packet loss, and jitter.</p> <p>This is an example output from a test cluster; please note that due to the various layers of networking involved, there is a performance impact that is expected:</p> <pre><code>Connecting to host 10.42.0.36, port 12345\n[  4] local 10.42.0.37 port 45568 connected to 10.42.0.36 port 12345\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  2.23 GBytes  19.1 Gbits/sec  300   2.88 MBytes\n[  4]   1.00-2.00   sec  2.49 GBytes  21.4 Gbits/sec  737   1.58 MBytes\n[  4]   2.00-3.00   sec  2.42 GBytes  20.8 Gbits/sec  524   2.33 MBytes\n[  4]   3.00-4.00   sec  2.17 GBytes  18.6 Gbits/sec  248   2.41 MBytes\n[  4]   4.00-5.00   sec  2.25 GBytes  19.3 Gbits/sec  151   2.45 MBytes\n[  4]   5.00-6.00   sec  2.36 GBytes  20.2 Gbits/sec   73   3.00 MBytes\n[  4]   6.00-7.00   sec  2.40 GBytes  20.6 Gbits/sec  181   2.84 MBytes\n[  4]   7.00-8.00   sec  2.29 GBytes  19.7 Gbits/sec   73   2.64 MBytes\n[  4]   8.00-9.00   sec  2.35 GBytes  20.2 Gbits/sec  110   2.44 MBytes\n[  4]   9.00-10.00  sec  2.27 GBytes  19.5 Gbits/sec  167   2.43 MBytes\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-10.00  sec  23.2 GBytes  19.9 Gbits/sec  2564             sender\n[  4]   0.00-10.00  sec  23.2 GBytes  19.9 Gbits/sec                  receiver\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020954/#conclusion","title":"Conclusion","text":"<p>In this article, we have shown you how to use iperf3 to test connectivity between Kubernetes pods in your cluster. We have used a Deployment to ensure that iperf3 pods are running on each node in the cluster. We hope that this article has helped demonstrate how to use iperf3 for testing connectivity between pods in your Kubernetes cluster.</p>"},{"location":"kbs/000020954/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020959/","title":"How to collect Control Plane component logs for an RKE cluster when logging based on determined or selected namespaces","text":"<p>This document (000020959) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020959/#situation","title":"Situation","text":"<p>By default, rancher-logging allows you to collect logs from all the control plane and node components for any clusters. However, you may want to select which namespaces to collect logs from, which for RKE clusters introduces an issue as the control plane components are deployed as containers and are not in any namespaces.</p>"},{"location":"kbs/000020959/#resolution","title":"Resolution","text":"<p>The simplest way to resolve this is by using the container_names directive with the logging app when creating the flow. This will allow you to list all the control plane or node components deployed as containers with the RKE clusters.</p> <p>Below is an example of a cluster flow collecting logs based on a list of selected namespaces, which will not, without the container_names directive, collect the control plane components.</p> <p>For this example, we have a cluster output\u00a0cattle-logging-cluster-test-output forwarding logs to an endpoint of our choice, and you will have to do the same to see the logs in your logging infrastructure.</p> <p>Note: Below is not an exhaustive list of all the RKE control plane containers and system namespaces, but the most common ones.</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterFlow\nmetadata:\n  name: cattle-logging-cluster-test-flow\n  namespace: cattle-logging\nspec:\n  filters:\n  match:\n    - select:\n         namespaces:\n           - kube-janitor\n           - kube-node-lease\n           - kube-public\n           - kube-system\n           - calico-system\n           - cattle-fleet-system\n           - cattle-impersonation-system\n           - cattle-system\n           - cattle-logging\n           - default\n           - opa\n         container_names:\n           - kube-apiserver\n           - etcd\n           - kubelet\n  globalOutputRefs:\n    - cattle-logging-cluster-test-output\n</code></pre>"},{"location":"kbs/000020959/#cause","title":"Cause","text":"<p>The control plane components are deployed as containers for an RKE cluster which makes the logging based on selected namespaces not pick their logs up.</p>"},{"location":"kbs/000020959/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020971/","title":"Enable CSR signing on an RKE cluster so certificates are issued","text":"<p>This document (000020971) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020971/#situation","title":"Situation","text":"<p>When creating a private key, a CertificateSigningRequest, and approving the CSR. You may notice in the output that the CSR is Approved but not Issued. For example, you may see the following:</p> <pre><code>kubectl get csr\nNAME                  AGE   REQUESTOR   CONDITION\nmy-csr                18m   admin       Approved\n</code></pre> <p>But you actually expect to see the following:</p> <pre><code>kubectl get csr\nNAME                  AGE   REQUESTOR   CONDITION\nmy-csr                18m   admin       Approved,Issued\n</code></pre>"},{"location":"kbs/000020971/#resolution","title":"Resolution","text":"<p>In an RKE cluster, you will need to provide the following flags for the\u00a0<code>kube-controller-manager:</code> <code>--cluster-signing-cert-file</code> and <code>--cluster-signing-key-file</code></p> <p>In order to do this from the Rancher UI:</p> <ol> <li>Go to Cluster Management</li> <li>Select the 3-dot menu next to the desired cluster and click Edit Config</li> <li>Click the Edit as YAML button</li> <li>Under the rancher_kubernetes_engine_config.services section, replace</li> </ol> <pre><code>kube-controller: {}\n</code></pre> <pre><code>with\n</code></pre> <pre><code>kube-controller:\n     extra_args:\n       cluster-signing-cert-file: /etc/kubernetes/ssl/kube-ca.pem\n       cluster-signing-key-file: /etc/kubernetes/ssl/kube-ca-key.pem\n</code></pre> <ol> <li>Click the Save button at the bottom of the screen</li> <li>Once the cluster finishes reconciling, you should be able to go through the steps again and have the certificate issued</li> </ol> <p>If this is on a cluster managed using rke up, you will have to put these values in the cluster.yml file and run rke up</p>"},{"location":"kbs/000020971/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020976/","title":"How to configure the logging app to use Custom PVC volume for Fluentd buffers","text":"<p>This document (000020976) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020976/#environment","title":"Environment","text":"<p>Rancher with Rancher-Logging chart v2</p>"},{"location":"kbs/000020976/#resolution","title":"Resolution","text":"<ol> <li>Go to Apps &gt; Charts, then Select Logging and click Install/Update</li> <li>Select the project to install the logging app and select \"Customize Helm Options before install\", and click Next</li> <li> <p>Select Edit YAML tab and modify the following:</p> </li> <li> <p>Search for disablePvc and change that from true to false</p> </li> </ol> <pre><code>disablePvc: false\n</code></pre> <ul> <li>Search for the bufferStorageVolume section and change it to the following (Modify the STORAGE_CLASS_NAME with your environement value):</li> </ul> <pre><code>\u00a0 bufferStorageVolume:\n\u00a0 \u00a0 pvc:\n\u00a0 \u00a0 \u00a0 spec:\n\u00a0 \u00a0 \u00a0 \u00a0 accessModes:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - ReadWriteOnce\n\u00a0 \u00a0 \u00a0 \u00a0 resources:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storage: 40Gi\n\u00a0 \u00a0 \u00a0 \u00a0 storageClassName: \"&lt;STORAGE_CLASS_NAME&gt;\"\n\u00a0 \u00a0 \u00a0 \u00a0 volumeMode: Filesystem\n</code></pre> <ol> <li>Modify other fields if required, and finally, Click on Next, then Install/Update.</li> </ol> <p>If the logging app is already installed, then the PVC will need to be created manually and associated with the fluentd StatefulSet, as per the following steps:</p> <ol> <li>From UI go to Storage &gt; PersistentVolumeClaims, and Create PVC to be used for the fluentd pod (PVC need to be on the namespace cattle-logging-system):</li> </ol> <pre><code>&gt; kubectl get pvc -n cattle-logging-system\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0STATUS \u00a0 VOLUME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CAPACITY \u00a0 ACCESS MODES \u00a0 STORAGECLASS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0AGE\nrancher-logging-root-fluentd-buffer \u00a0 Bound \u00a0 \u00a0pvc-4b78c686-7b68-4f7d-a40f-32c0a5470831 \u00a0 40Gi \u00a0 \u00a0 \u00a0 RWO \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0rancher-storage-class \u00a0 10s\n</code></pre> <ol> <li> <p>From UI, go to Workload &gt; StatefulSets, then\u00a0Edit the fluentd StatefulSet YAML ( rancher-logging-root-fluentd) and update the claimName for the rancher-logging-root-fluentd-buffer volume with the PVC name.</p> </li> <li> <p>Change the following two lines:</p> </li> </ol> <pre><code>\u00a0 \u00a0 \u00a0 - emptyDir: {}\n\u00a0 \u00a0 \u00a0 \u00a0 name: rancher-logging-root-fluentd-buffer\n</code></pre> <ul> <li>To the following:</li> </ul> <pre><code>\u00a0 \u00a0 \u00a0 - name: rancher-logging-root-fluentd-buffer\n\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: \"rancher-logging-root-fluentd-buffer\"\n</code></pre> <ol> <li>Finally, click on save, and a new fluentd pod will be scheduled with to use PV for buffer volume.</li> </ol> <p>To confirm if fluentd is using PV for buffer volume, you can run the following command and review the mount and volumes output sections:</p> <pre><code>kubectl describe pod rancher-logging-root-fluentd-0 -n cattle-logging-system\n</code></pre> <ul> <li>If fluentd is using PV for buffer volume, the output will be:</li> </ul> <pre><code>\u00a0 rancher-logging-root-fluentd-buffer:\n\u00a0 \u00a0 Type: \u00a0 \u00a0 \u00a0 PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n\u00a0 \u00a0 ClaimName: \u00a0rancher-logging-root-fluentd-buffer\n\u00a0 \u00a0 ReadOnly: \u00a0 false\n</code></pre> <ul> <li>If fluentd is using EmptyDir for buffer volume, the output will be:</li> </ul> <pre><code>\u00a0 rancher-logging-root-fluentd-buffer:\n\u00a0 \u00a0 Type: \u00a0 \u00a0 \u00a0 EmptyDir (a temporary directory that shares a pod's lifetime)\n\u00a0 \u00a0 Medium:\n\u00a0 \u00a0 SizeLimit: \u00a0&lt;unset&gt;\n</code></pre> <p>Reference:</p> <ul> <li>https://banzaicloud.com/docs/one-eye/logging-operator/logging-infrastructure/fluentd/</li> </ul>"},{"location":"kbs/000020976/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020985/","title":"LDAP error when logging into the Rancher UI after a Rancher upgrade","text":"<p>This document (000020985) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020985/#situation","title":"Situation","text":"<p>After upgrading Rancher to version 2.6.9 or higher, the following error is seen after logging into the Rancher UI:</p> <pre><code>ldap error Error creating ssl connection: LDAP Result Code 200 \"Network Error\": tls: server selected unsupported protocol version 301\n</code></pre>"},{"location":"kbs/000020985/#resolution","title":"Resolution","text":"<p>Upgrade the version of TLS that the LDAP server uses to at least 1.2.</p>"},{"location":"kbs/000020985/#cause","title":"Cause","text":"<p>This problem happens when the LDAP server used for authentication with Rancher uses a version of TLS lower than 1.2. Rancher 2.6.9 uses Go 1.19 where the crypto/tls library that is used requires a minimum version of TLS 1.2 for LDAP connections.</p>"},{"location":"kbs/000020985/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020988/","title":"How to troubleshoot rancher-logging","text":"<p>This document (000020988) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020988/#situation","title":"Situation","text":"<p>After installing the rancher-logging app and creating your resources, flows(clusterflows)/outputs(clusteroutputs), how do you troubleshoot if you are not seeing your logs at the final destination?</p>"},{"location":"kbs/000020988/#resolution","title":"Resolution","text":"<p>The rancher-logging app troubleshooting can be divided into three phases as below:</p> <ol> <li>User resources review: The first part is to ensure there is no error for the resources you created and that they are active.</li> </ol> <pre><code>1. ClusterOutputs and Outputs review. They should be active with no errors. Please correct the errors if any\n\nkubectl get clusteroutput -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0 \u00a0      ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-c-output \u00a0 true\nkubectl get output -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0 \u00a0    ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-output \u00a0 true\n\n2. Clusterflows and flows review. They should be active with no errors. Please correct the errors if any\n\nkubectl get clusterflow -o wide -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0 \u00a0    ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-c-flow \u00a0 true\nkubectl get flow -o wide -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0   \u00a0ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-flow \u00a0 true\n</code></pre> <ol> <li>FluentD and Fluentbit pods review: The fluentbit is a daemonset and should be running on each node, while you should have at least one FluentD pod running as the fluentbit pods will collect logs from each node and forward them\u00a0to the FluentD pod to be sent to their final destination</li> </ol> <pre><code>Your output could look different depending on what type Kubernetes cluster you have. However, you should have a fluentbit pod on each node and at least one FluentD pod\n\nkubectl get pods -n cattle-logging-system\nNAME                                                READY   STATUS      RESTARTS   AGE\nrancher-logging-655578478b-7k46r                    1/1     Running     0          89s\nrancher-logging-k3s-journald-aggregator-957gz       1/1     Running     0          89s\nrancher-logging-root-fluentbit-lczl5                1/1     Running     0          70s\nrancher-logging-root-fluentd-0                      2/2     Running     0          70s\nrancher-logging-root-fluentd-configcheck-ac2d4553   0/1     Completed   0          84s\n</code></pre> <ol> <li>Logs review: At this step, you review to ensure no errors in the logs for the FluentD or Fluentbit pods. For Fluentbit, you will probably need to review each if you suspect the logs are not being collected from fluentbit.</li> </ol> <p>``` 1. kubectl exec rancher-logging-root-fluentd-0 -n cattle-logging-system -- cat /fluentd/log/out</p> <p>This will dump the logs out of the fluentd container</p> <ol> <li>kubectl -n cattle-logging-system logs rancher-logging-root-bit-lczl5</li> </ol> <p>You should run it against each fluentbit pod if you suspect that the issue is on fluentbit ```</p>"},{"location":"kbs/000020988/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020994/","title":"How to Restore the fleet-local namespace if it gets deleted accidently","text":"<p>This document (000020994) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020994/#environment","title":"Environment","text":"<p>Rancher 2.6.x and 2.7.x</p>"},{"location":"kbs/000020994/#situation","title":"Situation","text":"<p>The fleet-local namespace in the local cluster is showing a Pending state in the Rancher dashboard, or a Terminating state with kubectl.</p>"},{"location":"kbs/000020994/#resolution","title":"Resolution","text":"<p>Prepare to clean up the related helm chart, to let Rancher reinstall it again.</p> <p>Requirements:</p> <ul> <li>A kubeconfig for the Rancher (local) management cluster</li> <li>The helm CLI</li> </ul> <p>If you don't have a kubeconfig for the local cluster, the steps here can be used to retrieve one from the control plane node in the cluster.</p> <p>1. Check that the kubeconfig is set for the correct cluster</p> <pre><code>export KUBECONFIG=/path/to/kubeconfig.yaml\nkubectl get nodes\n</code></pre> <p>2. Delete the fleet chart using helm</p> <pre><code>helm uninstall\u00a0fleet -n\u00a0cattle-fleet-system\n</code></pre> <p>3. Perform a rollout restart of the Rancher deployment, this will trigger the recreation of fleet-local and its associated contents</p> <pre><code>kubectl rollout restart deploy/rancher -n cattle-system\n</code></pre> <p>Note, this may cause a small interruption for users as the websocket tunnel to Rancher will be re-established.</p> <p>4. Check the fleet/fleet-crd apps are installed and active</p>"},{"location":"kbs/000020994/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020994/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020998/","title":"How to enable View in API","text":"<p>This document (000020998) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020998/#situation","title":"Situation","text":"<p>By default, the View in API is disabled in the rancher UI</p>"},{"location":"kbs/000020998/#resolution","title":"Resolution","text":"<p>Follow the steps below to enable it:</p> <ol> <li>Click on your user avatar in the upper right corner.</li> <li>Click on Preferences.</li> <li>Under the Advanced section, check the Enable Developer Tools &amp; Features. This will enable the View In API for the resources.</li> </ol>"},{"location":"kbs/000020998/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020999/","title":"Secrets in downstream clusters is repeatedly overwritten","text":"<p>This document (000020999) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020999/#environment","title":"Environment","text":"<p>Long standing setups, upgraded from Rancher versions &lt;2.6</p>"},{"location":"kbs/000020999/#situation","title":"Situation","text":"<p>A secret is replaced with\u00a0 a new one with the same name. When \"kubectl rollout restart deployment -n cattle-system rancher\" is run in the management cluster the secret is replaced with the old one. Rancher also overwrites the secret every few hours, on its own.</p>"},{"location":"kbs/000020999/#resolution","title":"Resolution","text":"<p>Check for a project secret having been configured for the project in the past.</p> <p>Navigate to the old UI (put /g behind the URL) and check for project secrets in the project(s) of the affected cluster.</p> <p>See also the Rancher documentation on this topic:</p> <p>https://ranchermanager.docs.rancher.com/v2.6/how-to-guides/new-user-guides/kubernetes-resources-setup/secrets#creating-secrets-in-projects</p>"},{"location":"kbs/000020999/#cause","title":"Cause","text":"<p>\"Project Secrets\" was a legacy feature from older Rancher &lt;2.6 versions.</p>"},{"location":"kbs/000020999/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021006/","title":"Cattle Cluster Agent flapping with runtime error: slice bounds out of range [:3] with capacity 0","text":"<p>This document (000021006) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021006/#environment","title":"Environment","text":"<p>This bug seems to affect all current Rancher versions as of March 2023.</p>"},{"location":"kbs/000021006/#situation","title":"Situation","text":"<p>A downstream cluster flaps between Active and Unavailable. The cattle-cluster-agent logs show errors like the following:</p> <pre><code>0308 15:04:59.702627 55 runtime.go:78] Observed a panic: runtime.boundsError{x:3, y:0, signed:true, code:0x2} (runtime error: slice bounds out of range [:3] with capacity 0)\ngoroutine 3160 [running]:\nk8s.io/apimachinery/pkg/util/runtime.logPanic({0x3cee760, 0xc00bef0660})\n/go/pkg/mod/k8s.io/apimachinery@v0.23.3/pkg/util/runtime/runtime.go:74 +0x7d\nk8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xfffffffe})\n/go/pkg/mod/k8s.io/apimachinery@v0.23.3/pkg/util/runtime/runtime.go:48 +0x75\npanic({0x3cee760, 0xc00bef0660})\n/usr/lib64/go/1.17/src/runtime/panic.go:1038 +0x215\ngithub.com/rancher/rancher/pkg/catalogv2/helm.decodeHelm3({0x0, 0xc9d32d})\n/go/src/github.com/rancher/rancher/pkg/catalogv2/helm/helm3.go:124 +0x1b1\ngithub.com/rancher/rancher/pkg/catalogv2/helm.fromHelm3Data({0x0, 0xc004d60540}, 0x3fc4c34)\n/go/src/github.com/rancher/rancher/pkg/catalogv2/helm/helm3.go:23 +0x25\ngithub.com/rancher/rancher/pkg/catalogv2/helm.ToRelease({0x47b3a20, 0xc004d60540}, 0x6c696877206e6564)\n/go/src/github.com/rancher/rancher/pkg/catalogv2/helm/release.go:74 +0x3eb\ngithub.com/rancher/rancher/pkg/controllers/dashboard/helm.(*appHandler).OnSecretChange(0xc00baa1950, {0xc005b81080, 0x2d}, 0xc004d60540)\n/go/src/github.com/rancher/rancher/pkg/controllers/dashboard/helm/apps.go:170 +0xa5\n</code></pre>"},{"location":"kbs/000021006/#resolution","title":"Resolution","text":"<p>This error seems to be caused by bad Helm release data on the downstream cluster. The first check should be if any releases have no release data stored. The below command will list all Helm release secrets when run against the downstream cluster. If the data column shows 0, that release secret has no release data.</p> <pre><code>kubectl get secrets -A | grep helm.sh/release.v1\n</code></pre> <p>All secrets with no release data on the downstream cluster need to be deleted to allow cattle-cluster-agent to start properly.</p> <pre><code>kubectl delete secrets -n &lt;NAMESPACE&gt; &lt;SECRET_NAME&gt;\n</code></pre> <p>Once the bad Helm release secrets are removed, cattle-cluster-agent pods should successfully start. If desired, the current cattle-cluster-agent pods in a CrashLoopBackOff can be deleted to speed up this process.</p>"},{"location":"kbs/000021006/#cause","title":"Cause","text":"<p>Occasionally, Helm release secrets are improperly stored, causing the release data to not be present. cattle-cluster-agent checks the Helm data everytime it starts, but will fail if there is no release data to check</p>"},{"location":"kbs/000021006/#additional-information","title":"Additional Information","text":"<p>Reported in GitHub issue 35971:\u00a0https://github.com/rancher/rancher/issues/35971</p> <p>Per the GitHub issue, this is scheduled for the 2023-Q2 releases</p>"},{"location":"kbs/000021006/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021012/","title":"rancher-monitoring installation on rke2 cluster stuck","text":"<p>This document (000021012) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021012/#situation","title":"Situation","text":"<p>The installation of rancher-monitoring is stuck as the pushprox-ingress-nginx-client pod is in a pending state</p>"},{"location":"kbs/000021012/#resolution","title":"Resolution","text":"<p>This happened when the rke2-ingress-nginx was not installed, and another ingress controller was selected instead.</p>"},{"location":"kbs/000021012/#cause","title":"Cause","text":"<p>The workaround is to remove the affinity below by editing the rancher-monitoring install YAML</p> <pre><code>clients:\n    affinity:\n      podAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                    - controller\n            namespaces:\n              - kube-system\n            topologyKey: kubernetes.io/hostname\n</code></pre>"},{"location":"kbs/000021012/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021013/","title":"Unable to edit Endpoints","text":"<p>This document (000021013) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021013/#environment","title":"Environment","text":"<p>Kubernetes Clusters v1.22+</p>"},{"location":"kbs/000021013/#situation","title":"Situation","text":"<p>As mitigation for CVE-2021-25740, Kubernetes removed a default rolebinding which allowed anyone with the built-in admin and edit roles to edit endpoints. This prevents users with the Project Owner or Project Member roles from being able to change Endpoint objects manually.</p>"},{"location":"kbs/000021013/#resolution","title":"Resolution","text":"<p>The Kubernetes team provided the following yaml to restore the permissions to the all users with the built-in edit or admin roles:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    kubernetes.io/description: |-\n      Add endpoints write permissions to the edit and admin roles. This was\n      removed by default in 1.22 because of CVE-2021-25740. See\n      https://issue.k8s.io/103675. This can allow writers to direct LoadBalancer\n      or Ingress implementations to expose backend IPs that would not otherwise\n      be accessible, and can circumvent network policies or security controls\n      intended to prevent/isolate access to those backends.\n      EndpointSlices were never included in the edit or admin roles, so there\n      is nothing to restore for the EndpointSlice API.\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n  name: custom:aggregate-to-edit:endpoints # you can change this if you wish\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"endpoints\"]\n    verbs: [\"create\", \"delete\", \"deletecollection\", \"patch\", \"update\"]\n</code></pre> <p>If you only want those with built-in admin role to have the permission restored, the yaml can be edited as follows:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    kubernetes.io/description: |-\n      Add endpoints write permissions to the edit and admin roles. This was\n      removed by default in 1.22 because of CVE-2021-25740. See\n      https://issue.k8s.io/103675. This can allow writers to direct LoadBalancer\n      or Ingress implementations to expose backend IPs that would not otherwise\n      be accessible, and can circumvent network policies or security controls\n      intended to prevent/isolate access to those backends.\n      EndpointSlices were never included in the edit or admin roles, so there\n      is nothing to restore for the EndpointSlice API.\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n  name: custom:aggregate-to-admin:endpoints # you can change this if you wish\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"endpoints\"]\n    verbs: [\"create\", \"delete\", \"deletecollection\", \"patch\", \"update\"]\n</code></pre> <p>The chosen yaml must be applied to the cluster where you want to restore these permissions.</p>"},{"location":"kbs/000021013/#cause","title":"Cause","text":"<p>Project Owners inherit the built-in Kubernetes admin role. Project members inherit the built-in Kubernetes edit role. These roles allow Project Owners and Project Members automatically have the majority of permissions needed on the appropriate namespaces without defining specific roles for each. The Kubernetes built-in roles are defined here:\u00a0https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings</p>"},{"location":"kbs/000021013/#additional-information","title":"Additional Information","text":"<p>For more information, see the Kubernetes documentation about this issue: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#write-access-for-endpoints</p>"},{"location":"kbs/000021013/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021018/","title":"How to set priorityClassName for fluentd and fluentbit pods during install for Logging app","text":"<p>This document (000021018) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021018/#situation","title":"Situation","text":"<p>When you install the Logging app through the Apps &amp; Marketplace in Rancher, is it possible to define priorityClassName values for the fluentd and fluentbit pods?</p>"},{"location":"kbs/000021018/#resolution","title":"Resolution","text":"<p>Yes, this is possible. During step 2 of the install, click Edit YAML and replace</p> <pre><code>loggingOverlay: {}\n</code></pre> <p>with:</p> <pre><code>loggingOverlay:\n  spec:\n    fluentbit:\n      podPriorityClassName: \"system-node-critical\"\n    fluentd:\n      podPriorityClassName: \"system-cluster-critical\"\n</code></pre>"},{"location":"kbs/000021018/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021022/","title":"How to collect kube-api audit logs with rancher-logging for an RKE/RKE2/K3S cluster","text":"<p>This document (000021022) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021022/#situation","title":"Situation","text":"<p>kube-api server audit logs are usually placed in a different directory than the one configured for rancher-logging when collecting</p>"},{"location":"kbs/000021022/#resolution","title":"Resolution","text":"<p>By configuring the following, you can enable the kube-api server audit logs collection from rancher-logging helm charts. The rancher-logging helm chart has it disabled by default:</p> <pre><code>kubeAudit:\n    auditFilename: 'audit-log.json'\n    enabled: enabled\n    fluentbit:\n      logTag: kube-audit\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          value: 'true'\n        - effect: NoExecute\n          key: node-role.kubernetes.io/etcd\n          value: 'true'\n    pathPrefix: '/var/log/kube-audit'\n</code></pre>"},{"location":"kbs/000021022/#cause","title":"Cause","text":"<p>The kube-api server audit logs aren't collected by rancher-logging as they are placed outside of the directory parsed by the logging operator by default</p>"},{"location":"kbs/000021022/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021029/","title":"How to stream k3s journal logs to Cloudwatch on Rancher","text":"<p>This document (000021029) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021029/#environment","title":"Environment","text":"<p>Rancher 2.6.x and 2.7.x</p>"},{"location":"kbs/000021029/#situation","title":"Situation","text":"<p>Send the k3s journal logs to AWS CloudWatch using Rancher's v2 logging integration</p>"},{"location":"kbs/000021029/#resolution","title":"Resolution","text":""},{"location":"kbs/000021029/#requirements","title":"Requirements:","text":"<ul> <li>Gathering k3s journal logs from each node in the cluster.</li> <li>Parsing\u00a0the logs to forward only the required fields.</li> <li>Forwarding the parsed data to cloudwatch.</li> </ul> <p>Rancher uses this\u00a0logging operator\u00a0that comes with the below CRDS:</p> <ul> <li>flow</li> <li>clusterFlow</li> <li>output</li> <li>clusterOutput</li> </ul> <p>You can read more about them\u00a0here.</p> <p>We will be using clusterFlow and clusterOutput as they are not namespaced. The clusterFlow CRD defines a logging flow for Fluentd with filters and outputs. Using this, we can define and apply filters to select only the desired data. Once parsed, data will be forwarded to the clusterOutput object. The clusterOutput CRD defines where to send the data. It supports several\u00a0plugins, but we will use Cloudwatch. You can read the spec\u00a0here.</p> <p>Now we have clusterFlow to parse the data and clusterOutput to define the destination of data. We need a way to get the journal logs from the nodes.</p> <p>HostTailer\u00a0CRD is provided by\u00a0https://banzaicloud.com/\u00a0and is supported on the Rancher. From the doc,\u00a0<code>HostTailer\u2019s main goal is to tail custom files and transmit their changes to stdout.</code>\u00a0This way, the logging-operator can process them. Example usage is\u00a0here. Similarly, you can use the\u00a0file-tailer\u00a0if you know the log file name.</p> <p>The difference between the two is host-tailer looks at specific systemd service logs like k3s.service logs, while for file-tailer, you need to specify the exact location of the log file like /var/log/nginx/access.log.</p> <p>Here is the YAML to get the systemd journal logs from each host. This will create a daemonset. Pods will fetch the logs from the journal log files of the specified service name and output them to stdout.</p> <pre><code>apiVersion: logging-extensions.banzaicloud.io/v1alpha1\nkind: HostTailer\nmetadata:\n  name: k3s-systemd-tailer\n  namespace: cattle-logging-system\nspec:\n  systemdTailers:\n    - name: k3s-systemd-tailer\n      maxEntries: 100\n      path: /run/log/journal/\n      systemdFilter: k3s.service\n</code></pre> <pre><code>\n</code></pre> <p>The log output will then be fed to clusterFlow, which parses the logs.</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterFlow\nmetadata:\n  name: host-tailer-flow\n  namespace: cattle-logging-system\nspec:\n  filters:\n    - parser:\n        key_name: message\n        reserve_time: true\n        parse:\n          type: json\n    - record_transformer:\n        remove_keys: _CMDLINE,_BOOT_ID,_MACHINE_ID,PRIORITY,SYSLOG_FACILITY,_UID,_GID,_SELINUX_CONTEXT,_SYSTEMD_SLICE,_CAP_EFFECTIVE,_TRANSPORT,_SYSTEMD_CGROUP,_SYSTEMD_INVOCATION_ID,_STREAM_ID,SYSLOG_IDENTIFIER,_COMM,_EXE\n  match:\n    - select:\n        labels:\n          app.kubernetes.io/name: host-tailer\n  globalOutputRefs:\n    - host-logging-cloudwatch\n</code></pre> <pre><code>\n</code></pre> <p>Here we are matching the app name to the name of the host-tailer daemonset, which is host-tailer. Once matched, we parse them using the\u00a0parser\u00a0plugin. We only need the\u00a0message\u00a0field from the logs, so\u00a0key_name\u00a0is specified as the\u00a0message, and the\u00a0parse\u00a0type is set to\u00a0json. After this, we remove unwanted fields from the message field using the remove_keys spec from the\u00a0record_transformer\u00a0plugin.</p> <p>The globalOutputRefs is set to the name of the clusterOutput.</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterOutput\nmetadata:\n  name: host-logging-cloudwatch\n  namespace: cattle-logging-system\nspec:\n  cloudwatch:\n    auto_create_stream: true\n    format:\n      type: json\n    buffer:\n      timekey: 30s\n      timekey_use_utc: true\n      timekey_wait: 30s\n    log_group_name: hosted-group\n    log_stream_name: host-logs\n    region: us-west-2\n</code></pre> <pre><code>\n</code></pre> <p>In the clusterOutput spec, we use cloudwatch and define log_group_name, log_stream_name, and region.</p>"},{"location":"kbs/000021029/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021036/","title":"rancher-logging: how to control fluentd logs flushing frequency","text":"<p>This document (000021036) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021036/#situation","title":"Situation","text":"<p>By default fluentd in rancher-logging flushes logs every 11 minutes as it uses timekey for the range of logs which is set to 10 minutes and timekey_wait set to 1 minute.</p> <pre><code> &lt;buffer tag,time&gt;\n      @type file\n      chunk_limit_size 8MB\n      path /buffers/clusterflow:cattle-logging-system:test-es-flow-1:clusteroutput:cattle-logging-system:test-es-1.*.buffer\n      retry_forever true\n      timekey 10m\n      timekey_wait 1m\n    &lt;/buffer&gt;\n</code></pre>"},{"location":"kbs/000021036/#resolution","title":"Resolution","text":"<p>You have two parameters to manage how often fluentd should flush logs. The timekey specifies the time range of logs that should be grouped in chunks. With timekey of 10m, the chunk will contain logs within a 10 minutes time range.</p> <p>Example:\u00a0timekey 10m: <code>[\"12:00:00\", ..., \"12:09:59\"]</code>, <code>[\"12:10:00\", ..., \"12:19:59\"]</code></p> <p>The <code>timekey_wait</code>\u00a0 configures the flush delay for events. Below is an example to illustrate this</p> <p>timekey 10m</p> <p>-------------------------------------------------------</p> <p>time range for chunk | timekey_wait | actual flush time</p> <p>12:00:00 - 12:09:59\u00a0 | 60s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 12:11:00</p> <p>The two parameters are to be specified at the output/clusteroutput level to manage the logs flushing frequency from fluentd. At the creation, you have a section called Output Buffer\u00a0( <code>Logging &gt; Output &gt; Create &gt; Output Buffer</code>\u00a0or\u00a0<code>Logging &gt; ClusterOutput &gt; Create &gt; Output Buffer</code>) where this can be changed, or if you are using kubectl command line, then you can update the buffer as shown in the example below.</p> <pre><code>&lt;buffer tag,time&gt;\n      @type file\n      chunk_limit_size 8MB\n      path /buffers/clusterflow:cattle-logging-system:test-es-flow-1:clusteroutput:cattle-logging-system:test-es-1.*.buffer\n      retry_forever true\n      timekey 3m\n      timekey_wait 1m\n    &lt;/buffer&gt;\n</code></pre>"},{"location":"kbs/000021036/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"}]}