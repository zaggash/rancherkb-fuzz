{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rancher KB Alt website","text":"<p>This website provide another look and feel and a better search to the SUSE Rancher KB articles.</p>"},{"location":"kbs/000020002/","title":"Pods return to the Initializing status even though containers in the pod are running","text":"<p>This document (000020002) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020002/#environment","title":"Environment","text":"<p>RKE1 clusters</p>"},{"location":"kbs/000020002/#situation","title":"Situation","text":""},{"location":"kbs/000020002/#after-successfully-starting-pods-return-to-the-initializing-status","title":"After successfully starting pods return to the 'Initializing status'.","text":""},{"location":"kbs/000020002/#when-reviewed-an-init-container-is-failing-to-start-even-though-other-containers-are-running","title":"When reviewed an init container is failing to start, even though other containers are running.","text":""},{"location":"kbs/000020002/#if-one-of-the-running-containers-fails-it-will-not-be-restarted-due-to-the-failed-init-container","title":"If one of the running containers fails it will not be restarted due to the failed init container.","text":""},{"location":"kbs/000020002/#resolution","title":"Resolution","text":"<p>This often happens after images has beeen pruned on the node.</p> <p>Manually pruning images on Kubernetes nodes should be avoided.</p> <p>The kubelet has a built in\u00a0image cleanup mechanism to remove unused containers and images.</p> <p>Where it's not possible to avoid manual clean up, init containers that are stopped should not be removed.</p> <p>A list of init container IDs can be generated with the following command:</p> <pre><code>kubectl get pods --all-namespaces -o jsonpath='{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' | cut -d/ -f3\n</code></pre> <p>The below script can be used to generate a list of containers to clean on a remote node, e.g.</p> <pre><code>NODE_TO_CLEAN=&lt;node_ip&gt;\nUSER=&lt;user&gt;\n\nINIT_CONTAINERS=$(kubectl get pods --all-namespaces -o jsonpath='{range .items[*].status.initContainerStatuses[*]}{.containerID}{\"\\n\"}{end}' | cut -d/ -f3)\nTERMED_PODS=$(ssh -o LogLevel=QUIET -t ${USER}@${NODE_TO_CLEAN} sudo docker ps -qa --filter status=exited --no-trunc | sed -e 's/\\r//g')\n\nCONTAINERS_TO_REMOVE=$(comm -23 &lt;(echo $TERMED_PODS | sort) &lt;(echo $INIT_CONTAINERS | sort) )\nPASS_CONTAINERS=$(typeset -p CONTAINERS_TO_REMOVE)\n\nssh -o LogLevel=QUIET -t ${USER}@${NODE_TO_CLEAN} bash &lt;&lt;EOF\n    $PASS_CONTAINERS\n    sudo docker rm $(echo \"\\${CONTAINERS_TO_REMOVE}\") &amp;&amp; sudo docker image prune -af\nEOF\n</code></pre>"},{"location":"kbs/000020002/#cause","title":"Cause","text":"<p>This happens when the init container is removed from the host machine.</p> <p>Because init containers have run to completion, and are terminated, this can happen when a\u00a0<code>docker system prune</code> or <code>docker container prune</code> is run on the node.</p> <p>When the kubelet sees that the init container in no longer exists it will try and rerun it. Depending on the init container operation this may fail on a pod that is already running (e.g linkerd).</p>"},{"location":"kbs/000020002/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020004/","title":"I have the alert 'NodeFilesystemSpaceFillingUp ', what does this mean?","text":"<p>This document (000020004) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020004/#situation","title":"Situation","text":""},{"location":"kbs/000020004/#issue","title":"Issue","text":"<p>When using Rancher Monitoring, you may see the Alert ' NodeFilesystemSpaceFillingUp\u00a0', however, you may not understand what this means and what the impact of this is.</p>"},{"location":"kbs/000020004/#background","title":"Background","text":"<p>The alert is an early warning for potentially more serious issues with the disk space on a node becoming full.</p> <p>It uses the following equation:</p> <p>(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 &lt; 15 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) &lt; 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)</p> <p>It can be divided into three parts that need to be true so that the alert triggers :</p> <ul> <li>node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 &lt; 15</li> </ul> <p>This condition evaluates to true if the node\u00b4s filesystem is below 15% of space available.</p> <ul> <li>predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) &lt; 0</li> </ul> <p>This condition measures the rate at which the disk is losing available space and, based on the trend from the last 6 hours, will warn you that, at that rate, in 4 hours (4*60*60) the disk will have no available space at all.</p> <ul> <li>node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0</li> </ul> <p>This is just a check that the node\u00b4s filesystem is currently not read-only. The metric\u00a0node_filesystem_readonly evaluates to 0 if it is not read-only, and 1 if it is.</p> <p>It is important to make sure that you investigate the affected node for any potential issues with excessive logging, under-provisioned disk space, or perhaps something else that could cause such a situation.</p>"},{"location":"kbs/000020004/#impact","title":"Impact","text":"<p>If the Node does run out of disk space, this will cause a 'Disk Pressure' event on the node.</p> <p>When a Node experiences Disk Pressure, it will evict all running containers and become unschedulable until the requisite disk space is made.</p> <p>This is a serious situation, especially if the cause of the failure is a rogue container over-logging.</p> <p>If an over-logging or failing workload is forced to reschedule on another node, they may all become unschedulable as the issue will follow the workload when it is rescheduled.</p> <p>In a typical Kubernetes installation, Disk Pressure is caused by available space being less than 10%.</p>"},{"location":"kbs/000020004/#investigative-steps","title":"Investigative Steps","text":"<p>The cause of this issue can vary heavily across different environments, but as the alert is node-specific, you should start there.</p> <p>The first place to investigate is a <code>df -h</code>, this will show you the percentage of disk space that is filled on your node, you may be able to identify immediately a place where disk space is no longer available.</p> <p>This is the fastest way to assess more urgent issues, and once you've identified the disk that may be reporting as nearly full, you can immediately take precautions, such as clearing out old log files or increasing the size of a disk.</p> <p>As you use Rancher Monitoring you can also look at Node-specific statistics graphed over time and make an assessment on when the issue began, compared to running workloads and other node logs.</p> <p>During operations seeing an increase in either logging or a gradual increase in storage used is often expected.</p> <p>For example, if the alert is sounding because a specific workload encountered issues, logged more, but then recovered and cleaned up the logs, you may no longer have an issue on your hands but could consider reducing the logging of the workload to prevent further alerts.</p> <p>The ' NodeFilesystemSpaceFillingUp' is a preemptive alert that will always require investigation and understanding to ensure the best operational health of your Nodes and Clusters.</p>"},{"location":"kbs/000020004/#resolution","title":"Resolution","text":""},{"location":"kbs/000020004/#short-term-solutions","title":"Short-Term Solutions","text":"<p>If you can identify the reason for this alert, the solution should hopefully be more straightforward. You may need to delete some old files on a Node, or you may need to reduce logging, for example, if debug logging is running.</p> <p>If you do hit a Disk Pressure event and you need to recover, you need to access the node directly and reduce the amount of space taken manually. When requisite space is made, you should either restart the Node or Docker on the Node to mark it as schedulable in Kubernetes again.</p>"},{"location":"kbs/000020004/#long-term-solutions","title":"Long-Term Solutions","text":"<p>Different solutions will mitigate this alert:</p> <ol> <li>Having larger disks</li> </ol> <p>While Rancher does not have specific requirements for disk space on a Node, it would be recommended to, at least, have 30GB or more to better mitigate this alert's occurrence.</p> <ol> <li>Exporting container logs</li> </ol> <p>Rancher provides a logging deployment you can configure to export your container logs, for example, to an in-house Elasticsearch Cluster.</p> <p>While these logs will be buffered locally, they will then be exported remotely, thereby reducing the amount of accumulated logs over time.</p> <ol> <li>Running regular cleanups on System Logs</li> </ol> <p>While out-of-scope of Rancher, a large amount of system logs can contribute to this alert sounding, it is encouraged to manage logging at an OS Level with either logrotate or by exporting logs.</p>"},{"location":"kbs/000020004/#further-reading","title":"Further reading","text":"<p>Kubernetes Out of Resource Handling Documentation: https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/</p>"},{"location":"kbs/000020004/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020005/","title":"Bind on Port 80 Fails Due to Permissions in NGINX Ingress","text":"<p>This document (000020005) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020005/#environment","title":"Environment","text":"<p>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</p>"},{"location":"kbs/000020005/#situation","title":"Situation","text":""},{"location":"kbs/000020005/#sometimes-an-admin-may-see-an-error-in-the-logs-like-the-one-below","title":"Sometimes an admin may see an error in the logs like the one below:","text":"<pre><code>nginx: [emerg] bind() to 0.0.0.0:80 failed (13: Permission denied)\n</code></pre>"},{"location":"kbs/000020005/#resolution","title":"Resolution","text":"<p>Remove the ingress container image from the node(s), using the following commands:</p> <pre><code># Find the name of the nginx-ingress-controller pod\nNGINX_INGRESS_CONTROLLER=\"$(kubectl get pods -n ingress-nginx | awk '/nginx-ingress-controller/ { print $1 }')\"\n\n# Remove nginx-ingress-controller pod and then clean up the container image\ndocker rm -f \"${NGINX_INGRESS_CONTROLLER}\" &amp;&amp; \\\ndocker system prune -af\n</code></pre>"},{"location":"kbs/000020005/#cause","title":"Cause","text":"<p>According to kubernetes/ingress-nginx GitHub Issue #3858, this is caused by a capabilities failure on one of the layers of the nginx-ingress-controller image, due to the xattrs not being copied correctly.</p>"},{"location":"kbs/000020005/#additional-information","title":"Additional Information","text":"<ul> <li>OCI Image Format Specification: Image Layer Filesystem Changeset</li> <li>xattr(7) - Linux manual page</li> </ul>"},{"location":"kbs/000020005/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020006/","title":"Controlplane/Etcd nodes are stuck at provisioning for a new cluster","text":"<p>This document (000020006) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020006/#environment","title":"Environment","text":"<ul> <li>Rancher version &gt;= v2.5.6</li> <li>An RKE, RKE2 or k3s downstream cluster created in Rancher using either existing nodes, or provisioned using an infrastructure provider</li> <li>No nodes have been added with the worker role</li> </ul>"},{"location":"kbs/000020006/#situation","title":"Situation","text":"<p>When creating a new cluster:</p> <ul> <li>RKE: It will get stuck in \"Provisioning\" status, while the control plane and etcd nodes will show a \"Registering\" status.</li> <li>RKE2/k3s: It will get stuck in \"Updating\" status, while the control plane and etcd nodes will show a \"Waiting for Node Ref\" status.</li> <li>Provisioning logs will show:</li> <li><code>[INFO ] waiting for at least one control plane, etcd, and worker node to be registered</code></li> </ul>"},{"location":"kbs/000020006/#resolution","title":"Resolution","text":"<p>As of Rancher v2.5.6, new RKE, RKE2 or k3s clusters require at least one node with the <code>worker</code> or <code>all</code> roles to begin provisioning. The reason for this is that the downstream cluster requires a worker node to host the Rancher cluster-agent, CoreDNS, metrics-server, and ingress-nginx. Without these workloads, a cluster will not provision correctly.</p>"},{"location":"kbs/000020006/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020009/","title":"How to enable HA for Alertmanager in Rancher lanched Monitoring","text":"<p>This document (000020009) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020009/#situation","title":"Situation","text":""},{"location":"kbs/000020009/#task","title":"Task","text":"<p>To increase the replica count of the alertmanager pod and enable HA so that if the node with the <code>Alertmanager</code> pod dies, alerts will still function.</p> <p>Increasing the replica count in the chart is the only change required as alerts are sent to ALL the alertmanager pods. They communicate to each other which alerts have already been dispatched, so they are not duplicated.</p>"},{"location":"kbs/000020009/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.7.x/v2.8.x</li> <li>Monitoring deployed via Rancher Apps &amp; Marketplace</li> </ul>"},{"location":"kbs/000020009/#resolution","title":"Resolution","text":"<p>You can enable HA alertmanager by increasing the replica count in the chart.</p> <ol> <li>Go to Monitoring Chart in the Apps Marketplace</li> </ol> <p> 2. Under Chart Options, click on the Edit as YAML button</p> <p> 3. In the alertmanager block, modify replicas to your preference. Here it is set to 2</p> <p> 4. Click on Upgrade button to apply changes</p> <p></p>"},{"location":"kbs/000020009/#further-reading","title":"Further reading","text":"<ul> <li>Alerting chart user-guide</li> </ul>"},{"location":"kbs/000020009/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020011/","title":"How to check connectivity from NGINX ingress controllers to all backend pods","text":"<p>This document (000020011) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020011/#situation","title":"Situation","text":""},{"location":"kbs/000020011/#need-to-check-if-nginx-controllers-actually-have-network-access-to-the-backend-pods-to-verify-the-overlay-network-is-working-and-to-check-the-overall-cluster-network-configuration","title":"Need to check if NGINX controllers actually have network access to the backend pods to verify the overlay network is working and to check the overall cluster network configuration.","text":""},{"location":"kbs/000020011/#resolution","title":"Resolution","text":"<p>This script is designed to walk through all the ingress controllers[1] in a cluster and test that it can curl the backend pods from the NGINX pods.</p> <p>Pre-requisites</p> <ul> <li>kubectl access to the cluster</li> </ul> <p>Run script</p> <pre><code>curl https://raw.githubusercontent.com/rancherlabs/support-tools/master/NGINX-to-pods-check/check.sh | bash\n</code></pre> <p>Example output</p> <p>Broken pod</p> <pre><code>bash ./check.sh -F Table\n####################################################\nPod: webserver-bad-85cf9ccdf8-8v4mh\nPodIP: 10.42.0.252\nPort: 80\nEndpoint: ingress-1d8af467b8b7c9682fda18c8d5053db7\nIngress: test-bad\nIngress Pod: nginx-ingress-controller-b2s2d\nNode: a1ubphylbp01\nStatus: Fail!\n####################################################\nbash ./check.sh -F Inline\nChecking Pod webserver-bad-8v4mh PodIP 10.42.0.252 on Port 80 in endpoint ingress-bad for ingress test-bad from nginx-ingress-controller-b2s2d on node a1ubphylbp01 NOK\n</code></pre> <p>Working pod</p> <pre><code>bash ./check.sh -F Table\n####################################################\nPod: webserver-bad-85cf9ccdf8-8v4mh\nPodIP: 10.42.0.252\nPort: 80\nEndpoint: ingress-1d8af467b8b7c9682fda18c8d5053db7\nIngress: test-bad\nIngress Pod: nginx-ingress-controller-b2s2d\nNode: a1ubphylbp01\nStatus: Pass!\n####################################################\nbash ./check.sh -F Inline\nChecking Pod webserver-good-65644cffd4-gbpkj PodIP 10.42.0.251 on Port 80 in endpoint ingress-good for ingress test-good from nginx-ingress-controller-b2s2d on node a1ubphylbp01 OK\n</code></pre> <p>Testing</p> <p>The following commands will deploy two workloads and ingresses. One that is working with a web server that is responding on port 80. And the other will have the webserver disabled, so it will fail to connect.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/rancherlabs/support-tools/master/NGINX-to-pods-check/example-deployment.yml\n</code></pre>"},{"location":"kbs/000020011/#additional-information","title":"Additional Information","text":"<p>[1] An Ingress Controller is the load balancing pod which enforces the routing rules outlined in an `Ingress` resource</p>"},{"location":"kbs/000020011/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020012/","title":"How to add a Grafana Dashboard for Logging v2","text":"<p>This document (000020012) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020012/#environment","title":"Environment","text":"<p>A cluster managed by Rancher v2.5.x+ with Monitoring and Logging v2 apps installed</p>"},{"location":"kbs/000020012/#situation","title":"Situation","text":""},{"location":"kbs/000020012/#vistualising-logging-statistics-can-be-useful-for-troubleshooting-and-capacity-planning-this-article-demonstrates-how-to-add-a-logging-dashboard-to-grafana-with-persistence-throughout-pod-restarts","title":"Vistualising Logging statistics can be useful for troubleshooting and capacity planning, this article demonstrates how to add a Logging Dashboard to Grafana with persistence throughout pod restarts.","text":"<p>The article focusses on a Logging Dashboard, however these same steps can be adapted to suit other Dashboards.</p>"},{"location":"kbs/000020012/#resolution","title":"Resolution","text":"<ul> <li>Upgrade the Logging v2 app to enable the service monitor by clicking 'Chart Options' and 'Edit as YAML', below is an example showing the <code>monitoring.serviceMonitorsection.enabled</code> field set to <code>true</code>:</li> </ul> <pre><code>monitoring:\n  serviceMonitor:\n    enabled: true\n</code></pre> <p>Once complete, the target ( <code>cattle-logging-system/rancher-logging</code>) should show up in the Prometheus Targets list with an <code>UP</code> state - this can take a few minutes</p> <p>Add a logging dashboard to Grafana by adding the JSON to a ConfigMap for persistence . In this example, the Logging Dashboard is used</p> <ul> <li> <p>Click the 'Download JSON' button for the Dashboard</p> </li> <li> <p>Replace the data source variable with <code>Prometheus</code> in the downloaded file</p> </li> </ul> <p>For example:</p> <pre><code>sed 's/${DS_PROMETHEUS}/Prometheus/' logging-dashboard_rev4.json &gt; logging.json\n</code></pre> <ul> <li>Create and label the ConfigMap using the updated <code>logging.json</code> file</li> </ul> <pre><code>kubectl create configmap --from-file logging.json logging -n cattle-dashboards\nkubectl label configmap  -n cattle-dashboards logging grafana_dashboard=1\n</code></pre> <ul> <li>Visit the Grafana UI, the Dashboard should now be available</li> </ul>"},{"location":"kbs/000020012/#further-reading","title":"Further reading","text":"<ul> <li>Monitoring in Rancher</li> <li>Logging in Rancher</li> </ul>"},{"location":"kbs/000020012/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020013/","title":"Configuring a buffer with total_limit_size in Logging v2","text":"<p>This document (000020013) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020013/#situation","title":"Situation","text":""},{"location":"kbs/000020013/#task","title":"Task","text":"<p>Some users run into an issue where fluentd buffers grow indefinetely and fill up the disk on nodes. This documentation explains how to limit this size in V2 Logging.</p>"},{"location":"kbs/000020013/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher 2.5.x and above</li> <li>Logging App installed with Cluster Explorer</li> </ul>"},{"location":"kbs/000020013/#resolution","title":"Resolution","text":"<p>When you're ready to configure your Output (the location that the logs are sent to) you can get there with the following steps after you have Logging installed:</p> <ol> <li>Cluster Explorer</li> <li>Dropdown in the top left</li> <li>Logging</li> <li>ClusterOutput/Output</li> <li>Create</li> </ol> <p>From here, you can edit the Output as YAML to access the buffer configurations. Using ElasticSearch as an example you can fill out the form easily, but then Edit as YAML:</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterOutput\nmetadata:\n    name: \"elasticsearch-output\"\n    namespace: \"cattle-logging-system\"\n  elasticsearch:\n    host: 1.2.3.4\n    index_name: some-index\n    port: 9200\n    scheme: http\n    buffer:\n      type: file\n      total_limit_size: 2GB\n</code></pre> <p>As far as the buffer size is concerned, the <code>total_limit_size</code> is the important parameter to change. 2GB is a good starting point and is unlikely to need to be changed, but you can adjust this based on your needs.</p>"},{"location":"kbs/000020013/#further-reading","title":"Further reading","text":"<p>As stated in the pre-reqs, this is only possible on the V2 Logging App which is available in Rancher 2.5. This uses the Banzai Cloud logging operator, which allows us to use CRDs to configure it.</p> <ul> <li>V2 Logging Documentation</li> <li>Banzai Cloud Operator Documentation</li> </ul>"},{"location":"kbs/000020013/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020014/","title":"Troubleshooting high ingress request times","text":"<p>This document (000020014) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020014/#environment","title":"Environment","text":"<ul> <li>An RKE or RKE2 cluster</li> <li>Use of the Nginx Ingress Controller</li> </ul>"},{"location":"kbs/000020014/#situation","title":"Situation","text":"<p>This article aims to provide steps to gather and analyse data to help troubleshoot ingress performance issues.</p>"},{"location":"kbs/000020014/#resolution","title":"Resolution","text":""},{"location":"kbs/000020014/#review-request-times","title":"Review request times","text":"<p>To narrow down on which requests are taking the longest, analyzing the ingress-nginx logs is very helpful.</p> <p>Retrieve requests with a high (&gt;2s) <code>upstream_response_time</code>. This log field represents the time taken for the response from the upstream target - the pod endpoint in the service.</p> <pre><code>kubectl logs -n ingress-nginx -l app=ingress-nginx -f --tail=2000 | awk '/- -/ &amp;&amp; $(NF-2)&gt;2.0'\n</code></pre> <p>The same can be done for <code>request_time</code>, this represents the time taken to complete the entire request, including the above <code>upstream_response_time</code>.</p> <pre><code>kubectl logs -n ingress-nginx -l app=ingress-nginx -f --tail=2000 | awk '/- -/ &amp;&amp; $(NF-7)&gt;2.0'\n</code></pre> <p>Please adjust the time to suit, where <code>&gt;2.0</code> will filter for any times greater than 2.0 seconds.</p> <p>Comparing the diference in timings between <code>request_time</code> and <code>upstream_response_time</code> can help to understand the issue further:</p> <ul> <li>Locate any potential upstream targets (pods), or nodes these may be running on, that are frequently associated with a higher <code>upstream_response_time</code></li> <li>If all upstream targets in a particular ingress/service are experiencing higher response times:</li> </ul> <p>- What dependencies does the application have? For example, external APIs, databases, other services, etc - Investigate the application logs - Simulate the same requests directly to pods to bypass ingress-nginx, are they also slow?</p> <ul> <li>If the <code>upstream_response_time</code> is much lower than <code>request_time</code>, the time is being spent elsewhere, check any tuning, performance or resource issues on the nodes</li> </ul> <p>Note: The <code>request_time</code> metric is also used to create the ingress controller graphs when Cluster Monitoring is enabled.</p>"},{"location":"kbs/000020014/#review-request-details","title":"Review request details","text":"<p>Along with the output in the previous step, it is also useful to analyse the request details, such as the request itself, source/destination IP address, response code, user agent, and the unique name for the ingress for common patterns.</p> <p>You may need to review these with the related application teams. For example, a request to retrieve a large amount of data, or perform a complex query may genuinely take a long time, these can potentially be ignored.</p> <p>Some requests may be opening a websocket, and in the scenario that the service scales up/down regularly, a small number of upstream targets could have a long-running connection creating an unfair distribution to occur on these targets.</p> <p>It's also worthwhile to consider the time when the issue occurs, the number of pods in the service, performance metrics, and requests/limits in place. For example, do the requests occur during a peak load time? Is HPA configured to scale the deployment? Is monitoring data available to identify trends and correlate with the logs?</p>"},{"location":"kbs/000020014/#check-ingress-nginx-logs","title":"Check ingress-nginx logs","text":"<p>With the focus previously on requests themselves, it is also useful to exclude the access logs and ensure there are no fundamental issues with ingress-nginx.</p> <p>The following command should exclude all access.log output, retrieving output from the ingress controller and the nginx error.log only.</p> <pre><code>kubectl logs -n ingress-nginx -l app=ingress-nginx -f --tail=100 | awk '!/- -/'\n</code></pre> <p>Please adjust the <code>--tail</code> flag as needed, this example retrieves the last 100 lines from each ingress-nginx pod.</p>"},{"location":"kbs/000020014/#real-time-view-of-all-requests","title":"Real-time view of all requests","text":"<p>Another option to get a broader overview is using a tool like goaccess. After installing the package, the below can be used to feed ingress-nginx logs to goaccess to get a real-time view of the logs.</p> <pre><code>kubectl logs -f -n ingress-nginx -l app=ingress-nginx --tail=2000 | goaccess --log-format=\"%h - - [%d:%t] \\\"%m %r %H\\\" %s %b \\\"%R\\\" \\\"%u\\\" %^ %T [%v]\" --time-format '%H:%M:%S %z' --date-format \"%d/%b/%Y\"\n</code></pre> <p>Please adjust the history of logs with the <code>--tail</code> flag.</p>"},{"location":"kbs/000020014/#measure-requests-to-ingress-nginx","title":"Measure requests to ingress-nginx","text":"<p>If you have isolated all areas so far, it might be worthwhile to focus on the Load Balancer or network devices that provide client access to ingress-nginx.</p> <p>The following articles contain curl commands to perform SNI-compliant requests and measure statistics, these requests could also be compared from the ingress-nignx logs (as above) to understand what portion of the time was spend with ingress-nginx handling the request.</p> <p>You may also be able to obtain metrics from your Load Balancer or infrastructure to troubleshoot this further.</p>"},{"location":"kbs/000020014/#additional-information","title":"Additional Information","text":"<ul> <li>How to troubleshoot HTTP request performance with curl statistics</li> <li>How to troubleshoot SNI enabled endpoints with curl and openssl</li> </ul>"},{"location":"kbs/000020014/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020015/","title":"What is the difference between Cluster registry and Global registry?","text":"<p>This document (000020015) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020015/#environment","title":"Environment","text":"<p>- Rancher 2.6.x,Rancher 2.7.x and Rancher 2.8.x</p>"},{"location":"kbs/000020015/#situation","title":"Situation","text":""},{"location":"kbs/000020015/#q-what-is-the-difference-between-a-global-registry-vs-a-cluster-wide-registry-where-would-i-use-one-over-the-other-global-registry","title":"Q) What is the difference between a global registry Vs a cluster-wide registry? Where would I use one over the other?    -\u00a0 Global Registry :","text":"<p>Global-level private registries allow administrators to store or proxy images through a centralized image repository. Global registries allow for air-gapped setups to pull images needed for cluster provisioning and end-user workloads without specifying the private registry server. This registry is used as the default pull location in place of DockerHub. The global private registry does not support image repositories requiring authentication. Use cluster-level registries if you need to authenticate against your image repository during cluster provisioning. Use registries within the cluster if you need to authenticate against your image repository for end-user workloads.</p> <p>Ref :\u00a0Docs to Global private registry configuration</p>"},{"location":"kbs/000020015/#-cluster-registry","title":"- Cluster Registry","text":"<p>Cluster-level private registries allow administrators to use a registry server with RKE-based clusters to provision system components required to run Kubernetes. One can find the\u00a0List of system images here.\u00a0\u00a0Administrators can pass in credentials if the registry server requires them. Outside of the RKE system images and RKE add-ons, the Rancher agent image used in cluster provisioning will use the cluster-level registry for custom clusters. Eventually, node drivers will also pull the Rancher agent image from the cluster-level registry.</p> <p>Note:</p> <p>There are specific images not covered by the cluster-level private registry that are part of the cluster provisioning process. Your cluster will need access to either DockerHub or have these images in your global registry (eg Busybox, shell, and pause)</p>"},{"location":"kbs/000020015/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020016/","title":"kube-apiserver \"socket: too many open files\" error messages","text":"<p>This document (000020016) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020016/#situation","title":"Situation","text":""},{"location":"kbs/000020016/#issue","title":"Issue","text":"<p>During normal operation of a Kubernetes cluster, you may experience intermittent stability issues and the kube-apiserver logs may contain messages of the following format:</p> <ul> <li><code>clientconn.go:1208] grpc: addrConn.createTransport failed to connect to {https://x.x.x.x:2379 &lt;nil&gt; 0 &lt;nil&gt;}. Err :connection error: desc = \"transport: Error while dialing dial tcp x.x.x.x:2379: socket: too many open files\". Reconnecting...</code></li> <li><code>clientconn.go:1208] grpc: addrConn.createTransport failed to connect to {https://x.x.x.x:2379 &lt;nil&gt; 0 &lt;nil&gt;}. Err :connection error: desc = \"transport: authentication handshake failed: context canceled\". Reconnecting...</code></li> <li><code>clientconn.go:1208] grpc: addrConn.createTransport failed to connect to {https://x.x.x.x:2379 &lt;nil&gt; 0 &lt;nil&gt;}. Err :connection error: desc = \"transport: authentication handshake failed: context deadline exceeded\". Reconnecting...</code></li> </ul>"},{"location":"kbs/000020016/#root-cause","title":"Root Cause","text":"<p>These symptoms can be caused by the kube-apiserver being blocked by configuration that limits the number of files a process can have open. This limit could also affect other components and OS services.</p> <p>This is typically a result of restrictive ulimits, or a high number of open connections.</p> <p>Below is a non-exhaustive list of places where the number of open files ulimit can be set for a Docker container.</p>"},{"location":"kbs/000020016/#system-ulimits-etcsecuritylimitsconf","title":"System ulimits (/etc/security/limits.conf):","text":"<p>This file defines the persisted configuration for the system-wide ulimits, such as file size limits, and how much memory can be used by the different components of the process, including the stack, data and text segments.</p> <p>The limit of interest is the <code>nofile</code> limit, which defines the number of files a process can have open at any given time. This can be set per user, or for all users( <code>*</code>) and there are two limits to define:</p> <ul> <li>Soft limit - These limits are ones that the user can move up or down within the range permitted by any pre-existing hard limits. A user can modify the soft limit by running the command <code>ulimit -n X</code> where X is the desired new value.</li> <li>Hard limit - These limits are set by the superuser and enforced by the Kernel. Users cannot exceed this.</li> </ul> <p>The <code>nofile</code> hard limit for the current user can be seen by running <code>ulimit -Hn</code> and the soft limit can be seen by running <code>ulimit -Sn</code>.</p> <p>More info on limits.conf can be found here.</p>"},{"location":"kbs/000020016/#k3s-and-rke2-configuration","title":"k3s and rke2 configuration","text":"<p>The k3s and rke2 install scripts both define LimitNOFILE=1048576 on the respective services. If you don't use the install scripts, you may need to configure ulimits as described below.</p>"},{"location":"kbs/000020016/#systemd-configuration","title":"Systemd configuration","text":"<p>By design, systemd will ignore ulimits set via <code>/etc/security/limits.conf</code>, and instead apply its own limits. These can be configured per-service or system-wide.</p> <p>The system-wide systemd nofile limit is defined in <code>/etc/systemd/system.conf</code> as <code>DefaultLimitNOFILE=X:Y</code>. Where X is the soft limit and Y is the hard limit.</p> <p>It is possible to set <code>nofile</code> for a specific service, either by defining <code>LimitNOFILE</code> within the service file itself or creating an override file. For example, defining it directly within the docker systemd service file (/lib/systemd/system/docker.service):</p> <pre><code>[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nBindsTo=containerd.service\nAfter=network-online.target firewalld.service containerd.service\nWants=network-online.target\nRequires=docker.socket\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutSec=0\nRestartSec=2\nRestart=always\n\nLimitNOFILE=infinity\n</code></pre> <p>Or creating a systemd override file (/etc/systemd/system/docker.d/override.conf):</p> <pre><code>[Service]\nLimitNOFILE=infinity\n</code></pre> <p>Note: The <code>docker.d</code> directory name may be slightly different between Linux distributions. It is usually recommended to create an override, as this will persist through system updates.</p> <p>Note: On older versions of systemd, <code>LimitNOFILE=infinity</code> results in a limit of <code>65535</code>. This is fixed as part of this commit which was merged in systemd v234. More info is available here.</p>"},{"location":"kbs/000020016/#docker-daemon-configuration","title":"Docker daemon configuration","text":"<p>It is possible to configure Docker to enforce its own open file limits on specific containers through the command line flags <code>--default-ulimit nofile=X:Y</code>.</p> <p>This can be applied to all containers by specifying the limit within the <code>/etc/docker/daemon.json</code> configuration file:</p> <pre><code>{\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 64000,\n      \"Soft\": 64000\n}\n</code></pre>"},{"location":"kbs/000020016/#resolution","title":"Resolution","text":"<p>If you have any non-default configuration that is applying nofile restrictions on either docker, or containers, revert these to the default configuration, or increase the limits and re-test.</p>"},{"location":"kbs/000020016/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020018/","title":"How to retrieve a kubeconfig from RKE v0.2.x+ or Rancher v2.2.x+","text":"<p>This document (000020018) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020018/#environment","title":"Environment","text":""},{"location":"kbs/000020018/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher v2.2.x or newer</li> <li>RKE v0.2.x or newer</li> <li>SSH access to one of the controlplane nodes</li> </ul>"},{"location":"kbs/000020018/#situation","title":"Situation","text":""},{"location":"kbs/000020018/#during-a-rancher-outage-or-other-disaster-event-you-may-lose-access-to-a-downstream-cluster-via-rancher-and-be-unable-to-manage-your-applications-this-process-allows-to-bypass-rancher-and-connects-directly-to-the-downstream-cluster","title":"During a Rancher outage or other disaster event, you may lose access to a downstream cluster via Rancher and be unable to manage your applications. This process allows to bypass Rancher and connects directly to the downstream cluster.","text":"<p>Alternative: you have lost your original kubeconfig file from your upstream cluster and you want to retrieve it.</p>"},{"location":"kbs/000020018/#resolution","title":"Resolution","text":""},{"location":"kbs/000020018/#retrieve-a-kubeconfig-using-jq","title":"Retrieve a kubeconfig - using jq","text":"<p>This option requires\u00a0<code>kubectl</code>\u00a0and\u00a0<code>jq</code>\u00a0to be installed on the server.</p> <p>Note: kubectl can be copied from the kubelet container</p> <pre><code>docker cp kubelet:/usr/local/bin/kubectl /usr/local/bin/\n</code></pre> <ul> <li>Get kubeconfig (Rancher 2.7.14+/Rancher 2.8.5+, RKE 1.4.19+/RKE 1.5.10+)</li> </ul> <pre><code>kubectl --kubeconfig $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl/kubecfg-kube-node.yaml get secrets -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\" &gt; kubeconfig_admin.yaml\n</code></pre> <ul> <li>Get kubeconfig (Earlier versions of Rancher and RKE)</li> </ul> <pre><code>kubectl --kubeconfig $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\" &gt; kubeconfig_admin.yaml\n</code></pre> <ul> <li>Run\u00a0<code>kubectl get nodes</code></li> </ul> <pre><code>kubectl --kubeconfig kubeconfig_admin.yaml get nodes\n</code></pre>"},{"location":"kbs/000020018/#retrieve-a-kubeconfig-without-jq","title":"Retrieve a kubeconfig - without jq","text":"<p>This option does not require\u00a0<code>kubectl</code>\u00a0or\u00a0<code>jq</code>\u00a0on the server because this uses the\u00a0<code>rancher/rancher-agent</code>\u00a0image to retrieve the kubeconfig.</p> <ul> <li>Get kubeconfig (Rancher 2.7.14+/Rancher 2.8.5+, RKE 1.4.19+/RKE 1.5.10+)</li> </ul> <pre><code>docker run --rm --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro --entrypoint bash $(docker inspect $(docker images -q --filter=label=org.opencontainers.image.source=https://github.com/rancher/hyperkube) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get secret -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\"' &gt; kubeconfig_admin.yaml\n</code></pre> <ul> <li>Get kubeconfig (Earlier versions of Rancher and RKE)</li> </ul> <pre><code>docker run --rm --net=host -v $(docker inspect kubelet --format '{{ range .Mounts }}{{ if eq .Destination \"/etc/kubernetes\" }}{{ .Source }}{{ end }}{{ end }}')/ssl:/etc/kubernetes/ssl:ro --entrypoint bash $(docker inspect $(docker images -q --filter=label=org.opencontainers.image.source=https://github.com/rancher/hyperkube.git) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\"' &gt; kubeconfig_admin.yaml\n</code></pre> <ul> <li>Run\u00a0<code>kubectl get nodes</code></li> </ul> <pre><code>docker run --rm --net=host -v $PWD/kubeconfig_admin.yaml:/root/.kube/config:z --entrypoint bash $(docker inspect $(docker images -q --filter=label=org.opencontainers.image.source=https://github.com/rancher/hyperkube) --format='{{index .RepoTags 0}}' | tail -1) -c 'kubectl get nodes'\n</code></pre>"},{"location":"kbs/000020018/#script","title":"Script","text":"<p>Run\u00a0<code>https://raw.githubusercontent.com/rancherlabs/support-tools/master/how-to-retrieve-kubeconfig-from-custom-cluster/rke-node-kubeconfig.sh</code>\u00a0and follow the instructions given.</p>"},{"location":"kbs/000020018/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020019/","title":"How to regenerate Service Account tokens in Kubernetes","text":"<p>This document (000020019) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020019/#situation","title":"Situation","text":""},{"location":"kbs/000020019/#at-times-service-account-tokens-could-be-rendered-invalid-this-could-be-due-to-restoring-etcd-backups-or-regenerating-ca-certificates-within-your-cluster","title":"At times Service Account tokens could be rendered invalid. This could be due to restoring etcd backups or regenerating CA certificates within your cluster.","text":"<p>This problem manifests in clients unable to authenticate against the Kubernetes API, for both pods within the cluster, or system components like the controller-manager or kube-proxy. A telling sign would be errors in these clients of the format <code>connect: connection refused</code> or within the Kubernetes API Server logs, showing a large number of clients failing TLS authentication.</p>"},{"location":"kbs/000020019/#resolution","title":"Resolution","text":"<p>With an admin kubeconfig sourced for the cluster facing issues, run the command below, to generate the list of kubectl commands required to delete all Service Account token secrets. After running the provided kubectl commands from the output, you will need to recreate pods, e.g. by deleting them, in order to regenerate the Service Account token.</p> <pre><code># kubectl get secret --all-namespaces | awk '{ if ($3 == \"kubernetes.io/service-account-token\") print \"kubectl -n\", $1 \" delete secret\", $2 }'\n</code></pre>"},{"location":"kbs/000020019/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020020/","title":"How to improve DNS resolution performance inside a kubernetes cluster?","text":"<p>This document (000020020) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020020/#situation","title":"Situation","text":""},{"location":"kbs/000020020/#task","title":"Task","text":"<p>This article gives a quick overview of DNS resolution inside a Kubernetes cluster. It also explains how performance can be improved.</p>"},{"location":"kbs/000020020/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A working Kubernetes cluster with CoreDNS or kube-dns installed.</li> </ul>"},{"location":"kbs/000020020/#overview","title":"Overview","text":"<p>When a workload is deployed on a Kubernetes cluster, the nameserver (DNS server) is set to the Service IP address of CoreDNS, along with the search options.</p> <p>Example:</p> <pre><code># cat /etc/resolv.conf\nnameserver 10.43.0.10\nsearch cattle-system.svc.cluster.local svc.cluster.local cluster.local members.linode.com\noptions ndots:5\n</code></pre> <ul> <li>The first search option is generally <code>&lt;namespace-where-workload-is-deployed&gt;.svc.&lt;cluster domain&gt;</code></li> <li>Second option is <code>svc.&lt;cluster domain&gt;</code></li> <li>Third one being just <code>&lt;cluster domain&gt;</code></li> <li>The rest of the options are copied over from what's configured on the node. (options from <code>/etc/resolv.conf</code> on the node)</li> </ul> <p>The above options are helpful in the following ways: - When a workload tries to connect to a service in the same namespace, it can simply reference it using the destination service name. Ex: hello-world-service - Similarly, when it tries to connect to a service in a different namespace, it's enough to append the namespace name after the service name. Example: database-service.finance-app</p> <p>The above conveniences come at a cost, though! Suppose the workload tries to resolve an external domain name from outside of the cluster. In that case, the DNS client first exhausts appending all the search options to this name before finally resolving to the domain name, this is known as search path expansion.</p> <p>Say the workload needs to resolve <code>mycompany.com</code>. The DNS client sends out a query for <code>mycompany.com.&lt;namespace&gt;.svc.&lt;cluster domain&gt;</code>. The DNS server responds correctly with a nonexistent domain message (NXDOMAIN) as it is not a known domain.</p> <p>Next, the DNS client tries to resolve <code>mycompany.com.svc.&lt;cluster domain&gt;</code>, followed by <code>mycompany.com.&lt;cluster domain&gt;</code>.</p> <p>As you can see, there are three additional queries before the search lookups added by Kubernetes are completed, on top of this there are <code>A</code> (IPv4) queries, as well as a <code>AAAA</code> (IPv6) queries sent out for each of the above.</p> <p>All these unnecessary queries add to the network traffic and increase the time to get the final query.</p>"},{"location":"kbs/000020020/#resolution","title":"Resolution","text":"<p>To avoid the DNS client iterating through each of the search options specified in <code>/etc/resolv.conf</code>, for external domain names it is helpful to add a trailing dot <code>.</code> to the name that's being resolved outside of the cluster, this ensures the DNS client performs an absolute lookup. Example: <code>login.mycompany.com.</code></p> <p>Similarly, when possible, use the full domain name for internal service names ( <code>&lt;destination-service-name&gt;.&lt;destination-namespace&gt;.svc.&lt;cluster domain&gt;</code>). As a further recommendation, some clusters can benefit from deploying NodeLocal DNS. This is particularly useful in a production cluster or a cluster with a high frequency of DNS queries.</p> <p>There are several ways that NodeLocal DNS improves the reliability and scalability of CoreDNS, please see the documentation for details.</p>"},{"location":"kbs/000020020/#further-reading","title":"Further reading","text":"<ul> <li>https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</li> <li>https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/</li> </ul>"},{"location":"kbs/000020020/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020021/","title":"How to configure expiry (TTL) on kubeconfig tokens in Rancher","text":"<p>This document (000020021) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020021/#situation","title":"Situation","text":""},{"location":"kbs/000020021/#task","title":"Task","text":"<p>In Rancher it is possible to configure an expiry (TTL) on Rancher-generated kubeconfig tokens for Rancher managed Kubernetes clusters. This article details how to configure kubeconfig token expiry as a Rancher administrator and how users can authenticate via <code>kubectl</code> when this is configured.</p>"},{"location":"kbs/000020021/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>The <code>kubectl</code> binary and <code>Rancher CLI</code> installed locally</li> </ul>"},{"location":"kbs/000020021/#resolution","title":"Resolution","text":""},{"location":"kbs/000020021/#disable-automatic-kubeconfig-token-generation-and-configure-ttl","title":"Disable automatic kubeconfig token generation and configure TTL","text":"<p>As a Rancher global admin, disable automatic kubeconfig token generation and configure the expiry time (TTL) for kubeconfig tokens, per the steps in the Rancher documentation here .</p>"},{"location":"kbs/000020021/#authenticating-via-the-rancher-cli-with-kubectl","title":"Authenticating via the Rancher CLI with kubectl","text":"<p>Once an admin has configured the kubeconfig TTL, users will need to download the Rancher CLI to authenticate against Rancher when using Rancher-generated kubeconfig files to connect to Rancher-managed clusters.</p> <ol> <li>Download the required Rancher CLI binary per the Rancher documentation .</li> <li>Ensure the <code>rancher</code> CLI binary is executable and in your PATH.</li> <li>Download a copy of the kubeconfig file for a cluster from the Rancher UI and add it to the default ~/.kube/config file or source it with <code>KUBECONFIG=/path/to/file</code>.</li> <li>Execute <code>kubectl get nodes</code> and observe you will be prompted for your Rancher username and password. If you are using an authentication provider you will also be prompted to select this versus local authentication. You can prevent this prompt by adding the <code>--auth-provider=&lt;provider&gt;</code> argument in the kubeconfig file, per the following example:</li> </ol> <pre><code>     args:\n    - token\n    - --auth-provider=openLdapProvider\n    - --server=rancher.example.com\n</code></pre> <ol> <li>After providing the username and password, the kubeconfig token will be generated and valid for the TTL ( <code>kubeconfig-token-ttl-minutes</code>) configured in Rancher.</li> <li>You can verify the configured expiry time of the kubeconfig token within the Rancher UI, under <code>API &amp; Keys</code>.</li> <li>Once the token expires, you will be prompted to log in again upon executing <code>kubectl</code> commands against the cluster, per step 4.</li> </ol> <p>N.B. By default the generated kubeconfig token is cached within the directory <code>.cache</code> in the working directory from which you invoke <code>kubectl</code>, when you are prompted to log in. As a result executing <code>kubectl</code> from a different directory, will re-prompt for authentication and generate a fresh token cache under <code>.cache</code>. To prevent this behavior, you can set the token cache location with the environment variable <code>RANCHER_CONFIG_DIR</code>, e.g. <code>export RANCHER_CONFIG_DIR=~/.rancher</code> to avoid being prompted for authentication when you change the working directory.</p>"},{"location":"kbs/000020021/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020022/","title":"How to enable debug logging when using Terraform?","text":"<p>This document (000020022) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020022/#environment","title":"Environment","text":"<p>Rancher Terraform Provider</p>"},{"location":"kbs/000020022/#situation","title":"Situation","text":""},{"location":"kbs/000020022/#if-you-encounter-an-issue-with-the-rancher2-terraform-provider-it-may-be-helpful-to-capture-debug-output-from-terraform-so-that-you-can-provide-this-to-rancher-support-this-article-details-how-to-enable-debug-output-and-set-the-log-location-for-terraform-commands-such-as-terraform-apply","title":"If you encounter an issue with the Rancher2 Terraform Provider it may be helpful to capture debug output from Terraform, so that you can provide this to Rancher Support. This article details how to enable debug output and set the log location for Terraform commands such as <code>terraform apply</code>.","text":""},{"location":"kbs/000020022/#resolution","title":"Resolution","text":"<p>You can set the Terraform log level and location via the <code>TF_LOG</code> and <code>TF_LOG_PATH</code> environment variables.</p>"},{"location":"kbs/000020022/#set-log-level-with-tf_log","title":"Set log level with <code>TF_LOG</code>","text":"<p>The environment variable <code>TF_LOG</code> defines the log level. Valid log levels are (in order of decreasing verbosity): <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Set the log level in your environment with the appropriate command (substituting your preferred log level):</p> <p>Bash: <code>export TF_LOG=\"DEBUG\"</code></p> <p>PowerShell: <code>$env:TF_LOG=\"DEBUG\"</code></p>"},{"location":"kbs/000020022/#redirect-terraform-logs-with-tf_log","title":"Redirect Terraform logs with <code>TF_LOG</code>","text":"<p>The environment variable <code>TF_LOG_PATH</code> specifies the file in which Terraform will write logs. If <code>TF_LOG_PATH</code> is not set, output is sent to standard output and error in the terminal. If the environment variable is set, Terraform will append logs from each run to the specified file.</p> <p>Set the Terraform log location in your environment with the appropriate command (substituting the path to your preferred file):</p> <p>Bash: <code>export TF_LOG_PATH=\"tmp/terraform.log\"</code></p> <p>PowerShell: <code>$env:TF_LOG_PATH=\"C:\\tmp\\terraform.log\"</code></p> <p>To set them permanently, you can add these environment variables to your <code>.profile</code>, <code>.bashrc</code>, PowerShell profile (if it exists, the path is stored in <code>$profile</code> environment variable) file, or the appropriate profile for your chosen shell.</p> <p>N.B. As this will append the log with the Terraform output every time you run a Terraform command, you may wish to configure log rotation for the chosen log file if this is enabled permanently. Alternatively, disable logging to file once you have finished troubleshooting.</p>"},{"location":"kbs/000020022/#further-reading","title":"Further reading","text":"<ul> <li>Debugging Terraform (official) documentation</li> </ul>"},{"location":"kbs/000020022/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020023/","title":"What do I need to provide to get started on Rancher Hosted Prime?","text":"<p>This document (000020023) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020023/#situation","title":"Situation","text":""},{"location":"kbs/000020023/#resolution","title":"Resolution","text":"<p>To get started on Rancher Hosted Prime, please provide the following information:</p> <ol> <li>The <code>rancher.cloud</code> vanity URL to be used for Web and API access. For example, mycompany.rancher.cloud. This cannot be changed, so please choose carefully.</li> <li>The region you would like your Rancher Hosted Prime environment to reside in. Current options are US West, US East, Canada Central, EU West (Dublin or Paris), EU Central, EU North, AP Southeast (Sydney or Singapore), and South Africa. Typically you'll want the region closest to where your downstream/managed clusters will reside.</li> <li>If you have special networking requirements, such as a site-to-site VPN or VPC peering between\u00a0Rancher Hosted Prime and your infrastructure, you can specify a RFC-1918 CIDR to be used in Rancher Hosted Prime. The CIDR must be at least /25 or larger network. You will want to choose a network block that does not overlap with your infrastructure.</li> <li>We perform regular maintenance on Hosted Rancher. Select one of two maintenance windows: Tuesday 00:00 - 01:00 UTC or Thursday 17:00 - 18:00 UTC. Note that windows are UTC-based and the times may shift by an hour twice a year if you reside in a region that observes Daylight Savings Time.</li> </ol> <p>To request your Hosted Rancher Prime environment, you can open a support ticket on our support portal.</p>"},{"location":"kbs/000020023/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020024/","title":"What are the networking requirements for using Rancher Hosted Prime?","text":"<p>This document (000020024) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020024/#resolution","title":"Resolution","text":"<p>The network requirements for Rancher Hosted Prime are going to depend on your use cases. The following is a list of common uses cases and what is required:</p> <ul> <li>To access the Rancher UI or API, you must have outbound TCP/443 connectivity to Rancher Hosted Prime.</li> <li>All downstream/managed clusters require outbound TCP/443 connectivity to Rancher Hosted Prime.</li> <li>If you are creating a node driver based cluster, Rancher Hosted Prime will need TCP/22 (SSH) connectivity to each node for the initial provisioning. Additionally, Rancher Hosted Prime will need connectivity to the orchestration API, for example, the vSphere API.</li> <li>For authentication provider integration, Rancher Hosted Prime will require connectivity to the authentication provider's endpoint. Generally, no networking setup is needed if using a SaaS authentication provider such as Azure Active Directory, Okta, or GitHub.</li> </ul> <p>More detailed documentation for networking requirements can be found in Rancher's port requirements docs.</p>"},{"location":"kbs/000020024/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020025/","title":"Can I use my own SSL/TLS certificates with Rancher Hosted Prime?","text":"<p>This document (000020025) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020025/#resolution","title":"Resolution","text":"<p>No, currently we use a rancher.cloud vanity hostname for Rancher Hosted Prime and fully manage the SSL/TLS certificate. The certificate is automatically created when your environment is provisioned and automatically renewed annually.</p>"},{"location":"kbs/000020025/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020027/","title":"Why longhorn volume still shows more space than the actual size after deleting the content?","text":"<p>This document (000020027) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020027/#environment","title":"Environment","text":"<ul> <li>A Kubernetes cluster with Longhorn installed</li> <li>An existing workload using a Persistent Volume (PV) of the Longhorn StorageClass (SC)</li> </ul>"},{"location":"kbs/000020027/#situation","title":"Situation","text":""},{"location":"kbs/000020027/#why-does-longhorn-report-higher-actual-size-for-a-volume-than-the-usage-reported-by-the-filesystem-even-after-deleting-content","title":"Why does Longhorn report higher <code>Actual Size</code> for a volume than the usage reported by the filesystem, even after deleting content?","text":""},{"location":"kbs/000020027/#resolution","title":"Resolution","text":"<p>Whilst Longhorn storage is thin-provisioned, a volume cannot shrink in size after content is removed.</p> <p>Per the Longhorn Documentation: \"This happens because Longhorn operates on the block level, not the filesystem level, so Longhorn doesn\u2019t know if the content has been removed by a user or not. That information is mostly kept at the filesystem level.\"</p> <p>You can demonstrate this behaviour, by attaching a 10GB Longhorn volume to a Pod, creating 6GB of files on the volume and then deleting 4GB, as follows:</p> <ol> <li>Increase the disk usage of the volume:</li> </ol> <pre><code>dd if=/dev/zero of=4gbfile bs=4G count=1\ndd if=/dev/zero of=2gbfile bs=2G count=1\n</code></pre> <ol> <li>Remove the <code>4gbfile</code> from the mounted volume:</li> </ol> <pre><code>rm 4gbfile\n</code></pre> <ol> <li> <p>Check the usage reported by the filesystem, by running <code>du -sh</code> on the mounted volume, which will show 2GB.</p> </li> <li> <p>Navigate to the Longhorn UI and check the <code>Actual Size</code>, reported under <code>Volume Details</code> in the Volume view, which will show 6GB of usage still.</p> </li> </ol>"},{"location":"kbs/000020027/#further-reading","title":"Further Reading","text":"<ul> <li>The Longhorn Documentation</li> </ul>"},{"location":"kbs/000020027/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020027/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020028/","title":"Does Rancher v2.x support two-factor (2FA) or multi-factor authentication (MFA)?","text":"<p>This document (000020028) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020028/#environment","title":"Environment","text":"<p>Rancher 2.x</p>"},{"location":"kbs/000020028/#situation","title":"Situation","text":""},{"location":"kbs/000020028/#does-rancher-v2x-support-two-factor-2fa-or-multi-factor-authentication-mfa_1","title":"Does Rancher v2.x support two-factor (2FA) or multi-factor authentication (MFA)?","text":""},{"location":"kbs/000020028/#resolution","title":"Resolution","text":"<p>The built-in Rancher local authentication provider does not have 2FA or MFA capabilities; however, it is possible to secure Rancher with 2FA/MFA, by using one of the Rancher-supported authentication providers with this capability.</p> <p>Google OAuth</p> <p>Active Directory (AD)</p> <p>Azure AD</p> <p>FreeIPA</p> <p>GitHub</p> <p>Keycloak</p> <p>PingIdentity</p> <p>Okta</p> <p>OpenLDA</p>"},{"location":"kbs/000020028/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020031/","title":"How to shutdown a Kubernetes cluster (Rancher Kubernetes Engine (RKE) CLI provisioned or Rancher v2.x Custom clusters)","text":"<p>This document (000020031) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020031/#situation","title":"Situation","text":""},{"location":"kbs/000020031/#task","title":"Task","text":"<p>This article provides instructions for safely shutting down a Kubernetes cluster provisioned via the Rancher Kubernetes Engine (RKE) CLI or a Rancher v2.x provisioned Custom Cluster.</p>"},{"location":"kbs/000020031/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster launched with the RKE CLI or from Rancher 2.x as a Custom Cluster</li> </ul>"},{"location":"kbs/000020031/#background","title":"Background","text":"<p>If you have a need to shut down the infrastructure running a Kubernetes cluster (datacenter maintenance, migration, etc.) this guide will provide steps in the proper order to ensure a safe cluster shutdown. This guide has command examples for RKE-deployed clusters but the order of operations and the process is similar for most Kubernetes distributions.</p> <p>Please ensure you complete an etcd backup before continuing this process. A guide regarding the backup and restore process can be found here.</p>"},{"location":"kbs/000020031/#solution","title":"Solution","text":"<p>N.B. If you have nodes that share worker, control plane, or etcd roles, postpone the <code>docker stop</code> and shutdown operations until worker or control plane containers have been stopped.</p>"},{"location":"kbs/000020031/#draining-nodes","title":"Draining nodes.","text":""},{"location":"kbs/000020031/#for-all-nodes-prior-to-stopping-the-containers-run","title":"For all nodes, prior to stopping the containers, run:","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"kbs/000020031/#to-identify-the-desired-node-then-run","title":"To identify the desired node, then run:","text":"<pre><code>kubectl drain &lt;node name&gt;\n</code></pre>"},{"location":"kbs/000020031/#this-will-safely-evict-any-pods-and-you-can-proceed-with-the-following-steps-to-a-shutdown","title":"This will safely evict any pods, and you can proceed with the following steps to a shutdown.","text":""},{"location":"kbs/000020031/#shutting-down-the-workers-nodes","title":"Shutting down the workers nodes","text":"<p>For each worker node:</p> <ol> <li>ssh into the worker node</li> <li>stop kubelet and kube-proxy by running <code>sudo docker stop kubelet kube-proxy</code></li> <li>stop docker by running <code>sudo service docker stop</code> or <code>sudo systemctl stop docker</code></li> <li>shutdown the system <code>sudo shutdown now</code></li> </ol>"},{"location":"kbs/000020031/#shutting-down-the-control-plane-nodes","title":"Shutting down the control plane nodes","text":"<p>For each control plane node:</p> <ol> <li>ssh into the control plane node</li> <li>stop kubelet and kube-proxy by running <code>sudo docker stop kubelet kube-proxy</code></li> <li>stop kube-scheduler and kube-controller-manager by running <code>sudo docker stop kube-scheduler kube-controller-manager</code></li> <li>stop kube-apiserver by running <code>sudo docker stop kube-apiserver</code></li> <li>stop docker by running <code>sudo service docker stop</code> or <code>sudo systemctl stop docker</code></li> <li>shutdown the system <code>sudo shutdown now</code></li> </ol>"},{"location":"kbs/000020031/#shutting-down-the-etcd-nodes","title":"Shutting down the etcd nodes","text":"<p>For each etcd node:</p> <ol> <li>ssh into the etcd node</li> <li>stop kubelet and kube-proxy by running <code>sudo docker stop kubelet kube-proxy</code></li> <li>stop etcd by running <code>sudo docker stop etcd</code></li> <li>stop docker by running <code>sudo service docker stop</code> or <code>sudo systemctl stop docker</code></li> <li>shutdown the system <code>sudo shutdown now</code></li> </ol>"},{"location":"kbs/000020031/#shutting-down-storage","title":"Shutting down storage","text":"<p>Shut down any persistent storage devices that you might have in your datacenter (such as NAS storage devices) if applicable. It iss important that you do this after shutting everything else down to prevent data loss/corruption for containers requiring persistency.</p> <p>N.B. If you are running a cluster that was not deployed through RKE then the order of the process is still the same, however the commands may vary. For instance, some distributions run kubelet and other control plane items as a service on the node rather than in docker. Check documentation for the specific Kubernetes distribution for information as to how to stop these services.</p>"},{"location":"kbs/000020031/#starting-a-kubernetes-cluster-up-after-shutdown","title":"Starting a Kubernetes cluster up after shutdown","text":"<p>Kubernetes is good about recovering from a cluster shutdown and requires little intervention, though there is a specific order in which things should be powered back on to minimize errors.</p> <ol> <li>Power on any storage devices if applicable.</li> </ol> <p>Check with your storage vendor on how to properly power on you storage devices and verify that they are ready.</p> <ol> <li>For each etcd node:</li> <li>Power on the system/start the instance.</li> <li>Log into the system via ssh.</li> <li>Ensure docker has started <code>sudo service docker status</code> or <code>sudo systemctl status docker</code></li> <li>Ensure etcd and kubelet\u2019s status shows Up in Docker <code>sudo docker ps</code></li> <li>For each control plane node:</li> <li>Power on the system/start the instance.</li> <li>Log into the system via ssh.</li> <li>Ensure docker has started <code>sudo service docker status</code> or <code>sudo systemctl status docker</code></li> <li>Ensure kube-apiserver, kube-scheduler, kube-controller-manager, and kubelet\u2019s status shows Up in Docker <code>sudo docker ps</code></li> <li>For each worker node:</li> <li>Power on the system/start the instance.</li> <li>Log into the system via ssh.</li> <li>Ensure docker has started <code>sudo service docker status</code> or <code>sudo systemctl status docker</code></li> <li>Ensure kubelet\u2019s status shows Up in Docker <code>sudo docker ps</code></li> <li>Log into the Rancher UI (or use kubectl) and check your various projects to ensure workloads have started as expected. This may take a few minutes depending on the number of workloads and your server capacity.</li> </ol>"},{"location":"kbs/000020031/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020032/","title":"How to block specific user agents from connecting through the nginx ingress controller","text":"<p>This document (000020032) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020032/#environment","title":"Environment","text":"<p>A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster with Nginx for its ingress controller.</p>"},{"location":"kbs/000020032/#situation","title":"Situation","text":""},{"location":"kbs/000020032/#at-times-its-necessary-to-block-specific-user-agents-from-connecting-to-workloads-within-your-cluster-whether-its-bad-actors-or-for-compliance-reasons-well-go-through-how-to-get-it-done-with-rancherrke-created-clusters","title":"At times it's necessary to block specific user agents from connecting to workloads within your cluster. Whether it's bad actors or for compliance reasons, we'll go through how to get it done with Rancher/RKE created clusters.","text":""},{"location":"kbs/000020032/#resolution","title":"Resolution","text":""},{"location":"kbs/000020032/#1-identify-the-user-agents-that-will-be-blocked","title":"1. Identify the user agents that will be blocked.","text":"<p>There are multiple ways to surface user agents needing to be blocked, the most practical being your nginx ingress controllers' logs. In the logs one would see something similar to the following entries:</p> <pre><code>172.16.10.101 - - [02/Jan/2021:21:51:22 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:84.0) Gecko/20100101 Firefox/84.0\" 367 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.000 200 1da439122bd7d7014f6627f32e4cefc3\n172.16.10.101 - - [02/Jan/2021:21:51:22 +0000] \"GET /favicon.ico HTTP/1.1\" 499 0 \"http://test.default.54.202.152.214.xip.io/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:84.0) Gecko/20100101 Firefox/84.0\" 341 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 0 0.000 - 92a1e851206da86fbec0610d346e2ddd\n172.16.10.101 - - [02/Jan/2021:21:51:26 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"curl/7.64.1\" 98 0.000 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.000 200 4c51046660b05cf2703dbedfae2272aa\n172.16.10.101 - - [02/Jan/2021:21:51:29 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"Wget/1.20.3 (darwin19.0.0)\" 164 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.000 200 5334e799b3268dab31d74a5d2239702b\n</code></pre> <p>We can see three unique user agents here; <code>curl</code>, <code>Wget</code>, and <code>Mozilla/5.0</code>.</p>"},{"location":"kbs/000020032/#2-modify-the-clusteryaml-to-include-the-nginx-option-to-block-user-agents","title":"2. Modify the cluster.yaml to include the Nginx option to block user agents","text":"<p>In the example configuration, we'll block connections from curl and Mozilla using regular expressions. It's essential to separate the list of agents we're looking to restrict with commas.</p> <pre><code>  ingress:\n    options:\n      block-user-agents: '~*curl.*,~*Mozilla.*'\n    provider: nginx\n</code></pre>"},{"location":"kbs/000020032/#3-test-the-configuration","title":"3. Test the configuration","text":"<p>Navigating to the workload or service behind the ingress now blocks the agents blacklisted.</p> <pre><code>172.16.10.101 - - [02/Jan/2021:22:23:00 +0000] \"GET / HTTP/1.1\" 200 45 \"-\" \"Wget/1.20.3 (darwin19.0.0)\" 164 0.000 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 45 0.004 200 df2b5c3fca1ba33683e8ee6d2708b214\n172.16.10.101 - - [02/Jan/2021:22:23:05 +0000] \"GET / HTTP/1.1\" 403 153 \"-\" \"curl/7.64.1\" 98 0.000 [] [] - - - - 1902bca1f3622bf5374f966358b10463\n172.16.10.101 - - [02/Jan/2021:22:31:56 +0000] \"GET / HTTP/1.1\" 403 153 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:84.0) Gecko/20100101 Firefox/84.0\" 478 0.001 [default-ingress-1db0bf370dd59aa8ff284a4bd4ccdc07-80] [] 10.42.0.10:80 0 0.004 304 9e0df037eaea1afedc5d3c93229dca80\n</code></pre>"},{"location":"kbs/000020032/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020033/","title":"How to remove and replace an unresponsive control plane / etcd node in the local Rancher server cluster, provisioned by the Rancher Kubernetes Engine (RKE) CLI","text":"<p>This document (000020033) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020033/#environment","title":"Environment","text":""},{"location":"kbs/000020033/#a-rancher-kubernetes-engine-rke-cli-provisioned-cluster","title":"A Rancher Kubernetes Engine (RKE) CLI provisioned cluster","text":"<ul> <li>A Highly Available control plane / etcd configuration, with an odd number of mixed role control plane / etcd nodes, commonly 3 or 5</li> <li>The cluster is quorate, i.e. with 3 control plane / etcd nodes only a single node is unresponsive, or with 5 control plane / etcd nodes upto two nodes are unresponsive</li> <li>The cluster configuration file (e.g. <code>cluster.yml</code>) and .rkestate file (e.g. <code>cluster.rkestate</code>)</li> <li>The RKE binary and SSH access to the nodes</li> </ul>"},{"location":"kbs/000020033/#situation","title":"Situation","text":""},{"location":"kbs/000020033/#this-article-details-how-to-remove-and-replace-an-unresponsive-control-plane-etcd-node-from-a-local-rancher-server-cluster-provisioned-via-the-rancher-kubernetes-engine-rke-cli","title":"This article details how to remove and replace an unresponsive control plane / etcd node from a local Rancher server cluster, provisioned via the Rancher Kubernetes Engine (RKE) CLI.","text":""},{"location":"kbs/000020033/#resolution","title":"Resolution","text":""},{"location":"kbs/000020033/#this-operation-is-relatively-simple-and-uses-the-example-clusteryaml-below-for-demonstration-purposes","title":"This operation is relatively simple, and uses the example <code>cluster.yaml</code> below for demonstration purposes.","text":"<p>N.B. Be sure to use your <code>cluster.yaml</code> and matching <code>cluster.rkestate</code> for the relevant cluster.</p> <p>In this demonstration example, the node that is failing has the address <code>1.2.3.3</code>:</p> <pre><code>nodes:\n    - address: 1.2.3.1\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n    - address: 1.2.3.2\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n    - address: 1.2.3.3\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n[...] # rest of cluster.yaml except control plane / etcd nodes restracted\n</code></pre>"},{"location":"kbs/000020033/#step-1-validate-the-cluster-is-quorate-and-confirm-the-unresponsive-node","title":"Step 1. Validate the cluster is quorate and confirm the unresponsive node","text":"<p>On the control plane / etcd nodes perform the following command, per the Rancher Troubleshooting Documentation to determine etcd endpoint health:</p> <pre><code>docker exec -e ETCDCTL_ENDPOINTS=$(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\") etcd etcdctl endpoint health\n</code></pre> <p>On the unresponsive node the command may fail to execute, on the healthy nodes you should see output of the following format indicating the health status of each node:</p> <pre><code>{\"level\":\"warn\",\"ts\":\"2020-12-31T12:11:41.840Z\",\"caller\":\"clientv3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"endpoint://client-c65a15b4-9646-4c71-914d-f3c892c04c2f/1.2.3.3:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: all SubConns are in TransientFailure, latest connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 1.2.3.3:2379: connect: connection refused\\\"\"}\nhttps://1.2.3.1:2379 is healthy: successfully committed proposal: took = 13.442336ms\nhttps://1.2.3.2:2379 is healthy: successfully committed proposal: took = 18.227226ms\nhttps://1.2.3.3:2379 is unhealthy: failed to commit proposal: context deadline exceeded\n</code></pre>"},{"location":"kbs/000020033/#step-2-remove-the-unresponsive-node","title":"Step 2. Remove the unresponsive node","text":"<p>Having confirmed which node is unresponsive in the cluster, remove this from the nodes block in the cluster configuration file ( <code>cluster.yaml</code>), per the example of <code>1.2.3.3</code> removed below:</p> <pre><code>nodes:\n    - address: 1.2.3.1\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n    - address: 1.2.3.2\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n\n[...] # rest of cluster.yaml except control plane / etcd nodes restracted\n</code></pre> <p>After updating the <code>cluster.yaml</code> file, execute an <code>rke up</code> run to remove the node:</p> <pre><code>rke up --config cluster.yaml\n</code></pre> <p>The above action will remove the problematic and unresponsive control plane / etcd node.</p>"},{"location":"kbs/000020033/#step-3-clean-and-add-the-removed-node-back-to-the-cluster","title":"Step 3. Clean and add the removed node back to the cluster","text":"<p>Once the <code>rke up</code> invocation has run through without any errors, and you can see the node removed from the Rancher UI or <code>kubectl get nodes</code> output, it is safe to move onto adding the node back in.</p> <p>First clean the removed node ( <code>1.2.3.3</code>) in our example, using the Extended Rancher 2 Cleanup script.</p> <p>After cleaning the node, add this back into the cluster configuration ( <code>cluster.yaml</code>) file:</p> <pre><code>nodes:\n    - address: 1.2.3.1\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n    - address: 1.2.3.2\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n    - address: 1.2.3.3\n      user: ubuntu\n      role:\n        - controlplane\n        - etcd\n[...] # rest of cluster.yaml except control plane / etcd nodes restracted\n</code></pre> <p>And run the rke up command again:</p> <pre><code>rke up --config cluster.yaml\n</code></pre>"},{"location":"kbs/000020033/#step-4-validate-final-cluster-state","title":"Step 4. Validate final cluster state","text":"<p>Once the <code>rke up</code> command has completed, without errors, you can now verify the node is visible and ready via <code>kubectl get nodes</code> and the Rancher UI.</p> <p>The etcd endpoint health commands on the control plane / etcd nodes should also show each endpoint as healthy, per the following example output:</p> <pre><code>https://1.2.3.1:2379 is healthy: successfully committed proposal: took = 13.442336ms\nhttps://1.2.3.2:2379 is healthy: successfully committed proposal: took = 18.227226ms\nhttps://1.2.3.3:2379 is healthy: successfully committed proposal: took = 22.065616ms\n</code></pre>"},{"location":"kbs/000020033/#further-reading","title":"Further reading","text":"<ul> <li>Extended Rancher 2 Cleanup script</li> <li>RKE Documentation on cluster state files (.rkestate)</li> </ul>"},{"location":"kbs/000020033/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020033/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020034/","title":"Preventing LoadBalancer service traffic from flowing through control plane and etcd nodes in a Kubernetes cluster with the AWS Cloud Provider","text":"<p>This document (000020034) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020034/#environment","title":"Environment","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster, provisioned on EC2 instances</li> <li>Separate worker nodes from control plane and etcd nodes</li> <li>The AWS Cloud Provider configured</li> </ul>"},{"location":"kbs/000020034/#situation","title":"Situation","text":"<p>This article details how to prevent LoadBalancer type service traffic from flowing through control plane and etcd nodes, in a cluster configured with the AWS Cloud Provider.</p>"},{"location":"kbs/000020034/#resolution","title":"Resolution","text":""},{"location":"kbs/000020034/#nodes-of-a-kubernetes-cluster-created-by-rancherrke-that-use-aws-as-the-cloud-provider-automatically-get-added-to-service-load-balancers-elb-the-behavior-results-in-both-controlplane-and-etcd-nodes-routing-end-user-application-traffic-breaking-the-role-separations-model-to-prevent-this-label-the-control-plane-and-etcd-nodes-with-the-label-node-rolekubernetesiomaster-and-the-cloud-controller-will-not-automatically-add-them-to-the-service-load-balancers","title":"Nodes of a Kubernetes cluster created by Rancher/RKE, that use AWS as the cloud provider, automatically get added to service load balancers (ELB). The behavior results in both controlplane and etcd nodes routing end-user application traffic, breaking the role separations model. To prevent this, label the control plane and etcd nodes with the label <code>node-role.kubernetes.io/master</code> and the cloud-controller will not automatically add them to the service load balancers.","text":""},{"location":"kbs/000020034/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020034/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020035/","title":"How to enable IPVS proxy mode for kube-proxy","text":"<p>This document (000020035) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020035/#environment","title":"Environment","text":"<ul> <li>A cluster managed using Rancher</li> </ul> <p>Or</p> <ul> <li>A cluster managed using Rancher Kubernetes Engine (RKE) CLI</li> </ul>"},{"location":"kbs/000020035/#situation","title":"Situation","text":""},{"location":"kbs/000020035/#the-default-proxy-mode-for-kube-proxy-in-kubernetes-and-clusters-is-iptables-and-this-is-also-the-case-for-clusters-created-with-rancher-2x-and-the-rancher-kubernetes-engine-rke-cli","title":"The default proxy mode for kube-proxy in Kubernetes and clusters is iptables, and this\u00a0is also the case for clusters created with Rancher 2.x and the Rancher Kubernetes Engine (RKE) CLI.","text":"<p>This article aims to provide all the needed steps and configuration to deploy or update a cluster to use IPVS proxy mode.</p> <p>Please note, IPVS provides load balancing functionality, with this in mind it does not cover all of the traffic handling maintained by kube-proxy. Some scenarios will still utilise iptables, such as services that require NAT, like NodePort and LoadBalancer services.</p>"},{"location":"kbs/000020035/#resolution","title":"Resolution","text":"<p>The <code>--proxy-mode</code> flag for kube-proxy is used to override the default iptables mode, using the below steps for Rancher or RKE the <code>--proxy-mode</code> flag can be provided to enable IPVS.</p> <p>Note: Enabling IPVS is best done when creating a cluster, the process to update an existing cluster does include some follow-up steps at the end of this article, please ensure to read these beforehand, and complete these when migrating to IPVS on an existing cluster.</p>"},{"location":"kbs/000020035/#rancher-v2x","title":"Rancher v2.x","text":"<p>Log into the Rancher UI:</p> <ul> <li>From the Global view click on the cluster</li> <li>Click the Edit Cluster button, and Edit as YAML</li> <li>Locate or create the <code>services.kubeproxy</code> field under <code>rancher_kubernetes_engine_config</code></li> </ul> <p>Add <code>extra_args</code> under <code>kubeproxy</code> to apply the IPVS changes to the kube-proxy component when it is started as a container on all nodes.</p> <p>This example uses the <code>lc</code> (least connection) load balancing algorithm, <code>rr</code> (round-robin) is the default.</p> <pre><code>    kubeproxy:\n      extra_args:\n        ipvs-scheduler: lc\n        proxy-mode: ipvs\n</code></pre> <ul> <li>Click Save, the above changes will be applied to the cluster</li> </ul>"},{"location":"kbs/000020035/#note-ensure-that-the-necessary-kernel-modules-such-as-ip_vs_lc-are-loaded-when-using-the-lc-least-connection-load-balancing-algorithm-rancher-kubernetes-engine-rke-cli","title":"Note:\u00a0Ensure that the necessary kernel modules (such as ip_vs_lc) are loaded when using the lc (least connection) load balancing algorithm    Rancher Kubernetes Engine (RKE) CLI","text":"<p>Edit the cluster.yaml configuration file for your cluster:</p> <ul> <li>Locate or create the <code>services.kubeproxy</code> field</li> </ul> <p>Add <code>extra_args</code> under <code>kubeproxy</code> to apply the IPVS changes to the kube-proxy component when it is started as a container on all nodes.</p> <p>This example uses the <code>lc</code> (least connection) load balancing algorithm, <code>rr</code> (round-robin) is the default.</p> <pre><code>    kubeproxy:\n      extra_args:\n        ipvs-scheduler: lc\n        proxy-mode: ipvs\n</code></pre> <ul> <li>Use the <code>rke up</code> command to apply the changes to the cluster</li> </ul>"},{"location":"kbs/000020035/#migrating-to-ipvs-on-an-existing-cluster","title":"Migrating to IPVS on an existing cluster","text":"<p>In recent Kubernetes versions when a proxy-mode is changed the managed iptables rules are not cleaned. To avoid inconsistency and unpredictable outcomes it is recommended to restart nodes that are in an existing cluster to ensure all service connectivity is accurate.</p> <p>If using using an immutable approach in your environment, replacing each node is also an option instead of restarting.</p> <p>Once the cluster has applied the above arguments to kube-proxy successfully and returned to the Active state, plan to drain, restart and/or replace each node during a maintenance period.</p> <p>This can be done on one node initially, and performed on one or more nodes at a time once tested.</p>"},{"location":"kbs/000020035/#additional-information","title":"Additional Information","text":"<ul> <li>IPVS proxy mode</li> <li>Comparing kube-proxy modes: iptables or IPVS</li> <li>IPVS-Based In-Cluster Load Balancing Deep Dive</li> </ul>"},{"location":"kbs/000020035/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020036/","title":"How to check the default CoreDNS configmap of a Rancher Kubernetes Engine (RKE) Kubernetes version","text":"<p>This document (000020036) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020036/#environment","title":"Environment","text":"<ul> <li>Rancher 2.6.x/ 2.7/2.8.x</li> <li>CoreDNS default configuration</li> </ul>"},{"location":"kbs/000020036/#situation","title":"Situation","text":""},{"location":"kbs/000020036/#task","title":"Task","text":"<p>You might have modified the default configmap for CoreDNS using Rancher Kubernetes Engine's (RKE) cluster configuration YAML ( <code>cluster.yml</code>). In this case, you may want to know the default configmap before upgrading Kubernetes. This verification step will help you to add all of the default/mandatory parameters to the modified configmap in RKE's <code>cluster.yml</code>, upon upgrade.</p>"},{"location":"kbs/000020036/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Python installed.</li> </ul>"},{"location":"kbs/000020036/#resolution","title":"Resolution","text":"<p>Download the <code>kontainer-metadata</code> according to the Rancher version you are running.</p> <p>Rancher 2.6.x:</p> <pre><code>curl -O https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json\n</code></pre> <p>Rancher v2.7.x:</p> <pre><code>curl -O https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/dev-v2.7/data/data.json\n</code></pre> <pre><code>Rancher v2.8.x:\n</code></pre> <pre><code>curl -O https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/dev-v2.8/data/data.json\n</code></pre> <pre><code>Get the available template list:\n</code></pre> <pre><code>python -c \"import sys, json; d=json.load(sys.stdin)['K8sVersionedTemplates']['coreDNS']; print (json.dumps(d,indent=4))\" &lt;data.json\n</code></pre> <pre><code>\n</code></pre> <p>Output:</p> <pre><code>{\n    \"&gt;=1.16.0-alpha &lt;1.17.0-alpha\": \"coredns-v1.16\",\n    \"&gt;=1.17.0-alpha &lt;1.20.15-rancher1-2\": \"coredns-v1.17\",\n    \"&gt;=1.20.15-rancher1-2 &lt;1.21.0-rancher1-1\": \"coredns-v1.8.3-rancher2\",\n    \"&gt;=1.21.0-rancher1-1 &lt;1.21.9-rancher1-2\": \"coredns-v1.8.3\",\n    \"&gt;=1.21.9-rancher1-2\": \"coredns-v1.8.3-rancher2\",\n    \"&gt;=1.8.0-rancher0 &lt;1.16.0-alpha\": \"coredns-v1.8\"\n}\n</code></pre> <p>Translation of one of the entries from the list is as follows:</p> <pre><code>\"&gt;=1.21.9-rancher1-2\": \"coredns-v1.8.3-rancher2\",\n</code></pre> <pre><code>\n</code></pre> <p>If the Kubernetes version is greater than or equal to <code>1.21.9-rancher1-2</code> , then the CoreDNS key we have to use in the next step is <code>coredns-v1.8.3-rancher2.</code></p> <pre><code>python -c \"import sys, json; print (json.load(sys.stdin)['K8sVersionedTemplates']['templateKeys']['coredns-v1.8.3-rancher2'])\" &lt;data.json\n</code></pre> <pre><code>\n</code></pre> <p>The configmap will be printed on the screen along with other YAML template specs associated with CoreDNS.</p> <p>Sample output:</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n          lameduck 5s\n        }\n        ready\n        kubernetes {{.ClusterDomain}} {{ if .ReverseCIDRs }}{{ .ReverseCIDRs }}{{ else }}{{ \"in-addr.arpa ip6.arpa\" }}{{ end }} {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n    {{- if .UpstreamNameservers }}\n        forward . {{range $i, $v := .UpstreamNameservers}}{{if $i}} {{end}}{{.}}{{end}}\n    {{- else }}\n        forward . \"/etc/resolv.conf\"\n    {{- end }}\n        cache 30\n        loop\n        reload\n        loadbalance\n    } # STUBDOMAINS - Rancher specific change\n---\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020036/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020038/","title":"How to enable Envoy access logging in Rancher deployed Istio","text":"<p>This document (000020038) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020038/#environment","title":"Environment","text":"<ul> <li>Rancher 2.7 / 2.8</li> <li>A Kubernetes cluster Managed by Rancher</li> <li>Rancher Monitoring Stack (pre-requisite for Istio) and Rancher Istio</li> </ul>"},{"location":"kbs/000020038/#situation","title":"Situation","text":""},{"location":"kbs/000020038/#this-article-details-how-to-enable-envoys-access-logging-for-rancher-deployed-istio-in-rancher","title":"This article details how to enable Envoy's access logging , for Rancher deployed Istio, in Rancher.","text":"<p>Configuring Telemetry API is recommended to enable Access logging for Envoy.</p> <p>To enable access logging, perform the following steps:</p> <ol> <li>With a kubeconfig for the Downstream cluster, create the following Telemetry configuration:</li> </ol> <pre><code>apiVersion: telemetry.istio.io/v1alpha1\nkind: Telemetry\nmetadata:\n     name: mesh-default\n     namespace: istio-system\nspec:\n     accessLogging:\n    - providers:\n      - name: envoy\n</code></pre> <p>The above uses the default <code>envoy</code> access log provider and only the default settings are configured. A similar configuration can also be applied to an individual namespace, or to an individual workload, to control logging at a fine-grained level.\u00a0 For more information about using the Telemetry API, see the Telemetry API overview. 2. After enabling access logging, you can test the configuration with the Istio <code>sleep</code> and <code>httpbin</code> sample applications, per the Istio documentation.\u00a0 Some sample logs after enabling Envoy access logs:</p> <pre><code>kubectl logs -l app=sleep -c istio-proxy\n\n2024-07-10T07:58:09.579778Z     info    cache   returned workload trust anchor from cache       ttl=23h59m59.420226455s\n2024-07-10T07:58:09.580066Z     info    ads     SDS: PUSH request for node:sleep-78ff5975c6-d9zrk.default resources:1 size:1.1kB resource:ROOTCA\n2024-07-10T07:58:09.580236Z     info    cache   returned workload trust anchor from cache       ttl=23h59m59.419767231s\n2024-07-10T07:58:09.588579Z     info    cache   returned workload trust anchor from cache       ttl=23h59m59.411432603s\n2024-07-10T07:58:10.622873Z     info    Readiness succeeded in 1.46762116s\n2024-07-10T07:58:10.623674Z     info    Envoy proxy is ready\n[2024-07-10T07:58:46.673Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 14 13 \"-\" \"curl/8.8.0\" \"62ee6bdb-0afe-495c-83e0-37bc131045c3\" \"httpbin:8000\" \"10.42.1.24:8080\" outbound|8000||httpbin.default.svc.cluster.local 10.42.1.23:44318 10.43.156.59:8000 10.42.1.23:42030 - default\n[2024-07-10T07:58:53.227Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 10 9 \"-\" \"curl/8.8.0\" \"addf66ef-fb90-43bd-85e0-35763d7f0fca\" \"httpbin:8000\" \"10.42.1.24:8080\" outbound|8000||httpbin.default.svc.cluster.local 10.42.1.23:57372 10.43.156.59:8000 10.42.1.23:35794 - default\n[2024-07-10T08:01:56.359Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 23 22 \"-\" \"curl/8.8.0\" \"385df7b2-9e0a-4506-811d-95d15f1da3ee\" \"httpbin:8000\" \"10.42.1.24:8080\" outbound|8000||httpbin.default.svc.cluster.local 10.42.1.23:60376 10.43.156.59:8000 10.42.1.23:46754 - default\n[2024-07-10T08:01:57.741Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 11 10 \"-\" \"curl/8.8.0\" \"e0f34346-032a-4af3-a27c-a40f000e61e1\" \"httpbin:8000\" \"10.42.1.24:8080\" outbound|8000||httpbin.default.svc.cluster.local 10.42.1.23:35960 10.43.156.59:8000 10.42.1.23:48636 - default\n\nkubectl logs -l app=httpbin -c istio-proxy\n\n2024-07-10T07:58:16.693643Z     info    cache   returned workload certificate from cache        ttl=23h59m59.30636378s\n2024-07-10T07:58:16.693942Z     info    ads     SDS: PUSH request for node:httpbin-54b5c865df-pjjhm.default resources:1 size:4.0kB resource:default\n2024-07-10T07:58:17.883782Z     info    Readiness succeeded in 1.645029716s\n2024-07-10T07:58:17.884493Z     info    Envoy proxy is ready\n[2024-07-10T07:58:46.680Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 3 2 \"-\" \"curl/8.8.0\" \"62ee6bdb-0afe-495c-83e0-37bc131045c3\" \"httpbin:8000\" \"10.42.1.24:8080\" inbound|8080|| 127.0.0.6:42839 10.42.1.24:8080 10.42.1.23:44318 outbound_.8000_._.httpbin.default.svc.cluster.local default\n[2024-07-10T07:58:53.229Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 2 2 \"-\" \"curl/8.8.0\" \"addf66ef-fb90-43bd-85e0-35763d7f0fca\" \"httpbin:8000\" \"10.42.1.24:8080\" inbound|8080|| 127.0.0.6:54765 10.42.1.24:8080 10.42.1.23:57372 outbound_.8000_._.httpbin.default.svc.cluster.local default\n[2024-07-10T08:00:17.209Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 3 2 \"-\" \"curl/8.8.0\" \"0ea867e3-4e8c-4e45-9655-057d08989f1a\" \"httpbin:8000\" \"10.42.1.24:8080\" inbound|8080|| 127.0.0.6:54011 10.42.1.24:8080 10.42.1.23:57372 outbound_.8000_._.httpbin.default.svc.cluster.local default\n[2024-07-10T08:00:18.779Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 2 2 \"-\" \"curl/8.8.0\" \"3bedc3ca-345d-4fba-8269-6ce4ecb1e04b\" \"httpbin:8000\" \"10.42.1.24:8080\" inbound|8080|| 127.0.0.6:38727 10.42.1.24:8080 10.42.1.23:44318 outbound_.8000_._.httpbin.default.svc.cluster.local default\n[2024-07-10T08:01:56.366Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 4 3 \"-\" \"curl/8.8.0\" \"385df7b2-9e0a-4506-811d-95d15f1da3ee\" \"httpbin:8000\" \"10.42.1.24:8080\" inbound|8080|| 127.0.0.6:35665 10.42.1.24:8080 10.42.1.23:60376 outbound_.8000_._.httpbin.default.svc.cluster.local default\n[2024-07-10T08:01:57.745Z] \"GET /status/418 HTTP/1.1\" 418 - via_upstream - \"-\" 0 135 2 2 \"-\" \"curl/8.8.0\" \"e0f34346-032a-4af3-a27c-a40f000e61e1\" \"httpbin:8000\" \"10.42.1.24:8080\" inbound|8080|| 127.0.0.6:47731 10.42.1.24:8080 10.42.1.23:35960 outbound_.8000_._.httpbin.default.svc.cluster.local default\n</code></pre>"},{"location":"kbs/000020038/#further-reading","title":"Further reading","text":"<ul> <li>Istio \"Getting Envoy's Access Logs\" Documentation</li> <li>Envoy Access Logging Documentation</li> </ul>"},{"location":"kbs/000020038/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020039/","title":"How to collect a stacktrace from the Rancher server process in Rancher v2.x","text":"<p>This document (000020039) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020039/#environment","title":"Environment","text":"<p>A Rancher v2.x instance</p>"},{"location":"kbs/000020039/#situation","title":"Situation","text":""},{"location":"kbs/000020039/#during-troubleshooting-it-may-be-useful-to-collect-a-stacktrace-from-a-running-rancher-instance-and-this-article-details-the-steps-for-creating-one","title":"During troubleshooting it may be useful to collect a stacktrace from a running Rancher instance, and this article details the steps for creating one.","text":""},{"location":"kbs/000020039/#resolution","title":"Resolution","text":"<ol> <li> <p>Open two shell sessions and source an admin kubeconfig for the Rancher server cluster in both.</p> </li> <li> <p>In one of the shell sessions, determine the name (e.g. <code>rancher-7b9f4764f5-rs2jx</code>) of the Rancher leader pod per the steps in the article here.</p> </li> <li> <p>In the same shell, start streaming the Rancher leader pod's logs to file, replacing with the pod name identified in step 2.:</p> </li> </ol> <pre><code>kubectl logs -n cattle-system --tail=-1 -f &lt;LEADER_POD_NAME&gt; &gt; rancher.log\n</code></pre> <ol> <li>In the other shell, send a SIGABRT signal to the rancher process in the leader pod, with the command below, to trigger a stacktrace, replacing with the pod name identified in step 2.:</li> </ol> <pre><code>kubectl -n cattle-system exec &lt;LEADER_POD_NAME&gt; -- bash -c 'kill -SIGABRT  $(pgrep -x rancher)'\n</code></pre> <ol> <li>When the stacktrace generation is complete the Rancher leader pod will restart and the <code>kubectl logs</code> command in step 3. will exit. You can then provide the <code>rancher.log</code> file, containing the trace, to Rancher Support.</li> </ol>"},{"location":"kbs/000020039/#validating-the-success-of-the-stacktrace","title":"Validating the success of the stacktrace","text":"<p>If you want to validate the stacktrace was successfully generated, you can confirm the presence of the <code>SIGABRT</code> signal and <code>goroutine</code> traces in the <code>rancher.log</code> file as below:</p> <pre><code>SIGABRT: abort\nPC=0x461781 m=0 sigcode=0\n\ngoroutine 0 [idle]:\nruntime.futex(0x7370308, 0x80, 0x0, 0x0, 0xc000000000, 0x7ffdd633d8f0, 0x435863, 0xc000446848, 0x7ffdd633d910, 0x40b04f, ...)\n    /usr/local/go/src/runtime/sys_linux_amd64.s:535 +0x21\nruntime.futexsleep(0x7370308, 0x7ffd00000000, 0xffffffffffffffff)\n    /usr/local/go/src/runtime/os_linux.go:44 +0x46\nruntime.notesleep(0x7370308)\n    /usr/local/go/src/runtime/lock_futex.go:151 +0x9f\nruntime.stoplockedm()\n    /usr/local/go/src/runtime/proc.go:2068 +0x88\nruntime.schedule()\n    /usr/local/go/src/runtime/proc.go:2469 +0x485\nruntime.park_m(0xc000be4780)\n    /usr/local/go/src/runtime/proc.go:2610 +0x9d\nruntime.mcall(0x0)\n    /usr/local/go/src/runtime/asm_amd64.s:318 +0x5b\n</code></pre> <p>This should end with a section similar to the following after the goroutine traces:</p> <pre><code>rax    0xca\nrbx    0x73701c0\nrcx    0x461783\nrdx    0x0\nrdi    0x7370308\nrsi    0x80\nrbp    0x7ffdd633d8d8\nrsp    0x7ffdd633d890\nr8     0x0\nr9     0x0\nr10    0x0\nr11    0x286\nr12    0x0\nr13    0x1\nr14    0xc000dde7e0\nr15    0x0\nrip    0x461781\nrflags 0x286\ncs     0x33\nfs     0x0\ngs     0x0\n</code></pre>"},{"location":"kbs/000020039/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020039/#rancher-documentation-on-troubleshooting-the-rancher-server-kubernetes-cluster","title":"Rancher Documentation on Troubleshooting the Rancher Server Kubernetes Cluster","text":""},{"location":"kbs/000020039/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020040/","title":"Can I move from self-managed Rancher to Rancher Prime Hosted?","text":"<p>This document (000020040) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020040/#resolution","title":"Resolution","text":"<p>Yes, we can do a one-time migration of your entire self-hosted Rancher Manager into the Rancher Prime Hosted service. Requirements for migration are:</p> <ol> <li>You must be running the latest stable version of Rancher (currently v2.10.3) for at least one week.</li> <li>Rancher cluster must be running Kubernetes v1.28 or higher for at least one week.</li> <li>All managed/downstream clusters must be running Kubernetes v1.28 or higher for at least one week.</li> <li>Have a kubeconfig file for all your managed (downstream) Kubernetes clusters. The kubeconfig file should point directly to the cluster\u2019s kube-apiserver endpoint and not to Rancher. For RKE clusters, see Authorized Cluster Endpoint for setup.</li> <li>Authentication integration must be with a public endpoint that Rancher Prime Hosted can reach.</li> <li>Global and cluster catalogs must have a public endpoint that Rancher Prime Hosted can reach.</li> <li>Clusters provisioned using the vSphere node driver cannot be managed unless the vSphere API endpoint is made available to Rancher Prime Hosted.</li> <li>All downstream/managed clusters must have outbound Internet access over port 443, or at least outbound access to Rancher Prime Hosted.</li> <li>You must provide the output of Helm Chart values used to install Rancher: <code>helm get values -n cattle-system rancher &gt; rancher-values.yaml</code></li> <li>You must provide a copy of the system summary report for your Rancher environment. See this article for instructions.</li> </ol> <p>To get started on migrating to Rancher Prime Hosted, open a support case on our support portal.</p>"},{"location":"kbs/000020040/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020041/","title":"How can I validate network policies within a Kubernetes cluster?","text":"<p>This document (000020041) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020041/#environment","title":"Environment","text":"<ul> <li>A Kubernetes cluster v1.15.2+</li> <li>kubectl access to the cluster</li> <li>Network Plugin (CNI) which supports Network Policies</li> </ul>"},{"location":"kbs/000020041/#situation","title":"Situation","text":"<p>Traffic flowing inside a kubernetes cluster is non-isolated by default so that each pod can communicate with every other pod. In some environments there is a need to ensure the proper isolation or restriction of each application in differing namespaces. Network Policies handle this microservice network segmentation and meet this need by defining the control to other entities' IP addresses (on OSI Layer 3) or network ports (on OSI Layer 4).</p> <p>Access for Pod communication with other entities are identified with Network Policies and defined by other pods, the relevant namespaces, and IP CIDR Blocks. Pods and Namespaces are specified using a selector, like <code>app=example</code> .</p> <p>Once the Network Policies are defined, how can a Kubernetes administrator test and validate them?</p>"},{"location":"kbs/000020041/#resolution","title":"Resolution","text":"<p>Illuminatio is an open-source project written in Python3 by Inovex. It can run standalone, and is also available as a docker container. It creates test-cases for both the network policies and their inverse rules, generates an illuminatio-runner daemonset, tests all the cases against the defined network policies, and reports back on success or failure for each rule and inverted-rule. Illuminatio can use the current kubectl config for cluster access while working in the shell session, or designate the config file with the optional --kubeconfig flag.</p>"},{"location":"kbs/000020041/#basic-usage","title":"Basic Usage","text":"<p>Assuming the Kubernetes admin has some network policies to test, the tool is very easy to use. It has three verbs to choose from, \"clean\", \"generate\" and, \"run\". The generate verb will only generate the tests, while clean removes them and run performs the test. Most users will want to use <code>illuminatio clean run</code> to start fresh, run the generated tests and report on their success. The results are also written to a configmap.</p> <p>The following are some common examples of Network Policies, and how Illuminatio can assist with validation. Examples are taken from this network policies recipes github repo, and applied to a kubernetes cluster, in the default namespace. Validation is performed with Illuminatio instead of a temporary pod.</p>"},{"location":"kbs/000020041/#deny-traffic-to-an-application","title":"Deny Traffic to an application","text":"<p>Save this file as <code>web-deny-all.yaml</code> and then apply the network policy with <code>kubectl -f web-deny-all.yaml</code>. Notice it is deploying to the default namespace. Prepare the pod for this example with a selector of app=web.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: web-deny-all\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  ingress: []\n</code></pre> <pre><code>kubectl run --generator=run-pod/v1 web --image=nginx --labels app=web --expose --port 80\n</code></pre> <p>Show the current network policies, then run cases for all of them. Illuminatio will deploy a deamonset and run all the test cases, any passing tests show \"success\" in the last column of the report. Note: success indicates the test was successful, even if testing a connection denial.</p> <pre><code>$ kubectl get netpol\nNAME           POD-SELECTOR   AGE\nweb-deny-all   app=web        13m\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nGenerated 1 cases in 0.0616 seconds\n\nFROM             TO               PORT\ndefault:app=web  default:app=web  -*\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 1 tests in 7.1175 seconds\nFROM             TO               PORT  RESULT\ndefault:app=web  default:app=web  -*    success\n</code></pre>"},{"location":"kbs/000020041/#limit-traffic-to-an-application","title":"Limit Traffic to an application","text":"<p>Allow app=bookstore pods to communicate with only other app=bookstore pods.</p> <p><code>kubectl run --generator=run-pod/v1 apiserver --image=nginx --labels app=bookstore,role=api --expose --port 80</code></p> <p>Save the following as api-allow.yaml and issue kubectl apply -f api-allow.yaml.</p> <p>Network policies are accumulative.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: api-allow\nspec:\n  podSelector:\n    matchLabels:\n      app: bookstore\n      role: api\n  ingress:\n  - from:\n      - podSelector:\n          matchLabels:\n            app: bookstore\n</code></pre> <pre><code>$ kubectl apply -f api-allow.yaml\nnetworkpolicy.networking.k8s.io/api-allow created\n\n$ kubectl get netpol\nNAME           POD-SELECTOR             AGE\napi-allow      app=bookstore,role=api   5s\nweb-deny-all   app=web                  18m\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nGenerated 5 cases in 0.0594 seconds\n\nFROM                                                             TO                              PORT\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*\ndefault:app=web                                                  default:app=web                 -*\ndefault:app=bookstore                                            default:app=bookstore,role=api  *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 5 tests in 13.2368 seconds\nFROM                                                             TO                              PORT  RESULT\n\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*    success\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*    success\ndefault:app=web                                                  default:app=web                 -*    success\ndefault:app=bookstore                                            default:app=bookstore,role=api  *     success\n</code></pre>"},{"location":"kbs/000020041/#allow-whitelisted-traffic-for-appweb","title":"Allow whitelisted traffic for app=web","text":"<p>This policy will whitelist the app=web pods from the first example, with a new web-allow-all.yaml file. This Network Policy also voids the first example, by allowing all traffic. Because the traffic connections are allowed, Illuminatio recognizes this and avoids generating the negative (inverted) test cases.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: web-allow-all\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  ingress:\n  - {}\n</code></pre> <pre><code>$ kubectl apply -f web-allow-all.yaml\nnetworkpolicy.networking.k8s.io/web-allow-all created\n\n$ kubectl get netpol\nNAME            POD-SELECTOR             AGE\napi-allow       app=bookstore,role=api   20m\nweb-allow-all   app=web                  32s\nweb-deny-all    app=web                  39m\n</code></pre> <pre><code>$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nNot generating negative tests for host ClusterHost(namespace=default, podLabels={'app': 'web'})as all connecti\nons to it are allowed\nGenerated 5 cases in 0.0551 seconds\n\nFROM                                                             TO                              PORT\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*\ndefault:app=bookstore                                            default:app=bookstore,role=api  *\n*:*                                                              default:app=web                 *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 5 tests in 13.4065 seconds\nFROM                                                             TO                              PORT  RESULT\n\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*    success\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*    success\ndefault:app=bookstore                                            default:app=bookstore,role=api  *     success\n*:*                                                              default:app=web                 *     success\n</code></pre>"},{"location":"kbs/000020041/#limit-access-to-a-namespace","title":"Limit access to a Namespace","text":"<p>This policy will deny all traffic from other namespaces, limiting to just the current namespace. In other words, the <code>secondary</code> namespace allows connections internally, denying any from the <code>default</code> namespace in previous examples. Note how Illuimnatio tests all network policies, cluster-wide in all namespaces.</p> <pre><code>kubectl create namespace secondary\n\nkubectl run --generator=run-pod/v1 web --namespace secondary --image=nginx \\\n    --labels=app=web --expose --port 80\n</code></pre> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  namespace: secondary\n  name: deny-from-other-namespaces\nspec:\n  podSelector:\n    matchLabels:\n  ingress:\n  - from:\n    - podSelector: {}\n</code></pre> <pre><code>$ kubectl apply -f deny-from-other-namespaces.yaml\nnetworkpolicy \"deny-from-other-namespaces\" created\"\n\n$ kubectl get netpol -n secondary\nNAME                         POD-SELECTOR   AGE\ndeny-from-other-namespaces   &lt;none&gt;         7s\n\n$ kubectl get netpol -n default\nNAME            POD-SELECTOR             AGE\napi-allow       app=bookstore,role=api   44m\nweb-allow-all   app=web                  24m\nweb-deny-all    app=web                  63m\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nNot generating negative tests for host ClusterHost(namespace=default, podLabels={'app': 'web'})as all connecti\nons to it are allowed\nGenerated 7 cases in 0.0621 seconds\n\nFROM                                                             TO                              PORT\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*\nilluminatio-inverted-secondary:*                                 secondary:*                     -*\ndefault:app=bookstore                                            default:app=bookstore,role=api  *\n*:*                                                              default:app=web                 *\nsecondary:*                                                      secondary:*                     *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 7 tests in 13.2361 seconds\nFROM                                                             TO                              PORT  RESULT\n\nilluminatio-inverted-default:illuminatio-inverted-app=bookstore  default:app=bookstore,role=api  -*    success\ndefault:illuminatio-inverted-app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-default:app=bookstore                       default:app=bookstore,role=api  -*    success\nilluminatio-inverted-secondary:*                                 secondary:*                     -*    success\ndefault:app=bookstore                                            default:app=bookstore,role=api  *     success\n*:*                                                              default:app=web                 *     success\nsecondary:*                                                      secondary:*                     *     success\n</code></pre>"},{"location":"kbs/000020041/#allow-all-traffic-from-a-certain-namespace","title":"Allow All Traffic from a certain Namespace","text":"<p>In this example, there are two namespaces, <code>dev</code> with purpose=testing and <code>prod</code> with purpose=production. The <code>default</code> namespace should allow connections from <code>production</code> but not <code>dev</code>. This is convenient for establishing policies along namespace boundaries. All previous network policies have been removed for this scenario.</p> <pre><code>kubectl run --generator=run-pod/v1 web --image=nginx \\\n    --labels=app=web --expose --port 80\n\nkubectl create namespace dev\nkubectl label namespace/dev purpose=testing\n\nkubectl create namespace prod\nkubectl label namespace/prod purpose=production\n</code></pre> <p>The contents of the web-allow-prod.yaml file.</p> <pre><code>kind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: web-allow-prod\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          purpose: production\n</code></pre> <pre><code>$ kubectl apply -f web-allow-prod.yaml\nnetworkpolicy \"web-allow-prod\" created\n\n$ kubectl get netpol -A\nNAMESPACE   NAME             POD-SELECTOR   AGE\ndefault     web-allow-prod   app=web        44s\n\n$ illuminatio clean run\nStarting cleaning resources with policies ['on-request', 'always']\nFinished cleanUp\nStarting test generation and run.\nGenerated 2 cases in 0.0645 seconds\n\nFROM                                       TO               PORT\nilluminatio-inverted-purpose=production:*  default:app=web  -*\npurpose=production:*                       default:app=web  *\n\nEnsure that Pods of DaemonSet illuminatio-runner are ready\n\nFinished running 2 tests in 7.1767 seconds\nFROM                                       TO               PORT  RESULT\nilluminatio-inverted-purpose=production:*  default:app=web  -*    success\npurpose=production:*                       default:app=web  *     success\n</code></pre> <p>To view the results of the test programmatically, check the configmap for the illuminatio namespace, before performing another \"clean\" operation.</p> <pre><code>$ kubectl get cm -n illuminatio\nNAME                               DATA   AGE\nilluminatio-cases-cfgmap           1      45s\nilluminatio-runner-s87rw-results   2      40s\nilluminatio-runner-z52gb-results   2      41s\n\n$ kubectl get cm -n illuminatio illuminatio-runner-s87rw-results -o yaml\napiVersion: v1\ndata:\n  results: |\n    illuminatio-inverted-purposeproduction:illuminatio-dummy-nqtc7:\n      10.43.168.221:\n        '-80':\n          nmap-state: filtered\n          string: 'Test 10.43.168.221:-80 succeeded\n\n            Couldn''t reach 10.43.168.221 on port 80. Expected target to not be reachable'\n          success: true\n    prod:illuminatio-dummy-tc5v9:\n      10.43.168.221:\n        '80':\n          nmap-state: open\n          string: 'Test 10.43.168.221:80 succeeded\n\n            Could reach 10.43.168.221 on port 80. Expected target to be reachable'\n          success: true\n  runtimes: |\n    overall: error\n    tests:\n      illuminatio-inverted-purposeproduction:illuminatio-dummy-nqtc7:\n        10.43.168.221: 2.1185858249664307\n      prod:illuminatio-dummy-tc5v9:\n        10.43.168.221: 0.2853882312774658\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2020-11-23T21:47:53Z\"\n  labels:\n    illuminatio-cleanup: always\n  managedFields:\n  - apiVersion: v1\n    fieldsType: FieldsV1\n    fieldsV1:\n      f:data:\n        .: {}\n        f:results: {}\n        f:runtimes: {}\n      f:metadata:\n        f:labels:\n          .: {}\n          f:illuminatio-cleanup: {}\n    manager: Swagger-Codegen\n    operation: Update\n    time: \"2020-11-23T21:47:53Z\"\n  name: illuminatio-runner-s87rw-results\n  namespace: illuminatio\n  resourceVersion: \"906614\"\n  selfLink: /api/v1/namespaces/illuminatio/configmaps/illuminatio-runner-s87rw-results\n  uid: 2c2f7434-d1ee-49c0-b77d-c11b7848f4da\n</code></pre>"},{"location":"kbs/000020041/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020041/#further-reading-and-other-useful-links","title":"Further Reading and Other Useful Links","text":"<ul> <li>Securing Kubernetes Cluster Networking</li> <li>Example Network Policy Recipes, ahmetb/kubernetes-network-policy-recipes</li> <li>Inovex Illuminatio, Kubernetes Network Policy Validator</li> <li>Inovex/Illuminatio GitHub Project Page</li> </ul>"},{"location":"kbs/000020041/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020042/","title":"Rancher v2.x provisioned vSphere cluster nodes stuck in provisioning, with \"Waiting for SSH to be available\", as a result of pre-existing cloud-init configuration in VM Template","text":"<p>This document (000020042) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020042/#environment","title":"Environment","text":"<p>A Rancher v2.x provisioned vSphere cluster, using the vSphere Node Driver.</p>"},{"location":"kbs/000020042/#situation","title":"Situation","text":""},{"location":"kbs/000020042/#upon-launching-a-vsphere-node-driver-cluster-in-rancher-v2x-nodes-within-the-cluster-are-stuck-in-provisioning-with-the-message-waiting-for-ssh-to-be-available-logging-into-the-nodes-via-ssh-and-checking-the-auth-log-directly-reveals-failed-ssh-connection-attempts-for-a-missing-docker-user","title":"Upon launching a vSphere Node Driver cluster in Rancher v2.x, nodes within the cluster are stuck in provisioning, with the message <code>Waiting for SSH to be available</code>. Logging into the nodes via SSH and checking the auth log directly reveals failed SSH connection attempts for a missing <code>docker</code> user.","text":""},{"location":"kbs/000020042/#resolution","title":"Resolution","text":"<p>Convert the Template back to a VM and run:</p> <pre><code>sudo cloud-init clean\n</code></pre> <p>This command will clean the Template of any existing cloud-inits, once complete you can convert the VM back to a template to try again.</p>"},{"location":"kbs/000020042/#cause","title":"Cause","text":"<p>When provisioning a vSphere Node Driver cluster Rancher v2.x uses cloud-init to generate an ssh-keypair for the user <code>docker</code> and copy this into the Virtual Machine on initial boot.</p> <p>In some Linux distributions, including Ubuntu Server 18.04, the standard OS installation process generates a cloud-init configuration. Installation of the OS is performed during the intitial setup of the VM Templates, prior to cluster provisioning via Rancher, and this existing cloud-init configuration within the Template can intefere with Rancher's ability to insert its own cloud-init.</p>"},{"location":"kbs/000020042/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020046/","title":"Resolving \"Conflict. The container name is already in use\" errors when updating a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020046) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020046/#situation","title":"Situation","text":""},{"location":"kbs/000020046/#issue","title":"Issue","text":"<p>When invoking <code>rke up</code>, to update a Rancher Kubernetes Engine (RKE) CLI provisioned Kubernetes cluster, or using Rancher to update a Rancher v2.x provisioned cluster, you encounter a <code>Conflict. The container name is already in use</code> error, of the following format:</p> <p>Format of error from RKE CLI, for RKE provisioned clusters</p> <pre><code>FATA[0219] [file-deploy] Failed to deploy file [/etc/kubernetes/audit-policy.yaml] on node [172.27.6.22]: Failed to create [file-deployer] container on host [172.27.6.22]: Failed to create Docker container [file-deployer] on host [172.27.6.22]: Error response from daemon: Conflict. The container name \"/file-deployer\" is already in use by container \"66b777d981aa0b0a9d6bc73e381e0f2bc8fc33ec00926aa0db51347607f8fcf8\". You have to remove (or rename) that container to be able to reuse that name.\n</code></pre> <p>Format of error in Rancher UI, for Rancher v2.x provisioned clusters</p> <pre><code>[Failed to create Certificates deployer container on host [172.27.3.21]: Failed to create Docker container [cert-deployer] on host [172.27.3.21]: Error response from daemon: Conflict. The container name \"/cert-deployer\" is already in use by container \"c3b35c454d6000266098573949d021f45b13a3c7f7306d7fdb58a5766f2f3312\". You have to remove (or rename) that container to be able to reuse that name.]\n</code></pre>"},{"location":"kbs/000020046/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI, or Rancher v2.x provisioned Kubernetes cluster</li> <li>SSH access to affected node(s) and Docker CLI access</li> </ul>"},{"location":"kbs/000020046/#root-cause","title":"Root cause","text":"<p>This error can occur when an <code>rke up</code> invocation or Rancher cluster update process is interrupted, leaving temporary cluster deployment containers on nodes.</p>"},{"location":"kbs/000020046/#resolution","title":"Resolution","text":"<ol> <li>SSH into the node.</li> <li>Remove stuck containers:</li> </ol> <pre><code>docker rm -f file-deployer cert-deployer\n</code></pre> <ol> <li>Trigger a cluster update:</li> <li>For RKE CLI provisioned clusters, invoke <code>rke up</code>.</li> <li>For Rancher provisioned clusters:<ul> <li>Browse to the cluster in the Rancher UI</li> <li>Click <code>Edit</code> from the action menu</li> <li>Click <code>Edit as YAML</code></li> <li>Find the option <code>addon_job_timeout</code> and edit the value, incrementing it by one</li> <li>Click <code>Save</code></li> </ul> </li> </ol>"},{"location":"kbs/000020046/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020047/","title":"How to disable Calico Telemetry in a Kubernetes cluster with the Canal or Calico CNI","text":"<p>This document (000020047) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020047/#environment","title":"Environment","text":"<p>A Kubernetes cluster, provisioned with the Calico or Canal CNI</p> <p>kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user</p>"},{"location":"kbs/000020047/#situation","title":"Situation","text":""},{"location":"kbs/000020047/#by-default-calico-reports-anonymous-telemetry-data-containing-the-calico-version-number-and-cluster-size-to-an-endpoint-at-projectcalicoorg-this-article-provides-details-on-how-to-disable-this-reporting","title":"By default, Calico reports anonymous telemetry data, containing the Calico version number and cluster size, to an endpoint at projectcalico.org.  This article provides details on how to disable this reporting.","text":""},{"location":"kbs/000020047/#resolution","title":"Resolution","text":"<p>To disable the Calico telemetry reporting, execute the following command against the cluster:</p> <pre><code>kubectl patch felixconfigurations.crd.projectcalico.org default  -p '{\"spec\":{\"usageReportingEnabled\": false}}' --type=merge\n</code></pre>"},{"location":"kbs/000020047/#additional-information","title":"Additional Information","text":"<p>Calico configuration documentation</p>"},{"location":"kbs/000020047/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020048/","title":"How to increase the inotify.max_user_watches and inotify.max_user_instances sysctls on a Linux host","text":"<p>This document (000020048) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020048/#environment","title":"Environment","text":"<p>Linux host</p>"},{"location":"kbs/000020048/#situation","title":"Situation","text":"<p>The sysctls <code>fs.inotify.max_user_instances</code> and <code>fs.inotify.max_user_watches</code> define user limits on the number of inotify resources and inotify file watches. If these limits are reached, you may experience processes failing with error messages related to the limits, for example:</p> <pre><code>ENOSPC: System limit for number of file watchers reached...\n</code></pre> <pre><code>The configured user limit (128) on the number of inotify instances has been reached\n</code></pre> <pre><code>The default defined inotify instances (128) has been reached\n</code></pre> <p>In the context of a Kubernetes cluster, this behaviour would exhibit as failing Pods, with inotify related errors in the Pod logs similar to the above. This article details how to check the current limits configured and how to increase these.</p>"},{"location":"kbs/000020048/#resolution","title":"Resolution","text":""},{"location":"kbs/000020048/#check-current-limits","title":"Check current limits","text":"<p>You can check the current inotify user instance limit, with the following:</p> <pre><code>cat /proc/sys/fs/inotify/max_user_instances\n</code></pre> <p>Similarly, the current inotify user watch limit can be checked as follows:</p> <pre><code>cat /proc/sys/fs/inotify/max_user_watches\n</code></pre>"},{"location":"kbs/000020048/#update-the-limits","title":"Update the limits","text":"<p>You can update the limits temporarily, with the following commands (setting the values to 8192 and 524288 respectively in this example):</p> <pre><code>sudo sysctl fs.inotify.max_user_instances=8192\nsudo sysctl fs.inotify.max_user_watches=524288\nsudo sysctl -p\n</code></pre> <p>In order to make the changes permanent, i.e. to persist a reboot, you can set <code>fs.inotify.max_user_instances=8192</code> and <code>fs.inotify.max_user_watches=524288</code> in the file <code>/etc/sysctl.conf</code>.</p> <p>After updating the limits, you can validate these on the host again, as above, with <code>cat /proc/sys/fs/inotify/max_user_instances</code> and <code>cat /proc/sys/fs/inotify/max_user_watches</code>.</p> <p>To check the value as reflected in a running container, exec into the container and cat the files:</p> <pre><code>docker exec -it &lt;CONTAINER ID&gt; cat /proc/sys/fs/inotify/max_user_instances\n</code></pre> <p>and</p> <pre><code>docker exec -it &lt;CONTAINER ID&gt; cat /proc/sys/fs/inotify/max_user_watches\n</code></pre> <p>If the updated limits are not reflected on a host after running <code>sysctl -p</code>, reboot the host after setting the limits in <code>/etc/sysctl.conf</code>.</p>"},{"location":"kbs/000020048/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020051/","title":"[JP] How to configure container log rotation for the Docker daemon","text":"<p>This document (000020051) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020051/#situation","title":"Situation","text":""},{"location":"kbs/000020051/#_1","title":"\u80cc\u666f","text":"<p>Docker\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u3067\u306f\u3001json-file\u30ed\u30b0\u30c9\u30e9\u30a4\u30d0\u30fc\u3092\u4f7f\u7528\u3057\u3066\u5236\u9650\u306a\u3057\u3067\u30b3\u30f3\u30c6\u30ca\u30fc\u30ed\u30b0\u3092\u8a18\u9332\u3059\u308b\u305f\u3081\u3001\u30ce\u30fc\u30c9\u3067disk-fill events\u304c\u767a\u751f\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u306e\u8a18\u4e8b\u3067\u306f\u3001Docker\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9\u3092\u69cb\u6210\u3057\u3066\u3001\u30b3\u30f3\u30c6\u30ca\u30fc\u30ed\u30b0\u30b5\u30a4\u30ba\u3092\u5236\u9650\u3057\u3001\u53e4\u3044\u30b3\u30f3\u30c6\u30ca\u30fc\u30ed\u30b0\u3092\u30ed\u30fc\u30c6\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u624b\u9806\u306b\u3064\u3044\u3066\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020051/#_2","title":"\u4e8b\u524d\u6e96\u5099","text":"<ul> <li>json-file\u30ed\u30b0\u30c9\u30e9\u30a4\u30d0\u30fc\u3092\u4f7f\u7528\u3057\u3066Docker\u3092\u5b9f\u884c\u3057\u3066\u3044\u308b\u30ce\u30fc\u30c9</li> <li><code>/etc/docker/daemon.json</code> \u3092\u7de8\u96c6\u3057\u3001Docker\u30c7\u30fc\u30e2\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u6a29\u9650</li> </ul>"},{"location":"kbs/000020051/#_3","title":"\u6ce8\u610f","text":"<ul> <li>\u65b0\u3057\u304f\u4f5c\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30ca\u306b\u5909\u66f4\u3092\u6709\u52b9\u306b\u3059\u308b\u306b\u306f\u3001Docker\u30c7\u30fc\u30e2\u30f3\u3092\u518d\u8d77\u52d5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> <li>\u91cd\u8981\u3000\u65e2\u5b58\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30b3\u30f3\u30c6\u30ca\u30ed\u30b0\u8a2d\u5b9a\u306f\u5909\u66f4\u306a\u3057\u306e\u305f\u3081\u3001\u65e2\u5b58\u306e\u30b3\u30f3\u30c6\u30ca\u306f\u81ea\u52d5\u7684\u306b\u65b0\u3057\u3044\u30ed\u30b0\u8a2d\u5b9a\u3092\u9069\u7528\u3055\u308c\u307e\u305b\u3093\u3002\u65b0\u3057\u3044\u8a2d\u5b9a\u3092\u4f7f\u3046\u306b\u306f\u30b3\u30f3\u30c6\u30ca\u3092\u518d\u30c7\u30d7\u30ed\u30a4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002</li> </ul>"},{"location":"kbs/000020051/#_4","title":"\u89e3\u6c7a\u65b9\u6cd5","text":"<ol> <li>Docker\u30c7\u30fc\u30e2\u30f3\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7de8\u96c6\u3057\u307e\u3059\uff1a</li> </ol> <pre><code>$ vim /etc/docker/daemon.json\n</code></pre> <ol> <li>\u6b21\u306e\u5185\u5bb9\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u8ffd\u52a0\u3057\u3066\u3001\u6700\u5927\u30b3\u30f3\u30c6\u30ca\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u309210MB\u306b\u3001\u30ed\u30fc\u30c6\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u6700\u591a\u306e\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u309210\u500b\u306b\u8a2d\u5b9a\u3057\u307e\u3059\u3002</li> </ol> <pre><code>{\n     \"log-driver\": \"json-file\",\n     \"log-opts\": {\n       \"max-size\": \"10m\",\n       \"max-file\": \"10\"\n     }\n}\n</code></pre> <ol> <li>docker\u30c7\u30fc\u30e2\u30f3\u3092\u518d\u8d77\u52d5\u3057\u3066\u3001\u8a2d\u5b9a\u3092\u65b0\u3057\u3044\u30b3\u30f3\u30c6\u30ca\u30fc\u306b\u9069\u7528\u3057\u307e\u3059\uff08\u4e0a\u8a18\u306e \u91cd\u8981 \u6ce8\u610f\u3092\u53c2\u7167\uff09\u3002</li> </ol> <pre><code>$ systemctl restart docker\n</code></pre>"},{"location":"kbs/000020051/#_5","title":"\u30d2\u30f3\u30c8","text":"<p>\u3053\u306e\u30ed\u30b0\u30ed\u30fc\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8a2d\u5b9a\u3092\u30d3\u30eb\u30c9/\u69cb\u6210\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306b\u5c0e\u5165\u3059\u308b\u3068\u3001\u30ce\u30fc\u30c9\u3092\u30d7\u30ed\u30d3\u30b8\u30e7\u30cb\u30f3\u30b0\u6642\u306b\u81ea\u52d5\u7684\u306b\u9069\u7528\u3055\u308c\u3001\u624b\u52d5\u3067\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u306a\u304f\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"kbs/000020051/#_6","title":"\u53c2\u8003","text":"<p>Docker JSON file log driver documentation</p>"},{"location":"kbs/000020051/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020052/","title":"How to create a custom cluster role in Rancher v2.x to grant permission on the metrics endpoint of the kube-apiserver in a Rancher managed cluster","text":"<p>This document (000020052) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020052/#environment","title":"Environment","text":"<ul> <li>A Rancher v2.x instance</li> <li>A Rancher-managed Kubernetes cluster, with Metrics Server deployed. This is deployed by default in Rancher-provisioned clusters.</li> </ul>"},{"location":"kbs/000020052/#situation","title":"Situation","text":""},{"location":"kbs/000020052/#this-article-details-how-to-create-a-cluster-role-to-grant-users-access-to-the-metrics-endpoint-of-the-kubernetes-api-server-in-rancher-managed-kubernetes-clusters","title":"This article details how to create a cluster role to grant users access to the <code>/metrics</code> endpoint of the Kubernetes API Server, in Rancher-managed Kubernetes clusters.","text":"<p>In Rancher v2.4.x - v2.7.x it should be possible to define a non-resource URL grant via role creation within the Rancher UI. However, this was affected by the issue tracked in\u00a0Issue #30321, and use of the Rancher v3 API is therefore required to create the role. This bug was solved in Rancher v2.8.0. Then, since that version, it is possible to create the role using the UI or the Rancher API, as explained here.</p> <ul> <li>Rancher v2.4.x - v2.7.x: creating the role is only possible using the Rancher API.</li> <li>Rancher 2.8.x and above: it is possible to create the role using the Rancher API or the Rancher UI.</li> </ul>"},{"location":"kbs/000020052/#resolution","title":"Resolution","text":"<ol> <li>The first step is to create a custom cluster role within Rancher, that grants <code>get</code> permission on the non-resource URL <code>/metrics</code> endpoint.</li> </ol> <p>As an admin user, generate an un-scoped Rancher API token, and execute the following API request via cURL, to create the required role. You will need to set CATTLE_ACCESS_KEY, CATTLE_SECRET_KEY and RANCHER_URL to reflect the generated API token and your Rancher URL. You can also edit the role name, as desired, which is set to <code>kube-api metrics</code> in this example.</p> <pre><code>export CATTLE_ACCESS_KEY=token-8jn92\nexport CATTLE_SECRET_KEY=l2r4nq9sx6pdhpm4bgwntvgk49qn6rvvmtsvlvkmjk9rjsfd7n65fz\nexport RANCHER_URL=rancher.example.com\ncurl -k -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n   -X POST \\\n   -H 'Accept: application/json' \\\n   -H 'Content-Type: application/json' \\\n   -d '{\"context\":\"cluster\",\"clusterCreatorDefault\":false,\" projectCreatorDefault\":false,\"name\":\"kube-api metrics\",\"rules\":[{\"nonResourceURLs\":[\"/metrics\"],\"type\":\"/v3/schemas/policyRule\",\"verbs\":[\"get\"]}]}' \\\n\"https://${RANCHER_URL}/v3/roletemplates\"\n</code></pre> <ol> <li> <p>After creating the cluster role, you can then grant this for a user or group. To do so, follow the steps in the Rancher documentation on assigning a cluster role to a user or group.</p> </li> <li> <p>Once the role is granted to a user, they will be able to test their access to the <code>/metrics</code> endpoint.</p> </li> </ol> <p>The user can access the endpoint, with the applicable cluster ID, via the Rancher proxied Kubernetes API Server endpoint, by generating a cluster-scoped or un-scoped API token. The user will need to set CATTLE_ACCESS_KEY, CATTLE_SECRET_KEY, RANCHER_URL and CLUSTER_ID to reflect the generated API token, Rancher URL and cluster ID.</p> <pre><code>export CATTLE_ACCESS_KEY=token-8jn92\nexport CATTLE_SECRET_KEY=l2r4nq9sx6pdhpm4bgwntvgk49qn6rvvmtsvlvkmjk9rjsfd7n65fz\nexport RANCHER_URL=rancher.example.com\nexport CLUSTER_ID=c-wwdjc\ncurl -k https://${RANCHER_URL}/k8s/clusters/${CLUSTER_ID}/metrics \\\n   -H \"Authorization: Bearer ${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\"\n</code></pre> <p>For Rancher-provisioned Kubernetes clusters with Authorized Cluster Endpoint enabled, the user can also query the endpoint by connecting to the Kubernetes API Server on the cluster's control plane nodes directly, using a cluster-scoped API token. The user will need to set CATTLE_ACCESS_KEY, CATTLE_SECRET_KEY and AUTHORIZED_ENDPOINT_ADDRESS to reflect the generated API token, and the authorized endpoint address.</p> <pre><code>export CATTLE_ACCESS_KEY=token-d6cls\nexport CATTLE_SECRET_KEY=b6gk6lmgrhsb4rjccktzkwxn5df7tm87msggq87lpmls2pkbpc5t5r\nexport AUTHORIZED_ENDPOINT_ADDRESS=controlplane-01.example.com\ncurl -k https://${AUTHORIZED_ENDPOINT_ADDRESS}:6443/metrics \\\n   -H \"Authorization: Bearer ${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\"\n</code></pre>"},{"location":"kbs/000020052/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020052/#further-reading","title":"Further reading","text":"<ul> <li>Rancher documentation on custom roles</li> </ul>"},{"location":"kbs/000020052/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020054/","title":"[JP] How to grant users access to Grafana with minimal permissions","text":"<p>This document (000020054) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020054/#situation","title":"Situation","text":""},{"location":"kbs/000020054/#_1","title":"\\* \u3053\u306e\u6587\u66f8\u306f\u5ec3\u6b62\u3055\u308c\u3066\u3044\u307e\u3059 *","text":""},{"location":"kbs/000020054/#monitoring-v2-view-monitoring-system-rbac","title":"Monitoring v2 \u306b\u306f \"View Monitoring\" \u30ed\u30fc\u30eb\u304c\u8ffd\u52a0\u3055\u308c\u3001 System \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u30e6\u30fc\u30b6\u30fc\u306b\u4ed8\u4e0e\u3067\u304d\u307e\u3059\u3002\u3053\u306e\u30ed\u30fc\u30eb\u306b\u3088\u308a\u30e6\u30fc\u30b6\u30fc\u306f\u53c2\u7167\u30a2\u30af\u30bb\u30b9\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u308c\u306b\u4f34\u3044\u5f53\u6587\u66f8\u306f\u4eca\u5f8c\u30e1\u30f3\u30c6\u30ca\u30f3\u30b9\u3055\u308c\u307e\u305b\u3093\u306e\u3067\u3001 \u3053\u3061\u3089 \u306e RBAC \u306b\u3064\u3044\u3066\u306e\u6587\u66f8\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002","text":""},{"location":"kbs/000020054/#_2","title":"\u30bf\u30b9\u30af","text":"<p>Kubernetes\u30af\u30e9\u30b9\u30bf\u5185\u306e\u30af\u30e9\u30b9\u30bf\u76e3\u8996\u3068Grafana\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3059\u308b\u305f\u3081\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u306b\u5f93\u3063\u3066\u3001\u65b0\u3057\u3044\u30e6\u30fc\u30b6\u30fc\u3092\u4f5c\u6210\u3057\u3001\u6700\u5c0f\u9650\u306e\u6a29\u9650\u3092\u4ed8\u4e0e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020054/#_3","title":"\u4e8b\u524d\u6e96\u5099","text":"<ul> <li> <p>Rancher v2.x</p> </li> <li> <p>\u30af\u30e9\u30b9\u30bf\u306eMonitoring\u304cenabled\u3067\u3042\u308b</p> </li> </ul>"},{"location":"kbs/000020054/#_4","title":"\u80cc\u666f","text":"<p>\u3042\u308b\u30e6\u30fc\u30b6\u306b\u30af\u30e9\u30b9\u30bf\u76e3\u8996\u306e\u30e1\u30c8\u30ea\u30af\u30b9\u3084\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3059\u308b\u6a29\u9650\u3092\u4ed8\u4e0e\u3057\u305f\u3044\u304c\u3001\u305d\u306e\u30e6\u30fc\u30b6\u304c\u4ed6\u306e\u60c5\u5831\u3092\u898b\u305f\u308a\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30a2\u30af\u30b7\u30e7\u30f3\u3092\u5b9f\u884c\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3088\u3046\u306b\u3057\u305f\u3044\u6642\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3053\u306e\u30ca\u30ec\u30c3\u30b8\u30fc\u30d9\u30fc\u30b9\u3067\u306f\u3001\u3053\u308c\u3092\u5b9f\u73fe\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"kbs/000020054/#_5","title":"\u89e3\u6c7a\u65b9\u6cd5","text":"<p>\u30e6\u30fc\u30b6\u304c\u307e\u3060\u4f5c\u6210\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001Rancher \u3067\u65b0\u3057\u3044\u306e\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30b0\u30ed\u30fc\u30d0\u30eb\u30d3\u30e5\u30fc\u306b\u79fb\u52d5\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u30e1\u30cb\u30e5\u30fc\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\u53f3\u4e0a\u306e <code>\u30e6\u30fc\u30b6\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u540d\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3001\u8868\u793a\u540d\u3092\u9078\u629e\u3057\u307e\u3059\u3002 <code>\u30b0\u30ed\u30fc\u30d0\u30eb\u6a29\u9650</code> \u3067\u306f User-Base \u3092\u9078\u629e\u3057\u3001 <code>\u30ab\u30b9\u30bf\u30e0</code> \u306f\u3059\u3079\u3066\u30c1\u30a7\u30c3\u30af\u3092\u5916\u3057\u305f\u307e\u307e\u306b\u3057\u307e\u3059\u3002\u30d5\u30a9\u30fc\u30e0\u306e\u4e0b\u90e8\u306b\u3042\u308b <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3002\u3053\u3053\u3067\u306f\u3001\u30e6\u30fc\u30b6\u540d <code>johndoe</code> \u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3068\u3057\u307e\u3059\u3002</p> <p>\u300c\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u300d\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u300c\u30ed\u30fc\u30eb\u300d\u3001\u3055\u3089\u306b\u300cProjects\u300d\u30bf\u30d6\u3092\u9078\u629e\u3057\u3001 <code>Project \u30ed\u30fc\u30eb\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002 <code>\u540d\u524d</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u300cServices Proxy\u300d\u3068\u5165\u529b\u3057\u307e\u3059\u3002 <code>Grant Resources</code> \u306e\u4e0b\u3067\u3001 <code>+\u30ea\u30bd\u30fc\u30b9\u306e\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002 <code>\u53d6\u5f97</code> \u3068 <code>\u30ea\u30b9\u30c8</code> \u30dc\u30c3\u30af\u30b9\u306b\u30c1\u30a7\u30c3\u30af\u3092\u5165\u308c\u3001 <code>\u30ea\u30bd\u30fc\u30b9</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306b services/proxy\u3068\u5165\u529b\u3057\u307e\u3059\u3002\u3053\u308c\u304c\u81ea\u52d5\u7684\u306b <code>serivces/proxy (Custom)</code> \u306b\u5909\u66f4\u3055\u308c\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4e0b\u90e8\u306e <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u3066\u65b0\u3057\u3044\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30ed\u30fc\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002</p> <p>\u6b21\u306b\u3001\u30af\u30e9\u30b9\u30bf\u306e\u30af\u30e9\u30b9\u30bf\u30d3\u30e5\u30fc\u3092\u958b\u304d\u3001\u30e1\u30cb\u30e5\u30fc\u304b\u3089\u300c\u30e1\u30f3\u30d0\u30fc\u300d\u3092\u9078\u629e\u3057\u307e\u3059\u3002\u53f3\u4e0a\u306e <code>\u30e1\u30f3\u30d0\u30fc\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002\u30e1\u30f3\u30d0\u30fc\u306e\u30c9\u30ed\u30c3\u30d7\u30c0\u30a6\u30f3\u3067 <code>johndoe</code> \u3092\u9078\u629e\u3057\u3001\u30af\u30e9\u30b9\u30bf\u6a29\u9650\u306e <code>\u30e1\u30f3\u30d0\u30fc</code> \u3092\u9078\u629e\u3059\u308b\u3002\u30d5\u30a9\u30fc\u30e0\u306e\u4e0b\u90e8\u306b\u3042\u308b <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3002</p> <p>\u30af\u30e9\u30b9\u30bf\u306e <code>System</code> \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u79fb\u52d5\u3057\u3001 <code>\u30e1\u30f3\u30d0\u30fc</code> \u30e1\u30cb\u30e5\u30fc\u304b\u3089 <code>\u30e1\u30f3\u30d0\u30fc\u3092\u8ffd\u52a0</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002 <code>\u30e1\u30f3\u30d0</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u306b johndoe \u3068\u5165\u529b\u3057\u3001 <code>Project \u6a29\u9650</code> \u3067 <code>Services Proxy</code> \u3092\u9078\u629e\u3057\u307e\u3059\u3002\u30d5\u30a9\u30fc\u30e0\u306e\u4e0b\u90e8\u306b\u3042\u308b <code>\u4f5c\u6210</code> \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u307e\u3059\u3002</p> <p>\u3053\u308c\u3067\u3001 <code>johndoe</code> \u30e6\u30fc\u30b6\u30fc\u306f Rancher \u306b\u30ed\u30b0\u30a4\u30f3\u3057\u3001Grafana \u30a2\u30a4\u30b3\u30f3\u304c\u8868\u793a\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002Grafana\u30a2\u30a4\u30b3\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3068\u3001\u65b0\u3057\u3044\u30d6\u30e9\u30a6\u30b6\u30a6\u30a3\u30f3\u30c9\u30a6\u304c\u958b\u304d\u3001\u30af\u30e9\u30b9\u30bf\u306e\u69d8\u3005\u306a\u30b0\u30e9\u30d5\u3084\u7d71\u8a08\u60c5\u5831\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u30e6\u30fc\u30b6\u30fc\u306f\u3001\u30af\u30e9\u30b9\u30bf\u5185\u306e\u65b0\u3057\u3044\u30ef\u30fc\u30af\u30ed\u30fc\u30c9\u306e\u8868\u793a\u3084\u8d77\u52d5\u306a\u3069\u306e\u4ed6\u306e\u64cd\u4f5c\u3092\u884c\u3046\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u3002</p>"},{"location":"kbs/000020054/#_6","title":"\u53c2\u8003","text":"<p>Rancher\u3084Kubernetes\u3067\u306eRBAC\u306e\u52d5\u4f5c\u65b9\u6cd5\u306e\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u30ea\u30f3\u30af\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ul> <li> <p>Role-Based Access Control (RBAC) in Rancher</p> </li> <li> <p>Using RBAC Authorization in Kubernetes</p> </li> </ul>"},{"location":"kbs/000020054/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020055/","title":"How to enable debug level logging for the kube-auth-api DaemonSet in Rancher provisioned Kubernetes clusters","text":"<p>This document (000020055) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020055/#situation","title":"Situation","text":""},{"location":"kbs/000020055/#task","title":"Task","text":"<p>The <code>kube-auth-api</code> DaemonSet is deployed to controlplane nodes, in Rancher provisioned Kubernetes clusters, to provide user authentication functionality for the authorized cluster endpoint. When troubleshooting an issue with authorized cluster endpoint authentication, it may be helpful to analyze the <code>kube-auth-api</code> logs at debug level, and this article details how to enable debug logging.</p>"},{"location":"kbs/000020055/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher\u00a0instance</li> <li>A Rancher provisioned Kubernetes cluster, either a custom cluster or on nodes in an infrastructure provider using a Node Driver</li> </ul>"},{"location":"kbs/000020055/#resolution","title":"Resolution","text":"<ol> <li> <p>Navigate to the workloads view of the System project, within the Rancher UI, for the relevant Rancher provisioned cluster.</p> </li> <li> <p>Locate the <code>kube-api-auth</code> DaemonSet, within the <code>cattle-system</code> namespace, click the three dots at the right side of the UI and select <code>Edit Config</code>, per the following screenshot:</p> </li> </ol> <p></p> <ol> <li> <p>Select the kube-api-auth container\u00a0in the main tab.</p> </li> <li> <p>Under the General section, scroll down until the Command section</p> </li> <li> <p>In the <code>Command</code> section, enter <code>/usr/bin/kube-api-auth, and\u00a0--debug serve</code> in the Arguments field, per the following screenshot, and click <code>Save</code>:</p> </li> </ol> <p></p> <ol> <li>The <code>kube-api-auth</code> pod(s) will restart with the new debug logging configuration. Viewing the <code>kube-api-auth</code> logs you should now observe log messages with <code>level=debug</code>.</li> </ol>"},{"location":"kbs/000020055/#further-reading","title":"Further reading","text":"<ul> <li>Rancher documentation on the <code>kube-api-auth</code> authentication webhook</li> <li>Rancher Server Architecture documentation</li> </ul>"},{"location":"kbs/000020055/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020056/","title":"How to determine the Rancher Leader pod in Rancher v2.x","text":"<p>This document (000020056) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020056/#environment","title":"Environment","text":"<p>- SUSE Rancher\u00a02.7.x</p> <p>- SUSE Rancher\u00a02.8.x</p> <p>- SUSE Rancher\u00a02.9.x</p>"},{"location":"kbs/000020056/#situation","title":"Situation","text":""},{"location":"kbs/000020056/#during-troubleshooting-it-may-be-useful-to-know-which-of-the-rancher-server-pods-running-as-part-of-the-rancher-deployment-in-the-cattle-system-namespace-is-the-current-leader-and-this-article-details-the-steps-to-determine-this","title":"During troubleshooting it may be useful to know which of the Rancher server pods, running as part of the <code>rancher</code> Deployment in the <code>cattle-system</code> namespace, is the current leader, and this article details the steps to determine this.","text":""},{"location":"kbs/000020056/#resolution","title":"Resolution","text":""},{"location":"kbs/000020056/#to-find-the-rancher-leader-pod-use-either-of-the-following-ways-from-rancher-ui","title":"To find the Rancher leader pod use either of the following ways :\u00a0    From Rancher UI :","text":"<p>1.\u00a0Navigate to the Rancher local cluster</p> <p>2. Click the More Resources on left side menu.</p> <p>3. Under the Coordination section -&gt; select Leases</p> <p>4. Locate the <code>cattle-controllers</code> Lease, under the <code>kube-system</code> namespace.</p> <p>5.\u00a0Expand the spec section and note the current leader pod name in the <code>holderIdentity</code> field.</p> <p>From kubectl shell :</p> <p>-\u00a0 Execute the below script from the rancher local cluster kubectl shell :</p> <pre><code>curl https://raw.githubusercontent.com/rancherlabs/support-tools/master/troubleshooting-scripts/determine-leader/rancher2_determine_leader.sh | sh\n</code></pre>"},{"location":"kbs/000020056/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020057/","title":"[JP] How to collect and share Rancher logs with Support","text":"<p>This document (000020057) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020057/#situation","title":"Situation","text":""},{"location":"kbs/000020057/#_1","title":"\u80cc\u666f","text":"<p>\u3042\u308b\u554f\u984c\u306e\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u3092Rancher\u30b5\u30dd\u30fc\u30c8\u306b\u4f9d\u983c\u3059\u308b\u5834\u5408\u3001\u8abf\u67fb\u3092\u7d99\u7d9a\u3059\u308b\u305f\u3081\u306b\u74b0\u5883\u60c5\u5831\u3084\u30ed\u30b0\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u30c7\u30fc\u30bf\u53ce\u96c6\u65b9\u6cd5\u306e\u7d71\u4e00\u5316\u3068\u7c21\u6613\u5316\u3059\u308b\u305f\u3081\u306b\u3001\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u30b9\u30af\u30ea\u30d7\u30c8\u304c\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b58\u5728\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u60c5\u5831\u53ce\u96c6\u304c\u5fc5\u8981\u3068\u306a\u308b\u3001\u5404\u30ce\u30fc\u30c9\u304b\u3089\u30ed\u30b0\u304c\u53ce\u96c6\u3055\u308c\u305f\u5f8c\u3001Rancher Support \u306e\u6307\u793a\u306b\u5f93\u3044\u3001\u3053\u306e\u51fa\u529b\u30d5\u30a1\u30a4\u30eb\u3092\u30b5\u30dd\u30fc\u30c8\u30c1\u30b1\u30c3\u30c8\uff08\u4e0a\u965020MB\uff09\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3059\u308b\u304b\u3001\u30b5\u30dd\u30fc\u30c8\u304c\u63d0\u4f9b\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u30ea\u30f3\u30af\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"kbs/000020057/#_2","title":"\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3","text":""},{"location":"kbs/000020057/#rancher-v2x","title":"Rancher v2.x","text":"<p>Rancher v2.x \u30af\u30e9\u30b9\u30bf\u5185\u306e\u30ce\u30fc\u30c9\u304b\u3089\u30ed\u30b0\u3092\u53ce\u96c6\u3059\u308b\u306b\u306f\u3001 Rancher v2.x log collector script \u3092\u4f7f\u7528\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u53ce\u96c6\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s\n</code></pre>"},{"location":"kbs/000020057/#rancher-v16","title":"Rancher v1.6","text":"<p>Rancher v1.6 \u30af\u30e9\u30b9\u30bf\u5185\u306e\u30ce\u30fc\u30c9\u304b\u3089\u30ed\u30b0\u3092\u53ce\u96c6\u3059\u308b\u306b\u306f\u3001 Rancher v1.6 log collector script \u3092\u4f7f\u7528\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u53ce\u96c6\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v1.6/logs-collector/rancher16_logs_collector.sh | sudo bash -s\n</code></pre>"},{"location":"kbs/000020057/#rancheros","title":"RancherOS","text":"<p>RancherOS\u7279\u6709\u306e\u554f\u984c\u3092\u8abf\u67fb\u3059\u308b\u305f\u3081\u306b\u3001 RancherOS log collector script \u3067\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>sudo curl https://raw.githubusercontent.com/rancher/os/master/scripts/tools/collect_rancheros_info.sh | sh\n</code></pre>"},{"location":"kbs/000020057/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020058/","title":"How to change the kubelet root directory when provisioning a Kubernetes cluster with the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x","text":"<p>This document (000020058) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020058/#environment","title":"Environment","text":"<p>The Rancher Kubernetes Engine (RKE) CLI or a SUSE R ancher v2.x instance.</p>"},{"location":"kbs/000020058/#situation","title":"Situation","text":""},{"location":"kbs/000020058/#this-article-details-how-to-change-the-path-of-the-kubelet-root-directory-which-defaults-to-varlibkubelet-when-provisioning-a-kubernetes-cluster-via-the-rancher-kubernetes-engine-rke-cli-or-suse-rancher-v2x","title":"This article details how to change the path of the kubelet root directory, which defaults to <code>/var/lib/kubelet</code>, when provisioning a Kubernetes cluster via the Rancher Kubernetes Engine (RKE) CLI or SUSE Rancher v2.x.","text":"<p>N.B. Updating the kubelet root directory is only supported on new cluster provisioning, and changing this path after initial cluster provisioning is not supported.</p>"},{"location":"kbs/000020058/#resolution","title":"Resolution","text":"<p>If you wish to use a separate filesystem for the kubelet root directory, you will need to ensure that this is mounted at the desired path on nodes prior to provisioning.</p> <p>Provisioning via SUSE Rancher v2.x</p> <ol> <li>Click <code>Edit as YAML</code> when configuring the cluster in the <code>Add Cluster</code> view.</li> <li>Under <code>kubelet</code> in the <code>services</code> block, add the desired kubelet root directory path to the <code>root-dir</code> argument of the <code>extra_args</code> block, and in <code>extra_binds</code>, per the following example:</li> </ol> <pre><code>services:\n     kubelet:\n       extra_args:\n         root-dir: \"/my/new/folder\"\n       extra_binds:\n    - \"/my/new/folder:/my/new/folder:shared,z\"\n</code></pre> <ol> <li>After configuring other cluster options as desired, click <code>Create</code> or <code>Next</code>, respectively for Node Driver or Custom clusters, to save the new cluster configuration.</li> </ol>"},{"location":"kbs/000020058/#provisioning-via-the-rke-cli","title":"Provisioning via the RKE CLI","text":"<ol> <li>Open the cluster configuration YAML file for the new cluster.</li> <li>Under <code>kubelet</code> in the <code>services</code> block, add the desired kubelet root directory path to the <code>root-dir</code> argument of the <code>extra_args</code> block, and in <code>extra_binds</code>, per the following example:</li> </ol> <p><code>services:      kubelet:        extra_args:          root-dir: \"/my/new/folder\"        extra_binds:     - \"/my/new/folder:/my/new/folder:shared,z\"</code></p> <ol> <li>After configuring other cluster options as desired, provision the cluster by invoking <code>rke up --config &lt;cluster configuration YAML file&gt;</code>.</li> </ol>"},{"location":"kbs/000020058/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020058/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020059/","title":"Who creates user accounts on Rancher Hosted Prime?","text":"<p>This document (000020059) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020059/#resolution","title":"Resolution","text":"<p>When a new Rancher Hosted Prime environment is created, it will have a default admin account, and a password will be provided to you for that account. You'll be asked to change the admin password when logging in for the first time. It is the customer's responsibility to create new user accounts. Most customers leverage Rancher's external authentication provider.</p>"},{"location":"kbs/000020059/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020061/","title":"Rancher Upgrade Checklist","text":"<p>This document (000020061) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020061/#situation","title":"Situation","text":"<p>This article details the high-level steps required when planning and performing a Rancher and/or Kubernetes upgrade.</p> <p>Note</p> <p>The versions contained in this document were current at the time of writing and are meant only as an example</p> <p>Rancher and Kubernetes use the semantic versioning format, where the version number is referred to major.minor.patch, for example: v2.11.x, x is the patch version</p>"},{"location":"kbs/000020061/#resolution","title":"Resolution","text":""},{"location":"kbs/000020061/#planning","title":"Planning","text":"<ul> <li>The support matrix gives a full list of the versions that are certified by Rancher and work best together</li> <li> <p>The recommended order of upgrades is: Rancher, Kubernetes, and then OS</p> </li> <li> <p>It is important to remain within the supported version combinations in the support matrix. For large version jumps, when planning the upgrade steps it may be best to split the upgrade into smaller portions, completing all of the areas in each portion before upgrading further.</p> </li> <li>For example, do not upgrade Rancher to a newer version if the Kubernetes versions of your management (local) and downstream clusters are not supported by the upgraded version of Rancher. Instead, upgrade Kubernetes to a version supported by both the current and upgraded versions of Rancher.</li> <li>All upgrades should be performed in a lab, testing, or non-prod environment first</li> <li>Adding a grace period between upgrades is recommended, to run workloads and confirm that the single upgrade did not cause any issues</li> <li>For example: add 24 hours between upgrading Rancher, the local cluster, and downstream clusters</li> <li>This reduces the number of changes occurring in a short timeframe and can aid in troubleshooting if an unexpected issue occurs</li> <li>It is recommended to pause any application deployments that use the Rancher API during an upgrade of Rancher</li> </ul> <p>Please see the following recommendations when planning version upgrades:</p> <ul> <li> <p>Rancher:</p> </li> <li> <p>Avoid skipping minor versions when upgrading</p> <ul> <li>For example: when upgrading from v2.9.x -&gt; v2.11.x we encourage upgrading v2.9.x -&gt; v2.10.x -&gt; v2.11.x</li> </ul> </li> <li>It is recommended to upgrade from the newest patch version of the current release to the newest patch version of the next minor release, particularly if the current patch version is many releases behind and may be missing important fixes. For example:<ul> <li>When upgrading from v2.9.2 -&gt; v2.10.x, we encourage you to first upgrade to v2.9.10 (newest v2.9.x patch version at the time of writing) as an intermediary step</li> <li>Then to upgrade from v2.9.10 to v2.10.6 (newest v2.10.x patch version at the time of writing) and not any earlier v2.10.x patch version, for example, v2.10.1</li> </ul> </li> <li> <p>Avoid upgrading to a pre-release or non-stable version as they are not yet fully tested and supported. This is generally recommended for any cluster/environment, however particularly important for production</p> <ul> <li>Pre-release versions can be identified by a -rc# or -alpha# following the Rancher version number (eg, v2.12.0-rc2)</li> </ul> </li> <li> <p>Kubernetes:</p> </li> <li> <p>It is recommended to upgrade incrementally and avoid skipping minor versions. This helps minimize potential issues by introducing gradual changes</p> <ul> <li>For example: when upgrading from v1.29.x -&gt; v1.32.x we encourage upgrading v1.29.x -&gt; v1.30.x -&gt; v1.31.x -&gt; v1.32.x</li> <li>Kubernetes plans approximately three minor version releases a year, so it is a good practice to also plan a minor version upgrade multiple times throughout the year</li> </ul> </li> <li> <p>RKE1 CLI:</p> </li> <li> <p>Perform one minor CLI version jump at a time</p> <ul> <li>For example: when upgrading from v1.5.x -&gt; v1.8.x instead do v1.5.x -&gt; v1.6.x -&gt; v1.7.x -&gt; v1.8.x</li> </ul> </li> </ul>"},{"location":"kbs/000020061/#data-collection","title":"Data collection","text":"<p>Before you start your upgrade, please collect the following pieces of information to best prepare yourself in case you need to open a support ticket.</p> <ul> <li>Scheduled change window:</li> <li>Current Rancher version ( `` shown bottom left in the Rancher dashboard):</li> <li>Target Rancher version:</li> <li>Installation option (single install/HA):</li> <li>Current Kubernetes version of Rancher local cluster (use <code>kubectl version</code>):</li> </ul>"},{"location":"kbs/000020061/#rancher-pre-upgrade","title":"Rancher Pre-Upgrade","text":"<ul> <li>Check if the Rancher UI is accessible</li> <li>Check if all clusters in UI are in an Active state</li> <li>Check if all pods in <code>kube-system</code> and <code>cattle-system</code> namespaces are running (in both Rancher and downstream clusters)</li> </ul> <pre><code>kubectl get pods -n kube-system\nkubectl get pods -n cattle-system\n</code></pre> <ul> <li>Verify the datastore has scheduled snapshots configured, and these are working.</li> <li> <p>RKE: If Rancher is deployed on a Kubernetes cluster built with RKE, verify etcd snapshots are enabled and working, on etcd nodes you can confirm with the following:</p> <pre><code>ls -l /opt/rke/etcd-snapshots\ndocker logs etcd-rolling-snapshots\n</code></pre> </li> <li> <p>k3s: If Rancher is deployed on a k3s Kubernetes cluster, ensure scheduled backups are configured and working. Please see the k3s documentation pages for further information on this.</p> </li> <li>RKE2: If Rancher is deployed on a RKE2 Kubernetes cluster, ensure scheduled backups are configured and working. Please see the RKE2 documentation pages for further information on this.</li> <li>Create a one-time datastore snapshot, please see the following documentation for RKE, RKE2, and k3s , and the single node Docker install options for more information</li> </ul>"},{"location":"kbs/000020061/#rancher-upgrade-steps","title":"Rancher Upgrade steps","text":"<ul> <li>The Rancher upgrade process is detailed in the upgrade documentation for both HA, and single node using Docker</li> </ul>"},{"location":"kbs/000020061/#rancher-reviewverify","title":"Rancher Review/Verify","text":"<p>After the upgrade is completed, go through the following checklist to verify your environment is in working order.</p> <ul> <li>Check if the Rancher UI is accessible</li> <li>You should be able to login into Rancher, view clusters, and browse to workloads</li> <li>Verify the Rancher version has changed in UI</li> <li>After logging into Rancher, review the version in the bottom left corner of the page</li> <li>Check if all clusters in UI are in an Active state</li> <li>Check if all pods in <code>kube-system</code> and <code>cattle-system</code> are running (in both Rancher and downstream clusters)</li> <li> <p>Check the <code>cattle-cluster-agent</code> and <code>cattle-node-agent</code> pods are running in all downstream clusters and running the latest version</p> </li> <li> <p>The rollout of the updated agent versions can take some time if there are a lot of downstream clusters or nodes</p> </li> <li>Create a one-time datastore snapshot, please see the following documentation for RKE,\u00a0 RKE2, and k3s, and the single node Docker install options for more information</li> </ul>"},{"location":"kbs/000020061/#rancher-rollback-steps","title":"Rancher Rollback steps","text":"<p>The Rancher rollback process is detailed in the rollback documentation, please follow the relevant link for Rancher installed on a Kubernetes cluster, or Docker</p>"},{"location":"kbs/000020061/#follow-up-tasks-optional","title":"Follow-up tasks (optional)","text":"<ul> <li>Upgrade the Rancher management cluster, this is often a follow-up to the Rancher upgrade. Please see the RKE, RKE2 ,\u00a0and k3s upgrade documentation for the upgrade process, as mentioned it is best to avoid skipping minor versions</li> <li>Upgrade the downstream clusters, please see the documentation for more information. A snapshot of both the local and downstream clusters before the upgrade is recommended to provides the maximum amount of recoverability options in the event of a rollback</li> <li>Docker/OS upgrades, please our article on performing rolling changes to nodes</li> </ul>"},{"location":"kbs/000020061/#other-useful-kbs","title":"Other useful KBs","text":"<p>[Rancher] Can Rancher Support validate our planned upgrade?</p> <p>[Rancher] Can Rancher Support join me as I do my upgrade?</p>"},{"location":"kbs/000020061/#additional-information","title":"Additional Information","text":"<p>Below is an example plan for upgrading from Rancher 2.9.5 to 2.11.2, and upgrading the local and downstream clusters from 1.27 to 1.31.</p> <p>Note Refer to the planning section above to plan each Rancher and Kubernetes upgrade. For example, between each upgrade (bullet point) take an etcd snapshot of the related cluster(s), add a grace period of 24 hours or longer between each upgrade, and check the critical applications/components in the cluster are healthy</p> <p>1) Upgrade Rancher to the latest version of the current release (2.9.9 at the time of writing)</p> <ul> <li>Upgrade Rancher 2.9.5 to Rancher 2.9.9</li> <li>Test for a minimum of 24 hours, preferably longer</li> </ul> <p>2) Upgrade local and downstream clusters to the maximum supported Kubernetes version on Rancher 2.9.x</p> <ul> <li>Upgrade Kubernetes on the local cluster from 1.27 to 1.28</li> <li>Upgrade Kubernetes on the local cluster from 1.28 to 1.29</li> <li>Upgrade Kubernetes on the local cluster from 1.29 to 1.30</li> <li>Upgrade Kubernetes on all downstream clusters from 1.27 to 1.28</li> <li>Upgrade Kubernetes on all downstream clusters from 1.28 to 1.29</li> <li>Upgrade Kubernetes on all downstream clusters from 1.29 to 1.30</li> </ul> <p>3) Upgrade Rancher to the latest version of the next release (2.10.6 at the time of writing)</p> <ul> <li>Upgrade Rancher 2.9.9 to Rancher 2.10.6</li> </ul> <p>4) Upgrade local and downstream clusters to the maximum supported Kubernetes version on Rancher 2.10.x</p> <ul> <li>Upgrade Kubernetes on the local cluster from 1.30 to 1.31</li> <li>Upgrade Kubernetes on all downstream clusters from 1.30 to 1.31</li> </ul> <p>5) Upgrade Rancher to the latest version of the next release (2.11.2 at the time of writing)</p> <ul> <li>Upgrade Rancher 2.10.6 to Rancher 2.11.2</li> </ul>"},{"location":"kbs/000020061/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020062/","title":"How to rotate the Rancher SSL certificate with a single node Docker installation","text":"<p>This document (000020062) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020062/#situation","title":"Situation","text":""},{"location":"kbs/000020062/#task","title":"Task","text":"<p>One installation method for Rancher 2.x is to run Rancher in a Docker container on a single node. This approach is designed for a short-lived development/test environment and bundles a minimal footprint of all the components needed by Rancher into the container image.</p> <p>When the default self-signed SSL certificate option is used, the lifetime of the SSL certificate is 1 year. If the container is run for a long period the certificate will need to be rotated. The below sections provide steps needed to rotate the certificate for different versions of Rancher.</p>"},{"location":"kbs/000020062/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x single node Docker installation</li> <li>Access to the node where Rancher is running to run Docker commands</li> <li>A backup of the Rancher container</li> </ul>"},{"location":"kbs/000020062/#resolution","title":"Resolution","text":"<p>To perform the certificate rotation, please ensure a backup of the Rancher container has been completed, this can be used as a rollback in the event any previous data needs to be restored.</p> <p>The process is different between different versions of Rancher, please select your version below as needed and set the container ID of the Rancher container.</p> <p>Rancher v2.4.x and above</p> <p>If the certificate is expiring in less than 90 days, certificate rotation occurs automatically. When expiry falls within this period, certificates will be rotated on the next start of the Rancher container.</p> <pre><code>rancher_container_id=xxx\n\ndocker restart ${rancher_container_id}\n</code></pre>"},{"location":"kbs/000020062/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020063/","title":"How to increase the log level for Calico components","text":"<p>This document (000020063) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020063/#environment","title":"Environment","text":"<p>Kubernetes cluster with the Calico Network Provider</p>"},{"location":"kbs/000020063/#situation","title":"Situation","text":""},{"location":"kbs/000020063/#during-network-troubleshooting-it-may-be-useful-to-increase-the-log-level-of-the-calico-components-this-article-details-how-to-set-verbose-debug-level-calico-component-logging-in-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-kubernetes-clusters","title":"During network troubleshooting it may be useful to increase the log level of the Calico components. This article details how to set verbose debug-level Calico component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","text":""},{"location":"kbs/000020063/#resolution","title":"Resolution","text":"<pre><code>N.B.\n As these instructions involve editing the Calico DaemonSet directly, the change\n will not persist cluster update events\u00a0As a result cluster updates should be\n avoided whilst collecting the debug level logs for troubleshooting.\n</code></pre>"},{"location":"kbs/000020063/#via-the-rancher-ui","title":"Via the Rancher UI","text":"<p>For a Rancher v2.x managed cluster, the Calico component log level can be adjusted via the Rancher UI, per the following process:</p> <ol> <li>Navigate to the <code>System</code> project of the relevant cluster within the Rancher UI.</li> <li>Locate the calico DaemonSet workload within the <code>kube-system</code> namespace, click the vertical elipses ( <code>\u22ee</code>) and select Edit.</li> <li>Click to Edit the <code>calico-node</code> container.</li> <li>Add <code>CALICO_STARTUP_LOGLEVEL = DEBUG</code> , <code>F\u200bELIX_LOGSEVERITYSCREEN = Debug</code>, <code>BGP_LOGSEVERITYSCREEN = Debug</code> in Environment Variables section and click <code>Save</code>.</li> </ol>"},{"location":"kbs/000020063/#via-kubectl","title":"Via kubectl","text":"<p>With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Calico component log level can be adjusted via kubectl, per the following process:</p> <ol> <li>Run <code>kubectl -n kube-system edit daemonset calico-node</code>.</li> <li>In the <code>env</code> definition for the <code>calico-node</code> container add an environment variable with the name <code>CALICO_STARTUP_LOGLEVEL</code> and value <code>DEBUG</code>, <code>F\u200bELIX_LOGSEVERITYSCREEN</code> and value <code>Debug</code> and <code>BGP_LOGSEVERITYSCREEN</code> and value <code>Debug</code>, e.g.:</li> </ol> <pre><code>[...]\n         containers:\n      - env:\n        [...]\n        - name: CALICO_STARTUP_LOGLEVEL\n          value: DEBUG\n        - name: BGP_LOGSEVERITYSCREEN\n          value: Debug\n        - name: FELIX_LOGSEVERITYSCREEN\n          value: Debug\n[...]\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"kbs/000020063/#additional-information","title":"Additional Information","text":"<ul> <li>Calico configuration documentation</li> <li>Calico component logs</li> </ul>"},{"location":"kbs/000020063/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020064/","title":"The Rancher v2.x Windows log collector script","text":"<p>This document (000020064) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020064/#environment","title":"Environment","text":"<p>Rancher 2.x</p> <p>Windows worker nodes</p>"},{"location":"kbs/000020064/#situation","title":"Situation","text":""},{"location":"kbs/000020064/#collecting-logs-from-a-rancher-2x-cluster-with-windows-worker-nodes-using-the-rancher-v2x-windows-worker-node-log-collection-script","title":"Collecting logs from a Rancher 2.x cluster with Windows worker nodes\u00a0using the Rancher v2.x Windows worker node log collection script.","text":"<p>N.B. The script needs to be downloaded and run directly on a Windows worker node using a Powershell session with Administrator Privileges.</p>"},{"location":"kbs/000020064/#resolution","title":"Resolution","text":"<p>To run the script, open a new Powershell window with Administrator Privileges and run the following command:</p> <pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force\niwr -useb https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/windows-log-collector/win-log-collect.ps1 -OutFile win-log-collect.ps1\n&amp; .\\win-log-collect.ps1\n</code></pre> <p>Upon successful completion, the log bundle will be written to the root of the C:\\ drive in a file named <code>rancher_&lt;hostname&gt;_&lt;datetime&gt;.tar.gz</code>.</p>"},{"location":"kbs/000020064/#additional-information","title":"Additional Information","text":"<p>See here for readme:</p> <p>https://github.com/rancherlabs/support-tools/tree/master/collection/rancher/v2.x/windows-log-collector</p>"},{"location":"kbs/000020064/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020067/","title":"Logs not forwarded by Rancher Logging in Rancher v2.x when Docker daemon logging driver is not set to json-file","text":"<p>This document (000020067) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020067/#environment","title":"Environment","text":"<p>Rancher v2.x managed cluster with Rancher logging enabled</p>"},{"location":"kbs/000020067/#situation","title":"Situation","text":""},{"location":"kbs/000020067/#the-rancher-v2x-logging-feature-enables-you-to-configure-log-forwarding-for-pods-as-well-as-system-component-containers-in-a-cluster-to-a-logging-endpoint-such-as-elasticsearch-or-splunk","title":"The Rancher v2.x Logging feature enables you to configure log forwarding for Pods, as well as system component containers, in a cluster to a logging endpoint such as Elasticsearch or Splunk.","text":"<p>This feature works by deploying a workload to each node in the cluster that mounts the container log directory from the host to parse the Docker container json log files. This is dependent upon use of the json-file Docker logging driver. In the event that the Docker daemon is configured with an alternative logging driver, the logging feature will be unable to parse the logs and will not forward these.</p> <p>However, under certain configurations (e.g., in CentOS and RHEL packaged Docker 1.13.1, the default log driver configured is journald), it could prevent log forwarding functioning. Meanwhile, whilst json-file is the default log driver in the upstream Docker packages, if an alternative has been configured on nodes this will also prevent the correct functioning of the log forwarding.</p> <p>You can verify the currently configured Docker logging driver on a node by running <code>docker info | grep Logging</code>, which will show output of the following format: <code>Logging Driver: journald</code>.</p> <p>In the event that json-file is not the configured logging driver, the output of <code>ls -la /var/log/containers/</code> on the node should also be empty. With json-file configured this would display symoblic links to paths under <code>/var/log/pods</code>, containing symbolic links which in turn point to the Docker container json log files.</p>"},{"location":"kbs/000020067/#resolution","title":"Resolution","text":""},{"location":"kbs/000020067/#centos-or-rhel-packaged-docker","title":"CentOS or RHEL packaged Docker","text":"<ol> <li>Update <code>/etc/sysconfig/docker</code>, to set <code>--log-driver=json-file</code> instead of <code>journald</code>.</li> <li>Restart the Docker daemon: <code>systemctl restart docker</code></li> <li>You should now see symlinked logs created under <code>/var/log/containers</code></li> </ol>"},{"location":"kbs/000020067/#upstream-docker","title":"Upstream Docker","text":"<ol> <li>Configure the json-file Docker logging driver in <code>/etc/docker/daemon.json</code> per the Docker documentation</li> <li>Restart the Docker daemon: <code>systemctl restart docker</code></li> <li>You should now see symlinked logs created under <code>/var/log/containers</code></li> </ol>"},{"location":"kbs/000020067/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020068/","title":"\"ERROR: XFS filesystem at /var has ftype=0, cannot use overlay backend\" error messages logged by the Docker daemon upon daemon startup","text":"<p>This document (000020068) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020068/#environment","title":"Environment","text":"<p>Cluster running with Docker daemon with the <code>overlay</code> or <code>overlay2</code> storage driver</p>"},{"location":"kbs/000020068/#situation","title":"Situation","text":"<p>During startup of the Docker daemon, an error message of the following format is present in the system logs:</p> <pre><code>Jun  13 13:55:47 hostname container-storage-setup: ERROR: XFS filesystem  at /var has ftype=0, cannot use overlay backend; consider different  driver or separate volume or OS reprovision\n</code></pre>"},{"location":"kbs/000020068/#resolution","title":"Resolution","text":"<p>An <code>xfs</code> formatted filesystem is only supported as backing for the <code>overlay</code> or <code>overlay2</code> Docker storage drivers if formatted with <code>d_type</code> set to <code>true</code>.</p> <p>The <code>d_type</code> value of an <code>xfs</code> filesystem can be verified with the <code>xfs_info</code> utility. Example output for this command can be found in the <code>xfs_info</code> man pages. If <code>ftype=1</code> the filesystem was formatted with <code>d_type</code> <code>true</code> and the filesystem is suitable for use as backing for the <code>overlay</code> or <code>overlay2</code> storage drivers. If the value is set to <code>0</code> the filesystem is not suitable for use with the <code>overlay</code> or <code>overlay2</code> storage drivers, and would need to be reformated with the flag <code>-n ftype=1</code>.</p> <p>Per the Docker documentation: \"Running on XFS without d_type support now causes Docker to skip the attempt to use the <code>overlay</code> or <code>overlay2</code> driver. Existing installs will continue to run, but produce an error. This is to allow users to migrate their data. In a future version, this will be a fatal error, which will prevent Docker from starting.\"</p>"},{"location":"kbs/000020068/#additional-information","title":"Additional Information","text":"<p>Docker documentation on the <code>overlay</code> and <code>overlay2</code> storage drivers</p>"},{"location":"kbs/000020068/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020070/","title":"How to enable support for use-forwarded-headers in ingress-nginx","text":"<p>This document (000020070) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020070/#environment","title":"Environment","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced</li> <li>For Rancher v2.x provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"kbs/000020070/#situation","title":"Situation","text":""},{"location":"kbs/000020070/#per-the-ingress-nginx-documentation-the-use-forwarded-headers-configuration-option-enables-passing-the-incoming-x-forwarded-headers-to-upstreams-use-this-option-when-nginx-is-behind-another-l7-proxy-load-balancer-that-is-setting-these-headers","title":"Per the [ingress-nginx documentation], the <code>use-forwarded-headers</code> configuration option enables passing \"the incoming X-Forwarded-* headers to upstreams. Use this option when NGINX is behind another L7 proxy / load balancer that is setting these headers.\"","text":"<p>This article details how to enable the <code>use-forwarded-headers</code> option in the ingress-nginx instance of Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020070/#resolution","title":"Resolution","text":""},{"location":"kbs/000020070/#configuration-for-rke-cli-provisioned-clusters","title":"Configuration for RKE CLI provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>use-forwarded-headers: true</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     options:\n       use-forwarded-headers: true\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Verify the new configuration:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020070/#configuration-for-rancher-v2x-provisioned-clusters","title":"Configuration for Rancher v2.x provisioned clusters","text":"<ol> <li>Log in to the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name.</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the <code>use-forwarded-headers</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     options:\n       use-forwarded-headers: true\n</code></pre> <ol> <li> <p>Click \"Save\" at the bottom of the page.</p> </li> <li> <p>Wait for cluster to finish upgrading.</p> </li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep use_forwarded_headers | grep true &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020070/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020070/#further-reading","title":"Further reading","text":"<ul> <li>ingress-nginx ConfigMap configuration documentation</li> </ul>"},{"location":"kbs/000020070/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020071/","title":"How to enable container log rotation with k3s or containerd","text":"<p>This document (000020071) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020071/#situation","title":"Situation","text":""},{"location":"kbs/000020071/#task","title":"Task","text":"<p>In a Kubernetes cluster running an alternative container runtime, such as containerd, instead of Docker, the kubelet manages container logs. The kubelet default values in relation to log rotation can be found in the upstream kubelet | Kubernetes\u00a0documentation,-%2D%2Dcontainer%2Druntime%20string). These values can be adjusted by adding flags to the kubelet process.</p>"},{"location":"kbs/000020071/#pre-requisites","title":"Pre-requisites","text":"<p>These steps have been validated for a k3s cluster using the default containerd runtime, in theory these same flags should work for any Kubernetes cluster which does not use Docker as the container runtime.</p>"},{"location":"kbs/000020071/#resolution","title":"Resolution","text":"<p>Two kubelet flags need to be added to configure log rotation, the flags will take effect only at start time.</p> <p>In the case of k3s, passing the needed flags can be done a number of ways, the most common perhaps is with the <code>INSTALL_K3S_EXEC</code> environment variable when installing k3s as a service. These same flags can be added to a previous install command to update the service configuration of an existing install of k3s.</p> <p>Note When updating an existing k3s install, the following command will restart k3s.</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--kubelet-arg \"container-log-max-files=4\" --kubelet-arg \"container-log-max-size=50Mi\"\" sh -\n</code></pre> <p>However, flags can also be supplied to the k3s binary directly if a service is not being used. A restart of k3s is required, using the updated flags.</p> <pre><code>k3s server --kubelet-arg container-log-max-files=4 --kubelet-arg container-log-max-size=50Mi\n</code></pre> <p>Note please adjust the values to suit your needs, for demonstration purposes the above commands used 4 log files of 50MB, allowing for 200MB of total space to be retained per container.</p>"},{"location":"kbs/000020071/#further-reading","title":"Further reading","text":"<p>Please reference the k3s and kubelet documentation pages to find more information on these flags.</p>"},{"location":"kbs/000020071/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020072/","title":"How to create an RKE template and template revision using the Rancher2 Terraform Provider","text":"<p>This document (000020072) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020072/#situation","title":"Situation","text":""},{"location":"kbs/000020072/#task","title":"Task","text":"<p>This article details how to create an RKE cluster template revision using the Rancher2 Terraform provider.</p>"},{"location":"kbs/000020072/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance, from v2.3.0 and above</li> <li>Terraform and the Rancher2 Terraform Provider, authenticated with a Rancher user who has permission to create RKE Templates and RKE Template Revisions</li> </ul>"},{"location":"kbs/000020072/#resolution","title":"Resolution","text":"<p>RKE cluster templates can be created using the Rancher2 Terraform Provider per the documentation on the <code>rancher2_cluster_template</code> resource.</p> <p>An example of this resource can be found below:</p> <pre><code>resource \"rancher2_cluster_template\" \"foo\" {\n  name = \"foo\"\n  members {\n    access_type = \"owner\"\n    user_principal_id = \"local://user-XXXXX\"\n  }\n  template_revisions {\n    name = \"V1\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"6h\"\n            retention = \"24h\"\n          }\n        }\n      }\n    }\n    default = true\n  }\n  description = \"Terraform cluster template foo\"\n}\n</code></pre> <p>Having configured the Rancher2 Terraform Provider and added the above example resource, adjusting as desired and replacing <code>local://user-XXXXX</code> with a valid user prinical ID, run <code>terraform apply</code> to create the RKE template.</p> <p>N.B. the <code>default = true</code> flag, which will specify this <code>V1</code> revision as the the default revision.</p> <p>To add additional revisions, each one will be nested as a new <code>template_revisions</code> block for that resource. Here is an example <code>V2</code> revision:</p> <pre><code>template_revisions {\n  name = \"V2\"\n  cluster_config {\n    rke_config {\n      network {\n        plugin = \"canal\"\n      }\n      services {\n        etcd {\n          creation = \"3h\"\n          retention = \"12h\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>So, the full resource block would now look like this:</p> <pre><code>resource \"rancher2_cluster_template\" \"foo\" {\n  name = \"foo\"\n  members {\n    access_type = \"owner\"\n    user_principal_id = \"local://user-XXXXX\"\n  }\n  template_revisions {\n    name = \"V1\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"6h\"\n            retention = \"24h\"\n          }\n        }\n      }\n    }\n    default = true\n\n  }\n  template_revisions {\n    name = \"V2\"\n    cluster_config {\n      rke_config {\n        network {\n          plugin = \"canal\"\n        }\n        services {\n          etcd {\n            creation = \"3h\"\n            retention = \"12h\"\n          }\n        }\n      }\n    }\n  }\n  description = \"Terraform cluster template foo\"\n}\n</code></pre> <p>Run <code>terraform apply</code> and observe this second <code>V2</code> revision created for the RKE template.</p> <p>N.B. the <code>default</code> revision is still set to <code>V1</code>; this can be changed as needed.</p>"},{"location":"kbs/000020072/#further-reading","title":"Further reading","text":"<ul> <li>Rancher RKE Template documentation .</li> <li>Rancher2 Terraform Provider <code>rancher2_cluster_template</code> resource documentation.</li> </ul>"},{"location":"kbs/000020072/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020073/","title":"How to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed cluster","text":"<p>This document (000020073) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020073/#environment","title":"Environment","text":""},{"location":"kbs/000020073/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster</li> </ul>"},{"location":"kbs/000020073/#situation","title":"Situation","text":""},{"location":"kbs/000020073/#task","title":"Task","text":"<p>This article details how to create a custom Project RBAC role to grant log access and exec permission on Pods, in a Rancher v2.x managed Kubernetes cluster.</p>"},{"location":"kbs/000020073/#resolution","title":"Resolution","text":""},{"location":"kbs/000020073/#resolution_1","title":"Resolution","text":"<p>In Rancher v2.x you can create a custom Project Role that provides the permissions to enable a user to view Pods, Pod logs and to exec into Pods. You can then grant this role to users on Projects to provide them this access where necessary.</p> <p>Pod Reader Permissions in Rancher UI</p> <ol> <li> <p>Navigate to Users &amp; Authentication &gt; Roles.</p> </li> <li> <p>From the Projects tab, select Add Project Role.</p> </li> <li> <p>Provide a name for the role.</p> </li> <li> <p>Under Grant Resources, select Add Resource and fill in the information for each of the following:</p> </li> </ol> Permission(s) Resource Get, Create pods/exec Get, List pods Get, List pods/log <ol> <li>Select Create at the bottom.</li> </ol>"},{"location":"kbs/000020073/#further-reading","title":"Further reading","text":"<ul> <li>Rancher Docs: Project Administration</li> <li>Rancher Docs: Cluster and Project Roles - Defining Custom Roles</li> </ul>"},{"location":"kbs/000020073/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020074/","title":"How to troubleshoot HTTP 400 response codes from ingress-nginx ingresses","text":"<p>This document (000020074) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020074/#environment","title":"Environment","text":"<p>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</p>"},{"location":"kbs/000020074/#situation","title":"Situation","text":"<p>This article details how to troubleshoot failing requests, with a HTTP 400 response code, when using an ingress to access a service in a Kubernetes cluster.</p> <p>This error message is typically returned due to a bad request, or as a result of an issue with the request headers or cookies.</p> <p>This article is not intended to be exhaustive as there are a wide variety of causes, however some possible issues are covered.</p>"},{"location":"kbs/000020074/#resolution","title":"Resolution","text":""},{"location":"kbs/000020074/#large-request-headers-exceeding-header-buffer","title":"Large request headers exceeding header buffer","text":"<p>Nginx has a client header buffer configuration of 4x 8KB by default. Per the Nginx documentation:</p> <ul> <li>\"A request line cannot exceed the size of one buffer, or the 414 (Request-URI Too Large) error is returned to the client.\"</li> <li>\"A request header field cannot exceed the size of one buffer as well, or the 400 (Bad Request) error is returned to the client.\"</li> </ul> <p>The buffer sizes can be extended by defining the <code>large-client-header-buffers</code> option in the <code>nginx-configuration</code> ConfigMap (see below).</p>"},{"location":"kbs/000020074/#large-uri-is-duplicated-into-a-new-header-passed-to-the-backend","title":"Large URI is duplicated into a new header passed to the backend","text":"<p>This issue is commonly seen where the app itself responds when queried directly with a large path, but returns a 400 error when queried through an ingress.</p> <p>By default, when ingress-nginx receives a request, it adds the original request's URI to the <code>X-Original-Uri</code> header that it passes on to the backend. This can result in the app being unable to handle the large sized headers, in addition to the long path.</p> <p>This behaviour can be disabled by setting the <code>proxy-add-original-uri-header</code> option to false in your <code>nginx-configuration</code> ConfigMap (see below).</p>"},{"location":"kbs/000020074/#adding-options-to-the-ingress-nginx-nginx-configuration-configmap","title":"Adding options to the ingress-nginx nginx-configuration ConfigMap","text":"<p>This configuration map is populated by RKE from configuration defined in the cluster config:</p>"},{"location":"kbs/000020074/#if-the-cluster-is-provisioned-by-rancher-v2x","title":"If the cluster is provisioned by Rancher v2.x:","text":"<ol> <li>Edit the cluster (navigate to the cluster within Rancher, select the triple-dot button and then \"Edit\")</li> <li>Select \"Edit as YAML\" to open the cluster configuration as YAML, instead of a form.</li> <li>Add the desired configuration within the ingress block in the following format:</li> </ol> <pre><code>     ingress:\n       provider: nginx\n       options:\n         name: value\n</code></pre> <p>Save the cluster configuration changes. RKE will go through and apply the config defined during its update process.</p>"},{"location":"kbs/000020074/#if-the-cluster-is-provisioned-by-the-rke-cli","title":"If the cluster is provisioned by the RKE CLI:","text":"<p>The process is largely the same as the Rancher process above, but the configuration is defined in the cluster.yml for this cluster:</p> <ol> <li>Open the cluster configuration yaml with your editor and add the ingress.options block:</li> </ol> <pre><code># cluster.yml\nnodes:\n\u00a0 \u00a0 - address: rancher-test\n\u00a0 \u00a0 \u00a0 role:\n\u00a0 \u00a0 \u00a0 \u00a0 - controlplane\n\u00a0 \u00a0 \u00a0 \u00a0 - etcd\n\u00a0 \u00a0 \u00a0 \u00a0 - worker\ningress:\n\u00a0 \u00a0 provider: nginx\n\u00a0 \u00a0 options:\n\u00a0 \u00a0 \u00a0 proxy-add-original-uri-header: false #Example\nssh_agent_auth: true\n</code></pre> <ol> <li>Apply this config with <code>rke up --config &lt;cluster configuration yaml&gt;</code></li> </ol> <p>Ensure you have an up-to-date cluster.rkestate within the same directory before running <code>rke up</code></p>"},{"location":"kbs/000020074/#additional-information","title":"Additional Information","text":"<ul> <li>ingress-nginx ConfigMap documentation</li> <li>RKE documentation on ingress-nginx configuration</li> </ul>"},{"location":"kbs/000020074/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020075/","title":"How to increase the log level for Canal components in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020075) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020075/#situation","title":"Situation","text":""},{"location":"kbs/000020075/#task","title":"Task","text":"<p>During network troubleshooting it may be useful to increase the log level of the Canal components. This article details how to set verbose debug-level Canal component logging, in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.</p>"},{"location":"kbs/000020075/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster with the Canal Network Provider</li> </ul>"},{"location":"kbs/000020075/#resolution","title":"Resolution","text":"<p>N.B. As these instructions involve editing the Canal DaemonSet directly, the change will not persist cluster update events, i.e. invocations of <code>rke up</code> for RKE CLI provisioned clusters, or changes to the cluster configuration for a Rancher provisioned cluster. As a result cluster updates should be avoided whilst collecting the debug level logs for troubleshooting.</p>"},{"location":"kbs/000020075/#via-the-rancher-ui","title":"Via the Rancher UI","text":"<p>For a Rancher v2.x managed cluster, the Canal component log level can be adjusted via the Rancher UI, per the following process:</p> <ol> <li>Navigate to the <code>System</code> project of the relevant cluster within the Rancher UI.</li> <li>Locate the canal DaemonSet workload within the <code>kube-system</code> namespace, click the vertical elipses ( <code>\u22ee</code>) and select Edit.</li> <li>Click to Edit the <code>calico-node</code> container.</li> <li>Add <code>CALICO_STARTUP_LOGLEVEL = DEBUG</code> in the Environment Variables section, click <code>Save</code>.</li> </ol> <pre><code>         containers:\n        - env:\n            - name: DATASTORE_TYPE\n              value: kubernetes\n            - name: USE_POD_CIDR\n              value: 'true'\n            - name: WAIT_FOR_DATASTORE\n              value: 'true'\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              value: none\n            - name: CLUSTER_TYPE\n              value: k8s,canal\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: '60'\n            - name: IP\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: 'true'\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: ACCEPT\n            - name: FELIX_IPV6SUPPORT\n              value: 'false'\n            - name: FELIX_LOGFILEPATH\n              value: none\n            - name: FELIX_LOGSEVERITYSYS\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: Warning\n            - name: FELIX_HEALTHENABLED\n              value: 'true'\n            - name: FELIX_IPTABLESBACKEND\n              value: auto\n            - name: CALICO_STARTUP_LOGLEVEL\n              value: DEBUG\n          envFrom:\n            - configMapRef:\n                name: kubernetes-services-endpoint\n                optional: true\n          image: rancher/mirrored-calico-node:v3.22.0\n          imagePullPolicy: IfNotPresent\n          lifecycle:\n            preStop:\n              exec:\n                command:\n                  - /bin/calico-node\n                  - '-shutdown'\n          livenessProbe:\n            exec:\n              command:\n                - /bin/calico-node\n                - '-felix-live'\n            failureThreshold: 6\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 10\n          name: calico-node\n</code></pre> <ol> <li>Click Edit Yaml for the canal DaemonSet again.</li> <li>This time click Edit on the <code>kube-flannel</code> container.</li> <li>In the Command section add <code>--v=10</code> to the <code>Entrypoint</code> e.g.: <code>/opt/bin/flanneld --ip-masq --kube-subnet-mgr --v=10</code>, and click <code>Save</code>.</li> </ol> <p>```         - command:             - /opt/bin/flanneld             - '--ip-masq'             - '--kube-subnet-mgr'             - '--v=10'</p> <p>```</p>"},{"location":"kbs/000020075/#via-kubectl","title":"Via kubectl","text":"<p>With a Kube Config file sourced for the relevant cluster, for a user with permission to edit the System project, the Canal component log level can be adjusted via kubectl, per the following process:</p> <ol> <li>Run <code>kubectl -n kube-system edit daemonset canal</code>.</li> <li>In the <code>env</code> definition for the <code>calico-node</code> container add an environment variable with the name <code>CALICO_STARTUP_LOGLEVEL</code> and value <code>DEBUG</code>, e.g.:</li> </ol> <pre><code>[...]\n         containers:\n      - env:\n        [...]\n        - name: CALICO_STARTUP_LOGLEVEL\n          value: DEBUG\n[...]\n</code></pre> <ol> <li>In the <code>command</code> definition for the <code>kube-flannel</code> container add <code>--v=10</code> to the command, e.g.:</li> </ol> <p><code>yaml    [...]       - commmand:         - /opt/bin/flanneld         - --ip-masq         - --kube-subnet-mgr         - --v=10 [...] <pre><code>4. Save the file.\n\nAfter Setting up the Debug log level, it can be checked viewing the 'Canal' pod logs:\n</code></pre> 2024-07-10 07:11:15.447 [INFO][9] startup/startup.go 425: Early log level set to debug 2024-07-10 07:11:15.447 [DEBUG][9] startup/load.go 124: No kubeconfig file at default path, leaving blank. 2024-07-10 07:11:15.447 [DEBUG][9] startup/client.go 30: Using datastore type 'kubernetes' 2024-07-10 07:11:15.450 [DEBUG][9] startup/k8s.go 628: Performing 'Get' for Node(foo) 2024-07-10 07:11:15.450 [DEBUG][9] startup/node.go 118: Received Get request on Node type 2024-07-10 07:11:15.483 [DEBUG][9] startup/k8s.go 98: Created k8s clientSet: &amp;{DiscoveryClient:0xc000c38ea0 admissionregistrationV1:0xc000609ad0 admissionregistrationV1beta1:0xc000609b30 internalV1alpha1:0xc000609b90 appsV1:0xc000609bf0 appsV1beta1:0xc000609c50 appsV1beta2:0xc000609cb0 authenticationV1:0xc000609d10 authenticationV1beta1:0xc000609d70 authorizationV1:0xc000609dd0 authorizationV1beta1:0xc000609e30 autoscalingV1:0xc000609f40 autoscalingV2beta1:0xc000609fc0 autoscalingV2beta2:0xc000410150 batchV1:0xc000410380 batchV1beta1:0xc0004105c0 certificatesV1:0xc0004107e0 certificatesV1beta1:0xc000410940 coordinationV1beta1:0xc000410a40 coordinationV1:0xc000410aa0 coreV1:0xc000410b00 discoveryV1:0xc000410b90 discoveryV1beta1:0xc000410bf0 eventsV1:0xc000410c50 eventsV1beta1:0xc000410cb0 extensionsV1beta1:0xc000410d30 flowcontrolV1alpha1:0xc000410d90 flowcontrolV1beta1:0xc000410e10 networkingV1:0xc000410ed0 networkingV1beta1:0xc000410f30 nodeV1:0xc000410fa0 nodeV1alpha1:0xc000411000 nodeV1beta1:0xc000411060 policyV1:0xc0004110e0 policyV1beta1:0xc000411140 rbacV1:0xc0004111c0 rbacV1beta1:0xc000411220 rbacV1alpha1:0xc000411290 schedulingV1alpha1:0xc0004112f0 schedulingV1beta1:0xc000411360 schedulingV1:0xc0004113c0 storageV1beta1:0xc000411420 storageV1:0xc000411480 storageV1alpha1:0xc0004114f0} 2024-07-10 07:11:15.483 [DEBUG][9] startup/k8s.go 628: Performing 'Get' for ClusterInformation(default) 2024-07-10 07:11:15.483 [DEBUG][9] startup/customresource.go 205: Get custom Kubernetes resource Key=ClusterInformation(default) Resource=\"ClusterInformations\" Revision=\"\" 2024-07-10 07:11:15.483 [DEBUG][9] startup/customresource.go 216: Get custom Kubernetes resource by name Key=ClusterInformation(default) Name=\"default\" Namespace=\"\" Resource=\"ClusterInformations\" Revision=\"\" 2024-07-10 07:11:15.487 [DEBUG][9] startup/migrate.go 820: major version is already &gt;= 3: v3.22.0</code></p>"},{"location":"kbs/000020075/#further-reading","title":"Further reading","text":"<ul> <li>Calico configuration documentation</li> <li>Flannel troubleshooting documentation</li> </ul>"},{"location":"kbs/000020075/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020076/","title":"How to enable legacy TLS versions for ingress-nginx in Rancher Kubernetes Engine (RKE) CLI and Rancher v2.x provisioned RKE Kubernetes clusters","text":"<p>This document (000020076) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020076/#environment","title":"Environment","text":"<p>An RKE Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</p>"},{"location":"kbs/000020076/#situation","title":"Situation","text":""},{"location":"kbs/000020076/#this-article-details-how-to-enable-tls-11-on-the-ingress-nginx-controller-in-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-rke-kubernetes-clusters","title":"This article details how to enable TLS 1.1 on the ingress-nginx controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned RKE Kubernetes clusters.","text":""},{"location":"kbs/000020076/#pre-requisites","title":"Pre-requisites:","text":"<ul> <li>For RKE CLI provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced</li> <li>For Rancher v2.x provisioned RKE clusters, you will require\u00a0cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"kbs/000020076/#resolution","title":"Resolution","text":""},{"location":"kbs/000020076/#configuration-for-rke-cli-provisioned-clusters","title":"Configuration for RKE CLI provisioned clusters","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>ssl-protocols</code> option for the ingress, as follows:</li> </ol> <pre><code>     ingress:\n       provider: nginx\n       options:\n         ssl-protocols: \"TLSv1.1 TLSv1.2\"\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ol> <li>Verify the new configuration:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020076/#configuration-for-rancher-provisioned-rke-clusters","title":"Configuration for Rancher-provisioned RKE clusters","text":"<ol> <li> <p>Login into the Rancher UI</p> </li> <li> <p>Go to Cluster Management</p> </li> <li>Click Edit Config for the relevant Rancher-provisioned RKE cluster</li> <li>Click Edit as YAML</li> <li>Include the <code>ssl-protocols</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     options:\n       ssl-protocols: \"TLSv1.1 TLSv1.2\"\n</code></pre> <ol> <li> <p>Click Save at the bottom of the page</p> </li> <li> <p>Wait for cluster to finish upgrading</p> </li> <li>Explore the cluster and launch a kubectl shell</li> <li>Run the following inside the kubectl shell to verify the new argument:</li> </ol> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep ssl_protocols | grep '1.1' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020076/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020078/","title":"How to confirm a version upgrade of Rancher v2.x is completed successfully","text":"<p>This document (000020078) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020078/#environment","title":"Environment","text":"<ul> <li>A Rancher v2.x instance, either a single Docker container or a Highly Available (HA) installation in Kubernetes.</li> <li>A Rancher version upgrade performed per the Rancher upgrade documentation.</li> </ul>"},{"location":"kbs/000020078/#situation","title":"Situation","text":"<p>This article details how to confirm that a Rancher version upgrade has successfully completed.</p>"},{"location":"kbs/000020078/#resolution","title":"Resolution","text":"<p>The following can be verified to confirm that the Rancher component containers have all been successfully upgrade to the newer version:</p> <ul> <li>Within the Rancher UI, confirm the version in the bottom-left corner displays the newer version.</li> <li>For a HA installation, confirm the rancher Deployment Pods within the cattle-system namespace of the Rancher cluster have all been updated to the newer version.</li> <li>Confirm that the Rancher agent workloads (the cattle-node-agent DaemonSet and cattle-cluster-agent Deployment in the cattle-system namespace) in all of the Rancher managed clusters have been updated to the newer version.</li> </ul>"},{"location":"kbs/000020078/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020079/","title":"How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile","text":"<p>This document (000020079) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020079/#situation","title":"Situation","text":""},{"location":"kbs/000020079/#question","title":"Question","text":"<p>How Nginx Ingress calculates the worker_processes and worker_rlimit_nofile</p>"},{"location":"kbs/000020079/#answer","title":"Answer","text":"<p>About the parameters;</p> <p>worker_processes</p> <p>This parameter determines the number of Nginx worker processes to spawn during startup.</p> <p>worker_rlimit_nofile</p> <p>This parameter controls the open file limit per worker process.</p> <p>More details can be found on Nginx documentation</p> <p>Both <code>worker_processes</code> and <code>worker_rlimit_nofile</code> are calculated dynamically by Nginx Ingress during startup.</p> <p>Based on the source code of Ingress Nginx;</p> <pre><code>worker_processes = Number of CPUs ($ grep -c processor /proc/cpuinfo)\nworker_rlimit_nofile = ( RLIMIT_NOFILE / worker_processes ) - 1024\n</code></pre> <p>where RLIMIT_NOFILE is the maximum allowed open files by the process ( <code>ulimit -n</code> )</p> <p>From Nginx Ingress shell, you can verify the same.</p> <pre><code># kubectl exec -it  -n ingress-nginx nginx-ingress-controller-8ln2b -- bash\nbash-5.0$ ulimit -n\n1048576\nbash-5.0$\nbash-5.0$ grep -c processor /proc/cpuinfo\n2        &lt;&lt;---- worker_processes\nbash-5.0$\nbash-5.0$ echo $(((1048576/2)-1024))\n523264    &lt;&lt;--- worker_rlimit_nofile\nbash-5.0$\nbash-5.0$ egrep \"worker_processes|worker_rlimit_nofile\" /etc/nginx/nginx.conf\nworker_processes 2;\nworker_rlimit_nofile 523264;\nbash-5.0$\n</code></pre>"},{"location":"kbs/000020079/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020080/","title":"How can I tell whether my app is installed with Helm v2 or Helm v3?","text":"<p>This document (000020080) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020080/#environment","title":"Environment","text":"<p>Rancher 2.7.x and 2.8.x</p>"},{"location":"kbs/000020080/#situation","title":"Situation","text":""},{"location":"kbs/000020080/#question","title":"Question","text":"<p>How can I tell whether my app was installed with Helm v2 or Helm v3?</p>"},{"location":"kbs/000020080/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>kubectl access to the cluster the app is deployed in</li> </ul>"},{"location":"kbs/000020080/#answer","title":"Answer","text":"<p>The easiest way is to check what version of Helm was used to deploy resources is to look at the <code>heritage</code> label. For example, to check whether Rancher was installed via Helm v2 or v3, run:</p> <pre><code>kubectl get deployment -n cattle-system rancher -o yaml | grep heritage\n</code></pre> <p>The heritage version defines what version of helm was used to install this chart.</p> <p><code>heritage: Tiller</code> - This is a Helm v2 resource</p> <p><code>heritage: Helm</code> - This is a Helm v3 resource</p>"},{"location":"kbs/000020080/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020081/","title":"Troubleshooting - Nodes won't join cluster or show unavailable","text":"<p>This document (000020081) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020081/#environment","title":"Environment","text":"<p>Custom RKE clusters\u00a0and clusters launched with a node driver (RKE).</p>"},{"location":"kbs/000020081/#situation","title":"Situation","text":""},{"location":"kbs/000020081/#issue-nodes-are-not-added-to-rancher-or-are-not-provisioned-correctly","title":"Issue - Nodes are not added to Rancher or are not provisioned correctly","text":"<p>The following article should help empower Rancher administrators diagnose and troubleshoot when a node is not added to Rancher or when a node is not provisioned correctly. We'll outline the process nodes undergo when they are added to a cluster.</p>"},{"location":"kbs/000020081/#resolution","title":"Resolution","text":""},{"location":"kbs/000020081/#tracing-the-steps-during-the-bootstrapping-of-a-node","title":"Tracing the steps during the bootstrapping of a node.","text":"<ul> <li>When working with custom RKE clusters, the way to add nodes to the cluster is by executing a docker run command generated for the created cluster. In case of a custom cluster, the command will be generated and displayed on the final step of cluster creation.</li> <li>In case of a cluster launched with a node driver, the command is generated and executed as the final command after creating the node and installing Docker.</li> </ul> <p>Example of displayed command on the final step of a cluster creation. Please note that not all roles may be present in the generated command, depending on what role(s) is/are selected.</p> <pre><code>sudo docker run -d \\\n --privileged \\\n --restart=unless-stopped \\\n --net=host \\\n -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:&lt;version&gt; --server https://&lt;server_url&gt; \\\n --token &lt;token&gt; \\\n --ca-checksum &lt;checksum_value&gt; \\\n --etcd \\\n --controlplane \\\n --worker\n</code></pre> <p>What happens next:</p> <ol> <li>The <code>docker run</code> command launches a bootstrap agent container. It will be identified with a randomly generated name.</li> <li>The entry point is a shell script which parses the flags and runs some validation tests on said flags and their provided values.</li> <li>A token is then used to authenticate against your Rancher server in order to interact with it.</li> <li>The agent retrieves the CA certificate from the Rancher server and places it in\u00a0<code>/etc/kubernetes/ssl/certs/serverca</code>, then the checksum is used to validate if the CA certificate retrieved from Rancher matches. This only applies when a self-signed certificate is in use.</li> <li>Runs an agent binary and connects to Rancher using a Web Socket connection.</li> <li>The agent then checks in with the Rancher server to see if the node is unique, and gets a node plan.</li> <li>The agent executes the node plan provided by the Rancher server.</li> <li>Docker run command will create the path <code>/etc/kubernetes</code> if it doesn't exist.</li> <li>Rancher will run cluster provisioning/reconcile based on the desired role for the node being added (etcd and control plane nodes only). This process will copy certificates down from the server via the built in rke cluster provisioning.</li> <li>On worker nodes, the process is slightly different. The agent requests a node plan from the Rancher server. The Rancher server generates the node config then sends it back down to the agent. The agent then executes the plan contained in the node config.<ul> <li>This involves the\u00a0certificate generation for the Kubernetes components\u00a0and the container create commands to create the following services:\u00a0kubelet, kube-proxy, and nginx-proxy.</li> </ul> </li> <li>The Rancher agent uses the node plan to write out a cloud-config to configure cloud provider settings.</li> </ol> <p>If the provisioning of the node succeeds, the node will be registering to the Kubernetes cluster and cattle-node-agent DaemonSet pods will be scheduled to the node, and the pod will remove and replace the agent container that was created via the Docker run command.</p> <p>The <code>share-mnt</code> binary (aka bootstrap phase 2):</p> <ul> <li>The <code>share-mnt</code> container runs the <code>share-root.sh</code> which creates filesystem resources that other container end up using: certificate folders, configuration files, etc.</li> <li>The container spings up another container that runs a share mount binary. This container makes sure <code>/var/lib/kubelet</code>\u00a0or <code>/var/lib/rancher</code>\u00a0have the right share permissions for systems like boot2docker.</li> </ul> <p>Note: All Kubernetes control plane components talk directly with the Kubernetes API server that is housed on the same node. This proxy is configured to front all k8s API servers within the cluster.</p> <p>If all goes well, the <code>share-mnt</code> bootstrap and <code>share-root</code>\u00a0container exit and the <code>share-root</code> container gets removed. The kubelet starts, registers with Kubernetes, and cattle-node-agent <code>DaemonSet</code> schedules a pod. The pod should then take over the websocket connection to the rancher server. This should end our provisioning journey and hopefully lead to a functional, happy cluster.</p>"},{"location":"kbs/000020081/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020082/","title":"How to deploy Nginx instead of Traefik as your ingress controller on K3s","text":"<p>This document (000020082) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020082/#environment","title":"Environment","text":"<p>K3s 1.27+ (may apply to other versions)</p>"},{"location":"kbs/000020082/#situation","title":"Situation","text":""},{"location":"kbs/000020082/#task","title":"Task","text":"<p>This knowledge base article will provide the directions for deploying NGINX instead of Traefik as your Kubernetes ingress controller on K3s. Please note that Traefik is the support ingress controller for K3s and NGINX is not officially supported by SUSE Rancher support.</p>"},{"location":"kbs/000020082/#background","title":"Background","text":"<p>By default, K3s uses Traefik as the ingress controller for your cluster. The decision to use Traefik over NGINX was based on multi-architecture support across x86 and ARM based platforms. Normally Traefik meets the needs of most Kubernetes clusters. However, there are unique use cases where NGINX may be required or preferred. If you don't think you need NGINX, it's recommended to stick with Traefik.</p>"},{"location":"kbs/000020082/#resolution","title":"Resolution","text":"<p>The first step to using NGINX or any alternative ingress controller is to tell K3s that you do not want to deploy Traefik. When installing K3s add the following <code>--no-deploy traefik</code> flag to the <code>INSTALL_K3S_EXEC</code> environment variable.\u00a0 Or in some cases, you may need a --disable traefik\u00a0option instead:</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--no-deploy traefik\" sh -s -\nor\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable traefik\" sh -s -\n</code></pre> <p>If you have already downloaded the k3s.sh install script, you can run the following:</p> <pre><code>INSTALL_K3S_EXEC=\"--no-deploy traefik\" k3s.sh\nor\nINSTALL_K3S_EXEC=\"--disable traefik\" k3s.sh\n</code></pre> <pre><code>\n</code></pre> <p>This will install the K3s server and form a single node cluster. You can confirm the cluster is operational (\"Ready\") by running:</p> <pre><code>$ kubectl get nodes\nNAME                         STATUS   ROLES                  AGE     VERSION\nubuntu-s-2vcpu-4gb-nyc1-01   Ready    control-plane,master   3h35m   v1.30.4+k3s1\n</code></pre> <pre><code>\n</code></pre> <p>Note, if you already had the kubectl binary installed on your host and it is not configured correctly, you may need to run <code>k3s kubectl</code> instead of <code>kubectl</code>.</p> <p>Next, confirm your out-of-box pods are running and Traefik is not running:</p> <pre><code>$ kubectl get pods -A\nNAMESPACE       NAME\nREADY   STATUS      RESTARTS   AGE\nkube-system     coredns-576bfc4dc7-n4k72\n1/1     Running     0          3h47m\nkube-system     local-path-provisioner-6795b5f9d8-mzwb8\n1/1     Running     0          3h47m\nkube-system     metrics-server-557ff575fb-ntkh6\n1/1     Running     0          3h47m\n</code></pre> <pre><code>\n</code></pre> <p>K3s has a nice feature that allows you to deploy Helm Charts by placing a <code>HelmChart</code> YAML in <code>/var/lib/rancher/k3s/server/manifests</code>. Create this file by running:</p> <pre><code>cat &gt;/var/lib/rancher/k3s/server/manifests/ingress-nginx.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ingress-nginx\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: ingress-nginx\n  namespace: kube-system\nspec:\n  chart: ingress-nginx\n  repo: https://kubernetes.github.io/ingress-nginx\n  targetNamespace: ingress-nginx\n  version: v4.11.2\n  set:\n  valuesContent: |-\n    fullnameOverride: ingress-nginx\n    controller:\n      kind: DaemonSet\n      hostNetwork: true\n      hostPort:\n        enabled: true\n      service:\n        enabled: false\n      publishService:\n        enabled: false\n      metrics:\n        enabled: true\n        serviceMonitor:\n          enabled: false\n      config:\n        use-forwarded-headers: \"true\"\nEOF\n</code></pre> <pre><code>\n</code></pre> <p>K3s periodically polls the manifests folder and applies the YAML in these files. After about a minute, you should see new pods running, including the NGINX Ingress Controller and default backend:</p> <pre><code>$ kubectl get pods -A\nNAMESPACE       NAME                                      READY   STATUS      RESTARTS   AGE\ningress-nginx   ingress-nginx-controller-t25nm            1/1     Running     0          34m\nkube-system     coredns-576bfc4dc7-n4k72                  1/1     Running     0          3h49m\nkube-system     helm-install-ingress-nginx-bfcgs          0/1     Completed   0          35m\nkube-system     local-path-provisioner-6795b5f9d8-mzwb8   1/1     Running     0          3h49m\nkube-system     metrics-server-557ff575fb-ntkh6           1/1     Running     0          3h49m\n</code></pre> <pre><code>\n</code></pre> <p>You'll also see a <code>helm-install-ingress-nginx</code> pod in your environment. K3s uses this pod to deploy the Helm Chart and it's normal for it to be in a READY=0/1 and STATUS=Completed state once the Helm Chart has been successfully deployed. In the event your Helm Chart failed to deploy, you can view the logs of this pod to troubleshoot further.</p>"},{"location":"kbs/000020082/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020082/#reference","title":"Reference","text":"<ul> <li>K3s documentation</li> <li>NGINX Ingress Controller</li> </ul>"},{"location":"kbs/000020082/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020083/","title":"How to deploy the AWS EBS CSI driver on K3s","text":"<p>This document (000020083) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020083/#situation","title":"Situation","text":""},{"location":"kbs/000020083/#task","title":"Task","text":"<p>This knowledge base article will provide the directions for deploying and testing the AWS EBS CSI driver and storage class on K3s.</p>"},{"location":"kbs/000020083/#requirements","title":"Requirements","text":"<ul> <li>K3s 1.18+ (may apply to other versions)</li> <li>Amazon Web Services (AWS) account with privileges to launch EC2 instances and create IAM policies.</li> </ul>"},{"location":"kbs/000020083/#background","title":"Background","text":"<p>K3s has all in-tree storage providers removed since Kubernetes is shifting to out of tree providers for Container Storage Interface (CSI) and Cloud Provider Interface (CPI). While in-tree providers are convenient, they add a lot of bloat to Kubernetes and will eventually be removed from upstream Kubernetes, possibly in 2021.</p> <p>This how-to guide will instruct you on installing and configuring the AWS EBS CSI driver and storage class. This will allow you to dynamically provision and attach an EBS volume to your pod without having to manually create a persistent volume (PV) and EBS volume in advance. In the event that your node crashes and your pod is re-launched on another node, your pod will be reattached to the volume assuming that node is running in the same availability zone used by the defunct node.</p>"},{"location":"kbs/000020083/#solution","title":"Solution","text":"<p>Assuming you want the CSI and storage class automatically deployed by K3s, copy the following YAML to a file in your manifests folder on one or all of your K3s servers. For example, <code>/var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml</code>:</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChart\nmetadata:\n  name: aws-ebs-csi-driver\n  namespace: kube-system\nspec:\n  chart: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/releases/download/v0.5.0/helm-chart.tgz\n  version: v0.5.0\n  targetNamespace: kube-system\n  valuesContent: |-\n    enableVolumeScheduling: true\n    enableVolumeResizing: true\n    enableVolumeSnapshot: true\n    extraVolumeTags:\n      Name: k3s-ebs\n      anothertag: anothervalue\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: ebs-storageclass\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>First, note at the time of this writing, v0.5.0 is the latest version of the driver. If there is a newer version available, you can replace this in the chart and version tags. See the AWS EBS CSI readme for documentation on the versions currently available. Second, you can customize the <code>enableVolumeScheduling</code>, <code>enableVolumeResizing</code>, <code>enableVolumeSnaphost</code>, and <code>extraVolumeTags</code> based on your needs. These parameters and others are documented in the Helm chart.</p> <p>Next, you need to give the driver IAM permissions to manage EBS volumes. This can be done one of two ways. You can either feed your AWS access key and secret key as a Kubernetes secret, or use an AWS instance profile. Since the first option involves passing sensitive keys in clear text and storing them directly in Kubernetes, the second option is usually preferred. I will go over both options. For either option, make sure your access keys or instance profile has the following permissions set in IAM:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AttachVolume\",\n        \"ec2:CreateSnapshot\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVolume\",\n        \"ec2:DeleteSnapshot\",\n        \"ec2:DeleteTags\",\n        \"ec2:DeleteVolume\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeSnapshots\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeVolumes\",\n        \"ec2:DescribeVolumesModifications\",\n        \"ec2:DetachVolume\",\n        \"ec2:ModifyVolume\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Reference: https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/example-iam-policy.json</p>"},{"location":"kbs/000020083/#option-1-kubernetes-secret","title":"Option 1: Kubernetes Secret","text":"<p>You can place your AWS access key and secret key into a Kubernetes secret. Create a YAML file with the following contents and run a kubectl apply. You can also place this inside your <code>/var/lib/rancher/k3s/server/manifests/aws-ebs-csi.yaml</code> file. Keep in mind this is not a terribly secure option and anyone with access to these files or secrets in the kube-system namespace will be able to obtain your AWS access keys.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-secret\n  namespace: kube-system\nstringData:\n  key_id: \"AKI**********\"\n  access_key: \"**********\"\n</code></pre>"},{"location":"kbs/000020083/#option-2-instance-profile","title":"Option 2: Instance Profile","text":"<p>This option to more secure and should not expose your keys in clear text or in a Kubernetes secret object. You'll need to make sure when your EC2 instances are launched, you've attached an instance profile that has the permissions defined above in the JSON block.</p>"},{"location":"kbs/000020083/#verifying-and-testing","title":"Verifying and Testing","text":"<p>You can now check your pods to see if the CSI pods are running. You should see something like this:</p> <pre><code># kubectl get pods -n kube-system | grep ebs\nebs-snapshot-controller-0                1/1     Running   0          15m\nebs-csi-node-k2gh5                       3/3     Running   0          15m\nebs-csi-node-xdcvn                       3/3     Running   0          15m\nebs-csi-controller-6f799b5548-46jqr      6/6     Running   0          15m\nebs-csi-controller-6f799b5548-h4nbb      6/6     Running   0          15m\n</code></pre> <p>Time to test things out. The following command can be run that should provision a 1GB EBS and attach it to your pod:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: ebs-storageclass\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: storage-test\nspec:\n  containers:\n  - name: \"storage-test\"\n    image: \"ubuntu:latest\"\n    command: [\"/bin/sleep\"]\n    args: [\"infinity\"]\n    volumeMounts:\n      - name: myebs\n        mountPath: /mnt/test\n  volumes:\n  - name: myebs\n    persistentVolumeClaim:\n      claimName: myclaim\nEOF\n</code></pre> <p>In your AWS console, you should see a new EBS volume has been created. After about a minute, you should be able to exec into your pod and see the volume mounted in your pod:</p> <pre><code># kubectl exec storage-test -- df -h\nFilesystem      Size  Used Avail Use% Mounted on\noverlay          31G  6.2G   25G  20% /\ntmpfs            64M     0   64M   0% /dev\ntmpfs           3.8G     0  3.8G   0% /sys/fs/cgroup\n/dev/nvme2n1    976M  2.6M  958M   1% /mnt/test\n/dev/root        31G  6.2G   25G  20% /etc/hosts\nshm              64M     0   64M   0% /dev/shm\ntmpfs           3.8G   12K  3.8G   1% /run/secrets/kubernetes.io/serviceaccount\ntmpfs           3.8G     0  3.8G   0% /proc/acpi\ntmpfs           3.8G     0  3.8G   0% /proc/scsi\ntmpfs           3.8G     0  3.8G   0% /sys/firmware\n</code></pre>"},{"location":"kbs/000020083/#cleaning-up","title":"Cleaning Up","text":"<p>Remove the test pod by running the following:</p> <pre><code>kubectl delete pod storage-test\n</code></pre> <p>Remove the PVC by running:</p> <pre><code>kubectl delete pvc myclaim\n</code></pre> <p>Check the AWS console and you should see your EBS volume has been removed automatically by the AWS EBS CSI driver.</p>"},{"location":"kbs/000020083/#reference","title":"Reference","text":"<ul> <li>K3s documentation</li> <li>AWS EBS CSI documentation</li> </ul>"},{"location":"kbs/000020083/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020084/","title":"How to perform a rolling change to nodes","text":"<p>This document (000020084) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020084/#situation","title":"Situation","text":""},{"location":"kbs/000020084/#task","title":"Task","text":"<p>In a Kubernetes cluster nodes can be treated as ephemeral building blocks providing the resources necessary for all workloads. Managing nodes in an immutable way is particularly common in a cloud environment.</p> <p>In an on premise environment however, nodes can be recycled and updated, in general it's typical that nodes have a longer lifecycle.</p> <p>There may be significant changes to nodes over time, for example: IP addresses, storage/filesystems migration to other hypervisors, data centers, large OS updates, or even migration between clusters.</p> <p>To perform large changes like this, this article aims to provide example steps to apply large changes like this safely in a rolling fashion.</p>"},{"location":"kbs/000020084/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A custom or imported cluster managed by Rancher, or an RKE/k3s cluster</li> <li>Access to the nodes in the cluster with sudo/root</li> <li>Permission to perform drain and delete actions on the nodes</li> </ul> <p>If there are any single replica workloads, whenever possible it is ideal to ensure at least 2 replicas are configured for availablity during rolling changes. These are best scheduled on separate nodes, a preferred anti-affinity can help with this.</p>"},{"location":"kbs/000020084/#steps","title":"Steps","text":"<p>While performing a rolling change to nodes you will need to determine a batch size, effectively how many nodes you wish to take out of service at a time. Initially, it is recommended to perform the change on one node as a canary first, and testing the change has the desired outcome before doing more at once.</p> <ol> <li> <p>If you wish to maintain the number of nodes in the cluster while performing the rolling change, at this point you may wish to add new nodes, this ensures that when nodes are out of service the cluster maintains at least the original number of available nodes.</p> </li> <li> <p>Drain the node, this can be done with <code>kubectl drain &lt;node&gt;</code>, or in the Rancher UI.</p> </li> </ol> <p>This is particularly important to avoid disruptions to services, by draining first, service endpoints are updated to remove the pods from services, stopped, started on a new node in the cluster, and added back to the service safely.</p> <p>If there are pods using local storage (commonly <code>emptyDir</code> volumes), and these should be drained, the <code>--delete-local-data=true</code> will be needed, beware: the data will be lost.</p> <ol> <li>Optional Delete the node(s) from the cluster, this can be done with <code>kubectl delete &lt;node&gt;</code>. This is needed for changes that cannot be performed on existing nodes, such as IP address, hostnames, moving nodes to another cluster, and large configuration updates. Any pods and Kubernetes components running on the nodes will be removed.</li> </ol> <p>Note: if this is an <code>etcd</code> node, ensure that the cluster has quorum and at least two remaining <code>etcd</code> nodes to maintain HA before performing this step.</p> <ul> <li>For an imported cluster, there is no automated cleanup so at this point you would remove the node from the cluster configuration<ul> <li>RKE, remove the node from the cluster.yaml file followed by an <code>rke up</code></li> <li>k3s, stop the k3s service and uninstall k3s using the script</li> </ul> </li> <li> <p>Optional If the node has been deleted in step 3, cleaning the node is important\u00a0to ensure all previous history of the cluster, CNI devices, volumes, and containers are removed. This is especially important if the node is to be re-used in another cluster.</p> </li> <li> <p>Perform the changes to the node, this could be automated with configuration management, scripted or manual steps.</p> </li> <li> <p>Once step 5 is complete, add the node back to the desired cluster.</p> </li> <li>In a custom cluster this can be done with the <code>docker run</code> command supplied in the Rancher UI</li> <li>For an imported cluster the steps are different<ul> <li>RKE, you would add this node to the cluster by configuring it in the cluster.yaml file, followed with an <code>rke up</code></li> <li>k3s, re-install k3s using the correct flags/variables</li> </ul> </li> <li> <p>Test the nodes with running workloads, and monitor before proceeding with the next node, or a larger batch size of nodes.</p> </li> <li> <p>If additional nodes were added in step 1, these can be removed from the cluster at this point by following steps 2, 3, and 4.</p> </li> </ul>"},{"location":"kbs/000020084/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020085/","title":"How is my Rancher Hosted Prime environment monitored?","text":"<p>This document (000020085) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020085/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime is monitored by multiple systems which will trigger an email/SMS notification to a SUSE Rancher Hosted DevOps on-call engineer in the event there's a problem with your environment. Prometheus and Grafana are used to monitor the health of the VMs running Rancher Hosted Prime and look at CPU, load, memory, and disk metrics. CloudWatch is used to monitor response times and database health inside the cloud infrastructure. Pingdom is used to monitor uptime and availability from multiple geographies. If at any time you notice a performance or availability problem with Rancher Hosted Prime, please open a support case on our support portal.</p>"},{"location":"kbs/000020085/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020086/","title":"How often is maintenance performed on Rancher Hosted Prime?","text":"<p>This document (000020086) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020086/#resolution","title":"Resolution","text":"<p>Paid customers with an SLA are given the choice of a one hour weekly maintenance window, so maintenance is done at most on a weekly basis with the exception of emergency maintenance to address an outage or high severity issue. Maintenance typically involves upgrading the underlying Kubernetes cluster or operating system patches and updates. Most maintenance can be done with little or no interruption to service.</p>"},{"location":"kbs/000020086/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020087/","title":"Is it possible to have alpha, beta, or release candidate (RC) available on Rancher Hosted Prime?","text":"<p>This document (000020087) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020087/#situation","title":"Situation","text":""},{"location":"kbs/000020087/#resolution","title":"Resolution","text":"<p>While it may be technically possible to run an alpha, beta, or release candidate version of Rancher on Rancher Hosted Prime, we don't typically offer it so that we can deliver our 99.9% uptime SLA. If you want to test a version of Rancher that is not GA, it's recommended that you use your own on-premise or cloud infrastructure.</p>"},{"location":"kbs/000020087/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020088/","title":"Can the admin password be reset if I\u2019m locked out of my Rancher Hosted Prime environment?","text":"<p>This document (000020088) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020088/#resolution","title":"Resolution","text":"<p>Yes, if you find yourself locked out of the admin account on Rancher Hosted Prime, the Rancher operations team can reset it for you using the method defined in our documentation . To initiate this request, please file a support case through the Rancher Support Portal.</p>"},{"location":"kbs/000020088/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020089/","title":"Who upgrades Kubernetes on my Rancher Hosted Prime downstream clusters?","text":"<p>This document (000020089) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020089/#resolution","title":"Resolution","text":"<p>Customers are responsible for upgrading all downstream clusters that Rancher Hosted Prime manages. RKE clusters can be easily upgraded and rolled back by using the UI or API. See Rancher docs for more details. K3s, RKE2, EKS, AKS, and GKE clusters can also be easily upgraded in the UI.</p>"},{"location":"kbs/000020089/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020090/","title":"Can Rancher Hosted Prime manage my on-premise clusters running on VMWare or bare metal servers?","text":"<p>This document (000020090) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020090/#resolution","title":"Resolution","text":"<p>Yes, there are a variety of ways you can accomplish this. You can provision your VMs or bare metal servers ahead of time and use the Custom Cluster option when creating your Kubernetes cluster. A shell command will be provided which you can run on each server to join the cluster. Your on-premise servers will only require outbound (egress) access to Rancher Hosted Prime.</p> <p>If you want to use the vSphere node driver to have Rancher Hosted Prime provision your infrastructure, Rancher Hosted Prime will need inbound (ingress) access to your on-premise infrastructure. This can be accomplished one of three ways:</p> <ol> <li>Open firewall rules on your corporate network.</li> <li>Establish a VPC peering connection between Rancher Hosted Prime and your AWS cloud account. This requires that your AWS cloud account is connected to your on-premise infrastructure through Direct Connect or VPN.</li> <li>Establish a VPN connection between Rancher Hosted Prime and your on-premise network.</li> </ol> <p>More details can be provided on each of these three options. See also Rancher Hosted Prime Whitepaper.</p>"},{"location":"kbs/000020090/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020091/","title":"Is there any limit on the number of downstream clusters or nodes Rancher Hosted Prime can manage?","text":"<p>This document (000020091) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020091/#situation","title":"Situation","text":""},{"location":"kbs/000020091/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime can manage up to 500 downstream clusters and a total of 5,000 nodes across all clusters. Check with your account executive on the node and cluster limits for your support contract.</p>"},{"location":"kbs/000020091/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020092/","title":"How often is Rancher Hosted Prime upgraded?","text":"<p>This document (000020092) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020092/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime is upgraded normally within two weeks after a stable release. There are typically one or two Rancher releases a quarter. SUSE will contact you to schedule the upgrade. In the future, we plan to have upgrades self-service by letting customers trigger an upgrade through the UI.</p>"},{"location":"kbs/000020092/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020093/","title":"How is Rancher Hosted Prime different than the open-source Rancher I can download for free?","text":"<p>This document (000020093) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020093/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime is built on the same Rancher open-source software that can be downloaded for free. Rancher Hosted Prime's value proposition is that SUSE installs, upgrades, backs up, monitors, and completely manages the software for you.</p>"},{"location":"kbs/000020093/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020094/","title":"Can I integrate Rancher Hosted Prime with my Active Directory, SAML, or LDAP based directory service?","text":"<p>This document (000020094) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020094/#resolution","title":"Resolution","text":"<p>Yes, the option to do authentication integration is available in Rancher Hosted Prime and you can find all the options and directions in our documentation. Integration with external authentication services such as Okta or Azure Active Directory are fairly trivial. For integration with a private or on-premise directory service, you may need to open ports in your firewall or use SUSE Rancher Hosted's network peering or VPN capabilities. SUSE can guide you through this setup if needed.</p>"},{"location":"kbs/000020094/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020095/","title":"Does Rancher Hosted Prime support multi-factor authentication (MFA)?","text":"<p>This document (000020095) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020095/#resolution","title":"Resolution","text":"<p>Local user accounts in Rancher Hosted Prime authenticate only using a login and password, so multi-factor authentication (MFA) is not supported. However, Rancher Hosted Prime can be integrated with many authentication providers that do support MFA, such as Microsoft Azure Active Directory. For a full list of authentication providers, see the Rancher 2.x Authentication Documentation.</p>"},{"location":"kbs/000020095/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020096/","title":"Do I have access to the \"local\" cluster in the management UI for my Rancher Hosted Prime environment?","text":"<p>This document (000020096) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020096/#resolution","title":"Resolution","text":"<p>No, your Rancher Hosted Prime environment will not display the \"local\" cluster in the UI and it is not accessible through the API. The local cluster is the Kubernetes cluster that is running the Rancher server workloads and is fully managed by the Rancher Hosted Prime DevOps team.</p>"},{"location":"kbs/000020096/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020097/","title":"What are the \"-promoted\" Cluster Roles in Rancher?","text":"<p>This document (000020097) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020097/#environment","title":"Environment","text":"<ul> <li>Rancher server with RKE clusters added</li> <li>Users added to a Project</li> </ul>"},{"location":"kbs/000020097/#situation","title":"Situation","text":""},{"location":"kbs/000020097/#when-i-query-for-cluster-roles-via-kubectl-i-see-some-entries-with-promoted-appended-to-them-what-are-these-and-why-is-rancher-creating-them","title":"When I query for Cluster Roles via kubectl, I see some entries with \"-promoted\" appended to them. What are these and why is Rancher creating them?","text":""},{"location":"kbs/000020097/#resolution","title":"Resolution","text":"<p>The ClusterRole with \"-promoted\" at the end, is created if the Project role given to a Project member contains any of these resources: storageClass, persistentVolumes, and apiServices.</p> <p>These resources are not scoped to a namespace. They do not belong to any Project but the entire Cluster. That is why Rancher creates an additional ClusterRole.</p>"},{"location":"kbs/000020097/#additional-information","title":"Additional Information","text":"<p>Managing Role-Based Access Control (RBAC)</p>"},{"location":"kbs/000020097/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020098/","title":"Can Rancher migrate my helm2 app to helm3?","text":"<p>This document (000020098) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020098/#environment","title":"Environment","text":"<p>Helm v2 app</p>"},{"location":"kbs/000020098/#situation","title":"Situation","text":"<p>Can I use Rancher to migrate a Rancher app I deployed from a Helm v2 catalog to Helm v3?</p>"},{"location":"kbs/000020098/#resolution","title":"Resolution","text":"<p>No, Rancher currently does not support migrating an app from Helm v2 to Helm v3. To migrate an app from Helm v2 to Helm v3, you would need to delete the app, re-add the catalog as a helm_v3 catalog and re-install the app.</p>"},{"location":"kbs/000020098/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020099/","title":"Loading the new Rancher Dashboard in an airgapped environment redirects to /fail-whale","text":"<p>This document (000020099) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020099/#situation","title":"Situation","text":""},{"location":"kbs/000020099/#issue","title":"Issue","text":"<p>When attempting to view the new Rancher dashboard in an airgapped environment, or one that requires a proxy to access the internet, the dashboard eventually times out and the user is redirected to https://rancher_server/fail-whale</p> <p></p>"},{"location":"kbs/000020099/#root-cause","title":"Root cause","text":"<p>As the dashboard is currently in beta testing, the code for it resides in our CDN instead of being included in our images. The service responsible for pulling this code currently does not support proxy configuration.</p>"},{"location":"kbs/000020099/#resolution","title":"Resolution","text":"<p>Until the dashboard goes into a General Release status, there is a requirement for internet connectivity to https://releases.rancher.com and https://github.com from both the Rancher cluster and also downstream controlplane nodes for this functionality to work.</p>"},{"location":"kbs/000020099/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020100/","title":"Slow etcd performance (performance testing and optimization)","text":"<p>This document (000020100) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020100/#situation","title":"Situation","text":""},{"location":"kbs/000020100/#issue","title":"Issue","text":"<p>If your etcd logs start showing messages like the following, your storage might be too slow for etcd or the server might be doing too much for etcd to operate properly:</p> <pre><code>2019-08-11 23:27:04.344948 W | etcdserver: read-only range request \"key:\\\"/registry/services/specs/default/kubernetes\\\" \" with result \"range_response_count:1 size:293\" took too long (1.530802357s) to execute\n</code></pre> <p>If your storage is really slow you will even see it throwing alerts in your monitoring system. What can you do to the verify the performance of your storage? If the storage is is not performing correctly, how can you fix it? After researching this I found an IBM article that went over this extensively. Their findings on how to test were very helpful. The biggest factor is your storage latency. If it is not well below 10ms in the 99th percentile, you will see warnings in the etcd logs. We can test this with a tool called fio which I will outline below.</p>"},{"location":"kbs/000020100/#testing-etcd-performance","title":"Testing etcd performance","text":"<ol> <li>Download and install the latest version of fio. This is important because older versions do not provide storage latency. I have a very simple script below to download and install this.</li> </ol> <pre><code>curl -LO https://github.com/rancherlabs/support-tools/raw/master/instant-fio-master/instant-fio-master.sh\nbash instant-fio-master.sh\n</code></pre> <ol> <li>Test the storage, create a directory on the device you want to test then run the fio command as shown below.</li> </ol> <pre><code>export PATH=/usr/local/bin:$PATH\nmkdir test-data\nfio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest\n</code></pre> <ol> <li>Below is an example output from an etcd,controlplane,worker node of a Rancher installation cluster running on an AWS ec2 instance type of t2.large.</li> </ol> <pre><code>[root@ip-172-31-14-184 ~]# fio --rw=write --ioengine=sync --fdatasync=1 --directory=test-data --size=100m --bs=2300 --name=mytest\nmytest: (g=0): rw=write, bs=(R) 2300B-2300B, (W) 2300B-2300B, (T) 2300B-2300B, ioengine=sync, iodepth=1\nfio-3.15-23-g937e\nStarting 1 process\nmytest: Laying out IO file (1 file / 100MiB)\nJobs: 1 (f=1): [W(1)][100.0%][w=2684KiB/s][w=1195 IOPS][eta 00m:00s]\nmytest: (groupid=0, jobs=1): err= 0: pid=21203: Sun Aug 11 23:47:30 2019\n     write: IOPS=1196, BW=2687KiB/s (2752kB/s)(99.0MiB/38105msec)\n       clat (nsec): min=2840, max=99026, avg=8551.56, stdev=3187.53\n         lat (nsec): min=3337, max=99664, avg=9191.92, stdev=3285.92\n       clat percentiles (nsec):\n         |  1.00th=[ 4640],  5.00th=[ 5536], 10.00th=[ 5728], 20.00th=[ 6176],\n         | 30.00th=[ 6624], 40.00th=[ 7264], 50.00th=[ 7968], 60.00th=[ 8768],\n         | 70.00th=[ 9408], 80.00th=[10304], 90.00th=[11840], 95.00th=[13760],\n         | 99.00th=[19328], 99.50th=[23168], 99.90th=[35584], 99.95th=[44288],\n         | 99.99th=[63744]\n       bw (  KiB/s): min= 2398, max= 2852, per=99.95%, avg=2685.79, stdev=104.84, samples=76\n       iops        : min= 1068, max= 1270, avg=1195.96, stdev=46.66, samples=76\n     lat (usec)   : 4=0.52%, 10=76.28%, 20=22.34%, 50=0.82%, 100=0.04%\n     fsync/fdatasync/sync_file_range:\n       sync (usec): min=352, max=21253, avg=822.36, stdev=652.94\n       sync percentiles (usec):\n         |  1.00th=[  400],  5.00th=[  420], 10.00th=[  437], 20.00th=[  457],\n         | 30.00th=[  478], 40.00th=[  529], 50.00th=[  906], 60.00th=[  947],\n         | 70.00th=[  988], 80.00th=[ 1020], 90.00th=[ 1090], 95.00th=[ 1156],\n         | 99.00th=[ 2245], 99.50th=[ 5932], 99.90th=[ 8717], 99.95th=[11600],\n         | 99.99th=[16581]\n     cpu          : usr=0.79%, sys=7.38%, ctx=119920, majf=0, minf=35\n     IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n         submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         issued rwts: total=0,45590,0,0 short=45590,0,0,0 dropped=0,0,0,0\n         latency   : target=0, window=0, percentile=100.00%, depth=1\nRun status group 0 (all jobs):\n     WRITE: bw=2687KiB/s (2752kB/s), 2687KiB/s-2687KiB/s (2752kB/s-2752kB/s), io=99.0MiB (105MB), run=38105-38105msec\nDisk stats (read/write):\n     xvda: ios=0/96829, merge=0/3, ticks=0/47440, in_queue=47432, util=92.25%\n</code></pre> <p>In the fsync data section you can see that the 99th percentile is 2245 or about 2.2ms of latency. This storage is well suited for an etcd node. The etcd documentation suggests that for storage to be fast enough, the 99th percentile of fdatasync invocations when writing to the WAL file must be less than 10ms.</p>"},{"location":"kbs/000020100/#resolution","title":"Resolution","text":"<p>What if your node's storage isn't fast enough? The simple solution is to upgrade the storage but that isn't always an option. If you are on the cusp of acceptable, there are things you can do to optimize your storage so that etcd is happy.</p> <ol> <li> <p>Don't run etcd on a node with other roles. A general rule of thumb is to never have the worker role on the same node as etcd. However many environments have etcd and controlplane roles on the same node and run just fine. If this is the case for your environment then you should consider separating etcd and controlplane nodes.</p> </li> <li> <p>If you've separated etcd and the controlplane node and are still having issues, you can mount a separate volume for etcd so that read write operations for everything else on the node do not impact etcd's performance. This is mostly applicable to Cloud hosted nodes since each volume mounted has its own allocated set of resources.</p> </li> <li> <p>If you are on a dedicated server and would like to separate etcd read write operations from the rest of the server, you should install a new storage device for etcd mounts.</p> </li> <li> <p>Always use SSD's for your etcd nodes, whether it is dedicated or in the cloud.</p> </li> <li> <p>Set the priority of the etcd container so that it is higher than other processes but not too high that it overwhelms the server.</p> </li> </ol> <pre><code>ionice -c2 -n0 -p `pgrep -x etcd`\n</code></pre>"},{"location":"kbs/000020100/#further-reading","title":"Further reading","text":"<p>Below is a list of links that I used for my research. I highly recommend reading these as they contain more information than I've posted in this article.</p> <ul> <li>IBM blog post on use of fio to test etcd storage performance</li> <li>etcd performance documentation</li> <li>etcd documentation on node sizing examples</li> <li>etcd metrics documentation</li> <li>etcd tuning documentation</li> <li>AWS blog post on the difference between burst and baseline performance in EC2 storage</li> </ul>"},{"location":"kbs/000020100/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020101/","title":"How to create docker goroutine, and memory heap, dumps","text":"<p>This document (000020101) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020101/#environment","title":"Environment","text":"<p>Rancher 2.x</p>"},{"location":"kbs/000020101/#situation","title":"Situation","text":""},{"location":"kbs/000020101/#task","title":"Task","text":"<p>It's important to observe Docker as it operates to help drive troubleshooting an issue. Here are some commands to generate memory heap and goroutine dumps without killing the Docker process.</p>"},{"location":"kbs/000020101/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Docker with an exposed socket (typically found at <code>/var/run/docker.sock</code>)</li> </ul>"},{"location":"kbs/000020101/#resolution","title":"Resolution","text":""},{"location":"kbs/000020101/#collecting-dumps","title":"Collecting dumps","text":""},{"location":"kbs/000020101/#heap-dump","title":"Heap dump","text":"<p>Heap dumps report a sampling of memory allocations of live objects.</p> <pre><code>curl --unix-socket /var/run/docker.sock http://./debug/pprof/heap?debug=2\n</code></pre>"},{"location":"kbs/000020101/#goroutine-dump","title":"Goroutine dump","text":"<p>The goroutine dump reports stack traces of all current goroutines for the docker process.</p> <pre><code>curl --unix-socket /var/run/docker.sock http://./debug/pprof/goroutine?debug=2\n</code></pre> <p>The output normally is output to <code>stdout</code>, where it can be redirected to a file.</p> <p>Depending on how Docker is configured, and where its configured to log to, the traces could end up in the <code>docker.log</code> file or with the system logs (syslog, journalctl, kern.log, messages, etc...).</p>"},{"location":"kbs/000020101/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020102/","title":"Are API audit logs enabled in Rancher Hosted Prime?","text":"<p>This document (000020102) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020102/#resolution","title":"Resolution","text":"<p>Yes, API audit logs are enabled in Rancher Hosted Prime at level 2. Level 2 includes log event metadata and request body, but does not include response metadata and response body. Rancher APIs typically do not contain personally identifiable information (PII). One exception is the user API which can contain a user's full name. More details on Rancher's API audit logging can be found in the Rancher Documentation . Audit logs are stored in the same region as your Rancher Hosted Prime environment and retained for 1 month. Only the SUSE Rancher team has access to these logs for troubleshooting purposes. Customers may request logs by filing a support case on SCC and providing a date and time range in UTC.</p>"},{"location":"kbs/000020102/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020103/","title":"What information is stored in Rancher Hosted Prime and where is it stored?","text":"<p>This document (000020103) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020103/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime stores the following information:</p>"},{"location":"kbs/000020103/#user-data","title":"User Data","text":"<ul> <li>First and last name of users (aka Display Name)</li> <li>Login id and password. Password is stored using one-way encryption and transported using TLS.</li> <li>Other user information from GitHub, Okta, Microsoft Active Directory, etc. if authentication integration is enabled.</li> </ul>"},{"location":"kbs/000020103/#cloud-provider-credentials-if-provided","title":"Cloud Provider Credentials (if provided)","text":"<ul> <li>Amazon Web Services Access Key and Secret Key</li> <li>Microsoft Azure Subscription ID, Client ID, Client Secret</li> <li>DigitalOcean Access Token</li> <li>Linode Access Token</li> <li>VMWare vSphere endpoint, Username, and Password</li> <li>Similar types of keys, tokens, or credentials for other cloud providers that are enabled by the customer.</li> </ul>"},{"location":"kbs/000020103/#other-application-data","title":"Other Application Data","text":"<ul> <li>Catalogs and Helm Charts</li> <li>CIS Scan Results</li> <li>Cluster Monitoring Metrics (if turned on)</li> <li>Cluster infrastructure, including node roles, node hardware specs, node software versions, workload metadata, workload logs.</li> <li>Anything else entered by the end-user in the Rancher user interface, API, or CLI which could change from version to version.</li> </ul> <p>Data is stored in our third-party cloud service provider on virtual machines managed by the Rancher Hosted Prime operations team in the region/country selected by the customer.</p>"},{"location":"kbs/000020103/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020104/","title":"Filesystem actions in containers fail with `Too many levels of symbolic links`","text":"<p>This document (000020104) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020104/#environment","title":"Environment","text":"<p>Docker container or Kubernetes Pod with a volume defined that is mounted on the host with autofs, typically backed by NFS</p>"},{"location":"kbs/000020104/#situation","title":"Situation","text":""},{"location":"kbs/000020104/#when-attempting-to-perform-a-filesystem-action-inside-a-container-with-a-volume-located-on-an-autofs-directory-the-error-too-many-levels-of-symbolic-links-is-thrown-and-the-action-fails","title":"When attempting to perform a filesystem action inside a container with a volume located on an autofs directory, the error <code>Too many levels of symbolic links</code> is thrown and the action fails.","text":"<pre><code>bash: cd: /data: Too many levels of symbolic links\n</code></pre>"},{"location":"kbs/000020104/#resolution","title":"Resolution","text":""},{"location":"kbs/000020104/#docker","title":"Docker","text":""},{"location":"kbs/000020104/#mount-the-volume-in-question-with-the-flag-slave-rslave-shared-or-rshared-to-ensure-that-mount-changes-are-propagated-to-the-container-example-docker-run-d-v-pathtoautofsdatashared-ubuntu","title":"Mount the volume in question with the flag <code>slave</code>, <code>rslave</code>, <code>shared</code>, or <code>rshared</code> to ensure that mount changes are propagated to the container.    Example:    <code>docker run -d -v /path/to/autofs:/data:shared ubuntu</code>","text":"<p>See the links at the bottom of this article for info on what each of these flags does</p>"},{"location":"kbs/000020104/#kubernetes","title":"Kubernetes","text":"<p>Define mountPropagation for the volume in question as either <code>HostToContainer</code> (same as Docker's <code>rslave</code>) or <code>Bidirectional</code> (same as Docker's <code>rshared</code>):</p> <pre><code>kind: Pod\napiVersion: v1\nmetadata:\n  name: test-app\nspec:\n  containers:\n    - name: test\n      image: busybox\n      volumeMounts:\n      - mountPath: \"/data\"\n        name: test-app-vol\n        mountPropagation: HostToContainer\n  volumes:\n    - name: test-app-vol\n      hostPath:\n        path: /data\n</code></pre>"},{"location":"kbs/000020104/#cause","title":"Cause","text":"<p>As the share that backs the autofs volume isn't mounted until the directory specified is accessed, it is typically not mounted when a container is run.</p> <p>With the default Docker bind-mount propagation of <code>rprivate</code>, containers do not receive mount changes for volumes from the host.</p>"},{"location":"kbs/000020104/#additional-information","title":"Additional Information","text":"<p>Docker bind propagation - https://docs.docker.com/storage/bind-mounts/#configure-bind-propagation</p> <p>Kubernetes mountPropagation - https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation</p> <p>Linux Kernel Shared Subtree - https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt</p>"},{"location":"kbs/000020104/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020105/","title":"Best Practices Rancher","text":"<p>This document (000020105) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020105/#environment","title":"Environment","text":"<p>Rancher 2.x</p>"},{"location":"kbs/000020105/#situation","title":"Situation","text":"<p>This article aims to provide a number of checks that can be evaluated to ensure best practices are in place when planning, building or preparing a Rancher 2.x and Kubernetes environment.</p>"},{"location":"kbs/000020105/#resolution","title":"Resolution","text":""},{"location":"kbs/000020105/#1-architecture","title":"1. Architecture","text":""},{"location":"kbs/000020105/#11-nodes","title":"1.1 Nodes","text":"<p>Understanding workload resource needs in downstream clusters upfront can help choose an appropriate node configuration; some nodes may need different configurations; however, all nodes of the same role are generally configured the same.</p> <p>Checks</p> <p>Standardize on supported versions and ensure minimum requirements are met:</p> <ul> <li>Confirm the OS is covered in the supported versions</li> <li>Resource needs can vary based on cluster size and workload, however, in general, no less than 8GB of memory and 2 vCPUs is recommended</li> <li>SSD storage is recommended, and should be considered a minimum requirement for server nodes, or nodes with the <code>etcd</code> role</li> <li>Firewall rules allow connectivity for nodes ( RKE, RKE2, k3s)</li> <li>A static IP for all nodes is required, if using DHCP, all nodes should have a reserved address</li> <li>Swap is disabled on the nodes</li> <li>Unique hostnames are used for every node within a cluster</li> <li>NTP is enabled on the nodes</li> </ul>"},{"location":"kbs/000020105/#12-separation-of-concerns","title":"1.2 Separation of concerns","text":"<p>The Rancher management cluster should be dedicated to running the Rancher deployment, additional workloads added to the cluster can contend for resources and impact the performance and predictability of Rancher.</p> <p>Applications that are part of the rancher catalog (e.g. rancher-monitoring, rancher-logging, neuvector, rancher-cis-benchmark) may be deployed into the Rancher Management Cluster as well, but it is important to ensure there are sufficient resources (cpu, memory, disk, network).</p> <p>This is also important to consider in downstream clusters, the etcd\u00a0and control plane\u00a0nodes (RKE), and server nodes (RKE2/k3s) should be dedicated to the purpose. For large clusters it may also be appropriate that each node has a single role, for example, separate nodes for the etcd and control plane roles.</p> <p>Checks</p> <p>Using the following commands on each cluster, check and confirm for any unexpected workloads running on the Rancher management cluster, or running on the server or etcd/control plane nodes of a downstream cluster.</p>"},{"location":"kbs/000020105/#rancher-management-local-cluster","title":"Rancher management (local) cluster","text":"<ul> <li>Check for any unexpected pods running in the cluster: <code>kubectl get pods --all-namespaces</code></li> <li>Check for any single points of failure or discrepancies in OS, kernel and CRI version: <code>kubectl get nodes -o wide</code></li> </ul>"},{"location":"kbs/000020105/#downstream-cluster","title":"Downstream cluster","text":"<ul> <li>Check for any unexpected pods running on server nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/master=true --no-headers | cut -d \" \" -f1)\n  do\n    kubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre> <p>Note: RKE does not use the node-role.kubernetes.io/master=true label used in the above command, the below commands select with labels in use by all distributions.</p> <ul> <li>Check for any unexpected pods running on etcd nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/etcd=true --no-headers | cut -d \" \" -f1)\n  do\n    kubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre> <ul> <li>Check for any unexpected pods running on control plane nodes:</li> </ul> <pre><code>for n in $(kubectl get nodes -l node-role.kubernetes.io/controlplane=true --no-headers | cut -d \" \" -f1)\n  do\n    kubectl get nodes --field-selector metadata.name=${n} --no-headers\n    kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=${n}; echo\ndone\n</code></pre>"},{"location":"kbs/000020105/#13-high-availability","title":"1.3 High Availability","text":"<p>Ensure nodes within a cluster are spread across separate failure boundaries where possible. This could mean VMs running on separate physical hosts, data centres, switches, storage pools, etc. If running in a cloud environment, instances in separate availability zones.</p> <p>For High Availability in Rancher, a Kubernetes install is required.</p> <p>Checks</p> <ul> <li>When deploying the Rancher management (local) cluster it is recommended to use the following configuration:</li> </ul> Distribution Recommendation RKE 3 nodes with all roles RKE2 3 server nodes (all roles) k3s (external datastore) 2 server nodes k3s (embedded etcd) 3 server nodes (all roles) <ul> <li>Confirm the components of all clusters and external datastores (k3s) are satisfying minimum HA requirements:</li> </ul> <p>RKE / RKE2</p> Component Minimum Recommended Notes etcd nodes 3 3 To maintain quorum it is important to have an uneven # of nodes, and to provide tolerance for at least 1 node failure control plane nodes 2 2 Allow tolerance for at least 1 node failure worker nodes 2 N/A Allow tolerance for at least 1 worker node failure, scale up to meet the workload needs <p>k3s</p> Component Minimum Recommended Notes external datastore 2 2 or greater (optional) The external datastore should provide failover to a standby using the datastore-endpoint server nodes 2 2 or greater (external datastore) Allow tolerance for at least 1 server node failure server nodes 3 3 (embedded etcd) To maintain quorum it is important to have an uneven # of nodes, and to provide tolerance for at least 1 node failure agent nodes 2 N/A Allow tolerance for at least 1 agent node failure, scale up to meet the workload needs <p>K3s allows for external (SQL) and embedded (etcd) datastore options, please refer to the appropriate notes in the table.</p>"},{"location":"kbs/000020105/#cloud-provider","title":"Cloud provider","text":"<p>The following commands can also be used with clusters configured with a cloud provider to review the instance type and availability zones of each node and identify any high availability concerns.</p> <p><code>kubectl get nodes --show-labels</code></p> <p>Labels may not be available on all cloud providers.</p>"},{"location":"kbs/000020105/#14-load-balancer","title":"1.4 Load balancer","text":"<p>To provide a consistent endpoint for the Rancher management cluster, a load balancer is highly recommended to ensure the Rancher agents, UI, and API connectivity can effectively reach the Rancher deployment.</p> <p>Checks</p> <p>The load balancer is configured:</p> <ul> <li>Within close proximity of the Rancher management cluster to reduce latency</li> <li>For high availability, with all Rancher management nodes configured as upstream targets</li> <li>With a health check to one of the following paths:</li> </ul> Distribution Health check path RKE <code>/healthz</code> RKE2 <code>/healthz</code> k3s (traefik) <code>/ping</code> <p>A health check interval is generally recommended at 30 seconds or less</p>"},{"location":"kbs/000020105/#15-proximity-and-latency","title":"1.5 Proximity and latency","text":"<p>For performance reasons, it is recommended to avoid spreading cluster nodes over long distances and unreliable networks. For example, nodes could be in separate AZs in the same region, the same datacenter, or separate nearby data centres.</p> <p>This is particularly important for etcd nodes\u00a0which are sensitive to network latency, the RTT between etcd nodes in the cluster will determine the minimum time to complete a commit.</p> <p>Checks</p> <ul> <li>Network latency and bandwidth is adequate between locations that the cluster nodes will be provisioned</li> </ul> <p>A tool like <code>mtr</code> to gather connectivity statistics between locations over a long sample period can be useful to report on the packet loss and latency.</p> <p>Generally latency between etcd nodes is recommended at 5s or less</p>"},{"location":"kbs/000020105/#16-datastore","title":"1.6 Datastore","text":"<p>It is important to ensure that the chosen datastore is capable of handling requests inline with the workload of the cluster.</p> <p>Allocation of resources, storage performance, and tuning of the datastore may be needed over time, this could be due to an increase in churn in a cluster, downstream clusters growing in size, or the number of downstream clusters Rancher is managing increases.</p> <p>Checks</p> <p>Confirm the recommended options are met for the distribution in use:</p>"},{"location":"kbs/000020105/#k3s-external-datastore","title":"k3s (external datastore)","text":"<p>With an external datastore the general performance requirements include:</p> <ul> <li>SSD or similar storage providing 1,000 IOPs or greater performance</li> <li>Datastore servers are assigned 2 vCPUs and 4GB memory or greater</li> <li>A low latency connection to the datastore endpoint from all k3s server nodes</li> </ul> <p>MySQL 5.7 is recommended. If running in a cloud provider, you may wish to utilise a managed database service.</p>"},{"location":"kbs/000020105/#rke-rke2-and-k3s-embedded-etcd","title":"RKE, RKE2 and k3s (embedded etcd)","text":"<p>To confirm the storage performance of etcd nodes is capable of handling the workload:</p> <ul> <li>A benchmark tool like <code>fio</code> can be used to accurately test the underlying disk for fsync latency. Alternatively, a basic self-test can be run on RKE and RKE2 with the respective commands below:</li> </ul>"},{"location":"kbs/000020105/#rke","title":"RKE","text":"<pre><code>docker exec -e ETCDCTL_ENDPOINTS=$(docker exec etcd etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ',') etcd etcdctl check perf\n</code></pre>"},{"location":"kbs/000020105/#rke2","title":"RKE2","text":"<pre><code>export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml\netcdcontainer=$(/var/lib/rancher/rke2/bin/crictl ps --label io.kubernetes.container.name=etcd --quiet)\n/var/lib/rancher/rke2/bin/crictl exec $etcdcontainer etcdctl --cert /var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key /var/lib/rancher/rke2/server/tls/etcd/server-client.key --cacert /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt check perf\n</code></pre> <ul> <li>Nodes with the <code>etcd</code> role have SSD or similar storage providing high IOPs and low latency</li> </ul> <p>On large downstream or Rancher environments, tuning etcd may be needed, including adding dedicated disk for etcd.</p>"},{"location":"kbs/000020105/#17-cidr-selection","title":"1.7 CIDR selection","text":"<p>The cluster, node, and service CIDRs cannot be changed once a cluster is provisioned.</p> <p>For this reason, it is important to future proof by reviewing the ranges to avoid routing overlaps with other areas of the network, and potential cluster IP exhaustion if the defaults are not suitable.</p> <p>Checks</p> <ul> <li>The default CIDR ranges do not overlap with any area of the network that needs to be routable from clusters</li> </ul> <p>The default CIDRs are below which often don't need to be changed, to ensure the are no issues with routing from pods you may wish to adjust the range and/or mask when creating clusters ( RKE, RKE2, k3s).</p> Network Default CIDR Cluster 10.42.0.0/16 Service 10.43.0.0/16 Node Mask /24 <p>Reducing the CIDR mask can lower the number of IPs available and therefore total number of pods and services in the cluster, or on each node. In a large cluster, the CIDR ranges may need to be increased.</p>"},{"location":"kbs/000020105/#18-authorized-cluster-endpoint","title":"1.8 Authorized cluster endpoint","text":"<p>At times connecting directly to a downstream cluster may be desired, this could be to reduce latency, avoid interruption if Rancher is unavailable, or avoid proxying the requests when a high frequency of API calls are expected - for example, external monitoring, automation or a CI/CD pipeline.</p> <p>Checks</p> <ul> <li>Check for any use cases where an authorized cluster endpoint is needed</li> </ul> <p>Access directly to the downstream cluster kube-apiserver can be configured using the secondary context in the kubeconfig file.</p>"},{"location":"kbs/000020105/#2-best-practices","title":"2. Best Practices","text":""},{"location":"kbs/000020105/#21-installing-rancher","title":"2.1 Installing Rancher","text":"<p>It is highly encouraged to install Rancher on a Kubernetes cluster in an HA configuration.</p> <p>When starting with a single node for the Rancher management (local) cluster, at the minimum it is highly recommended to install on a single node Kubernetes cluster to improve configuration and management of the Rancher environment.</p> <p>The intended use case of the single node Docker install is for short-lived testing environments, migration from a Docker to a Kubernetes install is not possible and will require migrating Rancher to the new cluster using the Backup Operator.</p> <p>Checks</p> <ul> <li>Rancher is installed on a Kubernetes cluster, even if that is a single node cluster</li> </ul>"},{"location":"kbs/000020105/#22-rancher-resources","title":"2.2 Rancher Resources","text":"<p>The minimum resource requirements for nodes in the Rancher management (local) cluster need to scale to match the number of downstream clusters and nodes, this may change over time and need reviewing as changes occur in the environment.</p> <p>Checks</p> <ul> <li>Verify that nodes in the Rancher management cluster meet at least the minimum requirements:</li> </ul> Resource CPU/Memory Port requirements"},{"location":"kbs/000020105/#23-chart-options","title":"2.3 Chart options","text":"<p>When installing the Rancher helm chart, the default options may not always be the best fit for specific environments.</p> <p>Checks</p> <ul> <li> <p>The Rancher helm chart is installed with the desired options</p> </li> <li> <p><code>replicas</code> - the default number of Rancher replicas ( <code>3</code>) may not suit your cluster, for example, a k3s cluster with an external datastore may only need a <code>replicas</code> value of <code>2</code> to ensure only one Rancher pod is running per k3s server node.</p> </li> </ul> <p>Note: it is generally recommended to not configure more than 3 Rancher replicas</p> <ul> <li><code>antiAffinity</code> - the default <code>preferred</code> scheduling can mean Rancher pods become imbalanced during the lifetime of a cluster, using <code>required</code> can ensure Rancher is always scheduled on unique nodes</li> </ul> <p>To confirm the options provided on an existing Rancher install, the following command can be used <code>helm get values rancher -n cattle-system</code></p>"},{"location":"kbs/000020105/#24-supported-versions","title":"2.4 Supported versions","text":"<p>When choosing or maintaining the components for Rancher and Kubernetes clusters the product lifecycle and support matrix can be used to ensure the versions and OS configurations are certified and maintained.</p> <p>Checks</p> <ul> <li>Current Rancher, OS and Kubernetes versions are under maintenance and certified</li> </ul> <p>As versions are a moving target, checking the current stable releases and planning for future upgrades on a schedule is recommended.</p> <p>The Rancher Upgrade Checklist can be a useful refresher when planning an upgrade.</p>"},{"location":"kbs/000020105/#25-recurring-snapshots-and-backups","title":"2.5 Recurring snapshots and backups","text":"<p>It is important to configure snapshots on a recurring schedule and store these externally to the cluster for disaster recovery.</p> <p>Checks</p> <ul> <li>Recurring snapshots are configured for the distribution in use</li> </ul> Distribution Configuration RKE Confirm recurring snapshots are enabled with an S3 compatible endpoint for off-node copies RKE2 Confirm recurring snapshots are enabled with an S3 compatible endpoint for off-node copies k3s (embedded etcd) Confirm recurring snapshots are enabled with an S3 compatible endpoint for off-node copies k3s (external datastore) Confirm backups on the external datastore are configured, this can differ depending on the chosen database <p>In addition to a recurring schedule, it's important to take one-time snapshots of etcd (RKE, RKE2 and k3s (embedded)) , or datastore (k3s) before and after significant changes.</p> <p>The Rancher backup operator can also be used on any\u00a0distribution or hosted provider to backup the related objects that Rancher needs to function, this can be used to restore to a previous backup point or migrate Rancher between clusters.</p>"},{"location":"kbs/000020105/#26-provisioning","title":"2.6 Provisioning","text":"<p>Provisioning clusters, nodes and workloads for Rancher and downstream clusters in a repeatable and automated way can improve the supportability of Rancher and Kubernetes. When configuration is stored in source control it can also assist with auditing changes.</p> <p>Checks</p> <p>Consider the below points for the Application, Rancher and Kubernetes environments:</p> <ul> <li>Manifests and configuration data for application workloads are stored in source control, treated as the source of truth for all containerized applications and deployed to clusters</li> <li>Infrastructure as Code for provisioning and configuration of Kubernetes clusters and workloads</li> <li>CI/CD pipeline to automate deployments and configuration changes</li> </ul> <p>The rancher2 terraform provider and pulumi package can be used to manage cluster provisioning and configuration as code with Rancher.</p> <p>The helm and kubernetes providers for terraform can be useful to deploy and manage application workloads, similar packages are available for pulumi.</p>"},{"location":"kbs/000020105/#27-managing-node-lifecycle","title":"2.7 Managing node lifecycle","text":"<p>When making significant planned changes it is important to drain nodes that are being affected to avoid disrupting in-flight connections, such as restarting Docker, patching, shutting down or removing nodes.</p> <p>For example, the <code>kube-proxy</code> component manages iptables rules on nodes to manage service endpoints, if a node is suddenly shutdown, stale endpoints and orphaned pods can be left in place for a period of time causing connectivity issues.</p> <p>In some cases during an unplanned issue, draining can be automated, such as when a node may be terminated, restarted, or shutdown.</p> <p>Checks</p> <ul> <li>A process is in place to drain before planned disruptive changes are performed on a node</li> <li>Where possible, node draining during the shutdown sequence is automated, for example, with a systemd or similar service</li> </ul>"},{"location":"kbs/000020105/#3-operating-kubernetes","title":"3. Operating Kubernetes","text":""},{"location":"kbs/000020105/#31-capacity-planning-and-monitoring","title":"3.1 Capacity planning and Monitoring","text":"<p>It is recommended to measure resource usage of all clusters by enabling monitoring in Rancher, or your chosen solution. It is recommended to alert on critical resource thresholds and events in the cluster.</p> <p>On supported platforms, Cluster Autoscaler can be used to ensure the number of nodes is right-sized for shceduled workloads. Combining this with Horizontal Pod Autoscaler provides both application and infrastructure scaling capabilities.</p> <p>Checks</p> <ul> <li>Monitoring is enabled for the Rancher and downstream clusters</li> <li>A receiver is configured to stay informed if an alarm or event occurs</li> <li>A process for adding/removing nodes is established, automated if possible</li> </ul>"},{"location":"kbs/000020105/#32-probes","title":"3.2 Probes","text":"<p>In the defence against service and pod related failures, liveness and readiness probes are very useful, these can be in the form of HTTP requests, commands, or TCP connections.</p> <p>Checks</p> <ul> <li>Liveness and Readiness probes are configured where necessary</li> <li>Probes do not rely on the success of upstream dependencies, only the running application in the pod</li> </ul>"},{"location":"kbs/000020105/#33-resources","title":"3.3 Resources","text":"<p>Assigning resource requests to pods allows the <code>kube-scheduler</code> to make more informed placement decisions, avoiding the \"bin packing\" of pods onto nodes and resource contention. For CPU requests, this also allocates a share of CPU time based on the request when the node is under contention.</p> <p>Limits also offer value in the form of a safety net against pods consuming an undesired amount of resources. One caveat specific to CPU limits, these can introduce CPU throttling when over used.</p> <p>Additionally, it can also be useful to\u00a0reserve capacity\u00a0on nodes to prevent allocating resources that may be consumed by the kubelet and other system daemons, like Docker.</p> <p>Checks</p> <ul> <li>Workloads define resource CPU and Memory requests where applicable, use CPU limits sparingly</li> <li>Nodes have system and daemon reservations where necessary</li> </ul> <p>When Rancher Monitoring is enabled, the graphs in Grafana \u00a0can be used to find a baseline of CPU and Memory for resource requests</p>"},{"location":"kbs/000020105/#34-os-limits","title":"3.4 OS Limits","text":"<p>Containerized applications can consume high amounts of OS resources, such as open files, connections, processes, filesystem space and inodes.</p> <p>Often the defaults are adequate; however, establishing a standardized image for all nodes can help establish a baseline for all configuration and tuning.</p> <p>Checks</p> <p>In general, the below can be used to confirm the OS limits allow for adequate headroom for the workloads</p> <ul> <li>File descriptor usage: <code>cat /proc/sys/fs/file-nr</code></li> <li> <p>User ulimits: <code>ulimit -a</code> Or, a particular process can be checked: <code>cat /proc/PID/limits</code></p> </li> <li> <p>Conntrack limits:</p> </li> </ul> <p><code>cat /proc/sys/net/netfilter/nf_conntrack_max</code></p> <p><code>cat /proc/sys/net/netfilter/nf_conntrack_count</code></p> <ul> <li>Filesystem space and inode usage: <code>df -h</code> and <code>df -ih</code></li> </ul> <p>Requirements for Linux can differ slightly depending on the distribution, refer to the requirements ( RKE , RKE2 , k3s) for more information.</p>"},{"location":"kbs/000020105/#35-log-rotation","title":"3.5 Log rotation","text":"<p>To prevent large log files from accumulating, it is recommended to rotate OS and container log files. Optionally an external log service can be used to stream logs off the nodes for a longer-term lifecycle and easier searching.</p> <p>Checks</p>"},{"location":"kbs/000020105/#containers","title":"Containers","text":"<ul> <li>Log rotation is configured for the container logs</li> <li>An external logging service is configured as needed</li> </ul> <p>To rotate container logs with RKE, configure log rotation in the <code>/etc/daemon.json</code> file with a size and retention configuration.</p> <p>The below can be used as an example for RKE2 and k3s in the config.yaml file, alternatively these can also be set directly when installing a standalone cluster using the install script:</p> <pre><code>kubelet-arg:\n  - container-log-max-files=4\n  - container-log-max-size=50Mi\n</code></pre> <ul> <li>Log rotation is configured for the container logs</li> <li>An external logging service is configured as needed</li> </ul>"},{"location":"kbs/000020105/#os","title":"OS","text":"<p>Rotation of log files on nodes is also important, especially if a long node lifecycle is expected.</p>"},{"location":"kbs/000020105/#36-dns-scalability","title":"3.6 DNS scalability","text":"<p>DNS is a critical service running within the cluster, as CoreDNS pods are scheduled throughout the cluster, the service availability depends on the accessibility of all CoreDNS pods in the service.</p> <p>This is where Nodelocal DNS cache is recommended for clusters that may service a high amount of DNS requests, or clusters very senstive to DNS issues.</p> <p>Checks</p> <p>If a cluster has experienced a DNS issue, or is known to handle a high amount of DNS queries:</p> <ul> <li>Check the output of <code>conntrack -S</code> on related nodes.</li> </ul> <p>High amounts of the <code>insert_failed</code> counter can be indicative of a conntrack race condition, deploying Nodelocal DNS cache is recommended to mitigate this.</p>"},{"location":"kbs/000020105/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020105/#additional-information","title":"Additional Information","text":"<p>For further information see the documentation for Best Practices</p>"},{"location":"kbs/000020105/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020106/","title":"What permissions are required to grant access to manage Cluster Logging in Rancher v2.x","text":"<p>This document (000020106) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020106/#situation","title":"Situation","text":""},{"location":"kbs/000020106/#question","title":"Question","text":"<p>By default, only Global Admins or Cluster Owners have access to configure and manage Cluster Logging in a Rancher v2.x managed cluster. This article details the permissions required to grant this access to other users.</p>"},{"location":"kbs/000020106/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster managed by Rancher v2.x</li> </ul>"},{"location":"kbs/000020106/#answer","title":"Answer","text":"<p>Cluster Logging configuration is managed by the ClusterLoggings Custom Resource in the management.cattle.io API Group. In order to create a role that grants permission to manage the logging configuration for a cluster, you should therefore grant all verbs on the CluserLoggings Resource in the management.cattle.io API group.</p> <p>You can define a custom Cluster Role via the Rancher UI, by navigating to the Global view, and selecting Security -&gt; Roles -&gt; Cluster, creating a custom role with these permissions. Granting this custom role on a cluster to a user or group will then provide access to manage the Cluster Logging configuration for that cluster.</p>"},{"location":"kbs/000020106/#further-reading","title":"Further Reading","text":"<ul> <li>Rancher v2.x Cluster Logging Documentation</li> <li>Rancher v2.x Role-Based Access Control (RBAC) Documentation</li> </ul>"},{"location":"kbs/000020106/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020107/","title":"How to pull the logs from the rancher-wins service on a Windows node in a Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020107) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020107/#environment","title":"Environment","text":"<p>A Rancher v2.6+ provisioned RKE2 cluster with Windows nodes</p>"},{"location":"kbs/000020107/#situation","title":"Situation","text":""},{"location":"kbs/000020107/#in-windows-kubernetes-clusters-the-rancher-wins-service-provides-a-method-for-rancher-to-operate-the-windows-host-whilst-troubleshooting-a-windows-cluster-issue-it-may-be-necessary-to-pull-the-logs-from-this-service-as-documented-in-this-article","title":"In Windows Kubernetes clusters the <code>rancher-wins</code> service provides a method for Rancher to operate the Windows host. Whilst troubleshooting a Windows cluster issue it may be necessary to pull the logs from this service, as documented in this article.","text":""},{"location":"kbs/000020107/#resolution","title":"Resolution","text":""},{"location":"kbs/000020107/#to-pull-the-logs-from-the-rancher-wins-service-execute-the-following-command-in-a-powershell-session-on-the-node","title":"To pull the logs from the <code>rancher-wins</code> service, execute the following command in a Powershell session on the node:","text":"<pre><code>Get-EventLog -LogName Application -Source 'rancher-wins' | format-table  -Property TimeGenerated, ReplacementStrings -Wrap &gt; wins.log\n</code></pre> <p>This will write the logs to the file <code>wins.log</code> in the working directory, which you can then provide in your Rancher Support Ticket, for analysis.</p>"},{"location":"kbs/000020107/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020108/","title":"How to enable CoreDNS query logging in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned RKE cluster","text":"<p>This document (000020108) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020108/#environment","title":"Environment","text":"<p>An RKE Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, using the CoreDNS dns add-on.</p>"},{"location":"kbs/000020108/#situation","title":"Situation","text":"<p>By default, DNS query logging is disabled in CoreDNS, this article details the steps to enable query logging for CoreDNS in an RKE Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x.</p>"},{"location":"kbs/000020108/#resolution","title":"Resolution","text":"<p>To enable DNS query logging the log plugin needs to be configured, by the addition of <code>log</code> to the Corefile in the coredns ConfigMap of the kube-system Namespace.</p> <p>For example, to use the default log plugin configuration and log all queries, the Corefile definition would be updated as follows:</p> <pre><code>.:53 {\n    log\n    errors\n    health\n    ready\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n      pods insecure\n      fallthrough in-addr.arpa ip6.arpa\n    }\n    prometheus :9153\n    forward . \"/etc/resolv.conf\" {\n      policy random\n    }\n    cache 30\n    loop\n    reload\n    loadbalance\n}\n</code></pre> <p>Steps to update the CoreDNS ConfigMap and persist these changes can be found in the article How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster.</p> <p>For the full list of available options when configuring the log plugin refer to the plugin documentation.</p>"},{"location":"kbs/000020108/#additional-information","title":"Additional Information","text":"<p>CoreDNS log plugin documentation</p>"},{"location":"kbs/000020108/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020109/","title":"How to use External TLS Termination with AWS","text":"<p>This document (000020109) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020109/#situation","title":"Situation","text":""},{"location":"kbs/000020109/#task","title":"Task","text":"<p>This document covers setting up Rancher using an AWS SSL certificate and an ALB (Application Load Balancer).</p>"},{"location":"kbs/000020109/#requirements","title":"Requirements","text":"<ul> <li>Running Rancher management servers on AWS</li> </ul>"},{"location":"kbs/000020109/#resolution","title":"Resolution","text":""},{"location":"kbs/000020109/#configure-the-ssl-certificate","title":"Configure the SSL certificate","text":"<ul> <li>If you are using your own certificate follow the AWS documentation to import the certificate.</li> <li>If you are using an AWS certificate following the AWS documentation to request a public ACM certificate.</li> </ul>"},{"location":"kbs/000020109/#create-the-target-group","title":"Create the Target Group","text":"<ol> <li>Log into the AWS Console to get started.</li> <li>Use Create a Target Group to create a Target group using the data in the tables below to complete the procedure:</li> </ol> <p>- Target Group Name: rancher-http-80     - Protocol: http     - Port: 80     - Target type: instance     - VPC: Choose your VPC     - Protocol (Health Check): http     - Path (Health Check): /healthz</p> <ol> <li>Use Register Targets to Rancher management servers making sure to use the port 80.</li> </ol>"},{"location":"kbs/000020109/#create-the-alb","title":"Create the ALB","text":"<ol> <li>From your web browser, navigate to the Amazon EC2 Console.</li> <li>From the navigation pane, choose LOAD BALANCING &gt; Load Balancers.</li> <li>Click Create Load Balancer.</li> <li>Choose Application Load Balancer.</li> <li> <p>Complete the Step 1: Configure Load Balancer form:</p> <p>- Basic Configuration  - Name: rancher-http  - Scheme: internet-facing  - IP address type: ipv4  - Listeners  - Add the Load Balancer Protocols and Load Balancer Ports below.  - HTTP: 80  - HTTPS: 443  - Availability Zones  - Select Your VPC and Availability Zones.</p> </li> <li> <p>Complete the Step 2: Configure Security Settings form.</p> <p>- Configure the certificate you want to use for SSL termination.</p> </li> <li> <p>Complete the Step 3: Configure Security Groups form.</p> </li> <li> <p>Complete the Step 4: Configure Routing form.</p> <p>- From the Target Group drop-down, choose Existing target group.  - Add target group rancher-http-80.</p> </li> <li> <p>Complete Step 5: Register Targets. Since you registered your targets earlier, all you have to do it click Next: Review.</p> </li> <li> <p>Complete Step 6: Review. Look over the load balancer details and click Create when you\u2019re satisfied.</p> </li> <li>After AWS creates the ALB, click Close.</li> </ol>"},{"location":"kbs/000020109/#configure-external-tls-termination-for-rancher","title":"Configure External TLS Termination for Rancher","text":"<p>You need to add the option <code>--set tls=external</code> to your Rancher install, per the following example: <code>helm install rancher rancher-latest/rancher --namespace cattle-system --set hostname=mmattox-example.support.rancher.space --version 2.3.6 --set tls=external</code></p>"},{"location":"kbs/000020109/#verification","title":"Verification","text":"<p>Run the following command to verify new certificate:</p> <pre><code>curl --insecure -v https://&lt;&lt;Rancher Hostname&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n</code></pre> <p>Example output:</p> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host mmattox-example.support.rancher.space left intact\n</code></pre> <p>NOTE: Some browsers will cache the certificate. Details on how to clear the SSL state in a browser can be found here.</p>"},{"location":"kbs/000020109/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020111/","title":"Kernel crash after \"unregister_netdevice: waiting for lo to become free. Usage count\"","text":"<p>This document (000020111) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020111/#situation","title":"Situation","text":""},{"location":"kbs/000020111/#issue","title":"Issue","text":"<p>In a Linux Kubernetes cluster that has frequent pod creations and deletions along with large amounts of pod network traffic, the following error may be logged to the host system logs:</p> <p><code>\"unregister_netdevice: waiting for lo to become free. Usage count = 1\"</code></p> <p>The kernel will typically be in a semi-hung state after this, causing major system instability.</p>"},{"location":"kbs/000020111/#resolution","title":"Resolution","text":"<p>This issue is fixed upstream in the Linux Kernel by this commit which was released in version 4.4.0.</p> <p>We recommend upgrading to the latest linux kernel available in your distribution.</p> <p>We have seen cases where certain kernel modules can cause this issue while loaded, even on a kernel that includes the fix above. If you are running a kernel higher than 4.4.0 and still seeing this issue, try disabling any third-party kernel modules to test.</p>"},{"location":"kbs/000020111/#projectos-specific-bugs","title":"Project/OS Specific bugs:","text":"<p>Docker - https://github.com/moby/moby/issues/5618</p> <p>Ubuntu - https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1403152</p> <p>- https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1711407</p> <p>RedHat - https://access.redhat.com/solutions/3105941 - https://access.redhat.com/solutions/3659011</p> <p>Centos - https://bugs.centos.org/view.php?id=12711</p>"},{"location":"kbs/000020111/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020113/","title":"Logging integration doesn't work if Docker Root is not default /var/lib/docker","text":"<p>This document (000020113) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020113/#situation","title":"Situation","text":""},{"location":"kbs/000020113/#issue","title":"Issue","text":"<p>As of the time of this writing, Rancher Logging is broken when the Docker root is configured to something other than <code>/var/lib/docker</code>.</p> <p>This issue is tracked in GitHub issue #21112.</p>"},{"location":"kbs/000020113/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Rancher 2.x managed/imported cluster with logging enabled.</li> <li>Docker root configured to something other than <code>/var/lib/docker</code> on the nodes (confirmed with <code>docker info | grep Root</code>).</li> </ul>"},{"location":"kbs/000020113/#workaround","title":"Workaround","text":"<p>These steps will assume you have the Docker data root set to <code>/other-docker-root</code>. Change <code>/other-docker-root</code> to whatever your custom path is:</p> <ol> <li> <p>Rancher UI -&gt; Cluster -&gt; System Project -&gt; Workloads -&gt; cattle-logging Namespace</p> </li> <li> <p>Find workload rancher-logging-fluentd-linux</p> </li> <li> <p>Edit YAML</p> </li> <li> <p>Edit volume dockerroot</p> </li> <li> <p>Change \"Path on the Node\" from <code>/var/lib/docker</code> to <code>/other-docker-root</code></p> </li> <li> <p>Add volume (with the following details):</p> </li> </ol> <pre><code>Volume Name: dockerrootcustom\nType: bind-mount\nPath on the Node: /other-docker-root\nMount Point: /other-docker-root\n</code></pre> <ol> <li>Click Save</li> </ol> <p>At this point logging should be working with your non-default Docker root directory. You should be able to verify this on your logging target. Keep in mind it may take a few minutes for logs to show up there as fluentd is configured to clear its buffer every 60 seconds by default.</p>"},{"location":"kbs/000020113/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020115/","title":"How to update the CoreDNS ConfigMap in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020115) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020115/#environment","title":"Environment","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) Kubernetes cluster provisioned by the RKE CLI or Rancher v2.x, using the CoreDNS add-on.</li> <li>kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user.</li> </ul>"},{"location":"kbs/000020115/#situation","title":"Situation","text":"<p>You might wish to update the Corefile configuration of CoreDNS, defined via the\u00a0coredns ConfigMap in the kube-system Namespace, for example, in order to enable query logging or update the resolver policy. This article details how to update this ConfigMap and persist changes in a Rancher Kubernetes Engine (RKE) cluster.</p>"},{"location":"kbs/000020115/#resolution","title":"Resolution","text":"<ol> <li>Capture the current CoreDNS ConfigMap definition, with the following <code>kubectl</code> command:</li> </ol> <pre><code>kubectl -n kube-system get configmap coredns -o go-template={{.data.Corefile}}\n</code></pre> <p>The output should look like the following:</p> <pre><code>.:53 {\n       errors\n       health\n       ready\n       kubernetes cluster.local in-addr.arpa ip6.arpa {\n         pods insecure\n         fallthrough in-addr.arpa ip6.arpa\n       }\n       prometheus :9153\n       forward . \"/etc/resolv.conf\" {\n         policy random\n       }\n       cache 30\n       loop\n       reload\n       loadbalance\n}\n</code></pre> <ol> <li>Edit the cluster configuration YAML, to define a custom add-on containing the CoreDNS ConfigMap, with your desired changes. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to the cluster view in the Rancher UI, open the edit cluster view and click <code>Edit as YAML</code>.</li> </ol> <p>Create the add-on with the content below, replacing the Corefile definition with the existing configuration retrieved in step 1. Then make the desired changes, in this example the resolver policy is updated from random, in the existing configuration, to sequential.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n     name: coredns\n     namespace: kube-system\ndata:\n     Corefile: |\n       .:53 {\n           errors\n           health\n           ready\n           kubernetes cluster.local in-addr.arpa ip6.arpa {\n             pods insecure\n             fallthrough in-addr.arpa ip6.arpa\n           }\n           prometheus :9153\n           forward . \"/etc/resolv.conf\" {\n             policy sequential\n           }\n           cache 30\n           loop\n           reload\n           loadbalance\n       }\n</code></pre> <ol> <li>Update the cluster with the new configuration. For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>). For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol>"},{"location":"kbs/000020115/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020116/","title":"How to run workloads on etcd or controlplane nodes, without the worker role, in a Rancher Kubernetes Engine (RKE) cluster","text":"<p>This document (000020116) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020116/#environment","title":"Environment","text":"<p>A Rancher Kubernetes Engine (RKE) cluster, provision with the RKE CLI or Rancher v2.x</p>"},{"location":"kbs/000020116/#situation","title":"Situation","text":"<p>Although it is usually not advised to run workloads on your controlplane and etcd nodes, there are occasionally scenarios when this is necessary. A few common examples are virus scanning, monitoring, and log collection workloads.</p>"},{"location":"kbs/000020116/#resolution","title":"Resolution","text":"<p>Both the controlplane and etcd nodes, which are not additionally designated the worker role, have taints. When RKE or Rancher provisions these nodes, it adds these taints automatically. Workloads that need to run on these nodes require tolerations for these taints. For Rancher managed clusters, you can see these taints within the Rancher UI on the cluster node view. The following kubectl command will also list the taints for each node.</p> <pre><code>$ kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\nNAME           TAINTS\nip-10-0-2-10   [map[effect:NoExecute key:node-role.kubernetes.io/etcd value:true]]\nip-10-0-2-11   [map[effect:NoSchedule key:node-role.kubernetes.io/controlplane value:true]]\nip-10-0-2-12   &lt;none&gt;\n</code></pre> <p>Per this output, each etcd node has the <code>NoExecute</code> taint <code>node-role.kubernetes.io/etcd=true</code> and each controlplane node has the <code>NoSchedule</code> taint <code>node-role.kubernetes.io/controlplane=true</code>.</p> <p>The Rancher UI does not have fields for adding tolerations, so you must specify the tolerations directly in the workload's YAML manifest. You can use the <code>Import YAML</code> button to deploy your workload, and make sure to add the following tolerations block in your manifest:</p> <pre><code>spec:\n...\n  template:\n...\n    spec:\n...\n      tolerations:\n      - operator: Exists\n...\n</code></pre> <p>If you have an existing workload, you can also select the <code>View/Edit YAML</code> option for the workload and apply the above change. This toleration will allow you to run the workload on any nodes with taints, so use with caution. If you are using Helm charts, you can also specify the same YAML in your Helm chart.</p>"},{"location":"kbs/000020116/#additional-information","title":"Additional Information","text":"<p>For more information on how taints and tolerations work in Kubernetes, see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</p>"},{"location":"kbs/000020116/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020117/","title":"How to override DNS results served by CoreDNS","text":"<p>This document (000020117) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020117/#situation","title":"Situation","text":""},{"location":"kbs/000020117/#task","title":"Task","text":"<p>By default, DNS requests for pods using CoreDNS will be made directly to the upstream nameservers configured in /etc/resolv.conf on the node.</p> <p>At times, it may not be possible to easily update records on the upstream nameservers, or specific records for the cluster may be needed. In these cases it's useful to override the results that CoreDNS will serve pods.</p>"},{"location":"kbs/000020117/#pre-requisites","title":"Pre-requisites","text":"<p>These steps should work for any cluster running CoreDNS where the <code>coredns</code> ConfigMap is used.</p>"},{"location":"kbs/000020117/#steps","title":"Steps","text":"<p>There are two approaches to achieve this, please read through both to understand which is best for your environment.</p> <p>Both approaches require editting the <code>coredns</code> ConfigMap, specifically the <code>Corefile</code> key. This can be done in the UI by clicking View/Edit YAML, Edit, or on the command line with kubectl.</p> <p>Along with these options, both plugins covered provide other features, like adjusting the TTL for records, see the documentation links for more information.</p>"},{"location":"kbs/000020117/#rewrite","title":"Rewrite","text":"<p>The rewrite plugin will perform a rewritten query to the upstream nameserver, and respond to the query with the results. The outcome would be similar to configuring a CNAME for the domain.</p> <pre><code>data:\n  Corefile: |\n    .:53 {\n        [...]\n        rewrite name archive.ubuntu.com internal-mirror.ubuntu.local\n    }\n</code></pre> <p>In this example, pods configured with the default Ubuntu mirror are now resolving to the internal mirror without any custom configuration.</p> <p>The benefit of this approach is that the upstream nameserver remains the source of truth for the results.</p>"},{"location":"kbs/000020117/#hosts","title":"Hosts","text":"<p>The hosts plugin provides the ability to define a list of IPs and domains in the form of /etc/hosts to respond as query results.</p> <pre><code>data:\n  Corefile: |\n    .:53 {\n        [...]\n        hosts {\n          10.0.0.1 archive.ubuntu.com\n          10.0.0.2 testing.com\n          fallthrough\n        }\n    }\n</code></pre> <p>A similar example, the internal IPs listed are provided as results.</p> <p>A downside to this approach is that the ConfigMap becomes a source of truth for these results, if changes in the environment are not reflected these entries could become stale. However, it does provide the most flexibility without needing to depend on any upstream nameserver to serve results.</p>"},{"location":"kbs/000020117/#persist-the-changes","title":"Persist the changes","text":"<p>In an RKE or Rancher environment, during cluster or addon upgrades, it's possible that changes to the <code>coredns</code> ConfigMap are updated to use the provided version.</p> <p>To persist the changes made to the ConfigMap, add the changes as a user-defined addon. The steps to do this are documented under How To Update CoreDNS's Resolver Policy article.</p>"},{"location":"kbs/000020117/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020119/","title":"How to enable NGINX support for HTTP headers with underscores","text":"<p>This document (000020119) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020119/#environment","title":"Environment","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher.</li> <li>For RKE provisioned clusters, you will require the RKE binary and access to the cluster configuration YAML, rkestate file and kubectl access with the kubeconfig for the cluster sourced.</li> <li>For Rancher provisioned clusters, you will require cluster owner or global admin permissions in Rancher</li> </ul>"},{"location":"kbs/000020119/#situation","title":"Situation","text":""},{"location":"kbs/000020119/#this-article-details-how-to-enable-http-headers-with-underscores-on-the-nginx-ingress-controller-in-rancher-kubernetes-engine-rke-cli-or-rancher-v2x-provisioned-kubernetes-clusters","title":"This article details how to enable HTTP headers with underscores on the nginx-ingress controller in Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes clusters.","text":""},{"location":"kbs/000020119/#resolution","title":"Resolution","text":""},{"location":"kbs/000020119/#configuration-for-rke-provisioned-clusters","title":"Configuration for RKE provisioned clusters","text":"<ul> <li>Edit the cluster configuration YAML file to include the \" <code>enable-underscores-in-headers: true\"</code> option for the ingress, as follows:</li> </ul> <pre><code>ingress:\n    provider: nginx\n    options:\n      enable-underscores-in-headers: true\n</code></pre> <ul> <li>Apply the changes to the cluster, by invoking \" <code>rke up\"</code>:</li> </ul> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre> <ul> <li>Recycle the nginx pods in-order to pick up new argument:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <ul> <li>Verify the new configuration:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre>"},{"location":"kbs/000020119/#configuration-for-rancher-provisioned-clusters","title":"Configuration for Rancher provisioned clusters","text":"<ul> <li>Login into the Rancher UI.</li> <li>Go to Global -&gt; Clusters -&gt; Cluster Name</li> <li>From the Cluster Dashboard edit the cluster by Clicking on \"\u22ee\" then select Edit.</li> <li>Click \"Edit as YAML\".</li> <li>Include the \" <code>enable-underscores-in-headers\"</code> option for the ingress, as follows:</li> </ul> <pre><code>ingress:\n    provider: nginx\n    options:\n      enable-underscores-in-headers: true\n</code></pre> <ul> <li>Click \"Save\" at the bottom of the page.</li> <li>Wait for cluster to finish upgrading.</li> <li>Go back to the Cluster Dashboard and click \"Launch kubectl\".</li> <li>Run the following inside the kubectl CLI to recycle the nginx pods in-order to pick up new argument:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name); do kubectl delete $pod -n ingress-nginx; echo \"Sleeping for 5 seconds\"; sleep 5; done\n</code></pre> <ul> <li>Run the following inside the kubectl CLI to verify the new argument:</li> </ul> <pre><code>for pod in $(kubectl get pods -l app=ingress-nginx -n ingress-nginx --no-headers -o name | awk -F '/' '{print $2}'); do echo -n \"Checking $pod .... \"; kubectl -n ingress-nginx exec \"$pod\" -- bash -c \"cat /etc/nginx/nginx.conf | grep underscores_in_headers | grep on &gt; /dev/null 2&gt;&amp;1 &amp;&amp; echo 'Good' || echo 'Bad'\"; done\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020119/#cause","title":"Cause","text":"<p>Underscore in HTTP headers is not allowed by the Nginx Ingress by default. Refer to\u00a0enable-underscores-in-headers.</p>"},{"location":"kbs/000020119/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020120/","title":"How to enable antiAffinity for Rancher v2.x server pods","text":"<p>This document (000020120) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020120/#environment","title":"Environment","text":"<p>Rancher 2.x</p>"},{"location":"kbs/000020120/#situation","title":"Situation","text":"<p>By default the Rancher server pods are deployed without podAntiAffinity rules. As a result of this multiple Rancher pods may be scheduled onto a single node, potentially leading to temporary service disruption if the node is unavailable or gets rebooted.</p>"},{"location":"kbs/000020120/#resolution","title":"Resolution","text":"<p>You need to add the option\u00a0<code>--set-string antiAffinity=required</code> to your Rancher install. Details on how to add this option to both new installations of Rancher, as well as existing deployment are provided below. See the Helm chart options documentation.</p>"},{"location":"kbs/000020120/#new-rancher-installation","title":"New Rancher installation","text":"<p>For new installations of Rancher, add the antiAffinity option to the <code>helm install</code> command, per the following example:</p> <pre><code>helm install rancher rancher-stable/rancher \\\n--namespace cattle-system \\\n--set hostname=&lt;FQDN&gt; \\\n--version 2.10.0 \\\n--set-string antiAffinity=required\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade.</p>"},{"location":"kbs/000020120/#update-existing-rancher-deployments","title":"Update existing Rancher deployments","text":"<p>To add the antiAffinity option to an existing deployment of Rancher, follow the Rancher upgrade documentation, using the <code>--version</code> flag to pin to the running Rancher version, preventing a version upgrade.</p> <ol> <li>Run <code>helm get values rancher</code> to get the current Rancher helm chart values, which will be used to generate the <code>helm upgrade</code> command with matching values.</li> <li>Generate and run the <code>helm upgrade</code> command with the chart values, including the pinned version and antiAffinity option, per the following example:</li> </ol> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n   --namespace cattle-system \\\n   --set hostname=&lt;FQDN&gt; \\\n   --version 2.10.0 \\\n   --set-string antiAffinity=required\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade. NOTE: We recommend saving this command for future Rancher upgrades to save time.</p>"},{"location":"kbs/000020120/#verification","title":"Verification","text":"<p>Run the command <code>kubectl get deployment -n cattle-system rancher -o yaml</code> and verify the following <code>podAntiAffinity</code> spec has been added:</p> <pre><code>[...]\nspec:\n affinity:\n   podAntiAffinity:\n     requiredDuringSchedulingIgnoredDuringExecution:\n     - labelSelector:\n         matchExpressions:\n         - key: app\n           operator: In\n           values:\n           - rancher\n       topologyKey: kubernetes.io/hostname\n[...]\n</code></pre>"},{"location":"kbs/000020120/#rollback","title":"Rollback","text":"<p>To remove the antiAffnitiy configuration you should remove the <code>--set-string antiAffinity=required</code> option from the <code>helm upgrade</code> command and re-run this, per the following example:</p> <pre><code>helm upgrade rancher rancher-stable/rancher \\\n--namespace cattle-system \\\n--set hostname=&lt;FQDN&gt; \\\n--version 2.10.0\n</code></pre> <p>NOTE: The Rancher version is pinned with the <code>--version</code> flag to prevent a version upgrade.</p>"},{"location":"kbs/000020120/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020121/","title":"How to create a cluster in SUSE Rancher v2.x using the Rancher CLI or v3 API","text":"<p>This document (000020121) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020121/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher v2.8.4</li> <li>The Rancher CLI installed for CLI cluster creation (this can be downloaded from the Rancher UI, via the <code>Download CLI</code> link in the lower-left corner by clicking the About button)</li> <li>curl installed to make Rancher v3 API requests for API cluster creation</li> <li>A Rancher API Key for a user with cluster creation permissions</li> <li>A Rancher Kubernetes Engine (RKE) cluster config file in YAML or JSON format (optional)</li> </ul>"},{"location":"kbs/000020121/#situation","title":"Situation","text":""},{"location":"kbs/000020121/#the-process-for-creating-kubernetes-clusters-via-the-rancher-ui-is-documented-in-setting-up-kubernetes-clusters-in-rancher","title":"The process for creating Kubernetes clusters via the Rancher UI is documented in \"Setting up Kubernetes Clusters in Rancher\" .","text":"<p>This article details the process for creating Kubernetes clusters in SUSE Rancher v2.8.x via the Rancher CLI or v3 API interfaces.</p>"},{"location":"kbs/000020121/#resolution","title":"Resolution","text":""},{"location":"kbs/000020121/#the-cluster-creation-process-is-detailed-below-for-both-the-rancher-cli-and-v3-api","title":"The cluster creation process is detailed below for both the Rancher CLI and v3 API.","text":""},{"location":"kbs/000020121/#cluster-creation-via-the-rancher-cli","title":"Cluster creation via the Rancher CLI","text":"<ol> <li>Log in to your Rancher Server:</li> </ol> <pre><code>rancher login &lt;server_url&gt; --token &lt;token&gt;\n</code></pre> <ol> <li>Create the cluster:</li> </ol> <p>To create a cluster with the default cluster configuration:</p> <pre><code>rancher cluster create &lt;new_cluster_name&gt;\n</code></pre> <p>If you are passing in an RKE cluster config file, do so as follows:</p> <pre><code>rancher cluster create --rke-config &lt;rke_config_file&gt; &lt;new_cluster_name&gt;\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020121/#cluster-creation-via-the-rancher-v3-api","title":"Cluster creation via the Rancher v3 API","text":"<ol> <li>Create a Rancher API Key , and save the access key and secret key as environment variables ( <code>export \"CATTLE_ACCESS_KEY=&lt;access_key&gt; &amp;&amp; export CATTLE_SECRET_KEY=&lt;secret_key&gt;</code>)\". Alternatively you can pass these directly into the curl request in place of the <code>${CATTLE_ACCESS_KEY}</code> and <code>${CATTLE_SECRET_KEY}</code> variables in the examples below.</li> <li>Send a POST request to the <code>/v3/clusters</code> API endpoint of your Rancher server:</li> </ol> <p>To create a cluster with the default cluster configuration:</p> <pre><code>curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n   -X POST \\\n   -H 'Accept: application/json' \\\n   -H 'Content-Type: application/json' \\\n   -d '{\"name\":\"test-cluster\"}' \\\n'https://&lt;rancher_server&gt;/v3/clusters'\n</code></pre> <pre><code>If you are passing in an RKE cluster config file, do so as follows:\n</code></pre> <pre><code>curl -u \"${CATTLE_ACCESS_KEY}:${CATTLE_SECRET_KEY}\" \\\n   -X POST \\\n   -H 'Accept: application/json' \\\n   -H 'Content-Type: application/json' \\\n   -d @&lt;rke_config_file&gt; \\\n'https://&lt;rancher_server&gt;/v3/clusters'\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020121/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020121/#additional-reading","title":"Additional Reading","text":"<ul> <li>The Rancher v2.8.x\u00a0API Documentation</li> <li>The Rancher v2.8.x\u00a0API Specification</li> </ul>"},{"location":"kbs/000020121/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020122/","title":"How to edit the upstream nameservers used by CoreDNS, in a Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020122) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020122/#situation","title":"Situation","text":""},{"location":"kbs/000020122/#task","title":"Task","text":"<p>By default, CoreDNS pods will inherit the nameserver configuration from the node. In certain circumstances, it might be desired to override this and use a specific set of nameservers for external queries.</p> <p>Note: These steps update the nameservers only for Pods that use either the <code>ClusterFirst</code> (default) or <code>ClusterFirstWithHostNet</code> DNS policy. Nameserver configuration for nodes and other Pods will not be affected.</p>"},{"location":"kbs/000020122/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, with the CoreDNS addon enabled.</li> </ul> <p>Note: New clusters can also be created using the same steps.</p>"},{"location":"kbs/000020122/#steps","title":"Steps","text":""},{"location":"kbs/000020122/#option-a-update-the-clusteryaml","title":"Option A: Update the cluster.yaml","text":"<p>The cluster configuration YAML provides the <code>upstreamnameservers</code> option, to configure a list of upstream nameservers, per the example below:</p> <ol> <li>Add the <code>upstreamnameservers</code> option, with the list of nameservers, to the cluster configuration YAML. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to Cluster Management in the Rancher UI, and Edit Config of the cluster, click <code>Edit as YAML</code>.</li> </ol> <pre><code>dns:\n     provider: coredns\n     upstreamnameservers:\n  - 1.1.1.1\n  - 8.8.8.8\n</code></pre> <ol> <li>Update the cluster with the new configuration. For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>). For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol> <p>Note: This option is recommended as it requires minimal change, see the RKE add-ons documentation for more information.</p>"},{"location":"kbs/000020122/#option-b-update-the-kubelet-resolvconf","title":"Option B: Update the kubelet resolv.conf","text":"<p>By default, the kubelet will refer to the <code>/etc/resolv.conf</code> file as the source for nameserver configuration.</p> <p>It is possible to override this by adding an <code>extra_args</code> option to the <code>kubelet</code> service, and this is also accomplished in the cluster configuration YAML.</p> <p>A custom resolv.conf file can then be used by the kubelet instead, per the example below:</p> <ol> <li>On each of the nodes in the cluster create the custom nameserver configuration file with a nameserver IP address:</li> </ol> <pre><code>echo \"nameserver 8.8.8.8\" &gt; /etc/k8s-resolv.conf\n</code></pre> <ol> <li>Add the <code>resolv-conf</code> flag to the <code>extra_args</code> option for the kubelet service, referencing the custom nameserver configuration file into the cluster configuration YAML. For RKE provisioned clusters, add this into the cluster.yml file. For a Rancher provisioned cluster, navigate to Cluster Management in the Rancher UI, and Edit Config of the cluster, click <code>Edit as YAML</code>.</li> </ol> <pre><code>services:\n  kubelet:\n    extra_args:\n      resolv-conf: /host/etc/k8s-resolv.conf\n</code></pre> <ol> <li>Update the cluster with the new configuration. For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code> ( ensure the cluster.rkestate file is present in the working directory when invoking <code>rke up</code>). For Rancher provisioned clusters, click <code>Save</code> in the Rancher UI <code>Edit as YAML</code> view.</li> </ol> <p>See the RKE services documentation for more information.</p> <p>Note: kubelet flags are being updated, as such a restart of the kubelet component will occur on each node.</p>"},{"location":"kbs/000020122/#option-c-update-the-node-resolvconf","title":"Option C: Update the node resolv.conf","text":"<p>If the nameserver configuration should be consistent between the OS and Kubernetes pods, updating the node <code>/etc/resolv.conf</code> file is recommended.</p> <p>This could be because nameservers are changing or that the caching configuration (for example systemd-resolved) is not desired.</p> <p>Changes to a systemd managed resolv.conf can be dependent on the Linux distribution and you should refer to the documentation for the distribution used in the cluster.</p> <p>Note: The kubelet component caches the <code>/etc/resolv.conf</code> file at start time, as such a restart of the kubelet component needs to occur on each node manually, after updating the /etc/resolv.conf file.</p> <p>This can be accomplished a number of ways:</p> <ul> <li><code>docker restart kubelet</code> on each node</li> <li>A drain and restart of each node</li> <li>Replacing nodes in the cluster with the updated configuration</li> </ul>"},{"location":"kbs/000020122/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020123/","title":"How to change etcd cipher suite","text":"<p>This document (000020123) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020123/#environment","title":"Environment","text":"<p>Rancher</p>"},{"location":"kbs/000020123/#situation","title":"Situation","text":""},{"location":"kbs/000020123/#hardening-etcd-cluster-communication","title":"Hardening ETCD cluster communication","text":""},{"location":"kbs/000020123/#resolution","title":"Resolution","text":""},{"location":"kbs/000020123/#synopsis","title":"Synopsis:","text":"<p>This article will walk Rancher administrators through hardening the cluster communication between etcd nodes. We'll go over configuring etcd to use specific ciphers which enable stronger encryption for securing intra-cluster etcd traffic.</p>"},{"location":"kbs/000020123/#configuring-etcd-rke-and-rancher-ui","title":"Configuring etcd (rke and Rancher UI):","text":"<p>To make the modifications we'll be configuring our rke cluster YAML spec. This setting would be defined, then applied at the command line with the rke CLI, or alternately via the Rancher UI. From within the Rancher UI, navigate to the cluster you're looking to modify, and click edit under the 3 dot menu. From there, you should see a button labeled 'Edit as Yaml'. At the cluster YAML spec view we define the cipher-suites parameter under the etcd service definition. We recommend testing this out in a non-vital cluster before rolling out on important clusters to become familiar with the process.</p> <pre><code>services:\n  etcd:\n    extra_args:\n      cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"\n      election-timeout: \"5000\"\n      heartbeat-interval: \"500\"\n</code></pre>"},{"location":"kbs/000020123/#note","title":"Note:","text":"<p>The cipher suites defined in the example could trade off speed for stronger encryption. Consider the level of ciphers in use and how they could impact the performance of an etcd cluster. Testing should be done to factor in the spec of your hosts (CPU, memory, disk, network, etc...) and the typical types of interacting with Kubernetes as well as the number of resources under management within the k8s cluster.</p>"},{"location":"kbs/000020123/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020124/","title":"How to block external connectivity with Calico","text":"<p>This document (000020124) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020124/#situation","title":"Situation","text":""},{"location":"kbs/000020124/#task","title":"Task","text":"<p>In cases where it is desired to control external connectivity from the cluster, such as to deny or allow specific IP addresses or ports from Pods using the CNI network, a <code>GlobalNetworkPolicy</code> object can be used to control the rules applied to all nodes in the cluster.</p> <p>The <code>GlobalNetworkPolicy</code> is provided by the Calico CRD deployed on RKE clusters.</p>"},{"location":"kbs/000020124/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE cluster configured with the Canal or Calico CNI</li> </ul>"},{"location":"kbs/000020124/#steps","title":"Steps","text":"<p>Configure a YAML manifest with the desired rules, using the <code>nets</code> and/or <code>ports</code> keys, the Calico documentation provides some more information on each field.</p> <p>In the below example, the EC2 metadata is being denied to prevent Pods from accessing the IAM profile credentials of the instance.</p> <pre><code>apiVersion: crd.projectcalico.org/v1\nkind: GlobalNetworkPolicy\nmetadata:\n  name: deny-ec2-metadata\nspec:\n  types:\n  - Egress\n  egress:\n  - action: Deny\n    destination:\n      nets:\n      - 169.254.169.254/32\n  - action: Allow\n    destination:\n      nets:\n      - 0.0.0.0/0\n</code></pre> <p>Deny 80/TCP connectivity external to the cluster</p> <pre><code>apiVersion: crd.projectcalico.org/v1\nkind: GlobalNetworkPolicy\nmetadata:\n  name: deny-http\nspec:\n  types:\n  - Egress\n  egress:\n  - action: Deny\n    protocol: TCP\n    destination:\n      ports:\n      - 80\n  - action: Allow\n    destination:\n      nets:\n      - 0.0.0.0/0\n</code></pre> <p>Apply the YAML file created and test connectivity from a Pod running within the cluster on the CNI network.</p> <p>Note: Pods running with <code>hostnetwork: true</code> will not be included in the <code>GlobalNetworkPolicy</code> as these Pods do not use the CNI network.</p>"},{"location":"kbs/000020124/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020125/","title":"How to Enable Pod Presets","text":"<p>This document (000020125) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020125/#environment","title":"Environment","text":"<p>RKE</p>"},{"location":"kbs/000020125/#situation","title":"Situation","text":""},{"location":"kbs/000020125/#task","title":"Task","text":"<p>This how-to article outlines how to enable pod presets on your cluster. This is done by enabling the <code>PodPreset</code> admission plugin and the <code>settings.k8s.io/v1alpha1</code> API for the kube-apiserver.</p>"},{"location":"kbs/000020125/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes version 1.10 and above</li> <li>Access to edit the cluster in yaml or the cluster.yaml file you used with RKE.</li> </ul>"},{"location":"kbs/000020125/#resolution","title":"Resolution","text":"<p>Get to the cluster yaml in Rancher by editing the cluster and selecting \"edit as yaml\" or opening the RKE cluster.yml file. Modify the kube-api section to resemble the following and hit save or run <code>up</code>:</p> <pre><code>services:\n  kube-api:\n    extra_args:\n      runtime-config: authorization.k8s.io/v1beta1=true,settings.k8s.io/v1alpha1=true\n      enable-admission-plugins: PodPreset,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize\n</code></pre> <p>Notice that <code>settings.k8s.io/v1alpha1/podpreset</code> and <code>PodPreset</code> are added to the runtime-config and admission plugins.</p>"},{"location":"kbs/000020125/#further-reading","title":"Further reading","text":"<p>More details can be found in the kubernetes docs on pod\u00a0presets.</p>"},{"location":"kbs/000020125/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020125/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020126/","title":"How to use nginx /dbg","text":"<p>This document (000020126) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020126/#situation","title":"Situation","text":""},{"location":"kbs/000020126/#what-is-the-dbg-command","title":"What is the /dbg command","text":"<p>/dbg is a program included in the ingress-nginx container image that can be used to show information about the nginx environment and the resulting nginx configuration, which can be helpful when debugging ingress issues in Kubernetes.</p>"},{"location":"kbs/000020126/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster that has ingress enabled with ingress-nginx as the ingress controller</li> <li>A cluster with Linux nodes, nginx will not run on Windows</li> <li>kubectl configured</li> </ul>"},{"location":"kbs/000020126/#using-dbg","title":"Using /dbg","text":"<p>This command needs to be run from inside one of the ingress-nginx pods, so first determine the pod to run it in.</p> <pre><code>&gt; kubectl get pods -n ingress-nginx\nNAME                                    READY   STATUS    RESTARTS   AGE\ndefault-http-backend-67cf578fc4-54jlz   1/1     Running   0          5d\nnginx-ingress-controller-56nss          1/1     Running   0          5d\nnginx-ingress-controller-hscfg          1/1     Running   0          4d21h\nnginx-ingress-controller-n4p22          1/1     Running   0          5d\n</code></pre> <pre><code>&gt; export NGINX_POD=nginx-ingress-controller-n4p22\n</code></pre> <p>If you are diagnosing specific connection issues, you can determine which controller is receiving the traffic by looking through the logs of each.</p>"},{"location":"kbs/000020126/#viewing-ingress-controller-status","title":"Viewing ingress-controller status","text":"<p>/dbg general will show the count of running controllers.</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg  general\n{\n  \"controllerPodsCount\": 3\n}\n</code></pre>"},{"location":"kbs/000020126/#viewing-backend-configuration","title":"Viewing backend configuration","text":"<p>/dbg backends list will list the discovered backends:</p> <p>```</p> <p>kubectl exec -n ingress-nginx $NGINX_POD /dbg backends list cattle-system-rancher-80 upstream-default-backend ```</p> <p>/dbg backends get will show the configuration for the named backend:</p> <pre><code> &gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg backends get cattle-system-rancher-80\n</code></pre>"},{"location":"kbs/000020126/#viewing-ingress-certificate-data","title":"Viewing ingress certificate data","text":"<p>/dbg certs will dump the x509 cert and key for a certificate that nginx has discovered from k8s secrets for the given hostname:</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg certs get &lt;fqdn&gt;\n</code></pre>"},{"location":"kbs/000020126/#viewing-dynamically-generated-nginx-configuration","title":"Viewing dynamically generated nginx configuration","text":"<p>/dbg conf will dump the dynamically generated nginx configuration. To view the configuration for a specific ingress hostname, you could run /dbg conf and then grep for the server_name:</p> <pre><code>&gt; kubectl exec -n ingress-nginx $NGINX_POD /dbg conf | grep \"server_name example.com\" -B2 -A20\n</code></pre>"},{"location":"kbs/000020126/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020127/","title":"Where is Rancher Hosted Prime hosted?","text":"<p>This document (000020127) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020127/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime is hosted in the Cloud. You can have your choice of regions available in North America, EMEA, and APAC geographies.</p>"},{"location":"kbs/000020127/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020128/","title":"How is uptime measured for my Rancher Hosted Prime environment?","text":"<p>This document (000020128) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020128/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime currently uses Pingdom for measuring uptime. Your Rancher Hosted Prime endpoint is tested every one minute and is considered down if a response is not returned within five seconds. Pingdom tests endpoints from over 100 locations across the world. Uptime for the month is calculated by dividing the number of uptime minutes by the total number of minutes in the month. For example, in the month of May, there are 44,640 minutes (60 X 24 X 31). If there were 5 minutes of downtime and 44,635 minutes of uptime, the uptime measurement would be 44,635 / 44,640 = 99.989%. Please also refer to your Service Agreement for the legal definitions for uptime.</p>"},{"location":"kbs/000020128/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020129/","title":"Does Rancher Hosted Prime offer an uptime SLA?","text":"<p>This document (000020129) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020129/#resolution","title":"Resolution","text":"<p>Yes, Rancher Hosted Prime offers a 99.9% uptime Service Level Agreement (SLA). For the details on our SLA, check your master service agreement or contact your Account Executive.</p>"},{"location":"kbs/000020129/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020130/","title":"Can I have more than one Rancher Hosted Prime environment?","text":"<p>This document (000020130) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020130/#resolution","title":"Resolution","text":"<p>Yes, you can have multiple Rancher Hosted Prime environments. Check with your Account Executive for pricing details. There are several use cases where this is preferred, such as having separate environments for development, quality assurance, and production. You will also want multiple Rancher Hosted Prime environments if you need to manage Kubernetes clusters in separate geographies, such as North America, Europe, and Asia.</p>"},{"location":"kbs/000020130/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020131/","title":"Does Rancher Hosted Prime offer a support SLA?","text":"<p>This document (000020131) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020131/#resolution","title":"Resolution","text":"<p>Yes, Rancher Hosted Prime offers the same support Service Level Agreement (SLA) to both Rancher Hosted Prime and customers who manage their own Rancher server instance. Support cases can be opened on the Support Portal. Details of the support offering can be found on the SUSE support page.</p>"},{"location":"kbs/000020131/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020132/","title":"How often are backups taken and retained on Rancher Hosted Prime?","text":"<p>This document (000020132) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020132/#resolution","title":"Resolution","text":"<p>Backups on Rancher Hosted Prime are taken hourly and retained for up to 1 year.</p>"},{"location":"kbs/000020132/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020133/","title":"How to configure the CoreDNS Autoscaler in a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster","text":"<p>This document (000020133) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020133/#situation","title":"Situation","text":""},{"location":"kbs/000020133/#task","title":"Task","text":"<p>During the life of a cluster, you may need to adjust the scaling parameters for the CoreDNS autoscaler. The autoscaler runs as an independent Deployment in the cluster, using the cluster-proportional-autoscaler container to scale up and down the related CoreDNS Deployment, using a linear or ladder pattern.</p>"},{"location":"kbs/000020133/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> <li>The cluster is configured with the coredns addon (enabled by default)</li> </ul> <p>Note When running <code>rke up</code> commands, ensure the <code>.rkestate</code> file for the cluster is present in the working directory as per the documentation here.</p>"},{"location":"kbs/000020133/#steps","title":"Steps","text":"<p>Note: When making the changes, the <code>coredns-autoscaler</code> pod will be restarted with updated command arguments, this will not cause any disruption to DNS resolution. Check the logs of the <code>coredns-autoscaler</code> pod after making changes to confirm they have taken effect.</p>"},{"location":"kbs/000020133/#rancher-provisioned-cluster-managed-by-rancher","title":"Rancher provisioned cluster managed by Rancher","text":"<ol> <li>Navigate to Cluster Management in the Rancher UI and click 'Edit Config' for the cluster.</li> <li>Click 'Edit as YAML'.</li> <li>Locate or add the <code>dns</code> field, using the below as an example to add the desired parameters below:</li> <li> <p>```yaml rancher_kubernetes_engine_config:      [...]      dns:        linear_autoscaler_params:          cores_per_replica: 128          max: 5          min: 1          nodes_per_replica: 4          prevent_single_point_failure: true <pre><code>5. Click 'Save' to update the cluster with the new configuration.\n\n\n#### RKE provisioned cluster managed by RKE\n\n1. Edit the cluster configuration YAML file to configure the `dns` field, using the below as an example to add the desired parameters below:\n\n\n```yaml\ndns:\n     linear_autoscaler_params:\n       cores_per_replica: 128\n       max: 5\n       min: 1\n       nodes_per_replica: 4\n       prevent_single_point_failure: true\n</code></pre></p> </li> <li> <p>Invoke <code>rke up --config &lt;cluster configuration YAML file&gt;</code> to update the cluster.</p> </li> </ol>"},{"location":"kbs/000020133/#additional-information","title":"Additional Information","text":"<p>RKE documentation: https://rke.docs.rancher.com/upgrades/configuring-strategy#replicas-for-dns-and-monitoring-addons</p>"},{"location":"kbs/000020133/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020134/","title":"Do SUSE employees have a login account for my Rancher Hosted Prime environment?","text":"<p>This document (000020134) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020134/#resolution","title":"Resolution","text":"<p>When your Rancher Hosted Prime environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your Rancher Hosted Prime environment. At your discretion, you can create an account for SUSE employees, with the permissions you feel comfortable with, to allow SUSE staff to perform troubleshooting activities. SUSE can also do a Teams or Zoom screen-sharing session to help troubleshoot any issues you have with Rancher Hosted Prime.</p>"},{"location":"kbs/000020134/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020135/","title":"Do SUSE employees have the credentials to my \u201cadmin\u201d account?","text":"<p>This document (000020135) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020135/#resolution","title":"Resolution","text":"<p>When your Rancher Hosted Prime environment is first provisioned, an admin account and password are provided to you. You will be prompted to change your password when you first log in. From this point on, SUSE employees do not have a login account for your Rancher Hosted Prime environment.</p>"},{"location":"kbs/000020135/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020136/","title":"What type of cluster is Rancher Hosted Prime running on?","text":"<p>This document (000020136) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020136/#resolution","title":"Resolution","text":"<p>Rancher Hosted Prime runs on top of SUSE Rancher's k3s which is a fully compliant, lightweight Kubernetes distribution. The cluster consists of two k3s server nodes with a MySQL cluster backend for the datastore.</p>"},{"location":"kbs/000020136/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020137/","title":"Why Does kubectl get Show a Different API Group for a Resource Than Specified?","text":"<p>This document (000020137) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020137/#situation","title":"Situation","text":"<p>Sometimes, when you use the <code>kubectl get</code> command to retrieve Kubernetes resources, the API group and version shown may differ from what you originally specified in the resource specification. For instance, you might create a Deployment resource using the <code>apps/v1</code> API group, but when you check the resource with <code>kubectl get deployment -o yaml</code>, it appears under the <code>extensions/v1beta1</code> API group:</p> <p>Original Deployment YAML:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\n[...]\n</code></pre> <pre><code>Output of 'kubectl get deployment -o yaml':\n</code></pre> <pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\n[...]\n</code></pre>"},{"location":"kbs/000020137/#resolution","title":"Resolution","text":"<p>Kubernetes resources, like Deployments, can exist in multiple API groups. If you don't specify an API group and version when using <code>kubectl</code>, it defaults to the first group listed in the Kubernetes API server's discovery documentation. In the current example, the Deployment resource is available under both the <code>apps/v1</code> and <code>extensions/v1beta1</code> API groups. For backward compatibility, the API server might list <code>extensions/v1beta1</code> first.</p> <p>To ensure that <code>kubectl</code> retrieves the resource from the desired API group, you can fully qualify the resource type. For example, to get Deployment resources from the <code>apps</code> API group, use ' <code>kubectl get deployments.apps -o yaml'</code>. To specify a particular version, you can use ' <code>kubectl get deployments.v1.apps -o yaml'</code>.</p>"},{"location":"kbs/000020137/#cause","title":"Cause","text":"<p>The issue arises because Kubernetes resources can exist in multiple API groups and versions. If you don't specify the API group and version when using <code>kubectl</code>, it defaults to the first group listed in the API server's discovery documentation. This can lead to the resource being displayed under a different API group or version than originally specified, mainly due to backward compatibility.</p>"},{"location":"kbs/000020137/#additional-information","title":"Additional Information","text":"<p>For more details on this behavior, you can check the Kubernetes GitHub Issue #58131.</p> <p>The Kubernetes developer documentation on API resource versioning provides additional insights and can be found here.</p>"},{"location":"kbs/000020137/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020138/","title":"How can I audit or examine RBAC Roles for different accounts within a Kubernetes cluster?","text":"<p>This document (000020138) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020138/#situation","title":"Situation","text":""},{"location":"kbs/000020138/#question","title":"Question","text":"<p>Access to different resources within Kubernetes is handled by role-based access control (RBAC).</p> <p>These resources are referenced by the resource name and API group, for example pods within the core/v1 Kubernetes API group or clusters within the management.cattle.io/v3 API group.</p> <p>A role can be applied (or bound) to different subjects, like a user, group or service account via role bindings, to grant varying degress of access to these resource types at a cluster or namespace level. The access a role grants on a particular resource type is defined by verbs, e.g. get, create, list, watch, delete, and patch etc.</p> <p>This article details methods by which you can audit or examine role-based access control (RBAC) roles for different accounts within a Kubernetes cluster.</p>"},{"location":"kbs/000020138/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster</li> <li>kubectl access to the cluster</li> </ul>"},{"location":"kbs/000020138/#answer","title":"Answer","text":"<p>To audit a specific account, the kubectl command can use the can-i option with the impersonation API to examine what verbs a user has access to, given a specific namespace.</p>"},{"location":"kbs/000020138/#basic-usage","title":"Basic Usage","text":"<p>Basic usage of the kubectl can-i option takes the following form:</p> <pre><code>kubectl auth can-i &lt;verb&gt; &lt;resource&gt; --as account --namespace=&lt;namespace&gt;\n</code></pre>"},{"location":"kbs/000020138/#can-my-user-perform-all-verbs-on-all-resources-am-i-an-admin","title":"Can my user perform all verbs on all resources? Am I an admin?","text":"<pre><code>kuboectl auth can-i \"*\" \"*\"\n</code></pre>"},{"location":"kbs/000020138/#can-the-helm-serviceaccount-delete-pods-in-the-current-namespace-or-cluster-wide","title":"Can the helm serviceaccount delete pods in the current namespace or cluster-wide?","text":"<pre><code>kubectl auth can-i delete pods --as helm\n</code></pre>"},{"location":"kbs/000020138/#is-user1234-an-admin-in-the-testing-namespace-can-they-perform-all-verbs-on-all-resources","title":"Is user1234 an admin in the \"testing\" namespace? Can they perform all verbs on all resources?","text":"<pre><code>kubectl auth can-i \"*\" \"*\" --namespace=testing --as user1234\n</code></pre>"},{"location":"kbs/000020138/#list-option-gives-insight-into-permissions-for-a-user-or-account","title":"List option gives insight into permissions for a user or account","text":"<pre><code>kubectl auth can-i --list --namespace=testing --as user1234\n</code></pre>"},{"location":"kbs/000020138/#additional-tools-for-querying-rbac","title":"Additional tools for querying RBAC","text":"<p>Other open-source third-party tools exist for auditing RBAC, many of which use the Krew plugin framework:</p> <ul> <li>access-matrix - output a CLI matrix of what users or roles have permissions</li> <li>rbac-lookup - perform lookups given subject queries</li> <li>who-can - see \"who-can\" perform a certain verb on a resource, like an opposite view of \"can-i\"</li> </ul> <p>Third-party tools also exist for creating visualizations of the RBAC configuration:</p> <ul> <li>RBack - parse the output from the kubectl commands as json, import into visualization in different formats</li> <li>RBAC-view - visualizing RBAC relationships via a dashboard interface</li> </ul>"},{"location":"kbs/000020138/#further-reading","title":"Further Reading","text":"<ul> <li>Offical Kubernetes RBAC documentation</li> <li>CNCF RBAC Blog post</li> <li>NCCGROUP Examples</li> <li>Krew Plugin Framework</li> <li>RBAC-View</li> <li>RBack</li> <li>who-can</li> <li>rakksess, acess-matrix plugin</li> <li>rbac-lookup</li> </ul>"},{"location":"kbs/000020138/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020139/","title":"What is the kube_config_cluster.yml file that is created after provisioning a cluster with the Rancher Kubernetes Engine (RKE) CLI?","text":"<p>This document (000020139) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020139/#situation","title":"Situation","text":""},{"location":"kbs/000020139/#question","title":"Question","text":"<p>The Rancher Kubernetes Engine (RKE) documentation references a file <code>kube_config_cluster.yml</code> that is generated after running <code>rke up</code>, this article explains what this file is and how to use it.</p>"},{"location":"kbs/000020139/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI provisioned Kubernetes cluster</li> <li>kubectl installed</li> </ul>"},{"location":"kbs/000020139/#answer","title":"Answer","text":"<p>When you provision a Kubernetes cluster using RKE, a kubeconfig file is automatically generated for your cluster.</p> <p>This file is created and saved as <code>kube_config_&lt;cluster&gt;.yml</code>, where <code>&lt;cluster&gt;</code> is the filename of your cluster configuration YAML file. This kubeconfig defines the connection and authentication details to interact with your cluster, using tools such as <code>kubectl</code>.</p> <p>By default, kubectl checks <code>~/.kube/config</code> for a kubeconfig file, but you can specify a different kubeconfig file using the --kubeconfig flag. For example:</p> <pre><code>kubectl --kubeconfig /custom/path/rke/kube_config_cluster.yml get pods\n</code></pre> <p>Or you can export the config path into the KUBECONFIG environment variable, removing the requirement to specify the --kubeconfig flag each time you run kubectl:</p> <pre><code>export KUBECONFIG=\"/custom/path/rke/kube_config_cluster.yml\"\n</code></pre>"},{"location":"kbs/000020139/#further-reading","title":"Further Reading","text":"<ul> <li> <p>RKE Documentation on the kubeconfig</p> </li> <li> <p>Kubernetes Documentation on kubeconfig files</p> </li> </ul>"},{"location":"kbs/000020139/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020140/","title":"What permissions are required for the API token when configuring the Rancher2 Terraform Provider?","text":"<p>This document (000020140) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020140/#situation","title":"Situation","text":""},{"location":"kbs/000020140/#question","title":"Question","text":"<p>When configuring the Rancher2 Terraform Provider, what permissions are required for the API token configured to authenticate with Rancher (as in the below example)?</p> <pre><code>provider \"rancher2\" {\n  api_url    = \"https://rancher.my-domain.com\"\n  access_key = \"${var.rancher2_access_key}\"\n  secret_key = \"${var.rancher2_secret_key}\"\n}\n</code></pre>"},{"location":"kbs/000020140/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x instance</li> <li>The Rancher2 Terraform Provider</li> </ul>"},{"location":"kbs/000020140/#answer","title":"Answer","text":"<p>The user account for which you generate the API token, to configure the Terraform provider, will need permissions granted on any resources that you intend to configure and manage via Terraform.</p>"},{"location":"kbs/000020140/#further-reading","title":"Further Reading","text":"<p>Rancher2 Terraform Provider Documentation</p>"},{"location":"kbs/000020140/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020141/","title":"How to configure container log rotation for the Docker daemon","text":"<p>This document (000020141) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020141/#environment","title":"Environment","text":"<p>Rancher 2.x</p> <p>RKE</p> <p>Docker</p>"},{"location":"kbs/000020141/#situation","title":"Situation","text":""},{"location":"kbs/000020141/#task","title":"Task","text":"<p>As the default setting on Docker is to log using the json-file log driver, without a container log limit, this can lead to disk-fill events on nodes. This article provides steps to configure any nodes running Docker to have a limited container log size and rotate out older container logs.</p>"},{"location":"kbs/000020141/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Node(s) running Docker, using the json-file log driver</li> <li>Permission to edit the <code>/etc/docker/daemon.json</code> and to restart the Docker daemon</li> </ul>"},{"location":"kbs/000020141/#warning","title":"Warning","text":"<ul> <li>You mustrestart the Docker daemon for the changes to take effect for newly created containers</li> <li>As the container logging configuration for pre-existing container is immutable, existing containers do not use the new logging configuration and will need to be redeployed to take on this new configuration.</li> </ul>"},{"location":"kbs/000020141/#resolution","title":"Resolution","text":""},{"location":"kbs/000020141/#resolution_1","title":"Resolution","text":"<ol> <li>Edit the Docker daemon configuration file:</li> </ol> <pre><code>$ vim /etc/docker/daemon.json\n</code></pre> <ol> <li>Add the following lines to the file, to configure a maximum container log file size of 10MB and maintain only 10 of these before deleting the oldest:</li> </ol> <pre><code>{\n     \"log-driver\": \"json-file\",\n     \"log-opts\": {\n       \"max-size\": \"10m\",\n       \"max-file\": \"10\"\n     }\n}\n</code></pre> <ol> <li>Restart the docker daemon to apply the settings to new containers (see Warnings above):</li> </ol> <pre><code>$ systemctl restart docker\n</code></pre>"},{"location":"kbs/000020141/#tips","title":"Tips","text":"<p>You could include this Docker daemon container log rotation configuration in your build/connfiguration management systems, to ensure this is automatically applied to nodes on provisioning, removing any requirement for manual configuration.</p>"},{"location":"kbs/000020141/#further-reading","title":"Further reading","text":"<p>Docker JSON file log driver documentation</p>"},{"location":"kbs/000020141/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020142/","title":"How to configure an internal Elastic Load Balancer (ELB) or Network Load Balancer (NLB) with an Istio Ingress Gateway in Rancher v2.3+","text":"<p>This document (000020142) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020142/#situation","title":"Situation","text":""},{"location":"kbs/000020142/#task","title":"Task","text":"<p>When configuring an Istio Ingress Gateway, a <code>LoadBalancer</code> type service is commonly configured to provide external access to the cluster.</p> <p>By default Kubernetes will provision an internet-facing Classic Load Balancer (CLB). The below steps provide guidance on the annotations needed to configure an internal CLB or Network Load Balancer (NLB) using private subnets.</p>"},{"location":"kbs/000020142/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.3+ managed Kubernetes cluster, runnning in AWS, with the AWS cloud provider configured</li> <li>Istio enabled in the cluster</li> <li>Tagging configured for the VPC and Subnets that will be used for the ELB or NLB</li> </ul> <p>Note: When using Load Balancers with the AWS cloud provider, it is important tag the private and public subnets in the VPC so that kube-controller-manager can correctly discover the specific subnets intended for use.</p> <p>For example the <code>kubernetes.io/role/internal-elb</code> and <code>kubernetes.io/role/elb</code> keys configured respectively, with the value of <code>1</code>.</p>"},{"location":"kbs/000020142/#steps","title":"Steps","text":""},{"location":"kbs/000020142/#enable-the-istio-ingress-gateway","title":"Enable the Istio Ingress Gateway","text":"<p>If the not already enabled, enable the Istio Ingress Gateway. In the drop down list for 'Service Type of Ingress Gateway', select <code>LoadBalancer</code>.</p>"},{"location":"kbs/000020142/#use-an-internal-load-balancer","title":"Use an internal Load Balancer","text":"<p>When editing the Istio Ingress Gateway, click the drop down for Custom Answers.</p> <p>Paste the below in the Variable field, this will automatically populate the value:</p> <pre><code>gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-internal\" = \"true\"\n</code></pre>"},{"location":"kbs/000020142/#use-an-nlb","title":"Use an NLB","text":"<p>To use an NLB, click 'Add Answer' and paste the below in the Variable field:</p> <pre><code>gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-type\" = nlb\n</code></pre> <p>Note: An NLB can be used as an internet-facing loadbancer by using only the above annotation, without adding the aws-load-balancer-internal annotation.</p>"},{"location":"kbs/000020142/#references","title":"References","text":"<p>Istio install options documentation</p> <p>Kubernetes load balancer documentation</p>"},{"location":"kbs/000020142/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020143/","title":"How to set server-tokens to false, to disable the the NGINX header in ingress-nginx responses, within a Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster","text":"<p>This document (000020143) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020143/#situation","title":"Situation","text":""},{"location":"kbs/000020143/#task","title":"Task","text":"<p>The ingress-nginx server-tokens option controls display of the NGINX server header, including version information, in the response to ingress requests. By default this header is enabled; however, due to security concerns in exposing version information, a user might want to disable this on the nginx-ingress-controllers of their Kubernetes cluster(s). This article details how to disable the header, via the server-tokens option, in Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned clusters.</p>"},{"location":"kbs/000020143/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020143/#resolution","title":"Resolution","text":""},{"location":"kbs/000020143/#rke-provisioned-clusters","title":"RKE provisioned clusters","text":"<ol> <li>Add the <code>server-tokens: \"false\"</code> option for nginx into the cluster configuration YAML file as follows:</li> </ol> <pre><code>ingress:\n       provider: nginx\n       options:\n         server-tokens: \"false\"\n</code></pre> <p>Example:</p> <pre><code>nodes:\n  - address: x.x.x.x\n    internal_address: x.x.x.x\n    user: ubuntu\n    role: [controlplane,worker,etcd]\ningress:\n    provider: nginx\n    options:\n      server-tokens: \"false\"\nservices:\netcd:\n    snapshot: true\n    creation: 6h\n    retention: 24h\n</code></pre> <ol> <li>Execute <code>rke up</code> to update the cluster with the new configuration. N.B. Ensure the <code>.rkestate</code> file for the cluster is present in the working directory when invoking <code>rke up</code> per the documentation here:</li> </ol> <pre><code>rke up --config &lt;cluster configuration YAML file&gt;\n</code></pre>"},{"location":"kbs/000020143/#rancher-v2x-provisioned-clusters","title":"Rancher v2.x provisioned clusters","text":"<ol> <li>Navigate to the Cluster within the Rancher UI and click 'Edit Cluster'.</li> <li>Click 'Edit as YAML'.</li> <li>Add the <code>server-tokens: \"false\"</code> option for nginx into the cluster configuration YAML file as follows:</li> </ol> <pre><code>rancher_kubernetes_engine_config:\n[...]\n  ingress:\n      provider: nginx\n      options:\n        server-tokens: \"false\"\n</code></pre> <ol> <li>Click 'Save' to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020143/#further-reading","title":"Further reading","text":"<p>ingress-nginx documentation on the server-tokens options</p>"},{"location":"kbs/000020143/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020144/","title":"How to enable debug level logging for the Rancher Cluster/Project Alerting Alertmanager instance, in a Rancher v2.x managed cluster?","text":"<p>This document (000020144) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020144/#situation","title":"Situation","text":""},{"location":"kbs/000020144/#task","title":"Task","text":"<p>This article details how to enable debug level logging on the Alertmanager instance in a Rancher v2.x managed Kubernetes cluster, which may assist when troubleshooting cluster or project alerting.</p>"},{"location":"kbs/000020144/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster</li> <li>Cluster or project alerting configured</li> </ul>"},{"location":"kbs/000020144/#resolution","title":"Resolution","text":"<ol> <li>Within the Rancher UI navigate to the System Project of the relevant cluster and click on the Apps view.</li> <li>Click 'Upgrade' on the cluster-alerting app.</li> <li>In the Answers section click 'Add Answer' and add the variable <code>alertmanager.logLevel</code> with a value of <code>debug</code>.</li> <li>Click upgrade to save the change and update the Alertmanager instance with the debug log level.</li> <li>Navigate to the cattle-prometheus namespace within the System Project for the cluster, and view the logs of the alertmanager-cluster-alerting-0 Pod running for the alertmanager-cluster-alerting StatefulSet. You should see <code>level=debug</code> log messages, such as in the following example, confirming debug level logging has been successfully configured:</li> </ol> <p><code>plaintext     level=debug  ts=2019-07-09T15:03:37.511451301Z caller=dispatch.go:104  component=dispatcher msg=\"Received alert\" alert=[433a194][active]     level=debug  ts=2019-07-09T15:03:38.511774835Z caller=dispatch.go:430  component=dispatcher  aggrGroup=\"{}/{group_id=\\\"c-5h85q:event-alert\\\"}/{rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\"}:{event_message=\\\"Scaled  up replica set mynginx2-7994cd84ff to 1\\\",  resource_kind=\\\"Deployment\\\",  rule_id=\\\"c-5h85q:event-alert_deployment-event-alert\\\",  target_name=\\\"mynginx2\\\", target_namespace=\\\"default\\\"}\" msg=flushing  alerts=[[433a194][active]]</code></p>"},{"location":"kbs/000020144/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020145/","title":"How to generate a Longhorn Support Bundle","text":"<p>This document (000020145) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020145/#situation","title":"Situation","text":""},{"location":"kbs/000020145/#task","title":"Task","text":"<p>When troubleshooting an issue with Longhorn, Rancher Support may request a Longhorn Support Bundle, which can be generated via the Longhorn UI, and contains system information and logs.</p>"},{"location":"kbs/000020145/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x managed Kubernetes cluster with Longhorn deployed</li> </ul>"},{"location":"kbs/000020145/#steps","title":"Steps","text":"<p>Generate the Support Bundle:</p> <ol> <li>Log in into the Rancher UI.</li> <li>Select the cluster with Longhorn depoyed.</li> <li>Select the Project where Longhorn is deployed (typically under the System project).</li> <li>Click on \"Apps\" button.</li> <li> <p>Find the Longhorn system app and click on the index.html button</p> </li> <li> <p>Click on the \"Generate Support Bundle\" in the bottom left of the screen</p> </li> <li> <p>Type in a description and click generate (issue url is optional)</p> </li> </ol> <p>Upload the Support Bundle</p> <p>Generally Longhorn Support Bundles files are small in size; however, if the pack is too large to upload directly to the ticket, please request a temporary upload location.</p>"},{"location":"kbs/000020145/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020146/","title":"How to add additional scrape configs to a Rancher cluster or project monitoring prometheus","text":"<p>This document (000020146) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020146/#situation","title":"Situation","text":""},{"location":"kbs/000020146/#task","title":"Task","text":"<p>The Rancher cluster and project monitoring tools, allow you to monitor cluster components and nodes, as well as workloads and custom metrics from any HTTP or TCP/UDP metrics endpoint that these workloads expose.</p> <p>This article will detail how to manually define additional scrape configs for either the cluster or project monitoring prometheus instance, where you want to scrape other metrics.</p> <p>Whether to define the additional scrape config at the cluster or project level would depend on the desired scope for the metrics and possible alerts. If you wish to scope the metrics scraped, and thus possible alerts configured for these metrics, to a project, you could configure the additional scrape config at the project monitoring level. If you wish to scope the metrics at the cluster level, so only those with cluster admin access could see the metrics or configure alerts, you could configure the additional scrape config at the cluster monitoring level.</p>"},{"location":"kbs/000020146/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.2.x, v2.3.x or v2.4.x managed cluster, with cluster monitoring enabled (and optionally project monitoring enabled, if you wish to configure the additonal scrape config at the project scope).</li> </ul>"},{"location":"kbs/000020146/#resolution","title":"Resolution","text":"<p>For both cluster and project monitoring the additional scrape config(s) are defined in the Answers section of the Monitoring configuration. This can be found as follows:</p> <ul> <li>Cluster Monitoring: As a user with permissions to edit cluster monitoring (global admins and cluster owners by default), navigate to the cluster view and click Tools -&gt; Monitoring from the menu bar. Click 'Show advanced options' at the bottom right.</li> <li>Project Monitoring: As a user with permissions to edit project monitoring (global admins, cluster owners and project owners by default), navigate to the project and click Tools -&gt; Monitoring from the menu bar. Click 'Show advanced options' at the bottom right.</li> </ul> <p>You can add an array of prometheus.additionalScrapeConfigs in the Answers section here.</p> <p>For example to define a scrape job of the following:</p> <pre><code> - job_name: \"prometheus\"\n   static_configs:\n   - targets:\n     - \"localhost:9090\"\n</code></pre> <p>You would add the following two definitions to the Answers section:</p> <p>prometheus.additionalScrapeConfigs[0].job_name = prometheus prometheus.additionalScrapeConfigs[0].static_configs[0].targets[0] = localhost:9090</p> <p>After adding the answers, click 'Save' and you should now be able to view the target and its status within the Prometheus UI under Status -&gt; Targets.</p>"},{"location":"kbs/000020146/#further-reading","title":"Further reading","text":"<p>Documentation on the Rancher cluster monitoring can be found here and for Rancher project monitoring here.</p>"},{"location":"kbs/000020146/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020147/","title":"How to enable SSL passthrough on the nginx-ingress controller in RKE and RKE2 clusters","text":"<p>This document (000020147) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020147/#environment","title":"Environment","text":"<ul> <li>A standalone or Rancher-provisioned RKE or RKE2 cluster, deployed within the bundled ingress-nginx controller</li> </ul>"},{"location":"kbs/000020147/#situation","title":"Situation","text":""},{"location":"kbs/000020147/#this-article-details-how-to-enable-ssl-passthrough-on-the-bundled-nginx-ingress-controller-in-an-rke-or-rke2-cluster","title":"This article details how to enable SSL passthrough on the bundled nginx-ingress controller in an RKE or RKE2 cluster","text":""},{"location":"kbs/000020147/#resolution","title":"Resolution","text":""},{"location":"kbs/000020147/#standalone-rke-clusters","title":"Standalone RKE clusters:","text":"<ol> <li>Edit the cluster configuration YAML file to include the <code>enable-ssl-passthrough: true</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     extra_args:\n       enable-ssl-passthrough: true\n</code></pre> <ol> <li>Apply the changes to the cluster, by invoking <code>rke up</code>:</li> </ol> <pre><code>rke up --config &lt;cluster configuration yaml file&gt;\n</code></pre>"},{"location":"kbs/000020147/#standalone-rke2-clusters","title":"Standalone RKE2 clusters:","text":"<ol> <li>Create the file /var/lib/rancher/rke2/server/manifest/rke2-ingress-nginx-config.yaml with the desired HelmChartConfig on server nodes within the cluster:</li> </ol> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n     name: rke2-ingress-nginx\n     namespace: kube-system\nspec:\n     valuesContent:  |-\n       controller:\n         extraArgs:\n           enable-ssl-passthrough: true\n</code></pre> <ol> <li>Restart the rke2-server process to trigger a helm installation Job for the rke2-ingress-nginx chart, applying the new configuration:</li> </ol> <pre><code>systemctl restart rke2-server\n</code></pre>"},{"location":"kbs/000020147/#rancher-provisioned-rke-clusters","title":"Rancher-provisioned RKE clusters:","text":"<ol> <li>Navigate to\u00a0Cluster Management in the Rancher UI</li> <li>Click\u00a0Edit Config for the relevant RKE cluster</li> <li>Click\u00a0Edit as YAML</li> <li>Include the <code>enable-ssl-passthrough: true</code> option for the ingress, as follows:</li> </ol> <pre><code>ingress:\n     provider: nginx\n     extra_args:\n       enable-ssl-passthrough: true\n</code></pre> <ol> <li>Click\u00a0Save</li> </ol>"},{"location":"kbs/000020147/#rancher-provisioned-rke2-clusters","title":"Rancher-provisioned RKE2 clusters:","text":"<ol> <li>Navigate to\u00a0Cluster Management in the Rancher UI</li> <li>Click\u00a0Edit Config for the relevant RKE cluster</li> <li>Click Additional Manifests and add the desired HemChartConfig manifest:</li> </ol> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n     name: rke2-ingress-nginx\n     namespace: kube-system\nspec:\n     valuesContent: |-\n        extraArgs:\n          enable-ssl-passthrough: true\n</code></pre> <ol> <li>Click Save</li> </ol>"},{"location":"kbs/000020147/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020149/","title":"Resolving conntrack table full error messages: 'nf_conntrack: table full, dropping packets'","text":"<p>This document (000020149) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020149/#situation","title":"Situation","text":""},{"location":"kbs/000020149/#issue","title":"Issue","text":"<p>When investigating a connectivity issue, you may experience errors like the below in the system logs:</p> <pre><code>nf_conntrack: table full, dropping packets\n</code></pre> <p>This error indicates the connection tracking table size has been exhausted. This can manifest with different symptoms, such as intermittent or consistent network timeouts.</p> <p>The conntrack table keeps state on open connections that the kernel is translating. This occurs often in a Kubernetes cluster when pods access an external endpoint, or another service within the cluster. These scenarios use NAT and stateful firewall rules which are maintained as entries in the conntrack table.</p>"},{"location":"kbs/000020149/#investigation","title":"Investigation","text":"<p>By default, the table size is calculated based on the memory allocated to the node. This does not fit all workloads demands, for example in a microservice environment typically a higher number of inter-service connections could be expected without consuming a high amount of memory.</p> <p>To output the current max table size:</p> <pre><code>cat /proc/sys/net/netfilter/nf_conntrack_max\n</code></pre> <p>To get a point in time count of the current entries in the table:</p> <pre><code>cat /proc/sys/net/netfilter/nf_conntrack_count\n</code></pre> <p>Note: With the <code>conntrack</code> package installed, you can also use <code>conntrack -C</code></p> <p>If the <code>nf_conntrack_count</code> and <code>nf_conntrack_max</code> are close, it is indicating that the current workload requires a larger table size.</p> <p>If the current number of entries are not approaching the table size, this could indicate that a burst of workload was experienced historically, in a containerized environment this can be common. For example, if the high-traffic Pods may now running on different nodes.</p>"},{"location":"kbs/000020149/#resolution","title":"Resolution","text":"<p>Increasing the conntrack table size is achieved with <code>sysctl</code>.</p> <p>Calculate a higher value, this can be applied to the node immediately with:</p> <pre><code>sysctl -w net.netfilter.nf_conntrack_max=&lt;value&gt;\n</code></pre> <p>To persist through reboot, add the tunable to either <code>/etc/sysctl.conf</code>, or a specific config file in <code>/etc/sysctl.d</code>.</p> <p>For example, if your Linux distribution follows the /etc/sysctl.d/ directory structure:</p> <pre><code>echo \"net.netfilter.nf_conntrack_max=&lt;value&gt;\" &gt; /etc/sysctl.d/10-conntrack-max.conf\nsysctl -p /etc/sysctl.d/10-conntrack-max.conf\n</code></pre> <p>This creates a new config file to set the table size at each boot.</p> <p>Additionally, if you configure nodes with configuration management, UserData, or build custom images etc., you may wish to add this to your usual approach to configure this for future nodes.</p>"},{"location":"kbs/000020149/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020150/","title":"Does Rancher Hosted Prime provide downstream clusters?","text":"<p>This document (000020150) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020150/#resolution","title":"Resolution","text":"<p>No, currently our Rancher Hosted Prime service only includes the Rancher Manager software. Downstream clusters are not provided as part of the service. You will either need to use Rancher Manager to provision Kubernetes clusters on-premise or in a cloud provider account that you own. You can also import existing Kubernetes clusters into Rancher Hosted Prime.</p>"},{"location":"kbs/000020150/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020151/","title":"How to grant users access to Grafana with minimal permissions","text":"<p>This document (000020151) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020151/#situation","title":"Situation","text":""},{"location":"kbs/000020151/#deprecation-note","title":"\\* Deprecation note *","text":""},{"location":"kbs/000020151/#there-is-now-a-view-monitoring-role-in-monitoring-v2-which-a-user-can-be-granted-on-the-system-project-this-grants-user-monitoring-access-thus-the-article-is-no-more-maintained-please-refer-to-this-for-more-information-on-rbac","title":"There is now a \"View Monitoring\" role in Monitoring v2, which a user can be granted on the System project. This grants user monitoring access. Thus, the article is no more maintained. Please, refer to this\u00a0for more information on RBAC.","text":""},{"location":"kbs/000020151/#task","title":"Task","text":"<p>You can follow these directions to create a new user and grant minimal permissions to view cluster monitoring and Grafana graphs in your Kubernetes cluster.</p>"},{"location":"kbs/000020151/#requirements","title":"Requirements","text":"<ul> <li>Rancher v2.x</li> <li>Monitoring enabled in your cluster</li> </ul>"},{"location":"kbs/000020151/#background","title":"Background","text":"<p>You may have a use case to grant permissions to a user to view cluster monitoring metrics and graphs, but don't want that same user to be able to see other information or perform any actions on your cluster. This how-to guide will show you how to achieve this.</p>"},{"location":"kbs/000020151/#solution","title":"Solution","text":"<ol> <li>If you have not already, create a new user in Rancher. Go to the Global view and click on the Users menu. Click the <code>Add Users</code> button in the top right corner. Select the desired Username, Password, and Display Name. For Global Permissions, select User-Base and leave all Custom permissions unchecked. Click the <code>Create</code> button at the bottom of the form. Let's assume we are using the username <code>johndoe</code>.</li> <li>Go to the Security menu and select Roles. Select the Projects tab and click the <code>Add Project Role</code> button. In the name field, enter Services Proxy. Under Grant Resources, click the <code>+ Add Resource</code> button. Check the Get and List boxes and enter <code>services/proxy</code> in the Resource field. Note, you'll see it changes this to <code>serivces/proxy (Custom)</code> which is normal. Click the <code>Create</code> button at the bottom to create the new project role.</li> <li>Next, go to the cluster view for your cluster and select Members from the menu. Click the <code>Add Members</code> button in the top right corner. In the Members dropdown, select <code>johndoe</code> and select Member for Cluster Permissions. Click the <code>Create</code> button at the bottom of the form.</li> <li>Now navigate to the System project in your cluster. Go to the Members menu and click the <code>Add Member</code> button. Enter <code>johndoe</code> in the Member field and select <code>Services Proxy</code> under Project Permissions. Click the <code>Create</code> button at the bottom of the form.</li> <li>The <code>johndoe</code> user should now be able to log into Rancher and see the cluster dashboard with the Grafana icons. Clicking the Grafana icons should open a new browser window that will show the user various graphs and statistics for the cluster. This user should not be able to perform other operations, like view or launch new workloads in the cluster.</li> </ol>"},{"location":"kbs/000020151/#further-reading","title":"Further Reading","text":"<p>For more detailed information on how RBAC works in Rancher and Kubernetes, see the following links:</p> <ul> <li>Role-Based Access Control (RBAC) in Rancher</li> <li>Using RBAC Authorization in Kubernetes</li> </ul>"},{"location":"kbs/000020151/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020152/","title":"Updating SSL cert in Rancher v2.x with the same CA","text":"<p>This document (000020152) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020152/#environment","title":"Environment","text":"<p>Rancher</p>"},{"location":"kbs/000020152/#situation","title":"Situation","text":"<p>Renew the Rancher SSL/TLS certificate with the same CA</p>"},{"location":"kbs/000020152/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> <li>Rancher on a Kubernetes Cluster see documentation for more information</li> <li>The new certificate must have the same root CA as the current certificate.</li> <li>A copy of the certificate and private key in Base64 format Doc</li> <li>A copy of the root and intermediate CA certificate (Sometimes called the certificate chain).</li> </ul>"},{"location":"kbs/000020152/#assumptions","title":"Assumptions","text":"<ul> <li>kubectl access to the Rancher local cluster</li> <li>The certificate is stored as server.crt</li> <li>The private key is stored as tls.key</li> <li>The root CA is stored as root-ca.crt</li> <li>The intermediate CA is stored as intermediate-ca.crt</li> </ul>"},{"location":"kbs/000020152/#resolution","title":"Resolution","text":""},{"location":"kbs/000020152/#install-steps","title":"Install Steps","text":"<ol> <li>Verify private key doesn't have a passphrase using the command listed below. If the following command asks for a passphrase, then it is password-protected, and this must be removed.</li> </ol> <pre><code>openssl rsa -in tls.key -noout\n</code></pre> <ol> <li>Remove the passphrase (skip this step if the previous command didn't ask for a passphrase):</li> </ol> <pre><code>mv tls.key tls-pass.key\nopenssl rsa -in tls-pass.key -out tls.key\nEnter your passphrase here\n</code></pre> <ol> <li>Create the certificate chain. If there is an\u00a0additional intermediate certs please add them at this step.</li> </ol> <p>NB: Order is important!</p> <pre><code>cat server.crt intermediate-ca.crt root-ca.crt &gt; tls.crt\n</code></pre> <ol> <li>Backup the current certificate:</li> </ol> <pre><code>kubectl -n cattle-system get secret tls-rancher-ingress -o yaml &gt; tls-rancher-ingress-bk.yaml\n</code></pre> <ol> <li>Remove the current certificate:</li> </ol> <pre><code>kubectl -n cattle-system delete secret tls-rancher-ingress\n</code></pre> <ol> <li>Install the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n   --cert=tls.crt \\\n   --key=tls.key\n</code></pre>"},{"location":"kbs/000020152/#verification-steps","title":"Verification Steps","text":"<ul> <li>Run the following command to verify the new certificate. (Replace Rancher with your Rancher URL):</li> </ul> <pre><code>curl --insecure -v https://&lt;&lt;Rancher&gt;&gt; 2&gt;&amp;1 | awk 'BEGIN { cert=0 } /^\\* SSL connection/ { cert=1 } /^\\*/ { if (cert) print }'\n</code></pre> <ul> <li>Example output:</li> </ul> <pre><code>* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\n* ALPN, server did not agree to a protocol\n* Server certificate:\n*  subject: OU=Domain Control Validated; CN=*.rancher.tools\n*  start date: Jul  2 00:42:01 2019 GMT\n*  expire date: May  2 00:19:41 2020 GMT\n*  issuer: C=BE; O=GlobalSign nv-sa; CN=AlphaSSL CA - SHA256 - G2\n*  SSL certificate verify ok.\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* old SSL session ID is stale, removing\n* Mark bundle as not supporting multiuse\n* Connection #0 to host lab.rancher.tools left intact\n</code></pre> <ul> <li>NOTE: Some browsers will cache the certificate. So you might have to close the browser and reopen it in order to get the new certificate.</li> </ul>"},{"location":"kbs/000020152/#rollback-steps","title":"Rollback Steps","text":"<ol> <li>Backup the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system get secret tls-rancher-ingress -o yaml &gt; tls-rancher-ingress-new.yaml\n</code></pre> <ol> <li>Remove the new certificate:</li> </ol> <pre><code>kubectl -n cattle-system delete secret tls-rancher-ingress\n</code></pre> <ol> <li>Re-install the old certificate:</li> </ol> <pre><code>kubectl -n cattle-system apply -f tls-rancher-ingress-bk.yaml\n</code></pre>"},{"location":"kbs/000020152/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020152/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020154/","title":"How to troubleshoot SNI enabled endpoints with curl and openssl","text":"<p>This document (000020154) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020154/#situation","title":"Situation","text":""},{"location":"kbs/000020154/#issue","title":"Issue","text":"<p>A modern webserver hosting or proxying to multiple backend domain names will often be configured to use SNI (Server Name Indication).</p> <p>SNI allows multiple SSL-protected domains to be hosted on the same IP address, and is commonly used in Kubernetes with ingress controllers, for example, the nginx ingress controller.</p> <p>As the SNI extension requires a slight change to the conversation between client and server - the hostname must be provided in the <code>Hello</code> message to correctly access the associated domain name.</p> <p>This can present an issue when troubleshooting a node or pod directly, where an IP address is used.</p>"},{"location":"kbs/000020154/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>The <code>curl</code> and/or <code>openssl</code> command installed</li> <li>Network access to the endpoint you wish to troubleshoot</li> </ul>"},{"location":"kbs/000020154/#steps","title":"Steps","text":"<p>To perform an SNI-compliant request using an IP address, use the following commands replacing the domain name and IP address.</p> <ul> <li>Using the <code>curl</code> command:</li> </ul> <pre><code>curl -v --resolve domain.com:443:&lt;ip address&gt; https://domain.com\n</code></pre> <ul> <li>Using <code>openssl</code> can be useful to obtain details about the certificate configured:</li> </ul> <pre><code>openssl s_client -showcerts -servername domain.com -connect &lt;ip address&gt;:443\n</code></pre>"},{"location":"kbs/000020154/#further-reading","title":"Further reading","text":"<p>More information on SNI can be found here.</p>"},{"location":"kbs/000020154/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020155/","title":"What is Rancher Prime Hosted?","text":"<p>This document (000020155) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020155/#resolution","title":"Resolution","text":"<p>Rancher Prime Hosted is a software-as-a-service offering from SUSE. As the name implies, Rancher Manager is completely hosted for you in the cloud. SUSE takes care of the installation, upgrade, and day-to-day operations of your Rancher Manager control plane. Using Rancher Manager, you can then add your own cloud-based, on-premise, AKS, GKE, or EKS clusters. For more information, see the\u00a0blog announcement or SUSE Rancher Hosted product page .</p>"},{"location":"kbs/000020155/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020156/","title":"Can I move an existing Kubernetes cluster to Rancher Hosted Prime?","text":"<p>This document (000020156) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020156/#resolution","title":"Resolution","text":"<p>Existing Kubernetes clusters can be imported into Rancher Hosted Prime. However, you cannot currently move a cluster that is already managed by Rancher to Rancher Hosted Prime. We are currently looking to enhance our management capabilities to allow users to move clusters between Rancher clusters. See GitHub issue 16471. As a workaround, you can redeploy workloads running in an existing cluster over to a new Rancher Hosted Prime managed cluster.</p>"},{"location":"kbs/000020156/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020158/","title":"How to use the calicoctl CLI in an RKE CLI or Rancher-provisioned RKE cluster","text":"<p>This document (000020158) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020158/#environment","title":"Environment","text":"<ul> <li> <p>An RKE Kubernetes cluster provisioned with the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x</p> </li> <li> <p>The Calico or Canal Container Networking Interface (CNI) Plugin (Canal is the default in both RKE CLI and Rancher provisioned RKE clusters).</p> </li> <li> <p>A cluster-admin level kube config sourced via $KUBECONFIG on a host running Docker</p> </li> </ul>"},{"location":"kbs/000020158/#situation","title":"Situation","text":"<p>The <code>calicoctl</code> CLI provides an interface for managing calico network and security policy.</p> <p>In RKE Kubernetes clusters provisioned by the Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x, and which use the Calico or Canal Container Networking Interface (CNI) Plugin, <code>calicoctl</code> can be used to configure Calico GlobalNetworkPolicy and NetworkPolicy resources.</p>"},{"location":"kbs/000020158/#resolution","title":"Resolution","text":"<p>N.B. The commands in this section should be run from a host running Docker, with a cluster-admin level kube config sourced.</p> <p>For this example, we will demonstrate creating an empty GlobalNetworkPolicy resource via <code>calicoctl</code>.</p>"},{"location":"kbs/000020158/#1-set-kubeconfig-environment-variable-to-the-cluster-admin-kube-config","title":"1. Set $KUBECONFIG environment variable to the cluster-admin kube config","text":"<p>With the cluster-admin level kube config file present on the host, execute <code>export KUBECONFIG=&lt;full path to cluster-admin kube config&gt;</code> replacing with the full path of the kube config.</p>"},{"location":"kbs/000020158/#2-create-the-desired-resource-in-the-working-directory","title":"2. Create the desired resource in the working directory","text":"<p>Create a YAML file in the working directory with the NetworkPolicy resource definition(s) you want to apply to the cluster.</p> <p>For this example create a file named <code>globalpolicy.yaml</code> in the working directory with the following contents:</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: GlobalNetworkPolicy\nmetadata:\n  name: allow-tcp-port-6379\n</code></pre>"},{"location":"kbs/000020158/#3-determine-the-calico-node-version-of-the-cluster","title":"3. Determine the calico-node version of the cluster","text":"<p>First get the version of the <code>calico-node</code> container running in the cluster.</p> <p>In a cluster with the Canal CNI Network Provider, run the following, with the admin kube config sourced:</p> <pre><code>CALICOVERSION=`kubectl -n kube-system get daemonset canal -o yaml | grep 'rancher/mirrored-calico-node:v' | tail -n1 | cut -d: -f3`\necho $CALICOVERSION\n</code></pre> <p>In a cluster with the Calico CNI Network Provider, run the following, with the admin kube config sourced:</p> <pre><code>CALICOVERSION=`kubectl -n kube-system get daemonset calico-node -o yaml | grep 'rancher/mirrored-calico-node:v' | tail -n1 | cut -d: -f3`\necho $CALICOVERSION\n</code></pre>"},{"location":"kbs/000020158/#4-run-calicoctl","title":"4. Run <code>calicoctl</code>","text":"<p>With the <code>calico-node</code> version determined and now set in the variable <code>$CALICOVERSION</code>, <code>calicoctl</code> can be invoked. This is done by running the <code>calico/ctl</code> image, with the version matching the <code>calico-node</code>. The kube config file is mounted into the container, as is the present working directory (at the path <code>/host</code>), so that the desired resource (in this example in the file globalpolicy.yaml) is available.</p> <p>To execute <code>calicoctl</code> run the following command, altering the filename as applicable to the resource you have created in the working directory:</p> <pre><code>docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION apply -f /host/globalpolicy.yaml\n</code></pre> <p>We can now view the GlobalNetworkPolicy resource by using <code>calicoctl get</code> as follows:</p> <pre><code>docker run --rm -v $KUBECONFIG:/root/.kube/config -v $(pwd):/host -e KUBECONFIG=/root/.kube/config -e DATASTORE_TYPE=kubernetes calico/ctl:$CALICOVERSION get globalnetworkpolicy allow-tcp-port-6379 -o yaml\n</code></pre> <p>This should return output similar to the following:</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: GlobalNetworkPolicy\nmetadata:\n  creationTimestamp: \"2020-04-08T15:12:45Z\"\n  name: allow-tcp-port-6379\n  resourceVersion: \"9033\"\n  uid: df2875a6-1142-4fe0-9f0c-5dc1372bd2c5\nspec:\n  types:\n  - Ingress\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020158/#additional-information","title":"Additional Information","text":"<ul> <li> <p>Calico's Get started with Calico network policy.</p> </li> <li> <p>The <code>calicoctl</code> user reference documentation.</p> </li> </ul>"},{"location":"kbs/000020158/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020160/","title":"How to run multiple ingress controllers","text":"<p>This document (000020160) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020160/#environment","title":"Environment","text":"<p>Rancher, Kubernetes</p>"},{"location":"kbs/000020160/#situation","title":"Situation","text":""},{"location":"kbs/000020160/#why-use-multiple-ingress-controllers","title":"Why use multiple ingress controllers?","text":"<p>At large numbers of ingresses and related workloads, a single ingress-controller can be a bottleneck in both throughput and reliability. It is recommended to shard ingresses across multiple ingress controllers in these scenarios.</p>"},{"location":"kbs/000020160/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster created by Rancher v2.x or RKE</li> <li>A Linux cluster, Windows is currently not supported</li> <li>Helm installed and configured</li> </ul>"},{"location":"kbs/000020160/#overview","title":"Overview","text":"<p>At a high level, the process for sharding ingresses is to build out one or more extra ingress controllers and logically separate your ingresses to split the load between your ingress controllers evenly. This separation is handled through annotations on the ingresses. When an nginx-ingress-controller pod starts up with an ingressClass set, it will only try to satisfy ingresses that are annotated with the same ingressClass. This allows you to run as many ingress-controllers as needed to satisfy your ingress needs.</p>"},{"location":"kbs/000020160/#creating-extra-nginx-ingress-controller-charts","title":"Creating extra nginx-ingress-controller charts","text":"<p>It is recommended to use the community nginx-ingress helm chart to install the extra ingress-controllers with NodePort services.</p> <p>This deployment method allows you to run multiple ingress controllers on a single node, as there are no conflicting ports. You are required to route traffic to the correct ingress controller ports through an external load balancer.</p> <p>Deploy a second default backend and ingress-controller from the nginx-ingress helm chart with the following values: <code>controller.ingressClass</code> - unique name of the ingress class, such as <code>ingress-nginx-2</code> <code>controller.service.type=NodePort</code></p> <p><code>controller.service.nodePorts.http</code> - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined, one will be randomly assigned</p> <p><code>controller.service.nodePorts.https</code> - define the NodePort between 30000-32767 you want to expose for http traffic. Optional, if not defined one will be randomly assigned</p> <p><code>controller.kind=DaemonSet</code></p> <p>For more configuration options, see the chart readme .</p> <p>An example daemon set install would be:</p> <pre><code>helm repo add stable https://kubernetes-charts.storage.googleapis.com\nhelm install nginx-ingress-second -n ingress-nginx stable/nginx-ingress --set controller.ingressClass=\"ingress-class-2\" --set controller.service.type=NodePort --set controller.kind=DaemonSet\n</code></pre> <p>This will create an ingress-nginx daemon set and service. This ingress controller will handle any ingress routed to it tagged with the annotation <code>kubernetes.io/ingress.class: ingress-class-2</code></p>"},{"location":"kbs/000020160/#sharding-ingresses","title":"Sharding Ingresses","text":"<p>It is recommended to shard (split) your ingresses in a way that evenly splits load and configuration size between ingress controllers.</p> <p>Sharding in this way means changing DNS and ingress hosts so that traffic for ingresses is sent to the correct ingress controllers, typically through an external load balancer.</p> <p>The process for sharding ingresses is to tag each ingress with the ingressClass for the ingress controller you want to route them through. For example:</p> <pre><code>apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: app_1_ingress\n  annotations:\n    kubernetes.io/ingress.class: \"ingress-class-2\"\nspec:\n</code></pre> <p>Once annotated with an ingressClass, these ingresses are now only handled by the ingress-controller that has that ingressClass.</p> <p>In the default configuration, the Rancher-provided nginx-ingress-controller will only handle ingresses that either have the default ingress.class annotation of <code>nginx</code> or do not have an ingress.class annotation at all.</p>"},{"location":"kbs/000020160/#next-steps","title":"Next steps","text":"<p>From here it is just a matter of ensuring that the traffic for each ingress is routed to the correct nodePort on the nodes that the daemonset is targeted against.</p> <p>If you did not specify a nodePort when deploying the chart, you can determine the nodePort that was assigned by checking the service created:</p> <pre><code>$ kubectl describe svc -n ingress-nginx nginx-ingress-second\nName:                     nginx-ingress-second-controller\nNamespace:                ingress-nginx\nLabels:                   app=nginx-ingress\n                          chart=nginx-ingress-1.35.0\n                          component=controller\n                          heritage=Helm\n                          release=nginx-ingress-second\nAnnotations:              field.cattle.io/publicEndpoints:\n                            [{\"addresses\":[\"13.210.157.241\"],\"port\":30155,\"protocol\":\"TCP\",\"serviceName\":\"ingress-nginx:nginx-ingress-second-controller\",\"allNodes\":tr...\nSelector:                 app.kubernetes.io/component=controller,app=nginx-ingress,release=nginx-ingress-second\nType:                     NodePort\nIP:                       10.43.139.23\nPort:                     http  80/TCP\nTargetPort:               http/TCP\nNodePort:                 http  30155/TCP\nEndpoints:                &lt;none&gt;\nPort:                     https  443/TCP\nTargetPort:               https/TCP\nNodePort:                 https  30636/TCP\nEndpoints:                &lt;none&gt;\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   &lt;none&gt;\n</code></pre> <p>In this example, the service is exposed on every node on ports 30155 for http and 30636 for https</p>"},{"location":"kbs/000020160/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020160/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020161/","title":"Why does the kubelet certificate still show as expired after performing a cluster certificate rotation in an Rancher Kubernetes Engine (RKE) CLI or Rancher v2.x provisioned cluster?","text":"<p>This document (000020161) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020161/#situation","title":"Situation","text":""},{"location":"kbs/000020161/#question","title":"Question","text":"<p>Why is Kubelet certificate still indicating expired after performing a cluster certificate rotation?</p>"},{"location":"kbs/000020161/#pre-requisite","title":"Pre-requisite","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) or Rancher v2.x provisioned Kubernetes cluster</li> </ul>"},{"location":"kbs/000020161/#answer","title":"Answer","text":"<p>Before Rancher v2.3.3 and RKE v1.0.0, cluster provisioning did not supply the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> arguments to the Kubelet container. As a result, the kubelet automatically generates the <code>kubelet.crt</code>, and <code>kubelet.key</code> files under the <code>/var/lib/kubelet/pki</code> \u200bdirectory and the certificate is not rotated during the certificate rotation.</p>"},{"location":"kbs/000020161/#how-to-verify-the-kubelet-certificate","title":"How to verify the Kubelet certificate","text":"<ul> <li> <p><code>openssl s_client -connect &lt;NODE IP&gt;:10250 | openssl x509 -text</code></p> </li> <li> <p><code>curl -vk https://&lt;NODE IP&gt;:10250</code></p> </li> </ul>"},{"location":"kbs/000020161/#resolution","title":"Resolution","text":"<p>You can rotate the kubelet certificate in RKE and Rancher provisioned clusters as follows:</p>"},{"location":"kbs/000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v220-v230-and-rke-v020-v032-provisioned-clusters","title":"How to rotate the kubelet certificate in Rancher v2.2.0 - v2.3.0 and RKE v0.2.0 - v0.3.2 provisioned clusters","text":"<p>For clusters provisioned and managed by Rancher prior to v2.3.3 or RKE prior to v1.0.0, you will need to manually delete the <code>kubelet.crt</code> and <code>kubelet.key</code> in <code>/var/lib/kubelet/pki</code> and restart the Kubelet container:</p> <pre><code>docker exec kubelet rm /var/lib/kubelet/pki/kubelet.crt\ndocker exec kubelet rm /var/lib/kubelet/pki/kubelet.key\ndocker restart kubelet\n</code></pre>"},{"location":"kbs/000020161/#how-to-rotate-the-kubelet-certificate-in-rancher-v232-provisioned-clusters","title":"How to rotate the kubelet certificate in Rancher v2.3.2+ provisioned clusters","text":"<p>For Rancher provisioned clusters managed by Rancher v2.3.3 and above, you can set the <code>generate_serving_certificate</code> kubelet option to <code>true</code> in the cluster configuration YAML to rotate the kubelet certificate.</p> <p>N.B. If <code>hostname_override</code> is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding.</p> <ol> <li> <p>For the affected cluster click 'Edit Cluster' from within the Rancher UI cluster view.</p> </li> <li> <p>Click 'Edit as YAML'.</p> </li> <li> <p>Set the <code>generate_serving_certificate</code> option to true for the kubelet, per the below:</p> </li> </ol> <pre><code>services:\n     kubelet:\n       generate_serving_certificate: true\n</code></pre> <ol> <li>Click 'Save' to intitate a cluster reconciliation and trigger rotation of the kubelet certificate.</li> </ol>"},{"location":"kbs/000020161/#how-to-rotate-the-kubelet-certificate-in-rke-v100-provisioned-clusters","title":"How to rotate the kubelet certificate in RKE v1.0.0+ provisioned clusters","text":"<p>For clusters managed by RKE v1.0.0 and above, you can set the <code>generate_serving_certificate</code> kubelet option to <code>true</code> in the cluster configuration YAML and invoke <code>rke up</code> to rotate the kubelet certificate.</p> <p>N.B. If <code>hostname_override</code> is configured for any nodes in the cluster, please read the requirements within the documentation here before proceeding.</p> <ol> <li>Set the <code>generate_serving_certificate</code> option to true for the kubelet, within the cluster configuration YAML file, per the below:</li> </ol> <pre><code>services:\n     kubelet:\n       generate_serving_certificate: true\n</code></pre> <ol> <li>Invoke <code>rke up --config &lt;cluster configuration yaml&gt;</code> to update the cluster configuration with the new kubelet option and trigger rotation of the kubelet certificate.</li> </ol>"},{"location":"kbs/000020161/#further-reading","title":"Further Reading","text":"<p>RKE Certificate Rotation Documentation. Rancher v2.x Certificate Rotation Documentation. Kubelet Service Certificate Requirements Documentation.</p>"},{"location":"kbs/000020161/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020162/","title":"How to clean a Rancher 2.x RKE cluster","text":"<p>This document (000020162) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020162/#environment","title":"Environment","text":"<p>Rancher 2.x</p> <p>RKE1 (all versions)</p>"},{"location":"kbs/000020162/#situation","title":"Situation","text":"<p>At times a node may need to be cleaned of all state to ensure it is consistent for further use in a cluster. This article and script are for Rancher 2.x.</p> <p>Please note, this script will delete all containers, volumes, images, network interfaces, and directories that relate to Rancher and Kubernetes. It can also optionally flush all iptables rules and delete container images. It is important to perform pre-checks, and backup the node as needed before proceeding with any steps below.</p>"},{"location":"kbs/000020162/#prerequisite","title":"Prerequisite","text":"<ul> <li>A node provisioned with the RKE distribution using Rancher or the RKE CLI.</li> <li>The node should no longer be a member of any cluster.</li> <li>A copy of the cleanup script, and root/sudo access.</li> <li>Check the running containers or Pods, these will be forcefully deleted in the following steps.</li> <li>Confirm you are on the correct node and are ready to proceed with cleaning all containers and all data specific to Kubernetes and Rancher/RKE.</li> </ul>"},{"location":"kbs/000020162/#resolution","title":"Resolution","text":"<p>The below steps use a script to automate the clean of a node, the commands used can be run manually as needed, follow the steps below cleaning a node that has been used previously in a cluster.</p> <ul> <li>Login to the node and download the cleanup script:</li> </ul> <p><code>curl -sLO https://github.com/rancherlabs/support-tools/raw/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh</code></p> <p>You should now have a copy of the script in the current directory.</p> <ul> <li>Run the script:</li> </ul> <p><code>sudo bash extended-cleanup-rancher2.sh</code></p> <p>If desired, the optional -f and -i flags can be used together or individually to flush iptables (-f) and delete container images (-i).</p> <p><code>sudo bash extended-cleanup-rancher2.sh -f -i</code></p> <ul> <li>Restart the node</li> </ul> <p>The node is now in a clean consistent state to be reused in a cluster.</p>"},{"location":"kbs/000020162/#additional-information","title":"Additional Information","text":"<p>For RKE2 and K3s use the uninstall script deployed on the node during install.</p>"},{"location":"kbs/000020162/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020163/","title":"How to troubleshoot using the namespace of a container","text":"<p>This document (000020163) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020163/#environment","title":"Environment","text":"<p>RKE2 environment with containerd and RKE environment with docker</p>"},{"location":"kbs/000020163/#situation","title":"Situation","text":"<p>When troubleshooting an issue, often a faithful reproduction and an exact environment are needed. This can be a challenge in a containerized environment, where tools and a shell environment may not be easily available within containers of a Pod.</p> <p>With more images going distroless, container images often don't have the necessary tools such as `dig`, `curl`, `telnet`, etc... since they not included. This can make troubleshooting a pod's networking connections very difficult, especially in an air-gapped environment.</p>"},{"location":"kbs/000020163/#resolution","title":"Resolution","text":"<p>RKE2 Environment</p> <p>With crictl and nsenter, the node's OS packages can be utilised to troubleshoot the pod container's networking by entering the container's network namespace without using kubectl exec.</p> <p>Setup</p> <ul> <li>Get container ID(s) from a pod</li> </ul> <pre><code>POD_NAME=&lt;pod name&gt;\nNAMESPACE=&lt;namespace for pod&gt;\n</code></pre> <pre><code>kubectl describe pod -n $NAMESPACE $POD_NAME | awk -F'//' '/containerd:/ {print $2}'\n</code></pre> <ul> <li>Copy and save the container ID for later. Next, identify the node that the pod is on</li> </ul> <pre><code>kubectl get pod -n $NAMESPACE $POD_NAME -o wide\n</code></pre> <p>If the node in the output is different from your current node, be sure to switch to a shell session on the corresponding node.</p> <p>Using crictl</p> <p>When using\u00a0<code>critcl</code>\u00a0in an RKE2 setup, you may need to run the following commands before using crictl</p> <pre><code>export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml\nPATH=\"$PATH:/var/lib/rancher/rke2/bin\"\n</code></pre> <ul> <li>From the node where the pod is deployed, run the following command to get the PID of a container from the setup steps</li> </ul> <pre><code>CONTAINER_ID=&lt;container ID&gt;\nPID=$(crictl inspect -o go-template --template '{{.info.pid}}' $CONTAINER_ID)\n</code></pre> <p>nsenter</p> <ul> <li>Run commands from the node within the network namespace context of the container/Pod with\u00a0<code>nsenter</code></li> </ul> <pre><code>nsenter -n -t $PID &lt;command&gt;\n</code></pre> <ul> <li>Run commands from the node within the context of all of the container/Pod namespaces with\u00a0<code>nsenter</code></li> </ul> <pre><code>nsenter -a -t $PID &lt;command&gt;\n</code></pre> <p>RKE Environment</p> <p>Two approaches can be taken:</p> <p>Sidecar Container</p> <p>By running a container in the same namespaces as another, it's possible to use that container for troubleshooting.</p> <p>The sidecar container can be started using the same network and PID namespaces while attaching the same volumes:</p> <ul> <li>Set the ID or name of the container you wish to troubleshoot:</li> </ul> <pre><code>ID=&lt;container ID or name&gt;\n</code></pre> <ul> <li>Run the sidecar container using the network, PID and volumes</li> </ul> <pre><code>docker run -it --net=container:$ID --pid=container:$ID --volumes-from=$ID alpine sh\n</code></pre> <p>It is now possible to troubleshoot with commands from the alpine container, within the context of the container or Pod with the issue.</p> <p>For example, if you were experiencing a network issue from this Pod, it is now possible to use tools available in the sidecar container to simulate the connection, view the network configuration and troubleshoot interactively.</p> <p>Substitute the alpine container as needed with an image of your choice.</p> <p>Note, this will attach the same volumes as the parent container, but the parent container read/write layers will not be accesible - to access the same container filesystem, see the nsenter example below.</p> <p>Use nsenter from the host</p> <p>Alternatively, tools are available on the host for the same use case with the nsenter command. The nsenter command is standard on most Linux distributions, for example, on Ubuntu, it is provided by the util-linux package.</p> <ul> <li>Set the ID or name of the container you wish to troubleshoot:</li> </ul> <pre><code>ID=&lt;container ID or name&gt;\n</code></pre> <ul> <li>Obtain the first process in the container (PID 1):</li> </ul> <pre><code>PID=$(docker inspect --format '{{ .State.Pid }}' $ID)\n</code></pre> <ul> <li>Run commands from the node within the network namespace context of the container/Pod with nsenter:</li> </ul> <pre><code>nsenter -n -t $PID &lt;command&gt;\n</code></pre> <ul> <li>Run commands from the node within the context of all of the container/Pod namespaces with nsenter:</li> </ul> <pre><code>nsenter -a -t $PID &lt;command&gt;\n</code></pre> <p>For example, when troubleshooting a network issue, tools like tcpdump, curl, dig, and mtr can be used to interactively troubleshoot the issue.</p> <p>Note, the\u00a0-a\u00a0flag is available in recent versions of\u00a0nsenter, if this does not succeed, use a flag for a specific namespace, check the\u00a0nsenter --help output.</p>"},{"location":"kbs/000020163/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020163/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020164/","title":"How to troubleshoot HTTP request performance with curl statistics","text":"<p>This document (000020164) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020164/#situation","title":"Situation","text":""},{"location":"kbs/000020164/#task","title":"Task","text":"<p>When troubleshooting a performance issue with a web-based endpoint, it's important to have metrics that assist in understanding what areas are related.</p> <p>This is where using a lightweight tool like curl, and it's ability to write out the statistics of a request can be very useful.</p>"},{"location":"kbs/000020164/#pre-requisites","title":"Pre-requisites","text":"<p>You will just need curl installed and available from the location performing the test.</p>"},{"location":"kbs/000020164/#steps","title":"Steps","text":"<p>Download the format file to use with curl:</p> <p><code>curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/files/curl-format.txt</code></p> <p>You should now have a <code>curl-format.txt</code> file locally in the current directory.</p> <p>Using the file and the <code>-w</code> flag, perform the desired request to the service, the example below displays the headers and statistics.</p> <p><code>curl -I -w \"@curl-format.txt\" https://rancher.com</code></p> <p>Timing statistics will be output with each run of the command, measurements are recorded in seconds.</p> <p>Note: run the command from a location that provides an accurate reproduction of the issue, to simulate the issue as closely as possible. Using the same request parameters are important - like the path and headers that might be used by client applications.</p>"},{"location":"kbs/000020164/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020166/","title":"Pod network connectivity non-functional as a result of sysctl net.ipv4.ip_forward=0","text":"<p>This document (000020166) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020166/#situation","title":"Situation","text":""},{"location":"kbs/000020166/#issue","title":"Issue","text":"<p>If the sysctl <code>net.ipv4.ip_forward</code> is set to 0 (disabled) on a Linux host, then IPv4 packet forwarding is disabled.</p> <p>As a result, on a Kubernetes nodes this will prevent Pod networking from functioning.</p> <p>You can confirm the current value of this sysctl on a Linux host, if you are experiencing a network issue, with the following:</p> <p><code>sysctl net.ipv4.ip_forward</code></p> <p>The output should show 1, for enabled.</p>"},{"location":"kbs/000020166/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster with a CNI (Container Network Interface) plugin configure, e.g. an RKE (Rancher Kubernetes Engine) or Rancher launched cluster.</li> <li>The systctl net.ipv4.ip_forward set to 0 (disabled) on the cluster hosts.</li> </ul>"},{"location":"kbs/000020166/#resolution","title":"Resolution","text":"<p>Check if the kernel parameter <code>net.ipv4.ip_forward</code> is set to 1 with:</p> <p><code>sysctl net.ipv4.ip_forward</code></p> <p>If the current value of net.ipv4.ip_forward is 0, then set to this to 1 with the following:</p> <p><code>sysctl net.ipv4.ip_forward=1</code></p> <p>To make it permanent across reboot, add the following line in <code>/etc/sysctl.conf</code>:</p> <p><code>net.ipv4.ip_forward=1</code></p> <p>With this sysctl correctly enabled, Pod ingress and egress will be able to function as expected.</p>"},{"location":"kbs/000020166/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020167/","title":"How to setup your network CIDR for a large cluster","text":"<p>This document (000020167) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020167/#situation","title":"Situation","text":""},{"location":"kbs/000020167/#task","title":"Task","text":"<p>If you are expecting to use Rancher to deploy a Kubernetes cluster with more than 256 nodes, you'll need to make sure you adjust the default cluster CIDR settings. The default settings only allows clusters of 256 nodes or less.</p>"},{"location":"kbs/000020167/#requirements","title":"Requirements","text":"<ul> <li>Rancher v2.x</li> <li>A lot of hardware or VMs!</li> </ul>"},{"location":"kbs/000020167/#background","title":"Background","text":"<p>Kubernetes provides each pod with an IP address and each node with a block of IP addresses. Each cluster is also provided a block of IP addresses that is distributed to each node.</p> <p>This is controlled by two settings, the <code>cluster_cidr</code> block and <code>node-cidr-mask-size</code>. By default, the <code>cluster_cidr</code> block is 10.42.0.0/16 and the <code>node-cidr-mask-size</code> is 24. This gives the cluster 256 blocks of /24 networks to distribute out to the pool of nodes. For example, node1 will get 10.24.0.0/24, node2 will get 10.42.1.0/24, node3 will get 10.42.2.0/24 and so on.</p>"},{"location":"kbs/000020167/#solution","title":"Solution","text":"<p>To support more than 256 nodes, you will need to use a larger cluster_cidr block, a smaller node-cidr-mask-size, or adjust both. For example, if you want to support up to 512 nodes you can set:</p> <ul> <li><code>cluster_cidr</code> to 10.40.0.0/15</li> <li><code>node-cidr-mask-size</code> to 24</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.42.0.0/16</li> <li><code>node-cidr-mask-size</code> to 25</li> </ul> <p>To support up to 1024 nodes, you can use a larger <code>cluster_cidr</code>, smaller <code>node-cidr-mask-size</code>, or combination of both:</p> <ul> <li><code>cluster_cidr</code> to 10.38.0.0/14</li> <li><code>node-cidr-mask-size</code> to 24</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.42.0.0/16</li> <li><code>node-cidr-mask-size</code> to 26</li> </ul> <p>OR</p> <ul> <li><code>cluster_cidr</code> to 10.40.0.0/15</li> <li><code>node-cidr-mask-size</code> to 25</li> </ul> <p>You should be aware of the following caveats when specifying your <code>cluster_cidr</code> and <code>node-cidr-mask-size</code> settings:</p> <ul> <li>Make sure you don't set your <code>cluster_cidr</code> to overlap with the default cluster service network of 10.43.0.0/16. That's why the examples above used 10.40.0.0/15 and 10.38.0.0/14. A CIDR of 10.42.0.0/15 will clash with the default cluster service CIDR.</li> <li>Make sure you don't set your <code>cluster_cidr</code> to overlap with IP address ranges already used in your enterprise infrastructure such as your node IPs, firewalls, load balancers, DNS, or other internal networks.</li> <li>Make sure your <code>node-cidr-mask-size</code> is large enough to accommodate the number of pods you want to run on each node. A size of 24 will give enough IP addresses for about 250 pods per node, which is well above the 110 maximum. However a size of 26 will only give you about 60 IPs, which is below the 110 maximum. If you plan to raise the default pod per node limit beyond 110, make sure sure your <code>node-cidr-mask-size</code> is large enough to support it. Note that pods that have <code>hostNetwork: true</code> do not count toward this total.</li> <li>Set it right the first time! Once your cluster has been deployed, these values cannot change. You'll need to decommission your cluster and start over again if you don't set it right.</li> <li>As of v1.17, Kubernetes supports clusters up to 5000 nodes. If you plan to go beyond this, you're venturing into unknown territory. For the latest large cluster best practices, see https://kubernetes.io/docs/setup/best-practices/cluster-large/</li> </ul> <p>Setting these values can be done when first creating the cluster. You'll need to click on the <code>Edit as YAML</code> button and merge in the following YAML:</p> <pre><code>rancher_kubernetes_engine_config:\n  services:\n    kube-controller:\n      cluster_cidr: 10.40.0.0/15\n      extra_args:\n        node-cidr-mask-size: 25\n</code></pre> <p>The above configuration should allow you to have about 120 pods per node and 1024 nodes in your cluster. That's over 100,000 pods, wow!</p>"},{"location":"kbs/000020167/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020170/","title":"How to upgrade Docker using Rancher's install script","text":"<p>This document (000020170) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020170/#environment","title":"Environment","text":"<ul> <li>A supported node with a version of Docker needing to be upgraded</li> <li>Curl or Wget installed</li> </ul>"},{"location":"kbs/000020170/#situation","title":"Situation","text":"<p>Rancher provides quick scripts for installing Docker, which are available for the most recent versions of Docker\u00a0https://rancher.com/docs/rancher/v2.x/en/installation/requirements/installing-docker/\u00a0Upgrading Docker on your machine using these scripts is equally as simple.</p>"},{"location":"kbs/000020170/#resolution","title":"Resolution","text":"<p>Just run the script with the version number you are trying to upgrade to. Let's say you're running 18.09 and want to upgrade to 19.03. Simply provide the version number as the name of the script to run. For example:</p> <p><code>curl https://releases.rancher.com/install-docker/27.2.sh | sh</code></p> <p>or</p> <p><code>wget -O- https://releases.rancher.com/install-docker/27.2.sh | sh</code></p> <p>This will throw a warning that Docker is already installed, stop the running Docker engine, and upgrade your version. Note that restarting Docker will also stop any running container or workloads running on this host.</p>"},{"location":"kbs/000020170/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020171/","title":"Information to provide when logging a support case","text":"<p>This document (000020171) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020171/#situation","title":"Situation","text":"<p>To allow us to provide quick and efficient support, we ask that customers provide as much information as possible when logging a ticket.</p> <p>Below are some things that we find we typically need from customers when more information is needed.</p>"},{"location":"kbs/000020171/#access","title":"Access","text":"<p>When troubleshooting interactively or gathering further information, access areas of the environment may be needed, such as:</p> <ul> <li>SSH access to the affected node(s)</li> <li>Access to the Rancher UI</li> <li>The kubectl CLI and kubeconfig available for the related cluster(s)</li> </ul>"},{"location":"kbs/000020171/#information-to-provide","title":"Information to provide","text":"<p>Please consider the key details and provide as much information as possible about the related areas, below are some common questions that can be asked to help form a description when raising a ticket:</p> <ul> <li>When did you first notice the issue? Please be as specific as possible, for example a time, date and timezone</li> <li>Is the issue related to Rancher, the management (local) cluster, or a downstream cluster managed by Rancher?</li> <li>Logs, screenshots, terminal output, error, etc. are very useful to assist troubleshooting:</li> <li>System logs are almost always required when diagnosing an issue, you can generate these using the Rancher log collector script</li> <li>For some issues it can help to capture traffic from the browser to the Rancher UI in a HAR file, the steps can be found here</li> <li>To capture as much context about the issue as possible, a screenshot to demonstrate the issue, or a copy/paste or attachment of errors or terminal output that demonstrate the issue can greatly reduce the time to resolution</li> <li>What are the installed versions? For example, Rancher version, Kubernetes version of the affected cluster(s)?</li> <li>The Rancher version can be found at the bottom left of the Rancher UI or by inspecting the container image tag of the Rancher pod/container</li> <li>The Kubernetes version can be found by:<ul> <li>Navigating to the relevant cluster in the Rancher dashboard</li> <li>Using kubectl get nodes</li> <li>Standalone RKE clusters can be checked also with kubectl, and rke version --config  <li>If the issue is related to an upgrade of Rancher or Kubernetes, what were the previous versions?</li> <li>If the issue is related to a downstream cluster, which distribution and how was the cluster built? RKE1, RKE2, k3s, hosted provider, imported or custom?</li> <li>If the issue is related to Longhorn, please create a Longhorn log bundle as explained here, and attach it to the case.</li> <li>Are there any GitHub issues or previous support tickets you think are related?</li> <li>Are there any events that you think may correlate with the issue? For example, an infrastructure outage, host reboot, OS upgrade, Docker upgrade, network changes, configuration changes?</li> <li>Have you taken any corrective action or steps?</li>"},{"location":"kbs/000020171/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020172/","title":"How to setup Rancher 2.x with Active Directory external authentication","text":"<p>This document (000020172) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020172/#situation","title":"Situation","text":""},{"location":"kbs/000020172/#overview-and-intention","title":"Overview and Intention","text":"<p>This is a quick guide aiming to get Rancher v2.x using external authentication via Active Directory with the least amount of effort. Of course there is much more to consider and configure in a production enterprise environment. For more detail on this function please refer to this Rancher article and fine tune as required. Configuring Active Directory</p>"},{"location":"kbs/000020172/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher v2.x, either a single node instance or High Availability (HA) cluster.</li> <li>Local account to log onto the Rancher Server (usually admin)</li> <li>A Windows Server running Active Directory</li> <li>Name of the domain you wish to join</li> <li>A restricted account that Rancher can use to bind and query the Directory with (Security Recommendations at the bottom of article)</li> <li>A standard user account that will be used to test and enable the authentication (i.e your domain account)</li> <li>Knowledge of where the users are in the Active Directory OU (Organisational Unit) structure</li> <li>Network connectivity from the Rancher worker nodes to the Active Directory Servers (There is probably more than one, run nslookup on the domain name)</li> <li>Also good to test ports 389 or 636 (TLS) as these need to be allowed</li> </ul>"},{"location":"kbs/000020172/#steps-on-how-to-get-rancher-talking-to-ad-quickly","title":"Steps on How To Get Rancher Talking to AD Quickly","text":""},{"location":"kbs/000020172/#tested-with-ad-running-windows-server-20162019","title":"(Tested with AD running Windows Server 2016/2019)","text":"<p>In this example I have used the below examples (yours will be different):</p> <ul> <li>my domain is 'rancher.local'</li> <li>All or my users are located under the Users OU in AD</li> <li>my bind account is 'svc-rancher'</li> </ul> <p>For more detail refer to Configuring Active Directory:</p> <ol> <li>Log into the Rancher UI using the initial local admin account.</li> <li>From the Global view, navigate to Security &gt; Authentication</li> <li>Select Active Directory. The Configure an AD server form will be displayed.</li> <li>Add in the Hostname or IP address into the Hostname field</li> <li>Add 'rancher/svc-rancher' to the Service Account Username field</li> <li>Add 'cn=users,dc=rancher,dc=local' to the User Search Base</li> <li>Goto Section 3 add your domain account username and password</li> <li>Click 'Authenticate with Active Directory'</li> </ol>"},{"location":"kbs/000020172/#security-tips-and-best-practices","title":"Security tips and Best Practices","text":"<p>WARNING: Once enabled all users in the Search base will be able to log into Rancher.</p> <ol> <li>Once auth is configured in Rancher change the relaxed default setting from 'Allow any valid Users' to login to 'only allow members of Cluster, Projects' to login. Access must now be specified instead of allowing any User onto the cluster.</li> <li>Under 'Global, Security, Roles' It is best to drop 'New User Default' setting from 'User' to 'User Base' which provide less privleges to new users and must increased as required not as a default.</li> <li>The bind account is critical for ongoing authentication so locking the account will break functionality.</li> <li>If this account gets locked or the password changes your AD authentication will be broken. Setting the account and the password not to expire and removing lockout policies prevent disruption.</li> <li>Remove interactive logon abilites as this account doesn't need to logon to a server and control it</li> </ol>"},{"location":"kbs/000020172/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020174/","title":"How to setup Nodelocal DNS cache with Rancher, RKE1 and RKE2","text":"<p>This document (000020174) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020174/#situation","title":"Situation","text":""},{"location":"kbs/000020174/#why-use-nodelocal-dns-cache","title":"Why use Nodelocal DNS cache?","text":"<p>Like many applications in a containerised architecture, CoreDNS or kube-dns runs in a distributed fashion. In certain circumstances, DNS reliability and latency can be impacted with this approach. The causes of this relate notably to conntrack race conditions or exhaustion, cloud provider limits, and the unreliable nature of the UDP protocol.</p> <p>A number of workarounds exist, however long term mitigation of these and other issues has resulted in a redesign of the Kubernetes DNS architecture, and the result being the Nodelocal DNS cache project.</p>"},{"location":"kbs/000020174/#requirements","title":"Requirements","text":"<ul> <li>A Kubernetes cluster provisioned by Rancher v2.x, or directly with RKE1 and RKE2</li> <li>A Linux cluster, Windows is currently not supported</li> <li>Access to the cluster</li> </ul>"},{"location":"kbs/000020174/#resolution","title":"Resolution","text":""},{"location":"kbs/000020174/#installing","title":"Installing","text":"<p>Once installed, pods will begin to resolve using the node-local-dns pod on the same node, below are details for RKE1 and RKE2 when provisioning using Rancher. These same steps can be applied in a similar way when directly provisioning a cluster.</p>"},{"location":"kbs/000020174/#rke1","title":"RKE1","text":"<p>When provisioning or configuring an existing cluster, edit the cluster configuration in the Rancher dashboard, and click the 'Edit as YAML' button. When provisioning an RKE cluster directly, edit the cluster.yaml file instead.</p> <p>Note: Updating the cluster using the below will create the <code>node-local-dns</code> Daemonset, and restart the <code>kubelet</code> container on each node.</p> <p>As in the documentation, update or add the <code>dns.nodelocal.ip_address</code> field using the following as an example:</p> <pre><code>  dns:\n  [..]\n    nodelocal:\n      ip_address: \"169.254.20.10\"\n</code></pre> <p>The kubelet will be updated to use the new IP address when configuring pod DNS resolution. Pods using the CoreDNS service address (default: 10.43.0.10) as the nameserver in <code>/etc/resolv.conf</code> will still resolve using the node-local-dns pod on the node. This is due to the way node-local-dns manages it's own interface and iptables rules.</p>"},{"location":"kbs/000020174/#rke2","title":"RKE2","text":"<p>Update the default HelmChart for CoreDNS, the nodelocal.enabled: true value will install node-local-dns in the cluster.</p> <p>When provisioning or configuring an existing cluster, edit the cluster configuration in the Rancher dashboard, and select Add-On Config. At the bottom of the page paste the following into the Additional Manifest text area:</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-coredns\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    nodelocal:\n      enabled: true\n</code></pre> <p>Save the changes, please see the documentation here for more details.</p> <p>When provisioning an RKE2 cluster directly, this file can be copied into the\u00a0/var/lib/rancher/rke2/server/manifests directory on each rke2-server node, manually or with user-data/configuration management.</p>"},{"location":"kbs/000020174/#testing","title":"Testing","text":"<p>Once installed, start a new pod to test DNS queries, for example:</p> <pre><code>kubectl run --restart=Never --rm -it --image=tutum/dnsutils dns-test -- dig google.com\n</code></pre> <p>To verify node-local-dns is available and handling DNS queries, here are some ways to confirm:</p> <ul> <li>Check for a nodelocaldns interface on a node, for example:</li> </ul> <pre><code># ip addr show nodelocaldns\n21: nodelocaldns: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default\n    link/ether e2:a9:45:f9:29:94 brd ff:ff:ff:ff:ff:ff\n    inet 169.254.20.10/32 scope global nodelocaldns\n       valid_lft forever preferred_lft forever\n    inet 10.43.0.10/32 scope global nodelocaldns\n       valid_lft forever preferred_lft forever\n</code></pre> <ul> <li> <p>Temporarily enable query logging for node-local-dns:</p> </li> <li> <p>Edit the node-local-dns ConfigMap to add the log plugin, locate and edit the ConfigMap in the kube-system namespace in the Rancher dashboard, or use kubectl edit configmap -n kube-system node-local-dns</p> </li> <li> <p>Add log to the cluster.local and :53 objects in the Corefile, for example for :53 (external queries):</p> </li> <li> <p><code>[...]           .:53 {               log               errors               cache 30</code></p> </li> <li>Check the node-local-dns pod logs once some DNS queries have been performed, the logs should indicate queries are being answered</li> <li>Perform the reverse of steps 1-2 to disable query logging</li> </ul>"},{"location":"kbs/000020174/#removing-nodelocal-dns-cache","title":"Removing Nodelocal DNS cache","text":"<p>To remove from a cluster, the reverse steps are needed:</p>"},{"location":"kbs/000020174/#rke1_1","title":"RKE1","text":"<p>Remove the <code>dns.nodelocal</code> field from the cluster configuration in the Rancher dashboard and save the change. When provisioning a cluster directly, run rke up to reconcile the change.</p>"},{"location":"kbs/000020174/#rke2_1","title":"RKE2","text":"<p>Remove the additional manifest in the Rancher dashboard, or delete the manifest file from all of the rke2-server nodes when provisioning the cluster directly.</p>"},{"location":"kbs/000020174/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020174/#troubleshooting","title":"Troubleshooting","text":"<p>Nodelocal DNS will perform external lookups on behalf of pods, this lookup occurs from the node-local-dns DaemonSet pod running on the same node as the pod.</p> <p>For internal lookups, CoreDNS will be used, node-local-dns pods will cache successful queries (30s), and negative queries (5s) by default. For an architecture overview please see the diagram here.</p> <p>In no specific order, the following can help understand a DNS issue further.</p>"},{"location":"kbs/000020174/#check-all-kube-dns-and-node-local-dns-objects","title":"Check all kube-dns and node-local-dns objects","text":"<p>Ensure there are no obvious issues with scheduling CoreDNS and node-local-dns pods in the cluster.</p> <pre><code>kubectl get all -n kube-system -l k8s-app=node-local-dns\nkubectl get all -n kube-system -l k8s-app=kube-dns\n</code></pre> <p>All node-local-dns and kube-dns pods should be ready and running, the kube-dns Service should exist. Check the events if needed to locate any warning or failed event messages.</p> <pre><code>kubectl describe ds -n kube-system -l k8s-app=node-local-dns\nkubectl describe rs -n kube-system -l k8s-app=kube-dns\n</code></pre>"},{"location":"kbs/000020174/#check-the-logs-and-configmap-of-kube-dns-and-node-local-dns-pods","title":"Check the logs and ConfigMap of kube-dns and node-local-dns pods","text":"<pre><code>kubectl logs -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=node-local-dns\n</code></pre> <pre><code>kubectl get configmap -n kube-system coredns -o yaml\nkubectl get configmap -n kube-system node-local-dns -o yaml\n</code></pre>"},{"location":"kbs/000020174/#enable-logging-and-perform-a-dns-test","title":"Enable logging and perform a DNS test","text":"<p>Note, query logging can increase the log output from CoreDNS, enabling this temporarily while investigating is suggested.</p> <ul> <li>Enable query logging for node-local-dns with the following steps:</li> <li>Edit the node-local-dns ConfigMap to add the log plugin, locate and edit the ConfigMap in the kube-system namespace in the Rancher dashboard, or use kubectl edit configmap -n kube-system node-local-dns</li> <li> <p>Add log to the cluster.local and :53 objects in the Corefile, for example for :53 (external queries):</p> <ul> <li><code>[...]       .:53 {           log           errors           cache 30</code></li> <li>Check the node-local-dns pod logs once some DNS queries have been performed, the logs should indicate queries are being answered</li> <li>Perform the reverse of steps 1-2 to disable query logging</li> <li>Query logging for CoreDNS can be enabled in a similar way, when Nodelocal DNS is enabled, this will only log internal (cluster.local) queries that were not already cached</li> <li>Run a DaemonSet to perform queries from a pod running on each node in the cluster</li> </ul> </li> </ul>"},{"location":"kbs/000020174/#ask-questions-to-further-eliminate-the-issue","title":"Ask questions to further eliminate the issue","text":"<ul> <li>Is it only DNS that is affected, or is all connectivity affected?</li> <li>Are internal, external, or all DNS queries failing?</li> <li>Are all nodes and workloads experiencing the issue, or a specific node or workload? Nodes use the upstream DNS configured in <code>/etc/resolv.conf</code>, queries failing from a node could indicate the issue is with upstream DNS</li> <li>What is the error reported by applications? If logs are aggregated, investigate the rate of the error in logs to identify timelines and impact</li> <li>Is the issue intermittent or constantly occurring? If the issue is intermittent, configure monitoring or a loop to identify when the issue occurs, when it does - what is the error? are internal, external or all queries affected?</li> </ul>"},{"location":"kbs/000020174/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020175/","title":"How to setup HAProxy for Rancher v2.x","text":"<p>This document (000020175) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020175/#situation","title":"Situation","text":""},{"location":"kbs/000020175/#task","title":"Task","text":"<p>Setup HAProxy as a frontend load balancer for Rancher v2.x.</p>"},{"location":"kbs/000020175/#overview","title":"Overview","text":""},{"location":"kbs/000020175/#install-haproxy","title":"Install HAProxy","text":""},{"location":"kbs/000020175/#ubuntu","title":"Ubuntu","text":"<pre><code>apt update\napt install -y haproxy\nsystemctl enable haproxy\nsystemctl start haproxy\n</code></pre>"},{"location":"kbs/000020175/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>yum update\nyum install haproxy -y\nsystemctl enable haproxy\nsystemctl start haproxy\n</code></pre>"},{"location":"kbs/000020175/#example-haproxy-config","title":"Example HAProxy Config","text":""},{"location":"kbs/000020175/#option-a-full-ssl","title":"Option A - Full SSL","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/</li> <li>Verify Rancher URL works when connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping\n</code></pre> <ul> <li>Copy cert and key into a single file called /etc/haproxy/cert.pem</li> <li>Add frontend to /etc/haproxy/haproxy.cfg:</li> </ul> <pre><code>frontend www-http\nbind *:80\nreqadd X-Forwarded-Proto:\\ http\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443 ssl crt /etc/haproxy/cert.pem\nreqadd X-Forwarded-Proto:\\ https\ndefault_backend rancher-https\n</code></pre> <ul> <li>Add backends to /etc/haproxy/haproxy.cfg:</li> </ul> <pre><code>backend rancher-http\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:80 check weight 1 maxconn 1024\nserver rancher02 192.168.1.104:80 check weight 1 maxconn 1024\nserver rancher03 192.168.1.105:80 check weight 1 maxconn 1024\n</code></pre> <pre><code>backend rancher-https\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:443 check weight 1 maxconn 1024 ssl verify none\nserver rancher02 192.168.1.104:443 check weight 1 maxconn 1024 ssl verify none\nserver rancher03 192.168.1.105:443 check weight 1 maxconn 1024 ssl verify none\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"kbs/000020175/#option-b-external-tls-termination","title":"Option B - External TLS Termination","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/options/chart-options/#external-tls-termination</li> <li>Verify Rancher URL works went connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl --header \"Host: rancher.example.com\" http://192.168.1.103/ping\n</code></pre> <ul> <li>Copy cert and key into a single file called /etc/haproxy/cert.pem</li> <li>Create frontends:</li> </ul> <pre><code>frontend www-http\nbind *:80\nreqadd X-Forwarded-Proto:\\ http\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443 ssl crt /etc/haproxy/cert.pem\nreqadd X-Forwarded-Proto:\\ https\ndefault_backend rancher-http\n</code></pre> <ul> <li>Create backends:</li> </ul> <pre><code>backend rancher-http\nmode http\noption httpchk HEAD /healthz HTTP/1.0\nserver rancher01 192.168.1.103:80 check weight 1 maxconn 1024\nserver rancher02 192.168.1.104:80 check weight 1 maxconn 1024\nserver rancher03 192.168.1.105:80 check weight 1 maxconn 1024\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"kbs/000020175/#option-c-tcp-pass-through","title":"Option C - TCP pass-through","text":"<ul> <li>Follow Rancher install doc https://rancher.com/docs/rancher/v2.x/en/installation/k8s-install/helm-rancher/</li> <li>Verify Rancher URL works when connecting directly to a Rancher node. For example:</li> </ul> <pre><code>curl -k --header \"Host: rancher.example.com\" https://192.168.1.103/ping\n</code></pre> <ul> <li>NOTE: The default gateway for all 3 Rancher nodes must be the load balancer. Doc: https://www.haproxy.com/blog/howto-transparent-proxying-and-binding-with-haproxy-and-aloha-load-balancer/</li> <li>Create frontends:</li> </ul> <pre><code>frontend www-http\nbind *:80\nmode tcp\noption tcplog\ntcp-request inspect-delay 5s\ndefault_backend rancher-http\n</code></pre> <pre><code>frontend www-https\nbind *:443\nmode tcp\noption tcplog\ntcp-request inspect-delay 5s\ndefault_backend rancher-https\n</code></pre> <ul> <li>Create backends:</li> </ul> <pre><code>backend rancher-http\nmode tcp\nbalance roundrobin\nsource 0.0.0.0 usesrc client\nserver rancher01 192.168.1.103:80\nserver rancher02 192.168.1.104:80\nserver rancher03 192.168.1.105:80\n</code></pre> <pre><code>backend rancher-https\nmode tcp\nbalance roundrobin\nsource 0.0.0.0 usesrc client\nserver rancher01 192.168.1.103:443\nserver rancher02 192.168.1.104:443\nserver rancher03 192.168.1.105:443\n</code></pre> <ul> <li>Test the configuration:</li> </ul> <pre><code>haproxy -f /etc/haproxy/haproxy.cfg -c\n</code></pre> <ul> <li>Reload HAProxy:</li> </ul> <pre><code>systemctl reload haproxy\n</code></pre> <p>Example config</p>"},{"location":"kbs/000020175/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Add the following to /etc/haproxy/haproxy.cfg before the frontend section.</li> </ul> <pre><code>listen stats\nbind :9000\nmode http\nstats enable\nstats hide-version\nstats realm Haproxy\\ Statistics\nstats uri /\nstats auth admin:admin\n</code></pre> <ul> <li>Go to http://load01.example.com:9000/</li> <li>Username/Password: admin/admin</li> <li>If there are firewall rules blocking port 9000, use ssh tunneling to proxy the connection:</li> </ul> <pre><code>ssh -f -N -L 9000:127.0.0.1:9000 root@192.168.1.101\n</code></pre> <ul> <li>Go to http://localhost:9000/</li> </ul>"},{"location":"kbs/000020175/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020176/","title":"How to collect a trace and heap from nginx ingress","text":"<p>This document (000020176) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020176/#situation","title":"Situation","text":""},{"location":"kbs/000020176/#task","title":"Task","text":"<p>When troubleshooting an ingress-nginx issue, collecting the trace and heap dump from ingress-nginx Pods may be requested. This can assist with understanding issues like excessive memory consumption.</p>"},{"location":"kbs/000020176/#pre-requisites","title":"Pre-requisites","text":"<p>Access to the node(s) where the ingress-nginx Pods are experiencing the issue, or access to the node on <code>10254/TCP</code> from a workstation.</p> <p>To collect the output, the below commands use <code>curl</code>, you may need to install the package. If needed, <code>wget</code> could be used instead.</p> <p>The <code>date</code> command is used to provide a consistent timestamp for the files, this could be changed or removed if the <code>date</code> command on the node doesn't support these flags.</p> <p>The issue should be occurring at the time for the collection to be useful when investigating.</p>"},{"location":"kbs/000020176/#steps","title":"Steps","text":"<p>SSH to the node(s), use the following commands to collect the trace and heap dump. If the issue is intermittent or fluctuating, repeat the commands as necessary to capture the collection when the issue is ocurring.</p>"},{"location":"kbs/000020176/#heap","title":"Heap","text":"<pre><code>curl -s http://localhost:10254/debug/pprof/trace?seconds=5 --output /tmp/nginx-trace.$(date -u --iso-8601=seconds)\n</code></pre>"},{"location":"kbs/000020176/#trace","title":"Trace","text":"<pre><code>curl -s http://localhost:10254/debug/pprof/heap --output /tmp/nginx-heap.$(date -u --iso-8601=seconds)\n</code></pre> <p>Note: if accessing the node on <code>10254/TCP</code> instead, be sure to update <code>localhost</code> with the IP Address of the node.</p> <p>If the files are too large to upload to the ticket, please request or use the provided temporary upload location.</p>"},{"location":"kbs/000020176/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020177/","title":"How to generate a HAR file","text":"<p>This document (000020177) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020177/#environment","title":"Environment","text":"<ul> <li>Web Browser (Firefox or Chrome)</li> <li>A browser that can reproduce the issue, we've covered Chrome and Firefox in this article.</li> <li>The issue should be occurring or reproducible at the time of the collection to contain an example of the issue.</li> </ul>"},{"location":"kbs/000020177/#situation","title":"Situation","text":"<p>When troubleshooting an issue that is reproducible in a browser, it is sometimes necessary to have additional information about the requests and responses. You may be requested to generate a HAR file recording to capture this and attach this to a ticket for analysis.</p> <p>Please note, the information collected in a HAR file can contain sensitive data like content, headers, and cookies. This is not always the case, and some information is transient only. However, please check and santise the information as necessary before uploading.</p>"},{"location":"kbs/000020177/#resolution","title":"Resolution","text":"<p>Open your browser ready to reproduce the issue.</p> <p>Shortcut</p> <p>Using the F12 button you can quickly open the Developer Tools on FireFox and Chrome.</p> <p>Chrome</p> <ul> <li>From the menu, select View &gt; Developer &gt; Developer Tools</li> <li>From the pane, click on the Network tab</li> <li>Locate the Preserve log setting in the upper left and ensure it is checked</li> <li>Locate the record button, it should be a red circle to indicate that it is currently recording, if it is grey, click it once to start recording</li> <li>Follow any steps needed to reproduce the issue during the recording</li> <li>Note: immediately before triggering the action that reproduces the issue, also reload the page (eg. via CTRL+R). For example, if the issue manifests when navigating to a page, then go to the the page containing the link leading to it, first reload the page, and then click to reproduce the issue.</li> <li>Once the issue has occurred, right click in the pane and select Save as HAR with Content</li> </ul> <p>Firefox</p> <ul> <li>From the menu, select Tools &gt; Web Developer &gt; Network</li> <li>The recording to start automatically with any further navigation in the browser</li> <li>Follow any steps needed to reproduce the issue with the network pane open</li> <li>Note: immediately before triggering the action that reproduces the issue, also reload the page (eg. via CTRL+R). For example, if the issue manifests when navigating to a page, then go to the the page containing the link leading to it, first reload the page, and then click to reproduce the issue.</li> <li>Once the issue has occurred, right click in the pane and select Save all as HAR</li> </ul> <p>Upload the HAR file</p> <p>Generally, HAR files are small in size, however if the file are too large to upload directly to the ticket, please request or use the provided temporary upload location.</p>"},{"location":"kbs/000020177/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020179/","title":"How to conduct performance testing with Clusterloader2","text":"<p>This document (000020179) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020179/#situation","title":"Situation","text":""},{"location":"kbs/000020179/#how-to-conduct-performance-testing-with-clusterloader2_1","title":"How to conduct performance testing with Clusterloader2","text":"<p>Clusterloader is an opensource performance testing tool to measure the performance metrics of your Kubernetes cluster.</p>"},{"location":"kbs/000020179/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Linux or Mac machine that has Golang and kubectl installed</li> <li>SSH key of the Kubernetes master node</li> <li>Kubeconfig file of the target cluster</li> </ul>"},{"location":"kbs/000020179/#steps","title":"Steps","text":"<ol> <li>Create a folder named k8s.io under <code>~/go/src/</code>:</li> </ol> <pre><code>mkdir ~/go/src/k8s.io\n</code></pre> <ol> <li>Clone the perf-test under k8s.io folder:</li> </ol> <pre><code>cd ~/go/src/k8s.io &amp;&amp; git clone https://github.com/galal-hussein/perf-tests.git\n</code></pre> <ol> <li>Navigate to the clusterloader2 directory:</li> </ol> <pre><code>cd ~/go/src/k8s.io/perf-tests/clusterloader2\n</code></pre> <ol> <li>Edit the testconfig according to the environment:</li> </ol> <pre><code>vim testing/load/config.yaml\n</code></pre> <ol> <li>Execute the clusterloader2 with appropriate options:</li> </ol> <pre><code>KUBE_SSH_USER=&lt;SSH USERNAME&gt; LOCAL_SSH_KEY=&lt;SSH KEY PATH&gt; go run cmd/clusterloader.go --nodes 3 --mastername=&lt;MASTER NODE NAME&gt; --kubeconfig=&lt;KUBECONFIG FILE PATH&gt; --provider=local --masterip=&lt;MASTER NODE IP ADDRESS&gt; --testconfig=testing/&lt;TESTING SUBJECT&gt;/config.yaml --report-dir=/tmp/reports 2&gt;&amp;1 | tee /tmp/tmp.log\n</code></pre> <ol> <li>The results of the testing will be stored in the <code>/tmp/reports</code> directory.</li> </ol>"},{"location":"kbs/000020179/#faq","title":"FAQ","text":"<pre><code>Errors: [config reading error: decoding failed: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal number -4611686018427388 into Go struct field Phase.Steps.Phases.ReplicasPerNamespace of type int32]\"\n</code></pre> <ul> <li>Change the value of <code>{{$NODES_PER_NAMESPACE := DefaultParam .NODES_PER_NAMESPACE 1}}</code> to match the number of the nodes in the config.yaml file</li> </ul> <pre><code>level=warning msg=\"Got errors during step execution: [measurement call APIResponsiveness - APIResponsiveness error: unexpected response: \\\"# HELP aggregator_openapi_v2_regeneration_count [ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason.\n</code></pre> <ul> <li>Comment out the APIResponsiveness section in config.yaml:</li> </ul> <pre><code>measurements:\n- Identifier: APIResponsiveness\n  Method: APIResponsiveness\n  Params:\n    action: reset\n</code></pre>"},{"location":"kbs/000020179/#further-reading","title":"Further reading","text":"<p>https://github.com/kubernetes/perf-tests</p>"},{"location":"kbs/000020179/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020180/","title":"How do I edit or upgrade clusters created via RKE Templates?","text":"<p>This document (000020180) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020180/#environment","title":"Environment","text":"<ul> <li>RKE1 cluster managed via RKE templates on Rancher 2.x.</li> </ul>"},{"location":"kbs/000020180/#situation","title":"Situation","text":""},{"location":"kbs/000020180/#unable-to-change-certain-kubernetes-cluster-options-under-the-cluster-management-cluster-edit-config-when-managing-clusters-via-rke-templates","title":"Unable to change\u00a0certain Kubernetes Cluster Options under the Cluster Management -&gt; Cluster -&gt; Edit config when managing clusters via RKE templates.","text":""},{"location":"kbs/000020180/#resolution","title":"Resolution","text":""},{"location":"kbs/000020180/#if-you-need-to-make-changes-or-upgrade-your-clusters-managed-via-rke-templates-you-need-to-perform-the-following-steps","title":"If you need to make changes or upgrade your clusters managed via RKE templates, you need to perform the following steps:","text":"<ul> <li>Navigate to Cluster management -&gt; RKE1 Configuration -&gt; RKE templates.</li> <li>Click the three-dot menu to make a new revision of your existing template (select Clone revision).</li> <li>Add the revision name, make the required changes, and save it.</li> <li>After saving the revision, navigate back to Cluster management -&gt; Select the cluster -&gt; Edit config. Under \"Cluster Options\", there will be a drop-down menu to select the\u00a0version of the template you want to use.</li> <li>Select your new version and Save.</li> <li>Once saved, your cluster will be updated with the changes you have made.</li> </ul>"},{"location":"kbs/000020180/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020181/","title":"How to configure Okta Auth with Rancher HA","text":"<p>This document (000020181) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020181/#environment","title":"Environment","text":"<p>Rancher v2.7, v2.8</p>"},{"location":"kbs/000020181/#situation","title":"Situation","text":""},{"location":"kbs/000020181/#when-configuring-okta-authentication-using-the-rancher-official-documentation-in-a-rancher-ha-environment-you-encounter-501-errors-when-trying-to-verify-and-enable-the-configuration","title":"When configuring Okta Authentication using the Rancher Official Documentation\u00a0in a Rancher HA environment you encounter 501 errors when trying to verify and enable the configuration.","text":""},{"location":"kbs/000020181/#resolution","title":"Resolution","text":"<ol> <li>Using the Nodes Tab in your Rancher Management Cluster cordon off the nodes you are not currently connected to, this will force traffic to be returned to the Requester.</li> <li>Run the test and enable procedure for Okta Configuration from Rancher and verify you can now login successfully.</li> <li>Uncordon the other Nodes and the settings will be synced across the cluster automatically.</li> <li>Verify the cluster is working as expected by logging in using an Okta sign-in.</li> </ol> <p>(Optional) To verify the settings have been synced to all nodes in the cluster you can cordon off all but another Node, not the one you used to configure, and attempt logging in using Okta. This process can be repeated for each node.</p>"},{"location":"kbs/000020181/#cause","title":"Cause","text":""},{"location":"kbs/000020181/#for-rancher-to-fully-enable-okta-authenication-it-requires-a-succesful-test-of-your-configuration-to-verify-the-information-is-correct-when-the-test-request-is-sent-from-one-of-your-rancher-servers-to-okta-the-returned-verification-is-routed-through-a-load-balancer-to-a-different-rancher-server-in-the-cluster-as-the-recipient-has-not-yet-been-configured-to-service-okta-authentication-it-will-return-a-501-for-the-request-and-the-rancher-server-that-acted-as-a-requester-will-fail-to-enable-as-it-could-not-complete-the-verification","title":"For Rancher to fully enable Okta Authenication it requires a succesful test of your configuration to verify the information is correct. When the test request is sent from one of your Rancher Servers to Okta the returned verification is routed through a Load Balancer to a different Rancher Server in the cluster. As the recipient has not yet been configured to service Okta Authentication it will return a 501 for the request and the Rancher Server that acted as a requester will fail to enable as it could not complete the verification.","text":""},{"location":"kbs/000020181/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020185/","title":"Why does a cluster or node show requested memory with a milli (m) unit?","text":"<p>This document (000020185) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020185/#environment","title":"Environment","text":"<ul> <li>RKE</li> <li>RKE2</li> </ul>"},{"location":"kbs/000020185/#situation","title":"Situation","text":"<p>Why does a cluster or node shows requested memory with a milli (m) unit?</p>"},{"location":"kbs/000020185/#resolution","title":"Resolution","text":"<p>CPU resources in Kubernetes are measured in millicpus, or 1/1000th of a CPU core, and 1 CPU Core = 1000m. The API will change any request for a decimal point into millicpus. For example 0.1 is converted into 100m. One hyperthread is considered one core, or 1000m.</p> <p>Memory resources in Kubernetes are mesured in bytes, and can be expressed as an integer with one of these suffixes: E, P, T, G, M, K - decimal suffixes, or Ei, Pi, Ti, Gi, Mi, Ki - binary suffixes (more commonly used for memory), or omit the suffix altogether. Lowercase \"m\" notation is not a recommended suffix for memory.</p> <p>The \"m\" notation for memory might indicate a misconfiguration, where CPU units are recommended to use that suffix (example: 200m), and Memory units are recommended to use \"Mi\" (example: 128Mi).</p>"},{"location":"kbs/000020185/#additional-information","title":"Additional Information","text":"<p>Explanation of Kubernetes CPU resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu ) Explanation of Kubernetes memory resources ( https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory )</p>"},{"location":"kbs/000020185/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020186/","title":"How To Update CoreDNS's Resolver Policy","text":"<p>This document (000020186) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020186/#environment","title":"Environment","text":"<p>CoreDNS on Standalone RKE or Rancher Provisioned RKE</p>"},{"location":"kbs/000020186/#situation","title":"Situation","text":""},{"location":"kbs/000020186/#this-article-outlines-how-to-change-corednss-forward-resolver-policy-while-coredns-offers-extensive-configuration-options-our-goal-is-to-substitute-the-default-random-policy","title":"This article outlines how to change CoreDNS's forward resolver policy. While CoreDNS offers extensive configuration options, our goal is to substitute the default random policy.","text":""},{"location":"kbs/000020186/#resolution","title":"Resolution","text":"<p>To change the policy to <code>sequential</code>, edit your cluster's yaml. For RKE provisioned clusters, it will be\u00a0<code>cluster.yaml</code> and for Rancher provisioned custom clusters , it will be found by editing the cluster.</p> <p>For RKE add the following to the end of the file, but for Rancher provisioned clusters, nest this in the <code>rancher_kubernetes_engine_config</code> section.</p> <pre><code>addons: |-\n  ---\n  apiVersion: v1\n  kind: ConfigMap\n  metadata:\n    name: coredns\n    namespace: kube-system\n  data:\n    Corefile: |\n      .:53 {\n          errors\n          health\n          ready\n          kubernetes cluster.local in-addr.arpa ip6.arpa {\n            pods insecure\n            fallthrough in-addr.arpa ip6.arpa\n          }\n          prometheus :9153\n          forward . \"/etc/resolv.conf\" {\n            policy sequential\n          }\n          cache 30\n          loop\n          reload\n          loadbalance\n      }\n</code></pre> <p>The lines of note here are the following, which are changed from just <code>forward . \"/etc/resolv.conf\"</code>.</p> <pre><code>          forward . \"/etc/resolv.conf\" {\n            policy sequential\n          }\n</code></pre> <p>At this point you should be able to just hit save in Rancher or run <code>rke up</code> and the change will be pushed to the cluster.</p>"},{"location":"kbs/000020186/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020186/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020187/","title":"How to update your etcd space alerts for better etcd monitoring","text":"<p>This document (000020187) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020187/#situation","title":"Situation","text":""},{"location":"kbs/000020187/#task","title":"Task","text":"<p>An alert you may have seen is <code>Database usage close to the quota 500M</code> in your Rancher 2.x cluster.</p> <p>This is a default etcd alert built into Rancher, you can find more info on the default alerts here: Rancher v2.x Default Alerts</p> <p>Upon further examination of the alert, you see the description of</p> <p>A warning alert is triggered when the size of etcd exceeds 500M.</p> <p>This alert is somewhat misleading as the default etcd size is 2GB. This alert is also at a severity level of Warning. Below, we suggest configuring your etcd alerts to better utilize the alert thresholds and to avoid the default alert constantly going off.</p>"},{"location":"kbs/000020187/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.x</li> </ul>"},{"location":"kbs/000020187/#resolution","title":"Resolution","text":"<p>You can access the alerts by going to <code>Tools -&gt; Alerts</code> at the cluster level. From here, clone the default alert three times. Once the 3 clones are created, disable the default alert so you always retain a clean reference alert.</p> <p>Alert for 1GB usage - INFO</p> <p>Alert for 1.5GB usage - WARNING</p> <p>Alert for 1.75GB usage - CRITICAL</p>"},{"location":"kbs/000020187/#if-you-are-running-out-of-space-please-reference-the-following-two-links-there-will-be-an-upcoming-kb-article-walking-through-the-resize-process-for-etcd","title":"If you are running out of space, please reference the following two links. There will be an upcoming KB article walking through the resize process for etcd.","text":"<p>Rancher v2.x etcd options</p> <p>etcd space quota documentation</p>"},{"location":"kbs/000020187/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020189/","title":"How to test websocket connections to Rancher v2.x","text":"<p>This document (000020189) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020189/#environment","title":"Environment","text":"<p>Rancher v2.x</p>"},{"location":"kbs/000020189/#situation","title":"Situation","text":"<p>Rancher depends heavily on websocket support for UI and CLI features within Rancher as well as managing and interacting with downstream clusters. This article provides a quick test to determine if websocket connections are working from a potential downstream node or client to the Rancher server cluster.</p>"},{"location":"kbs/000020189/#resolution","title":"Resolution","text":""},{"location":"kbs/000020189/#executing-the-test","title":"Executing the test","text":"<p>First you will need to create an API token to authenticate against Rancher. Start by logging into the Rancher UI. Once logged in, navigate to the API &amp; Keys section by clicking the user icon in the top right of the pane, then click on the API &amp; Keys menu item. Generate a new \"no scope\" key by clicking the Add Key button, providing a name for the token and clicking Create. Copy the bearer token to a safe location.</p> <p>In a Linux shell from the desired test node execute the following, substituting the bearer token and fully qualified domain name of your Rancher endpoint with these environmental variables:</p> <pre><code>export TOKEN=&lt;your token here&gt;\nexport FQDN=&lt;your Rancher fully qualified domain name here&gt;\n</code></pre> <p>Next execute the test using the following command:</p> <pre><code>curl -s -i -N \\\n  --http1.1 \\\n  -H \"Connection: Upgrade\" \\\n  -H \"Upgrade: websocket\" \\\n  -H \"Sec-WebSocket-Key: SGVsbG8sIG15IHdvcmxkIQ==\" \\\n  -H \"Sec-WebSocket-Version: 13\" \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Host: $FQDN\" \\\n  -k https://$FQDN/v3/subscribe\n</code></pre> <p>If websockets work this will successfully connect to the Rancher server and print a steady stream of json output reflecting configuration items being sent from the server. In the event of a failed connection this should print a meaningful error you can act upon to get websockets working between your client and Rancher server.</p> <p>The below is an example of the output from the test upon a successfully established websocket:</p> <pre><code>HTTP/1.1 101 Switching Protocols\nDate: Wed, 27 Nov 2024 15:17:15 GMT\nConnection: upgrade\nUpgrade: websocket\nSec-WebSocket-Accept: XOGNqi8tcvor2gv8PhnKsmWD8xs=\nStrict-Transport-Security: max-age=31536000; includeSubDomains\n\n{\"name\":\"resource.change\",\"data\":{\"baseType\":\"userAttribute\",\"created\":\"2024-11-13T16:27:15Z\",\"createdTS\":1731515235000,\"creatorId\":null,\"id\":\"user-xxxxx\",\"labels\":{\"cattle.io/creator\":\"norman\"},\"lastLogin\":\"2024-11-27T08:35:36Z\",\"lastLoginTS\":1732696536000,\"links\":{\"self\":\"https://yourdomain.example.com/v3/userAttributes/user-xxxxx\"},\"name\":\"user-xxxxx\",\"needsRefresh\":false,\"ownerReferences\":[{\"apiVersion\":\"management.cattle.io/v3\",\"kind\":\"User\",\"name\":\"user-xxxxx\",\"type\":\"/v3/schemas/ownerReference\",\"uid\":\"4c8e2f11-abd0-4a87-b273-76179ad8ffe2\"}],\"type\":\"userAttribute\",\"uuid\":\"d31b7e9a-3c1f-4f2a-b4bd-5397f873a802\"}\n}\n</code></pre>"},{"location":"kbs/000020189/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020191/","title":"The Rancher v2.x Linux log collector script","text":"<p>This document (000020191) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020191/#situation","title":"Situation","text":""},{"location":"kbs/000020191/#rancher-v2x-linux-log-collector","title":"Rancher v2.x Linux log collector","text":"<p>Note:</p> <p>This command requires curl or wget to be installed, and internet access from the node.</p> <p>This script is intended to collect logs from Rancher Kubernetes Engine (RKE) CLI provisioned clusters, K3s clusters, RKE2 clusters, Rancher provisioned Custom, and Node Driver clusters.</p> <p>This script may not collect all necessary information when run on nodes in Hosted Kubernetes Provider clusters.</p> <p>Logs can be collected from a Linux node using the Rancher v2.x log collector script.</p> <p>The script needs to be downloaded and run directly on the node, using the root user or sudo.</p> <p>Output will be written to <code>/tmp</code> as a tar.gz archive named <code>&lt;hostname&gt;-&lt;date&gt;.tar.gz</code>, the default output directory can be changed with the <code>-d</code> flag.</p>"},{"location":"kbs/000020191/#resolution","title":"Resolution","text":""},{"location":"kbs/000020191/#download-and-run-the-script","title":"Download and run the script","text":"<ul> <li>Download the script as: <code>rancher2_logs_collector.sh</code></li> </ul> <p>Using <code>wget</code>:</p> <pre><code>wget --backups https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh\n</code></pre> <p>Using <code>curl</code>:</p> <pre><code>curl -OLs https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh\n</code></pre> <ul> <li>Run the script:</li> </ul> <pre><code>sudo bash rancher2_logs_collector.sh\n</code></pre>"},{"location":"kbs/000020191/#optional-download-and-run-the-script-in-one-command","title":"Optional: Download and run the script in one command","text":"<pre><code>curl -Ls rnch.io/rancher2_logs | sudo bash\n</code></pre>"},{"location":"kbs/000020191/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020191/#options","title":"Options","text":"<p>The available flags that can be passed to the script can be found in the Rancher v2.x log collector script README.</p>"},{"location":"kbs/000020191/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020192/","title":"The Rancher v2.x systems summary script","text":"<p>This document (000020192) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020192/#environment","title":"Environment","text":"<p>A Rancher v2.x instance</p>"},{"location":"kbs/000020192/#situation","title":"Situation","text":"<p>Understanding your cluster/node distribution on an on-going basis assists Rancher in sending you any prescriptive advisories related to scale and performance.</p> <p>System information can be collected from a Rancher v2.x server node using the Rancher v2.x systems summary script v2.</p>"},{"location":"kbs/000020192/#resolution","title":"Resolution","text":"<p>The pod can be deployed from a host with a valid kubeconfig pointing to the local cluster (Kubernetes cluster hosting Rancher). You can deploy the pod and get the output by running the following commands:</p> <pre><code># Deploy the pod in the cluster\nkubectl apply -f https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information-v2/deploy.yaml\n\n# Wait for the pod to reach Succeeded status\nwhile [[ $(kubectl get pod rancher-systems-summary-pod -n cattle-system -o 'jsonpath={..status.phase}') != \"Succeeded\" ]]; do\n  echo \"Waiting for rancher-systems-summary-pod to complete...\"\n  sleep 5\ndone\n\n# Grab the logs from the pod\nkubectl logs pod/rancher-systems-summary-pod -n cattle-system\n\n# Clean up the pod\nkubectl delete pod/rancher-systems-summary-pod -n cattle-system\n</code></pre>"},{"location":"kbs/000020192/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020193/","title":"What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?","text":"<p>This document (000020193) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020193/#situation","title":"Situation","text":""},{"location":"kbs/000020193/#question","title":"Question","text":"<p>What is the process performed by Rancher v2.x when upgrading a Rancher managed Kubernetes cluster?</p>"},{"location":"kbs/000020193/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Running Rancher v2.0.x - v2.3.x. Note, Kubernetes upgrades will be changing in v2.4.x, see Further Reading below.</li> </ul> <p>OR</p> <ul> <li>RKE CLI v0.2.x+</li> </ul>"},{"location":"kbs/000020193/#answer","title":"Answer","text":"<p>Rancher, either through the UI or API, can be used to upgrade a Kubernetes cluster that was provisioned using the \"Custom\" option or on cloud infrastructure such as AWS EC2 or Azure. This can be accomplished by editing the cluster and selecting the desired Kubernetes version. Clusters provisioned with the RKE CLI can also be upgraded by editing the kubernetes_version key in the cluster YAML file. This will trigger an update of all the Kubernetes components in the order listed below:</p>"},{"location":"kbs/000020193/#etcd-plane","title":"Etcd plane","text":"<p>Each etcd container is updated, one node at a time. If the etcd version has not changed between versions of Kubernetes, no action is taken. The process consists of:</p> <ol> <li>Downloading etcd image</li> <li>Stopping and renaming old etcd container (backend datastore is preserved on host)</li> <li>Creating and starting new etcd container</li> <li>Running etcd health check</li> <li>Removing old etcd container</li> </ol> <p>For RKE CLI provisioned clusters, the etcd-rolling-snapshot container is also upgraded if a new version is available.</p>"},{"location":"kbs/000020193/#control-plane","title":"Control plane","text":"<p>Every Kubernetes update will require the control plane components to be updated. All control plane nodes are updated in parallel. The process consists of:</p> <ol> <li>Downloading hyperkube image, which is used by all control plane components.</li> <li>Stopping and renaming old kube-apiserver container</li> <li>Creating and starting new kube-apiserver container</li> <li>Running kube-apiserver health check</li> <li>Removing old kube-apiserver container</li> <li>Stopping and renaming old kube-controller-manager container</li> <li>Creating and starting new kube-controller-manager container</li> <li>Running kube-controller-manager health check</li> <li>Removing old kube-controller-manager container</li> <li>Stopping and renaming old kube-scheduler container</li> <li>Creating and starting new kube-scheduler container</li> <li>Running kube-scheduler health check</li> <li>Removing old kube-scheduler container</li> </ol>"},{"location":"kbs/000020193/#worker-plane","title":"Worker plane","text":"<p>Every Kubernetes update will require the worker components to be updated. These components run on all nodes, including the control plane and etcd. Nodes are updating in parallel. The process consists of:</p> <ol> <li>Downloading hyperkube image (if not already present)</li> <li>Stopping and renaming old kubelet container</li> <li>Creating and starting new kubelet container</li> <li>Running kubelet health check</li> <li>Removing old kubelet container</li> <li>Stopping and renaming old kube-proxy container</li> <li>Creating and starting new kube-proxy container</li> <li>Running kube-proxy health check</li> <li>Removing old kube-proxy container</li> </ol>"},{"location":"kbs/000020193/#addons-user-workloads","title":"Addons &amp; user workloads","text":"<p>Once Kubernetes etcd, control plane, and worker components have been updated, the latest manifests for addons are applied. This includes, but is not limited to KubeDNS/CoreDNS, Nginx Ingress, Metrics Server, and CNI plugin (Calico, Weave, Flannel, Canal). Depending on the manifest deltas and the upgrade strategy defined in the manifest, pods and their corresponding containers may or may not be removed and recreated. Please be aware that some of these addons are critical for your cluster to operator correctly and you may experience brief outages if these workloads are restarted. For example, when KubeDNS/CoreDNS is restarted, you could have issues resolving hostname to IP addresses. When the Nginx Ingress is restarted, layer 7 http/https traffic from outside your cluster to your workloads may get interrupted. When your CNI plugin is restarted on each node, the workloads running on the node may temporarily not be able to reach workloads running on other nodes. The best way to minimize outages or disruptions is to make sure you have proper fault tolerance in your cluster.</p> <p>The kubelet automatically destroys and recreates all user workload pods when the spec hash value is changed. This value will change for a pod if the Kubernetes upgrade involves any field changes in the pod manifest, such as a new field or the removal of a deprecated field. As a best practice, it's best to assume all your pods and containers will be destroyed and recreated during a Kubernetes upgrade. This is more likely to happen for major/minor releases and less likely for patch releases.</p>"},{"location":"kbs/000020193/#further-reading","title":"Further Reading","text":"<p>Upgrade refactor in v2.4: https://github.com/rancher/rancher/issues/23038</p> <p>Kubeadm upgrades: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p>"},{"location":"kbs/000020193/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020194/","title":"What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?","text":"<p>This document (000020194) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020194/#environment","title":"Environment","text":"<p>Running Rancher v2.x HA deployed using Helm.</p>"},{"location":"kbs/000020194/#situation","title":"Situation","text":"<p>What is the process performed during a Rancher v2.x upgrade and what is the impact to the managed downstream Kubernetes clusters?</p>"},{"location":"kbs/000020194/#resolution","title":"Resolution","text":"<p>The bulk of the Rancher HA installation and upgrade are performed by using Helm. The core piece of the Rancher Helm Chart is the Rancher deployment. Please note the following characteristics of this Helm Chart:</p> <ul> <li>Deployment is set to a replica of 3. This means Kubernetes will attempt to run and maintain three rancher pods.</li> <li>Deployment is set to do a rolling update with a max surge of 25% and max unavailability of 25%. This means:</li> <li>During an upgrade, pods are updated in chunks, not all at once.</li> <li>During an update, no more than 4 pods will be running at once</li> <li>During an update, no fewer than 2 pods will be available at once</li> <li>Deployment has an anti-affinity for the node's hostname. This means Kubernetes will attempt to place each pod on a separate host. For three pods and three hosts, that means one pod on each host.</li> </ul> <p>Given the information above on how the manifests are defined, below is the expected sequence of events during a Rancher upgrade:</p>"},{"location":"kbs/000020194/#rancher-ha-cluster","title":"Rancher HA cluster","text":"<ol> <li>A new rancher pod is created</li> <li>An old rancher pod is terminated</li> <li>A new second rancher pod is created</li> <li>A second old rancher pod is terminated</li> <li>A new third rancher pod is created</li> <li>A third old rancher pod is terminated</li> </ol>"},{"location":"kbs/000020194/#downstream-clusters","title":"Downstream clusters","text":"<p>Rancher will also apply two other important manifests to all managed clusters. These are described below:</p>"},{"location":"kbs/000020194/#cattle-cluster-agent-deployment","title":"cattle-cluster-agent deployment","text":"<ul> <li>Deployment is set to a replica of 2</li> <li>Deployment is set to do a rolling update with a max surge of 25% and a max unavailability of 25%. See Rancher's deployment description above for the behavior of these settings.</li> </ul>"},{"location":"kbs/000020194/#cattle-node-agent-daemonset-only-for-rke1-clusters","title":"cattle-node-agent daemonset - only for RKE1 clusters","text":"<ul> <li>Daemonset will deploy one agent per node</li> <li>Daemonset is set to a rolling update with max unavailable of 1 pod. That means during an update, one pod is updated at a time.</li> </ul> <p>Once Rancher is upgraded, Rancher will check each cluster it manages to make sure the cattle-cluster-agent and cattle-node-agents are up to date. For RKE2/k3s clusters, if the cluster is not in a \"Provisioning\" state, meaning another cluster update is in progress, it will deploy the latest cattle-cluster-agent into the cluster. For RKE1, it will also deploy the latest cattle-cluster-agent and cattle-node-agent manifests into the cluster. All managed clusters are updated in parallel and not sequentially.</p> <p>Other workloads running in the cluster should not be impacted.</p>"},{"location":"kbs/000020194/#additional-information","title":"Additional Information","text":"<p>Kubernetes deployments - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</p> <p>Kubernetes daemonsets - https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</p>"},{"location":"kbs/000020194/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020195/","title":"How to rollback the Kubernetes version of a Rancher v2.x provisioned RKE1 cluster","text":"<p>This document (000020195) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020195/#environment","title":"Environment","text":"<p>Rancher 2.x launched RKE1 clusters.</p>"},{"location":"kbs/000020195/#situation","title":"Situation","text":""},{"location":"kbs/000020195/#to-roll-back-your-kubernetes-cluster-version-upgrade-you-need-to-first-have-taken-an-etcd-snapshot-before-the-upgrade-you-should-keep-the-reference-of-the-snapshot-name-that-was-created-as-your-pre-upgrade-snapshot-in-my-cluster-which-has-cluster-id-c-q8st7-my-snapshot-name-was-c-q8st7-ml-qdxdh-our-example-upgrade-is-from-v12716-rancher1-1-to-v12812-rancher1-1","title":"To roll back your Kubernetes cluster version upgrade, you need to first have taken an etcd snapshot before the upgrade. You should\u00a0keep the reference of\u00a0the snapshot name that was created as your \"pre-upgrade\" snapshot. In my cluster which has cluster ID: <code>c-q8st7</code> , my snapshot name was <code>c-q8st7-ml-qdxdh</code>. Our example upgrade is from <code>v1.27.16-rancher1-1</code>\u00a0to <code>v1.28.12-rancher1-1</code>.","text":""},{"location":"kbs/000020195/#resolution","title":"Resolution","text":""},{"location":"kbs/000020195/#a-kubernetes-cluster-rollback-will-most-definitely-cause-downtime-in-the-cluster-as-you-are-restoring-a-snapshot-from-before-the-upgrade-and-the-cluster-will-have-to-go-through-the-process-of-reconcilation","title":"A Kubernetes Cluster Rollback will most definitely cause downtime in the cluster, as you are restoring a snapshot from before the upgrade, and the cluster will have to go through the process of reconcilation.","text":""},{"location":"kbs/000020195/#rollback-operation","title":"Rollback operation","text":"<p>To rollback, you must:</p> <ul> <li>Edit Cluster from Cluster Management.</li> <li>Edit as YAML</li> <li>Find the key\u00a0<code>kubernetes_version</code>\u00a0in the YAML and set it back to <code>v1.27.16-rancher1-1</code> (or whatever your desired restore version is)</li> <li>Find the <code>restore</code> key in the YAML. You will need to update the following configuration from:</li> </ul> <pre><code>rancher_kubernetes_engine_config:\n    restore:\n    restore: false\n</code></pre> <p>To the below configuration which instructs the cluster to use the Snapshot that was taken prior upgrade to perform a rollback:</p> <pre><code>rancher_kubernetes_engine_config:\n    restore:\n    restore: true\n    snapshot_name: \"c-q8st7:c-q8st7-ml-qdxdh\"\n</code></pre> <p>Note the\u00a0<code>snapshot_name</code> has the cluster ID prefixed to it with a \" <code>:\"</code> <code>.</code></p> <ul> <li>Finally, you can save the cluster, and observe the snapshot restore + K8s version rollback occur.</li> </ul>"},{"location":"kbs/000020195/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020196/","title":"HTTP 401 \"clusterID does not match\" error using cluster-scoped Rancher API token in Rancher v2.x","text":"<p>This document (000020196) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020196/#environment","title":"Environment","text":"<ul> <li> <p>A Rancher v2.x instance</p> </li> <li> <p>A cluster-scoped Rancher API token</p> </li> </ul>"},{"location":"kbs/000020196/#situation","title":"Situation","text":"<p>When attempting to perform operations against the Rancher v2.x API, with a cluster-scoped API token, you receive a HTTP 401 response code with a body of the following format:</p> <pre><code>{\n  \"type\":\"error\",\n  \"status\":\"401\",\n  \"message\":\"clusterID does not match\"\n}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020196/#resolution","title":"Resolution","text":"<p>Only use a cluster-scoped API token where you wish to restrict usage of the token to the Kubernetes API for that cluster, or the Rancher v3 cluster endpoint. To permit access to other API endpoints, or to use a token for API access to multiple clusters, create a Rancher API token that is not cluster-scoped.</p>"},{"location":"kbs/000020196/#cause","title":"Cause","text":"<p>The primary purpose of cluster-scoped API tokens is to permit access to the Kubernetes API for a specific cluster via Rancher, i.e. via the endpoint <code>https://&lt;rancher_url&gt;/k8s/clusters/&lt;cluster_id&gt;</code> for the matching cluster. Cluster-scoped tokens can be used to interact directly with the Kubernetes API of clusters configured with an Authorized Cluster Endpoint.</p> <p>In addition, a cluster-scoped token also works for resources under the Rancher v3 API endpoint for that cluster, at <code>https://&lt;rancher_url&gt;/v3/clusters/&lt;cluster_id&gt;</code>.</p> <p>The token is not valid for the other available API endpoints or other clusters. Attempts to perform API operations on other clusters or endpoints with a cluster-scoped token will result in the HTTP 401 <code>\"clusterID does not match\"</code> error.</p>"},{"location":"kbs/000020196/#additional-information","title":"Additional Information","text":"<p>You can read more on the Rancher v2.x API within the API documentation.</p>"},{"location":"kbs/000020196/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020198/","title":"Launching kubectl for cluster within Rancher UI fails in a cluster after following the CIS Benchmark Hardening Guide for Kubernetes","text":"<p>This document (000020198) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020198/#situation","title":"Situation","text":""},{"location":"kbs/000020198/#issue","title":"Issue","text":"<p>Attempting to launch kubectl in the Rancher v2.x UI, for a cluster upon which the Rancher CIS Hardening Guide has been applied, results in a <code>Closed Code: 1006</code> message. Further, using the browser developer tools to inspect requests when opening this page reveals the API request to initiate the connection (https:///v3/clusters/?shell=true) receiving a HTTP 403 response.</p>"},{"location":"kbs/000020198/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An RKE CLI or Rancher v2.x launched Kubernetes cluster, with the Rancher v2.1.x, v2.2.x or v2.3.x CIS Hardening Guide applied.</li> </ul>"},{"location":"kbs/000020198/#root-cause","title":"Root cause","text":"<p>This behaviour is caused by CIS Control 1.1.12, which specifies that the DenyEscalatingExec Admission Controller should be enabled on the Kubernetes API Server.</p> <p>The terminal for the Rancher UI is provided by exec'ing into a cattle-node-agent Pod, whilst Pods within this DaemonSet run in Privileged mode. As a result the exec to open the terminal session is denied by the DenyEscalatingExec Admission Controller.</p>"},{"location":"kbs/000020198/#workaround","title":"Workaround","text":"<p>You can workaround the issue by removing <code>DenyEscalatingExec</code> from the list of <code>enable-admission-plugins</code> in <code>extra_args</code> for the <code>kube-api</code> service.</p>"},{"location":"kbs/000020198/#resolution","title":"Resolution","text":"<p>This issue is tracked in the Rancher GitHub issue #19439.</p>"},{"location":"kbs/000020198/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020199/","title":"How to increase the log level of Kubernetes components in an RKE CLI or Rancher provisioned Kubernetes cluster","text":"<p>This document (000020199) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020199/#situation","title":"Situation","text":""},{"location":"kbs/000020199/#task","title":"Task","text":"<p>When troubleshooting an issue with an RKE CLI or Rancher provisioned Kubernetes cluster, it may be helpful to increase the verbosity of logging on one or more of the Kubernetes components, above the default level. This article details the process of increasing logging on both those components that use the Kubernetes hyperkube image (kubelet, kube-apiserver, kube-controller-manager, kube-scheduler, kube-proxy) as well as the etcd component.</p>"},{"location":"kbs/000020199/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned by the RKE CLI or Rancher v2.x</li> </ul>"},{"location":"kbs/000020199/#resolution","title":"Resolution","text":""},{"location":"kbs/000020199/#kubernetes-api-server-controller-manager-scheduler-kube-proxy-and-kubelet","title":"Kubernetes API Server, Controller Manager, Scheduler, Kube Proxy and Kubelet","text":"<p>The Kubernetes core components, which run using the Kubernetes hyperkube image, will log ERROR, WARNING and INFO messages. The verbosity of the INFO level log output is controlled by the <code>--v</code> flag, which is set to an integer from 0 to 9. In an RKE CLI or Rancher launched Kubernetes cluster, the <code>--v</code> flag is configured to <code>2</code> by default. At this level, the components will log <code>useful steady state information about the service and important log messages that may correlate to significant changes in the system.</code></p> <p>In order to troubleshoot an issue, it may be useful to increase the verbosity flag to one of the following:</p> Verbosity Description --v=3 Extended information about changes. --v=4 Debug level verbosity. --v=6 Display requested resources. --v=7 Display HTTP request headers. --v=8 Display HTTP request contents. --v=9 Display HTTP request contents without truncation of contents."},{"location":"kbs/000020199/#update-the-v-flag-in-an-rke-cli-launched-cluster","title":"Update the <code>--v</code> flag in an RKE CLI launched cluster","text":"<ol> <li>First set the <code>--v</code> flag for the desired components within the <code>cluster.yml</code>. For each of the services you wish to change the verbosity on, you should add an extra_args option with <code>v: \"&lt;value&gt;\"</code> in the services block, per the example below. The appropriate name for each service within this block can be found within the RKE documentation. N.B. Please see the separate section below for updating the log verbosity of the etcd component</li> </ol> <pre><code>     services:\n       kube-api:\n         extra_args:\n           v: '9'\n</code></pre> <ol> <li>Having set the flag in the cluster.yml, run <code>rke up --config cluster.yml</code> to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020199/#update-the-v-flag-in-a-rancher-launched-cluster","title":"Update the <code>--v</code> flag in a Rancher launched cluster","text":"<p>Navigate to the cluster within the Rancher UI and click <code>Edit Cluster</code>, then <code>Edit as YAML</code>. For each of the services you wish to change the verbosity on, you should add an extra_args option with <code>v: \"&lt;value&gt;\"</code> in the services block of the cluster, per the example below.</p> <p>N.B. Please see the separate section below for updating the log verbosity of the etcd component.</p> <pre><code>  services:\n    kube-api:\n      extra_args:\n        v: '9'\n</code></pre> <p>The appropriate name for each service within this block can be found within the RKE documentation.</p> <p>Having set the verbosity flag, click <code>Save</code> at the bottom of the page, to update the cluster.</p>"},{"location":"kbs/000020199/#etcd","title":"etcd","text":"<p>The etcd component is configured to log at an INFO level by default, in an RKE CLI or Rancher launched Kubernetes cluster, but this can be set to DEBUG level by setting the <code>--debug=true</code> flag.</p>"},{"location":"kbs/000020199/#update-etcd-verbosity-in-an-rke-cli-launched-cluster","title":"Update etcd verbosity in an RKE CLI launched cluster","text":"<ol> <li>First set the <code>--debug=true</code> flag, within the <code>cluster.yml</code> cluster configuration file, under <code>extra_args</code> for the etcd service, per the following example:</li> </ol> <pre><code>     services:\n       etcd:\n         extra_args:\n           debug: 'true'\n</code></pre> <ol> <li>Having set the flag in the cluster.yml, run <code>rke up --config cluster.yml</code> to update the cluster with the new configuration.</li> </ol>"},{"location":"kbs/000020199/#update-etcd-verbosity-in-a-rancher-launched-cluster","title":"Update etcd verbosity in a Rancher launched cluster","text":"<p>Navigate to the cluster within the Rancher UI and click <code>Edit Cluster</code>, then <code>Edit as YAML</code>. Set the <code>--debug=true</code> flag under <code>extra_args</code>, for the etcd service, per the following example:</p> <pre><code>  services:\n    etcd:\n      extra_args:\n        debug: 'true'\n</code></pre> <p>Having set the debug flag, click <code>Save</code> at the bottom of the page, to update the cluster.</p>"},{"location":"kbs/000020199/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020201/","title":"\"log unreadable. It is excluded and would be examined next time.\" warning messages, for kubelet and kube-proxy, in rancher-logging-fluentd Pod logs of worker nodes","text":"<p>This document (000020201) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020201/#situation","title":"Situation","text":""},{"location":"kbs/000020201/#issue","title":"Issue","text":"<p>In a Rancher v2.x provisioned Kubernetes cluster, with Rancher Cluster Logging configured, the <code>rancher-logging-fluentd</code> Pod logs on Linux worker role only nodes show warning messages of the following format:</p> <pre><code>2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kubelet_5c47838dd4af749a7a0d1c457b04a6d7b905e680157718063c8e5d9eb61268fa.log unreadable. It is excluded and would be examined next time.\n2019-12-05 10:58:27 +0000 [warn]: #0 /var/lib/rancher/rke/log/kube-proxy_d2beb2e667eefbd6d95355082af4bc61c367fc4c220d9f1d165d15a8c8be2ab1.log unreadable. It is excluded and would be examined next time.\n</code></pre> <p>The output of <code>ls /var/lib/rancher/rke/log/</code> on affected workers shows that these files referenced in the warning log messages are broken symlinks. In addition, <code>docker ps</code> output shows the container ID for the currently running <code>kubelet</code> and <code>kube-proxy</code> containers does not match the IDs in these filenames.</p>"},{"location":"kbs/000020201/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x provisioned Kubernetes cluster, using either custom nodes or nodes hosted in an infrastructure provider.</li> <li>Rancher Cluster Logging configured on the cluster, with the 'Include System Log' option set.</li> <li>Worker role only nodes in the cluster.</li> </ul>"},{"location":"kbs/000020201/#root-cause","title":"Root cause","text":"<p>These warning level messages in the <code>rancher-logging-fluentd</code> Pod are the result of the issue tracked in Rancher GitHub issue #22549.</p> <p>The container log symlinks in <code>/var/lib/rancher/rke/log/</code> for cluster component containers ( <code>kubelet</code>, <code>kube-proxy</code>, <code>nginx-proxy</code>) on worker nodes in Rancher launched clusters are not cleaned up when these components are re-created, i.e. due to a Kubernetes version upgrade, or other configuration update for these components.</p> <p>As a result these broken symlinks persist and cause the <code>log unreadable</code> warning messages when the <code>rancher-logging-fluentd</code> Pod attempts to parse files in the <code>/var/lib/rancher/rke/log/</code> directory.</p> <p>This warning message itself is harmless and can be ignored.</p>"},{"location":"kbs/000020201/#resolution","title":"Resolution","text":"<p>The request to handle automatic clean-up of these log symlinks on worker nodes, in Rancher provisioned clusters, is tracked in Rancher GitHub issue #22549.</p>"},{"location":"kbs/000020201/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020202/","title":"Many rancher-agent containers running on Rancher v2.x provisioned RKE cluster, where stopped containers are regularly deleted on hosts","text":"<p>This document (000020202) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020202/#environment","title":"Environment","text":"<ul> <li>A Rancher v2.x provisioned Rancher Kubernetes Engine (RKE) cluster.</li> <li>Repeated deletion of stopped containers on hosts in the cluster, e.g. use of <code>docker system prune</code>, either manually or as part of an automated process such as a cronjob.</li> </ul>"},{"location":"kbs/000020202/#situation","title":"Situation","text":""},{"location":"kbs/000020202/#issue","title":"Issue","text":"<p>On a Rancher v2.x provisioned cluster, a host shows a large number of containers running the <code>rancher-agent</code> image, per the following output of <code>docker ps | grep rancher-agent</code>:</p> <pre><code>$ docker ps | grep rancher-agent\n...\naeffe9725521        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   About a minute ago   Up About a minute                       sleepy_hopper\n130120f49b71        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   6 minutes ago        Up 6 minutes                            stoic_hypatia\n498b923d9b6e        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   11 minutes ago        Up 11 minutes                            laughing_elbakyan\n3453865e5f70        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   16 minutes ago        Up 16 minutes                            wonderful_gagarin\nf925209cd16a        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   21 minutes ago       Up 21 minutes                           silly_shannon\n7d7fb5d4bf04        rancher/rancher-agent:v2.3.3         \"run.sh --server htt\u2026\"   26 minutes ago       Up 26 minutes                           gifted_elgamal\n...\n</code></pre> <p>A <code>docker inspect &lt;container_id&gt;</code> for these containers, shows the Path and Args are of the following format:</p> <pre><code>\"Path\": \"run.sh\",\n\"Args\": [\n    \"--server\",\n    \"https://167.172.96.240\",\n    \"--token\",\n    \"gwrp7zlnwvsnzh2nhbvwcgdw45ccv6cq9pztzdd92j6xlv69xxhvnp\",\n    \"--ca-checksum\",\n    \"bbc8c7ca05c87a7140154554fa1a516178852f2710538c57718f4c874c29533c\",\n    \"--no-register\",\n    \"--only-write-certs\"\n],\n</code></pre>"},{"location":"kbs/000020202/#resolution","title":"Resolution","text":"<p>To trigger automatic removal of the <code>rancher-agent</code> containers, the <code>node-agent</code> container on the host can be restarted. Identifying the running agent container with <code>docker ps | grep k8s_agent_cattle-node</code> restart the container with <code>docker restart &lt;container_id&gt;</code>.</p> <p>In addition, you can prevent further creation of multiple <code>rancher-agent</code> container instances by removing whichever process is triggering the deletion of stopped containers.</p>"},{"location":"kbs/000020202/#cause","title":"Cause","text":"<p>This behaviour is a result of the issue reported in Rancher GitHub issue #15364.</p> <p>The <code>share-mnt</code> container is created on a Rancher provisioned Kubernetes cluster, and exits upon completion, but is not removed such that it can be invoked again.</p> <p>Meanwhile, the Rancher <code>node-agent</code> Pod on a host will spawn a new <code>share-mnt</code> container, if the <code>share-mnt</code> is removed. Upon starting, the <code>share-mnt</code> process spawns a <code>rancher-agent</code> container to write certificates. This agent container will run indefinitely until the <code>node-agent</code> is triggered to reconnect to the Rancher server or the <code>node-agent</code> process is restarted.</p> <p>As a result, where the <code>share-mnt</code> container on a host is removed repeatedly, either manually or by an automated process, this will result in multiple running <code>rancher-agent</code> containers.</p>"},{"location":"kbs/000020202/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020203/","title":"Is it safe to update the Docker bridge IP range on hosts in an RKE or Rancher v2.x launched Kubernetes cluster?","text":"<p>This document (000020203) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020203/#situation","title":"Situation","text":""},{"location":"kbs/000020203/#question","title":"Question","text":"<p>The <code>docker0</code> bridge network has a default IP range of <code>172.17.0.0/16</code>. These ranges will be routed to these interfaces, per the below example of the <code>route</code> output. If the range(s) overlap with the internal IP space usage in your own network, the host will not be able to route packets to other hosts in your network that lie within these ranges. As a result you may wish to change the bridge range(s) to enable successful routing to hosts within these.</p> <pre><code>$ route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n...\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n...\n</code></pre>"},{"location":"kbs/000020203/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>This article is only applicable to Kubernetes cluster launched by RKE v1.x, or Rancher v2.x</li> </ul>"},{"location":"kbs/000020203/#answer","title":"Answer","text":"<p>Updating the <code>docker0</code> bridge IP range\u00a0is possible in an RKE or Rancher v2.x provisioned Kubernetes cluster, where no cluster containers are in fact running attached to the Docker bridge network. The only impact of the change should be some downtime, as you will be required to restart the Docker daemon for the change to take effect.</p> <p>For other operating systems, where Docker is installed from the upstream Docker repositories, you should update the <code>bip</code> configuration in <code>/etc/docker/daemon.json</code> per the dockerd documentation.</p> <p>On CentOS, RHEL and SLES you should also check the configuration in /etc/sysconfig/docker to ensure <code>--bip</code> has not been configured there.</p>"},{"location":"kbs/000020203/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020204/","title":"Is it safe to disable inter-container connectivity (icc) on the Docker daemon in an RKE or Rancher v2.x launched Kubernetes cluster?","text":"<p>This document (000020204) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020204/#environment","title":"Environment","text":"<p>A Rancher Kubernetes Engine (RKE) cluster, provisioned by the RKE CLI or Rancher v2.x</p>"},{"location":"kbs/000020204/#situation","title":"Situation","text":"<p>The Docker daemon provides a\u00a0configuration option <code>icc</code> which permits a user to disable inter-container connectivity (icc) on the Docker bridge network. Is is safe to disable this Docker daemon option in an RKE or Rancher v2.x launched Kubernetes cluster?</p>"},{"location":"kbs/000020204/#resolution","title":"Resolution","text":"<p>Setting <code>icc</code> to false in the docker daemon.json configuration, or as an argument to to dockerd, is possible but is unnecessary in an RKE or Rancher v2.x provisioned Kubernetes cluster, as containers are not run attached to the Docker bridge network. Therefore, whilst this step is often included in standard 'hardening Docker daemon' guides, it is not relevant to operating an RKE or Rancher launched Kubernetes cluster.</p>"},{"location":"kbs/000020204/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020205/","title":"Users assigned the Project Owner or Member role on a project are able to create namespaces on any project, in the same cluster, to which they have access","text":"<p>This document (000020205) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020205/#environment","title":"Environment","text":"<ul> <li>A cluster managed by Rancher v2.x</li> <li>A user granted the Project Member or Owner role on one project, and access e.g. the Read-only role, on another project</li> </ul>"},{"location":"kbs/000020205/#situation","title":"Situation","text":"<p>A user assigned the Project Owner or Member role on one project is able to create namespaces on any project, in the same cluster, to which they have access.</p> <p>For example, if a user has been granted the Project Member role on a Project named Dev in a cluster, and the Read-only role on a project named Test in that cluster, they will be able to create namespaces on both the Dev and Test projects.</p>"},{"location":"kbs/000020205/#resolution","title":"Resolution","text":"<p>Per the caveat explanation in the Rancher v2.x documentation:</p> <p>Users assigned the Owner or Member role for a project automatically inherit the namespace creation role. However, this role is a Kubernetes ClusterRole, meaning its scope extends to all projects in the cluster. Therefore, users explicitly assigned the owner or member role for a project can create namespaces in other projects they\u2019re assigned to, even with only the Read Only role assigned.</p>"},{"location":"kbs/000020205/#additional-information","title":"Additional Information","text":"<p>Read more on Cluster and Project Roles in the Rancher v2.x. documentation.</p>"},{"location":"kbs/000020205/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020207/","title":"Node labels and taints reset on reboot with AWS cloudprovider in Kubernetes lower than v1.12.0","text":"<p>This document (000020207) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020207/#situation","title":"Situation","text":""},{"location":"kbs/000020207/#issue","title":"Issue","text":"<p>In a Kubernetes cluster, running on AWS EC2 instances, with the AWS cloudprovider configured, labels and taints for a node are reset when the EC2 instance is rebooted.</p>"},{"location":"kbs/000020207/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Kubernetes version lower than v1.12.0</li> <li>Cluster running on AWS EC2 instances, with the AWS cloudprovider configured</li> </ul>"},{"location":"kbs/000020207/#root-cause","title":"Root Cause","text":"<p>This behaviour is caused by the AWS cloudprovider in Kubernetes versions prior to v1.12.0, in which a stopped EC2 instance is deleted from the Kubernetes cluster, and then re-created when started again. As a result of this deletion and re-creation labels and taints on the node are lost during the reboot. Details of the issue and fix can be found in Kubernetes Pull Request #66835.</p>"},{"location":"kbs/000020207/#resolution","title":"Resolution","text":"<p>In order to resolve this issue, the cluster should be upgraded to Kubernetes version v1.12.0 or above.</p> <p>For clusters provisioned via the RKE CLI, users can upgrade the cluster to a Kubernetes version of v1.12.0 or higher with RKE v0.1.10 or above.</p> <p>For clusters provisioned via Rancher, users can upgrade the cluster to a Kubernetes version of v1.12.6 or higher with Rancher v2.1.7 or above.</p>"},{"location":"kbs/000020207/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020210/","title":"RKE errors connecting to the Docker socket whilst updating clusters with the Aqua Enforcer deployed","text":"<p>This document (000020210) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020210/#situation","title":"Situation","text":""},{"location":"kbs/000020210/#issue","title":"Issue","text":"<p>During invocations of <code>rke up</code> via the RKE CLI or whilst modifying Rancher provisioned Kubernetes clusters, the process fails upon attempted creation of a Kubernetes component container with an error of the following format:</p> <pre><code>2019-04-30T15:19:17.9826528Z time=\"2019-04-30T15:19:17Z\" level=fatal msg=\"[etcd] Failed to bring up Etcd Plane: Failed to create [etcd] container on host [rancher.example.com]: Failed to create [etcd] container on host [rancher.example.com]: error during connect: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create?name=etcd: EOF\n</code></pre>"},{"location":"kbs/000020210/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster provisioned via the RKE CLI or Rancher</li> <li>The Aqua Enforcer workload deployed in the cluster, with AQUA_RUNC_INTERCEPTION environment variable set to 0</li> </ul>"},{"location":"kbs/000020210/#root-cause","title":"Root cause","text":"<p>The issue is caused by Aqua Enforcer's use of the Docker socket to perform runtime enforcement operations preventing RKE from successfully connecting to the Docker socket upon some requests.</p>"},{"location":"kbs/000020210/#resolution","title":"Resolution","text":"<p>To resolve this issue set the AQUA_RUNC_INTERCEPTION environment variable on the Aqua Enforcer daemonset to 1. With this setting the Aqua Enforcer will interact directly with runC to perform runtime enforcement operations, and not with the Docker daemon via the Docker socket. This is the default behaviour in new versions of the Aqua Enforcer, as it brings stability and performance benefits. More information on this setting can be found at https://docs.aquasec.com/docs/40-ga#section-new-aqua-enforcer-architecture-for-enhanced-stability-and-performance</p>"},{"location":"kbs/000020210/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020211/","title":"Editing Rancher launched Kubernetes cluster in infrastructure provider restricted to creating user","text":"<p>This document (000020211) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020211/#situation","title":"Situation","text":""},{"location":"kbs/000020211/#issue","title":"Issue","text":"<p>When attempting to edit a Rancher launched Kubernetes cluster, hosted on nodes in an infrastructure provider neither the <code>Cluster Options</code> nor <code>Node Pools</code> sections are available and configurable in the edit cluster view, if logged in as a different user to the cluster creator.</p>"},{"location":"kbs/000020211/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Rancher v2.x launched Kubernetes cluster, provisioned on nodes hosted in an infrastructure provider</li> <li>Access to the Rancher UI as a user different to the cluster creator</li> </ul>"},{"location":"kbs/000020211/#root-cause","title":"Root cause","text":"<p>Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider.</p> <p>Node templates are user-scoped and, as a result, where <code>userA</code> creates a node template in Rancher it is not accessible by <code>userB</code>. This prevents Rancher launched Kubernetes clusters, provisioned on nodes in an infrastructure provider by <code>userA</code> from being edited by other users, as only <code>userA</code> has access to the node template configuration.</p>"},{"location":"kbs/000020211/#resolution","title":"Resolution","text":"<p>An enhancement request to enable users, other than the cluster creator, to edit Rancher launched Kubernetes clusters is tracked in Rancher GitHub Issue #12038.</p> <p>Where it is necessary for another user to edit the cluster, i.e. the original user who created the cluster has left the business, it is possible to re-associate the node template with a different user. If you encounter this situation, please open a ticket with Rancher Support for assistance.</p>"},{"location":"kbs/000020211/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020212/","title":"Blank provider listed for cluster when logged in as user who did not create cluster in Rancher v2.x","text":"<p>This document (000020212) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020212/#environment","title":"Environment","text":"<ul> <li>A Rancher v2.x launched Rancher Kubernetes Enginer (RKE) cluster, provisioned on nodes hosted in an infrastructure provider</li> <li>Access to the Rancher UI as a user different to the cluster creator</li> </ul>"},{"location":"kbs/000020212/#situation","title":"Situation","text":"<p>When viewing a Rancher launched Kubernetes cluster, provisioned on\u00a0nodes hosted in an infrastructure provider, as a user other than the cluster creator, the infrastructure provider name is blank or shows as Imported.</p>"},{"location":"kbs/000020212/#cause","title":"Cause","text":"<p>Node templates contain the configuration parameters for provisioning nodes in a specific cloud provider.</p> <p>Node templates are user-scoped and, as a result, where <code>userA</code> creates a node template in Rancher it is not accessible by <code>userB</code>.</p> <p>Meanwhile, the Rancher v2.x UI determines the provider for a Rancher launched Kubernetes cluster by mapping <code>nodes</code> to <code>node templates</code> to <code>node drivers</code>.</p> <p>The user-scoping of node templates therefore prevents users, other than the creator, from viewing the provider of the cluster.</p>"},{"location":"kbs/000020212/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020214/","title":"'Error: snapshot missing hash but --skip-hash-check=false' when performing `rke etcd snapshort-restore` with .zip extension included in snapshot name","text":"<p>This document (000020214) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020214/#environment","title":"Environment","text":"<ul> <li>A Kubernetes cluster provisioned with the Rancher Kubernetes Engine (RKE) CLI</li> </ul>"},{"location":"kbs/000020214/#situation","title":"Situation","text":""},{"location":"kbs/000020214/#when-performing-an-etcd-snapshot-restore-via-rke-including-the-zip-file-extension-in-the-snapshot-name-parameter-ie-rke-etcd-snapshot-restore-name-snapshotzip-and-a-snapshot-filename-of-snapshotzip-the-restoration-fails-with-an-error-of-the-following-format","title":"When performing an etcd snapshot restore via RKE, including the\u00a0<code>.zip</code> file extension in the snapshot name parameter, i.e. <code>rke etcd snapshot-restore --name snapshot.zip</code> and a snapshot filename of <code>snapshot.zip</code>, the restoration fails with an error of the following format:","text":"<pre><code>FATA[0020] [etcd] Failed to restore etcd snapshot: Failed to run etcd restore container, exit status is: 128, container logs: Error: snapshot missing hash but --skip-hash-check=false\n</code></pre>"},{"location":"kbs/000020214/#resolution","title":"Resolution","text":"<p>This issue is caused by the incorrect inclusion of the <code>.zip</code> file extension to the snapshot name parameter.</p> <p>The snapshot name parameter ( <code>--name</code>) should contain the snapshot name, excluding the file extension.</p> <p>In the example of a snapshot filename of <code>snapshot.zip</code> the correct name parameter is therefore just <code>snapshot</code>, i.e. <code>rke etcd snapshot-restore --name snapshot</code>.</p>"},{"location":"kbs/000020214/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020217/","title":"How to install or upgrade to a specific Rancher v2.x version","text":"<p>This document (000020217) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020217/#environment","title":"Environment","text":"<p>Rancher (All version)</p>"},{"location":"kbs/000020217/#situation","title":"Situation","text":""},{"location":"kbs/000020217/#issue","title":"Issue","text":"<p>By default the installation and upgrade documentation references the installation of, or upgrade to, the most recently released latest or stable tagged version of Rancher. This article details how to install a specific version, both in a single node and high availability installation.</p> <p>For details on the difference between the latest and stable releases please see the documentation on 'Choosing a Version' .</p> <p>N.B. We strongly recommend you only run product releases tagged \u201cStable\u201d in your production and any other business-critical environments. Any product release with the \u201cLatest\u201d tag should only be used for testing the latest releases.\"</p>"},{"location":"kbs/000020217/#resolution","title":"Resolution","text":""},{"location":"kbs/000020217/#single-node-install","title":"Single Node Install","text":"<p>To install or upgrade to a specific Rancher version in a single node install, you can specify the exact version number of the image to run, <code>rancher/rancher:vX.X.X</code>, i.e.:</p> <pre><code>docker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  --privileged \\\n  rancher/rancher:latest  or rancher/rancher:v2.7.0\n</code></pre>"},{"location":"kbs/000020217/#high-availability-ha-install","title":"High Availability (HA) Install","text":"<p>To install or upgrade to a specific version in a High Availability install, you can specify the <code>--version X.X.X</code> parameter when running the <code>helm install</code> or <code>helm upgrade</code> command, i.e.:</p> <pre><code>helm install rancher rancher-&lt;CHART_REPO&gt;/rancher \\\n\u00a0 --namespace cattle-system \\\n\u00a0 --set hostname=rancher.my.org \\\n\u00a0 --set bootstrapPassword=admin \\\n\u00a0 --version 2.7.0\n</code></pre>"},{"location":"kbs/000020217/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020218/","title":"Why are namespaces created via the kubectl CLI not assigned to a project?","text":"<p>This document (000020218) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020218/#environment","title":"Environment","text":"<p>SUSE Rancher 2.7.x</p> <p>SUSE Rancher 2.8.x</p>"},{"location":"kbs/000020218/#situation","title":"Situation","text":""},{"location":"kbs/000020218/#when-a-user-creates-a-kubernetes-namespace-via-the-rancher-ui-api-or-cli-the-namespace-is-created-within-a-specified-rancher-project-in-the-cluster-however-when-a-user-creates-a-namespace-via-the-kubectl-cli-kubectl-create-ns-namespace-it-is-created-outside-of-any-project-why-is-this","title":"When a user creates a Kubernetes namespace via the Rancher UI, API or CLI the namespace is created within a specified Rancher project in the cluster; however, when a user creates a namespace via the kubectl CLI ( <code>kubectl create ns &lt;namespace&gt;</code>) it is created outside of any project, why is this?","text":""},{"location":"kbs/000020218/#resolution","title":"Resolution","text":""},{"location":"kbs/000020218/#-if-you-want-to-create-namespaces-within-a-rancher-project-with-a-command-line-tool-then-you-should-use-the-rancher-cli-httpsranchercomdocsrancherv2xencli-or-cluster-admin-can-move-the-namespace-into-a-project-within-the-projectsnamespaces-view-for-the-cluster","title":"- If you want to create namespaces within a Rancher Project with a command-line tool, then you should use the Rancher CLI ( https://rancher.com/docs/rancher/v2.x/en/cli/ ) OR cluster admin can move the namespace into a project, within the 'Projects/Namespaces' view for the cluster.","text":""},{"location":"kbs/000020218/#cause","title":"Cause","text":"<p>-\u00a0It is expected behavior that namespaces created via the kubectl CLI are created outside of a Project.</p> <p>-\u00a0Projects are Rancher abstractions and do not exist natively within Kubernetes, as a result when you create a namespace via the kubectl CLI,\u00a0 it is not associated with any project in Rancher.</p>"},{"location":"kbs/000020218/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020219/","title":"'SIGSEGV: segmentation violation' in prometheus container of the prometheus-project-monitoring-0 Pod when enabling Project monitoring on the System Project","text":"<p>This document (000020219) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020219/#situation","title":"Situation","text":""},{"location":"kbs/000020219/#issue","title":"Issue","text":"<p>Enabling project monitoring in a Rancher v2.2 cluster, in which cluster monitoring is enabled, fails with the Prometheus Pod in a CrashLoopBackOff.</p> <p>The <code>prometheus</code> container in the <code>prometheus-project-monitoring</code> StatefulSet fails with an error of the following format:</p> <pre><code>panic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x669c12]\ngoroutine 437 [running]:\nnet/http.(*Client).deadline(0x0, 0xc005381070, 0x40bb8f, 0xc0055e3600)\n/usr/local/go/src/net/http/client.go:187 +0x22\nnet/http.(*Client).do(0x0, 0xc005cdaa00, 0x0, 0x0, 0x0)\n/usr/local/go/src/net/http/client.go:527 +0xab\nnet/http.(*Client).Do(0x0, 0xc005cdaa00, 0x23, 0xc002802230, 0x9)\n/usr/local/go/src/net/http/client.go:509 +0x35\ngithub.com/prometheus/prometheus/scrape.(*targetScraper).scrape(0xc0060fa960, 0x1fd4a60, 0xc00010ec60, 0x1fb2760, 0xc0002eb110, 0x0, 0x0, 0x0, 0x0)\n/app/scrape/scrape.go:471 +0x111\ngithub.com/prometheus/prometheus/scrape.(*scrapeLoop).run(0xc00616a100, 0xdf8475800, 0x2540be400, 0x0)\n/app/scrape/scrape.go:813 +0x487\ncreated by github.com/prometheus/prometheus/scrape.(*scrapePool).sync\n/app/scrape/scrape.go:336 +0x45d\n</code></pre>"},{"location":"kbs/000020219/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>Cluster monitoring enabled and Project monitoring enabled on the System project</li> </ul>"},{"location":"kbs/000020219/#resolution","title":"Resolution","text":"<p>Project monitoring is not compatible with the Rancher System project and should not be enabled in the System project. Starting with Rancher v2.3.0 monitoring of the System project is performed by cluster monitoring, when this is enabled, and the UI prevents enabling of project monitoring on the System project.</p>"},{"location":"kbs/000020219/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020220/","title":"Is it possible to migrate a Rancher launched Kubernetes cluster between Rancher instances?","text":"<p>This document (000020220) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020220/#situation","title":"Situation","text":""},{"location":"kbs/000020220/#question","title":"Question","text":"<p>Is it possible to migrate a Rancher launched Kubernetes cluster from one Rancher server instance to another, e.g. to launch a custom cluster using one Rancher server, and then at a later time, to migrate this to be managed instead via a different Rancher instance?</p>"},{"location":"kbs/000020220/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched and managed by Rancher v2.x</li> </ul>"},{"location":"kbs/000020220/#answer","title":"Answer","text":"<p>No, it is not possible to migrate a cluster between Rancher server instances. A feature request for this is tracked in GitHub Issue #16471.</p> <p>Currently, if you launch a Kubernetes cluster in one Rancher instance, then later attempt to use the imported cluster feature to import this cluster into another Rancher instance, you will lose any ability to add or remove nodes from the cluster, perform etcd backups or disaster recovery, or to edit any of the cluster configuration. We would therefore strongly recommend against this. Instead, we recommend performing regular Rancher server backups, so that you can recover the Rancher server cluster in a disaster recovery scenario, ensuring successful on-going management of downstream clusters launched by the server.</p>"},{"location":"kbs/000020220/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020221/","title":"Is it possible to update the Cluster (Pod) CIDR or Service CIDR for an RKE, RKE2, or K3s cluster?","text":"<p>This document (000020221) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020221/#environment","title":"Environment","text":"<p>An RKE, RKE2, or K3s cluster</p>"},{"location":"kbs/000020221/#situation","title":"Situation","text":""},{"location":"kbs/000020221/#the-cluster-pod-cidr-default-10420016-and-service-cidr-default-10430016-ranges-for-a-cluster-can-be-specified-in-the-cluster-configuration-when-launching-an-rke-rke2-or-k3s-cluster-whether-provisioned-directly-or-via-rancher-v2x-is-it-possible-to-change-these-values-after-the-cluster-has-been-provisioned","title":"The Cluster (Pod) CIDR (default 10.42.0.0/16) and Service CIDR (default 10.43.0.0/16) ranges for a cluster can be specified in the cluster configuration when launching an RKE, RKE2, or K3s cluster, whether provisioned directly or via Rancher v2.x.\u00a0Is it possible to change these values after the cluster has been provisioned?","text":""},{"location":"kbs/000020221/#resolution","title":"Resolution","text":"<p>Updating either the Cluster (Pod) CIDR or Service CIDR after the cluster has been provisioned is not supported and you should be careful to set these as required when first configuring the cluster.</p>"},{"location":"kbs/000020221/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020222/","title":"\"fatal: could not read Username for 'http://host:port': No such device or address\" error for pipeline Publish Catalog Template step in Rancher v2.2 when Git URL contains a port","text":"<p>This document (000020222) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020222/#situation","title":"Situation","text":""},{"location":"kbs/000020222/#issue","title":"Issue","text":"<p>If a pipeline is configured in Rancher v2.2 with a Publish Catalog Template step, in which the specified Git URL contains a port, the step will fail to execute with an error of the format <code>fatal: could not read Username for 'http://host:port': No such device or address\"</code>.</p>"},{"location":"kbs/000020222/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A cluster managed by Rancher v2.2</li> <li>A pipeline configured with a Publish Catalog Template step, in which the Git URL contains a port</li> </ul>"},{"location":"kbs/000020222/#resolution","title":"Resolution","text":"<p>A patch was developed for this issue, and is available in Rancher v2.3.0 and above.</p>"},{"location":"kbs/000020222/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020225/","title":"How to migrate from CentOS/RHEL-packaged to upstream Docker","text":"<p>This document (000020225) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020225/#environment","title":"Environment","text":"<ul> <li>A Kubernetes cluster launched with the Rancher Kubernetes Engine (RKE) CLI, or a Rancher v2.x launched Kubernetes cluster on custom nodes</li> <li>Nodes running CentOS 7.x or RHEL 7.x, with Docker installed from the CentOS/RHEL extras repository.</li> </ul>"},{"location":"kbs/000020225/#situation","title":"Situation","text":"<p>Deprecation notice</p> <p>Docker is not packaged in the RHEL 8 or 9 repositories, and starting with\u00a0Rancher v2.7 the RHEL-packaged version of Docker (1.13) for RHEL 7 has been removed from the Rancher Support Matrix. Therefore, customers must migrate from RHEL-packaged Docker 1.13 to the upstream Docker version in their RKE clusters.</p> <p>This article will describe the process by which you can migrate a CentOS or RHEL node in an RKE cluster from running the CentOS/RHEL-packaged Docker package to the upstream package from Docker.</p> <p>To perform this migration you will be required to first uninstall the CentOS/RHEL packaged Docker, before installing the upstream version. This process is destructive and will remove all container state from the host. As a result, the process outlined below will guide you through first removing the node from the Rancher cluster, before conducting the package migration, then finally re-adding the node to the cluster.</p>"},{"location":"kbs/000020225/#resolution","title":"Resolution","text":""},{"location":"kbs/000020225/#cluster-launched-by-the-rke-cli","title":"Cluster launched by the RKE CLI","text":""},{"location":"kbs/000020225/#create-a-backup","title":"Create a Backup","text":"<p>As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the RKE documentation here and you should copy the snapshot off an etcd node to a safe location outside the cluster.</p>"},{"location":"kbs/000020225/#perform-migration-on-each-cluster-node-in-turn","title":"Perform migration on each cluster node in turn","text":"<ol> <li>Check if you should first add an additional node to the cluster, to replace the node during its migration:</li> </ol> <p>Controlplane or etcd nodes In the case that the node is a controlplane or etcd node, it is recommend that you first add an additional node to replace this, or add the role(s) to an existing node, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node to the cluster configuration YAML\u00a0and run <code>rke up</code> to provision this.</p> <p>Worker nodes If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node to the cluster configuration YAML\u00a0and run <code>rke up</code> to provision this.</p> <ol> <li> <p>Remove the node which you are migrating from the cluster, to do so remove the node from the cluster configuration YAML and then run <code>rke up</code> to reconcile the cluster.</p> </li> <li> <p>Once the <code>rke up</code> invocation in step 2. completes successfully, run the Extended Rancher 2 cleanup script on the node that you are migrating, to clean up Rancher state.</p> </li> <li>Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS\u00a0or using the Rancher installation script for Docker.</li> <li>Add the node back to the cluster configuration YAML and run <code>rke up</code> to provision it.</li> </ol>"},{"location":"kbs/000020225/#custom-cluster-launched-by-rancher","title":"Custom cluster launched by Rancher","text":""},{"location":"kbs/000020225/#create-a-backup_1","title":"Create a Backup","text":"<p>As with any cluster maintenance, it is recommended that you first take an etcd snapshot of the cluster, to recover from in the event of an issue. A snapshot can be created for the cluster, per the Rancher documentation here\u00a0and you should copy the snapshot off an etcd node to a safe location outside the cluster, if S3 backups are not configured for the cluster.</p>"},{"location":"kbs/000020225/#perform-migration-on-each-cluster-node-in-turn_1","title":"Perform migration on each cluster node in turn","text":"<ol> <li>Check if you should first add an additional node to the cluster, to replace the node during its migration:</li> </ol> <p>Controlplane or etcd nodes In the case that the node is a controlplane or etcd node, it is recommended that you first add an additional node to replace this, to ensure that quorum is maintained in the event of failure of another node during the process. If the node is the single etcd or controlplane node in the cluster, then adding an additional node to replace it is not an optional step. Add the new etcd and/or controlplane role node by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the replacement node.</p> <p>Worker nodes If the worker nodes within the cluster are heavily loaded, or if the node is the sole worker role node, you should provision an additional worker node, to replace the node during the migration. Add the new worker role node by running the Rancher agent command from the 'Edit Cluster' view, with the worker role, on the replacement node.</p> <ol> <li> <p>Remove the node that you are migrating from the cluster, to do so delete it from the node list for the cluster within Rancher.</p> </li> <li> <p>Once the cluster reconciliation triggered by step 2. is complete, and the cluster no longer shows as updating within Rancher, run the Extended Rancher 2 cleanup script on the node that you are migrating to clean up Rancher state.</p> </li> <li>Switch to the upstream Docker package on the node, by following the Docker Engine installation documentation for CentOS\u00a0or using the Rancher installation script for Docker.</li> <li>Add the node back by running the Rancher agent command from the 'Edit Cluster' view, with the appropriate roles, on the node.</li> </ol>"},{"location":"kbs/000020225/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020226/","title":"How to rotate certificates for clusters launched by RKE v0.1.x or Rancher v2.0.x and v2.1.x","text":"<p>This document (000020226) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020226/#situation","title":"Situation","text":""},{"location":"kbs/000020226/#task","title":"Task","text":"<p>Kubernetes clusters use multiple certificates to provide both encryption of traffic to the Kubernetes components as well as authentication of these requests. These certificates are auto-generated for clusters launched by Rancher and also clusters launched by the Rancher Kubernetes Engine (RKE) CLI.</p> <p>In Rancher v2.0.x and v2.1.x, the auto-generated certificates for Rancher-launched Kubernetes clusters have a validity period of one year, meaning these certificates will expire one year after the cluster is provisioned. The same applies to Kubernetes clusters provisioned by v0.1.x of the Rancher Kubernetes Engine (RKE) CLI.</p> <p>If you created a Rancher-launched or RKE-provisioned Kubernetes cluster about 1 year ago, and have not already rotated the certificates, you need to rotate the certificates. If no action is taken, then when the certificates expire, the cluster will go into an error state and the Kubernetes API for the cluster will become unavailable. Rancher recommends that you rotate the certificates before they expire to avoid an unexpected service interruption. The rotation is a one time operation, and the newly-generated certificates will be valid for the next 10 years.</p>"},{"location":"kbs/000020226/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Kubernetes cluster launched by RKE CLI v0.1.x, or Rancher v2.0.x and v2.1.x</li> </ul>"},{"location":"kbs/000020226/#resolution","title":"Resolution","text":"<p>Full details on who to rotate the certificates for the both RKE and Rancher launched clusters can be found in the Rancher blog post \"Manual Rotation of Certificates in Rancher Kubernetes Clusters\".</p>"},{"location":"kbs/000020226/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020231/","title":"Network ingress traffic from 192.168.0.0/16 always SNAT'd in Kubernetes clusters with canal network provider","text":"<p>This document (000020231) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020231/#environment","title":"Environment","text":"<p>An RKE or RKE2 cluster, using the canal network provider</p>"},{"location":"kbs/000020231/#situation","title":"Situation","text":""},{"location":"kbs/000020231/#network-ingress-traffic-to-a-kubernetes-cluster-with-the-canal-network-provider-from-ip-addresses-in-the-range-1921680016-is-always-snatd-even-in-instances-where-this-is-not-desired","title":"Network ingress traffic to a Kubernetes cluster with the canal network provider, from IP addresses in the range <code>192.168.0.0/16</code>, is always SNAT'd, even in instances where this is not desired.","text":"<p>For example on NodePort services configured with <code>externalTrafficPolicy: Local</code> the source IP should be preserved without SNAT, per the Kubernetes documentation. With this issue the source IP is SNAT'd even in instances of NodePort services configured with <code>externalTrafficPolicy: Local</code>.</p>"},{"location":"kbs/000020231/#resolution","title":"Resolution","text":"<p>A permanent solution to prevent programming of the problematic <code>cali-nat-outgoing</code> iptables rules and is tracked in Rancher Issue #20500.</p> <p>In order to workaround the issue in existing clusters, the Calico ippool configuration can be edited to disable outgoing nat, which removes programming of the <code>cali-nat-outgoing</code> iptables rules. To implement this workaround run kubectl against the affected to edit the <code>default-ipv4-ippool</code> object: <code>kubectl edit ippools default-ipv4-ippool</code>. Edit the line <code>natOutgoing: true</code> to set <code>natOutgoing: false</code> and save the change. Calico will detect the configuration update and remove the <code>cali-nat-outgoing</code> iptables rules.</p>"},{"location":"kbs/000020231/#cause","title":"Cause","text":"<p>When a cluster is provisioned with the canal network provider selected, Flannel is used for networking and Calico for network policy enforcement. IP address management is therefore managed by Flannel.</p> <p>The <code>calico-node</code> container in the canal pod is still configured with an (un-used) IP pool, which defaults to <code>192.168.0.0/16</code>. By default Calico programs iptables rules in the <code>cali-nat-outgoing</code> chain of the <code>nat</code> table on cluster nodes to perform SNAT on traffic from this IP pool. The purpose of these rules is to masquerade egress traffic from pods where Calico is used for networking (and not just network policy). As a result in a canal network provider cluster, where the <code>calico-node</code> container is present for network policy enforcement, these rules are programmed and any ingress traffic from the range <code>192.168.0.0/16</code> will match and be SNAT'd.</p>"},{"location":"kbs/000020231/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020232/","title":"Is it possible to perform RKE etcd snapshots to an s3 endpoint with a certificate signed by a custom CA?","text":"<p>This document (000020232) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020232/#environment","title":"Environment","text":"<p>Rancher Kubernetes Engine (RKE) clusters provisioned via the RKE CLI or Rancher v2.x</p>"},{"location":"kbs/000020232/#situation","title":"Situation","text":"<p>Is it possible to perform\u00a0etcd snapshots to an S3 endpoint with a certificate signed by a custom certificate authority (CA)?</p>"},{"location":"kbs/000020232/#resolution","title":"Resolution","text":""},{"location":"kbs/000020232/#rancher-provisioned-clusters","title":"Rancher-provisioned clusters","text":"<p>In Rancher v2.2.5 and above it is possible to specify a custom CA for the S3 endpoint within the S3 backup options. Expanding 'Show advanced options' under the 'Edit Cluster' view, a 'Custom CA Certificate' field is shown when the s3 backup target is selected, enabling you to enter the certificate or upload this from file.</p>"},{"location":"kbs/000020232/#rke-cli-provisioned-clusters","title":"RKE CLI-provisioned clusters","text":"<p>With the RKE CLI v0.2.5 and above it also possible to specify a custom CA for the S3 endpoint within the S3 backup options. To do you specify the certificate via the <code>custom_ca</code> field in the <code>s3backupconfig</code> block of the cluster configuration YAML. The cert should be provided as string, with newlines replaced with \\n, per the example below:</p> <pre><code>services:\n  etcd:\n    backup_config:\n      interval_hours: 12\n      retention: 6\n      s3backupconfig:\n        access_key: S3_ACCESS_KEY\n        secret_key: S3_SECRET_KEY\n        bucket_name: s3-bucket-name\n        region: \"\"\n        endpoint: s3.amazonaws.com\n        custom_ca: \"-----BEGIN CERTIFICATE-----\\nMIIDazCCAlOgAwIBAgIUMoCmUpa4u2UJWqNIkizFbpeJkwowDQYJKoZIhvcNAQEL\\nBQAwRTELMAkGA1UEBhMCQVUxEzARBgNVBAgMClNvbWUtU3RhdGUxITAfBgNVBAoM\\nGEludGVybmV0IFdpZGdpdHMgUHR5IEx0ZDAeFw0xOTA5MTgwOTI4NDBaFw0yMjA3\\nMDgwOTI4NDBaMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEw\\nHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQwggEiMA0GCSqGSIb3DQEB\\nAQUAA4IBDwAwggEKAoIBAQDIW8aN2vszkiNAqykYvqivZgWPRqEukPSAZz39Qtyx\\nkv2wl3B29chBzw5+vjG6veaUnWufOpGeiwglL2PEBOMI0a62zmmm3ttyJDy1lY+A\\ncuxZ1+hveWjWrA2B2bN69/wdkQTQu6ZLoguk+8mRFBZ7ghu6YTZQfczBsHlDxUpA\\n77qQunE4RmcQzOBHoWmMkSSxSGMBsVIj2rRihtVqpgbrMr3/LtCqzqsF+UcroJPC\\nIIBd8bSFlcgkWLnJdqlSa8s1PUodcKD3q6mbMZPDudraszuRgLyC5pIylGQOk+XF\\nMjf2I8zkkAV4QtfSpgBpNXbZEZ3a6CPhveDZqoZN4rxTAgMBAAGjUzBRMB0GA1Ud\\nDgQWBBTD/EagPfxclAlfViV5kKLq0YwBYzAfBgNVHSMEGDAWgBTD/EagPfxclAlf\\nViV5kKLq0YwBYzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQB0\\nyJ6vjtmuvBEKuNgWwIJLh2CqZubUL+lUQGi1NhdFzkXj7+fLeLjqsmbi2Xj/qQ5n\\nooI/p4MeHfYrUqqS7nqTBIsRZQZDZcKUYTZWzDRBdQZtxvEsB1WUq5+nsCQqVuZO\\n+ICsXQFL45xDKaWOoRMH8z9JksYf2CSKeRWViAFElC/IDwf8d5mtufe17h5vlyPR\\nLaIMJ37vyAosN6h8icztVHRzfcIjp1KLqwaGfaOrNSCv8zja9YsD6kbYL64lKND4\\nHiOJy3oSjjjTNdnXjIO44Ngo7L4TWF1CshFlsRF3a5/Jw+NmsEV46Vq41YcuRX9E\\n5JYZWzGRsPDeG4vrzWrV\\n-----END CERTIFICATE-----\"\n</code></pre>"},{"location":"kbs/000020232/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020239/","title":"How to change the log level for Rancher v2.x","text":"<p>This document (000020239) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020239/#environment","title":"Environment","text":"<p>SUSE Rancher v2.x</p>"},{"location":"kbs/000020239/#situation","title":"Situation","text":""},{"location":"kbs/000020239/#task","title":"Task","text":"<p>By default the Rancher v2.x server log level is set to <code>info</code>; however, when investigating an issue it may be helpful to increase the log verbosity to <code>debug</code>. This article details how to control the log verbosity on Rancher v2.x containers.</p>"},{"location":"kbs/000020239/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A running instance of Rancher server v2.x, either a single node instance or High Availability (HA) cluster.</li> </ul>"},{"location":"kbs/000020239/#resolution","title":"Resolution","text":"<p>The log verbosity is set within a running Rancher server container by use of the <code>loglevel</code> command:</p> <pre><code>loglevel --set &lt;verbosity&gt;\n</code></pre> <p>Using <code>kubectl</code> with your cluster's context, you can update the log level of all your Rancher server containers by running the following:</p> <pre><code>kubectl -n cattle-system get pods -l app=rancher --no-headers -o custom-columns=name:.metadata.name | while read rancherpod; do kubectl -n cattle-system exec $rancherpod -c rancher -- loglevel --set debug; done\n</code></pre> <p>where verbosity is one of <code>error</code>, <code>info</code> or <code>debug</code>.</p> <p>Instructions on how to run this command in either a single node or High Availability installation of Rancher can be found within the Rancher documentation under the \"Logging\" troubleshooting guide.</p> <p>If the log level is increased to <code>debug</code> for troubleshooting purposes, you should be sure to reduce to <code>info</code> after the necessary logs have been captured, in order to reduce disk usage and minimise noise when reading the logs.</p>"},{"location":"kbs/000020239/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020241/","title":"Istio-init container fails to start when SElinux is enabled on RHEL 7.x","text":"<p>This document (000020241) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020241/#environment","title":"Environment","text":"<p>OS: RHEL 7.x</p> <p>ISTIO Chart Version: v1.8.300</p> <p>Rancher Version : v2.5.7</p>"},{"location":"kbs/000020241/#situation","title":"Situation","text":"<p>Starting a workload on Istio-enabled namespace fails as the istio-init container failed to start.</p> <p>The istio-init container shows below error;</p> <pre><code>The error is:\nnat\n-N ISTIO_INBOUND\n-N ISTIO_REDIRECT\n-N ISTIO_IN_REDIRECT\n-N ISTIO_OUTPUT\n-A ISTIO_INBOUND -p tcp --dport 15008 -j RETURN\n-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001\n-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006\n-A PREROUTING -p tcp -j ISTIO_INBOUND\n-A ISTIO_INBOUND -p tcp --dport 22 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15090 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15021 -j RETURN\n-A ISTIO_INBOUND -p tcp --dport 15020 -j RETURN\n-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT\n-A OUTPUT -p tcp -j ISTIO_OUTPUT\n-A ISTIO_OUTPUT -o lo -s 127.0.0.6/32 -j RETURN\n-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT\n-A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -o lo ! -d 127.0.0.1/32 -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT\n-A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN\n-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN\n-A ISTIO_OUTPUT -j ISTIO_REDIRECT\nCOMMIT\n\niptables-restore --noflush /tmp/iptables-rules-1618985143596701894.txt019825926\niptables-restore: line 25 failed\niptables-save\n</code></pre>"},{"location":"kbs/000020241/#resolution","title":"Resolution","text":"<p>Execute the modprobe in the below order to fix this without a reboot.</p> <pre><code>modprobe br_netfilter\nmodprobe nf_nat_redirect\nmodprobe xt_REDIRECT\nmodprobe xt_owner\n</code></pre> <p>To load the modules automatically during boot, create a file inside /etc/modules-load.d/ as shown below.</p> <pre><code>cat &gt;/etc/modules-load.d/istio-iptables.conf &lt;&lt;EOF\nbr_netfilter\nnf_nat_redirect\nxt_REDIRECT\nxt_owner\nEOF\n</code></pre>"},{"location":"kbs/000020241/#cause","title":"Cause","text":"<p>The issue is caused by SELinux which prevents the istio-init to load kernel modules that are needed for the iptables rules.</p>"},{"location":"kbs/000020241/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020243/","title":"Can I receive an application backup of my SUSE Rancher Hosted environment if I decide not to renew at the end of my term?","text":"<p>This document (000020243) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020243/#situation","title":"Situation","text":""},{"location":"kbs/000020243/#resolution","title":"Resolution","text":"<p>Yes, the SUSE can provide an application backup\u00a0of your SUSE Rancher Hosted environment. You can follow the Rancher documentation\u00a0to restore your environment into a Rancher server running in your datacenter or cloud account.</p> <p>To request a backup of your SUSE Rancher Hosted environment, open a support ticket on our support portal.</p>"},{"location":"kbs/000020243/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020296/","title":"[Rancher] How do I change severity level on an open case?","text":"<p>This document (000020296) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020296/#situation","title":"Situation","text":"<p>I have reviewed the severity levels listed below and I would now like to know\u00a0how I can change the severity level of an open case myself.</p> Severity Description Sev 1 (Critical) The solution is in production or is mission-critical to your business. It is inoperable, and the situation is resulting in a total disruption of work. There is no workaround available. Sev 2 (High) Operations are severely restricted, but work can continue. Core functionalities can operate in a restricted fashion despite important features being unavailable. A workaround is available that ensures no immediate business impact. Sev3 (Medium) The solution does not work as designed, resulting in a minor loss of usage. It may also be a significant software defect that impacts the Customer when performing some actions and has no workaround. Sev 4 (Low) There is no loss of service, and this may be a request for documentation, general information, product enhancement request, Software defects with workarounds or medium or low functionality impact, etc."},{"location":"kbs/000020296/#resolution","title":"Resolution","text":"<p>During the creation of a support case, you may select the severity of the support case via the drop-down menu. The creation of a Sev1 or a Sev2 support case will trigger our escalation workflows. If during the life of your support case the initial severity is no longer accurate, you may post a new comment with the string\u00a0bump_to_sev1\u00a0or\u00a0bump_to_sev2\u00a0to escalate a ticket to higher severity. This string can be stand-alone as a comment or can be a part of a sentence. \u00a0There is no need for any special characters (such as \" \" or * *).</p> <p>During the life of the case, you can also recategorize it to sev3 or sev4 by using\u00a0bump_to_sev3\u00a0or\u00a0bump_to_sev4. Using these\u00a0bump_to_\u00a0strings for sev1 and sev2 will trigger our escalation workflows, where sev3 and sev4 will update the severity with no escalation workflows.</p>"},{"location":"kbs/000020296/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020302/","title":"How to add custom labels to Alerts in Monitoring v2","text":"<p>This document (000020302) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020302/#environment","title":"Environment","text":"<p>A Rancher v2.5+ managed cluster, with the rancher-monitoring chart installed</p>"},{"location":"kbs/000020302/#situation","title":"Situation","text":"<p>Alerts in Monitoring v2 contain standard labels. In some cases, users may want to inject custom labels, such as Kubernetes cluster names to easily identify the environment or other custom labels for Alert routing.</p>"},{"location":"kbs/000020302/#resolution","title":"Resolution","text":"<p>Use the value defaultRules.additionalRuleLabels\u00a0 in the rancher-monitoring chart to inject custom labels.</p> <p>For example, to inject cluster name, open the Monitoring App in Apps from Cluster explorer.\u00a0Under \"Edit as YAML\", add the custom label as below:</p> <pre><code>defaultRules:\n\u00a0 additionalRuleLabels:\n\u00a0 \u00a0 cluster: \"My_Test_cluster\"\n</code></pre> <p>Then click on \"Deploy\" or \"Upgrade\" if App is already installed.</p> <p>If your receiver is webhook, then the alerts will have the custom labels as shown in the below example alert.</p> <pre><code>...\n...\nstatus\":\"firing\",\n\"labels\":{\n\u00a0 \"alertname\":\"NodeClockNotSynchronising\",\n\u00a0 \"cluster\":\"My_Test_cluster\", &lt;&lt;&lt;------\n\u00a0 \"container\":\"node-exporter\",\n\u00a0 \"endpoint\":\"metrics\",\n\u00a0 \"instance\":\"192.168.110.157:9796\",\n\u00a0 \"job\":\"node-exporter\",\n\u00a0 \"namespace\":\"cattle-monitoring-system\",\n\u00a0 \"pod\":\"rancher-monitoring-prometheus-node-exporter-lg2g6\",\n\u00a0 \"prometheus\":\"cattle-monitoring-system/rancher-monitoring-prometheus\",\n\u00a0 \"service\":\"rancher-monitoring-prometheus-node-exporter\",\n...\n...\n</code></pre>"},{"location":"kbs/000020302/#additional-information","title":"Additional Information","text":"<p>GitHub issue #3325 is opened to add additional labels via UI.</p>"},{"location":"kbs/000020302/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020350/","title":"Streaming server stopped unexpectedly: listen tcp x.x.x.x:0: bind: cannot assign requested address","text":"<p>This document (000020350) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020350/#environment","title":"Environment","text":"<p>Rancher v2.6+</p> <p>Kubernetes v1.20+</p>"},{"location":"kbs/000020350/#situation","title":"Situation","text":"<p>Adding a new node to an existing cluster stuck in \u201cregistering\u201d.</p> <p>Kubelet pod is in\u00a0<code>Restarting</code>\u00a0state on the new node.</p> <pre><code>$ docker ps -a |grep kubelet\n</code></pre> <pre><code>66bd40b36e76 rancher/hyperkube:v1.20.4-rancher1 \"/opt/rke-tools/entr\u2026\" 7 minutes ago Restarting (255) 32 seconds ago kubelet\n</code></pre> <p>The kubelet is failing with below error.</p> <pre><code>$ docker logs kubelet\n</code></pre> <pre><code>\"2021-07-26T11:13:48.270162766Z F0726 11:13:48.270086   40730 docker_service.go:415] Streaming server stopped unexpectedly: listen tcp 27.0.0.1:0: bind: cannot assign requested address\"\n</code></pre>"},{"location":"kbs/000020350/#resolution","title":"Resolution","text":"<p>Update\u00a0<code>/etc/hosts</code>\u00a0file with the correct entry.</p> <pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n</code></pre> <p>After updating the file, the kubelet will restart itself and pick up the modified /etc/host file to bind to the loopback IP.</p>"},{"location":"kbs/000020350/#cause","title":"Cause","text":"<p>File /etc/hosts in the new node had below incorrect entry.</p> <pre><code>27.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n</code></pre>"},{"location":"kbs/000020350/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020373/","title":"How to drain a node in an RKE1 cluster from the local node using a docker command","text":"<p>This document (000020373) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020373/#environment","title":"Environment","text":"<p>An RKE CLI or Rancher-provisioned RKE1 cluster</p>"},{"location":"kbs/000020373/#situation","title":"Situation","text":"<p>Need a way to cordon or drain a node when OS patching is automated, and the automation doesn't have access to the Rancher API.</p>"},{"location":"kbs/000020373/#resolution","title":"Resolution","text":"<p>Integrate the below command in the automation to drain the node in the OS patching workflow.</p> <p>Rancher v2.7.14+/Rancher v2.8.5+, RKE v1.4.19+/RKE v1.5.10+ :</p> <pre><code>docker exec kubelet bash -c 'kubectl --kubeconfig &lt;(kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get secret -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\") drain $(hostname -s) --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s'\n</code></pre> <p>Earlier versions of Rancher and RKE:</p> <pre><code>docker exec kubelet bash -c 'kubectl --kubeconfig &lt;(kubectl --kubeconfig /etc/kubernetes/ssl/kubecfg-kube-node.yaml get configmap -n kube-system full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .currentState.certificatesBundle.\\\"kube-admin\\\".config | sed -e \"/^[[:space:]]*server:/ s_:.*_: \\\"https://127.0.0.1:6443\\\"_\") drain $(hostname -s) --delete-local-data=true --force=true --grace-period=60 --ignore-daemonsets=true --timeout=120s'\n</code></pre> <p>Please note that the below flags need to be changed according to your requirements.</p> <pre><code>--delete-local-data=true\n--force=true\n--grace-period=60\n--ignore-daemonsets=true\n--timeout=120s\n</code></pre>"},{"location":"kbs/000020373/#cause","title":"Cause","text":"<p>Attempting to drain nodes using the kubeconfig file \" /etc/kubernetes/ssl/kubecfg-kube-node.yaml\" will result in errors like below, since the \" system:node\" role is not authorized to access the needed API groups.</p> <pre><code>cannot delete daemonsets.apps \"nginx-ingress-controller\" is forbidden: User \"system:node\" cannot get resource \"daemonsets\" in API group \"apps\" in the namespace \"ingress-nginx\"\n</code></pre> <p>But this kubeconfig file can access the secret/config-map \"full-cluster-state\" in the \" kube-system\" namespace, containing the kubeconfig file that has the privilege to do the drain operation.</p>"},{"location":"kbs/000020373/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020415/","title":"How to rotate rancher-webhook (cattle-webhook-tls) certificates?","text":"<p>This document (000020415) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020415/#environment","title":"Environment","text":"<p>Rancher versions \u2264 2.6.2</p>"},{"location":"kbs/000020415/#situation","title":"Situation","text":"<p>An upgrade of Rancher or editing roles in Rancher UI fails with the below error.</p> <pre><code>Internal error occurred: failed calling webhook \"rancherauth.cattle.io\": Post \"https://rancher-webhook.cattle-system.svc:443/v1/webhook/validation?timeout=10s\": x509: certificate has expired or is not yet valid: current time 2021-10-25T07:43:50Z is after 2021-10-06T20:20:47Z\n</code></pre>"},{"location":"kbs/000020415/#resolution","title":"Resolution","text":"<ul> <li>Set the kubectl context for the\u00a0Rancher management cluster (local cluster).</li> <li>Take the backup of existing secret</li> </ul> <pre><code>kubectl get secret -n cattle-system cattle-webhook-tls -o yaml &gt; cattle-webhook-tls.yaml\n</code></pre> <ul> <li><code>Delete the secret that contains expired certificate <pre><code>\n</code></pre> kubectl delete secret -n cattle-system cattle-webhook-tls <pre><code>- Delete the\u00a0`rancher.cattle.io`\u00a0mutating webhook\n</code></pre> kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io --ignore-not-found=true rancher.cattle.io <pre><code>- Delete the rancher webhook Pod to regenerate the expired certificate.\n</code></pre> kubectl delete pod -n cattle-system -l app=rancher-webhook</code></li> </ul>"},{"location":"kbs/000020415/#cause","title":"Cause","text":"<p>This issue is caused by the expired certificate of the rancher webhook.</p>"},{"location":"kbs/000020415/#additional-information","title":"Additional Information","text":"<p>In Rancher v2.6.3 and up, rancher-webhook deployments will automatically renew their TLS certificate when it is within 30 or fewer days of its expiration date.</p>"},{"location":"kbs/000020415/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020416/","title":"Downstream clusters flapping between available and unavailable state","text":"<p>This document (000020416) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020416/#environment","title":"Environment","text":"<p>Rancher version: v2.5.6</p> <p>Management cluster K8S version: 1.21.5</p>"},{"location":"kbs/000020416/#situation","title":"Situation","text":"<p>After upgrading the Kubernetes version of the Rancher management cluster, the downstream cluster status in the WebUI flaps between the available and unavailable states.</p> <p>Rancher Pod logs show errors like the below;</p> <pre><code>Failed to connect to peer wss://x.x.x.x/v3/connect [local ID=y.y.y.y]: websocket: bad handshake\n</code></pre>"},{"location":"kbs/000020416/#resolution","title":"Resolution","text":"<p>Upgrade Rancher to v2.6.x</p> <p>A workaround until Rancher upgarde is to reduce the Rancher deployment replicas to one.</p>"},{"location":"kbs/000020416/#cause","title":"Cause","text":"<p>Rancher is storing the service account token from the initial Pod, and then trying to reuse that on subsequent requests even though that pod has been deleted.</p> <p>As of Kubernetes version v1.21, service account tokens are pod-specific, and are invalidated when the pod is deleted, which is why Rancher is unable to use it and thus unable to reach other Rancher replica instances via web-socket.</p>"},{"location":"kbs/000020416/#additional-information","title":"Additional Information","text":"<p>The issue is tracked in the GitHub issue\u00a026082</p>"},{"location":"kbs/000020416/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020432/","title":"Rancher Support FAQ","text":"<p>This document (000020432) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020432/#resolution","title":"Resolution","text":"## Quick Links- SUSE Rancher Product Lifecycle Information- SUSE Rancher Product Support Matrices- SUSE Rancher Support Advisories- SUSE Rancher Hosted FAQ- SUSE Customer Center\u00a0(SCC) (Support Portal)\u00a0- SCC sign-in help- Rancher 2.x Linux log collector script- Rancher 2.x Windows log collector script- Rancher 2.x Systems summary script"},{"location":"kbs/000020432/#severity-levels-designations-and-escalations","title":"Severity Levels, Designations, and Escalations","text":"<ul> <li>How do I change the severity level on an open case?</li> <li>Could you illustrate the severity levels with subject lines of sample support cases?</li> </ul>"},{"location":"kbs/000020432/#product-releases-and-support-phases","title":"Product Releases and Support Phases","text":"<ul> <li>What is the difference between \"stable\" and \"latest\" release tags?</li> <li>What does 'Developer Support\" mean?</li> <li>What are the definitions in the new Rancher Prime Release Cycle?</li> </ul>"},{"location":"kbs/000020432/#request-for-assistance","title":"Request for Assistance","text":"<ul> <li>Can Rancher Support validate our planned upgrade?</li> <li>We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?</li> <li>Can Rancher Support join me as I do my upgrade?</li> <li>We need help validating our deployment design and operational readiness, can Rancher Support help?</li> </ul>"},{"location":"kbs/000020432/#rancher-and-rancheros","title":"Rancher and RancherOS","text":"<ul> <li>Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?</li> <li>Could you help us understand the development and support status of RancherOS for 2020 and beyond?</li> <li>Does Rancher support migration from single node installation to high availability installation?</li> <li>Does Rancher Support cover single node installations?</li> <li>Is Rancher software itself completely free?</li> <li>Does Rancher include any 3rd party, commercial software components?</li> </ul>"},{"location":"kbs/000020432/#kubernetes","title":"Kubernetes","text":"<ul> <li>I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?</li> <li>As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?</li> <li>Is Rancher Support only for RKE-provisioned clusters?</li> <li>Could you help us understand the Rancher approach to supporting specific k8s versions and how Rancher keeps up with new k8s releases?</li> </ul>"},{"location":"kbs/000020432/#security","title":"Security","text":"<ul> <li>How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?</li> <li>How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?</li> <li>Can we run Antivirus on our cluster nodes?</li> </ul>"},{"location":"kbs/000020432/#included-open-source-software-components","title":"Included Open Source Software Components","text":"<ul> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?</li> <li>Is support for Prometheus/Grafana available only with a valid support subscription for Rancher 2.x?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?</li> <li>What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?</li> <li>Will Rancher fix problems root-caused to be in nginx?</li> <li>Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?</li> <li>Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?</li> <li>We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?</li> </ul>"},{"location":"kbs/000020432/#support-matrix","title":"Support Matrix","text":"<ul> <li>Would\u00a0 Rancher Support provide assistance if we use components that are not listed in the Rancher support matrix?</li> <li>Would Rancher Support provide assistance if we change the default configurations of one or more of these components listed in Rancher support matrix?</li> </ul>"},{"location":"kbs/000020432/#certified-integrations","title":"Certified Integrations","text":"<ul> <li>What are the certified integrations with persistent volume plugins covered by Rancher Support?</li> <li>What are the certified integrations with storage class provisioners covered by Rancher Support?</li> <li>What are the certified integrations with authentication providers covered by Rancher Support?</li> <li>Could you clarify what you generally mean by a \"certified integration\" to another software system or service?</li> </ul>"},{"location":"kbs/000020432/#node-drivers-and-infrastructure","title":"Node Drivers and Infrastructure","text":"<ul> <li>Does Rancher Support covers OpenStack clusters?</li> <li>I see node drivers tagged as \"Built-in\". What does that mean?</li> <li>I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?</li> </ul>"},{"location":"kbs/000020432/#kubernetes-cloud-providers","title":"Kubernetes Cloud Providers","text":"<ul> <li>What are the Kubernetes cloud providers supported by Rancher?</li> </ul>"},{"location":"kbs/000020432/#requests-for-enhancements-rfes","title":"Requests for Enhancements (RFEs)","text":"<ul> <li>I filed an RFE as a support case. What should I expect on how it will be followed up on?</li> </ul>"},{"location":"kbs/000020432/#localization","title":"Localization","text":"<ul> <li>What language(s) is Rancher Support service offered in?</li> </ul>"},{"location":"kbs/000020432/#licensing-and-usage","title":"Licensing and Usage","text":"<ul> <li>Does my support subscription to Rancher include support for Longhorn?</li> <li>Can we have a mix of unsupported and supported nodes at our choice/discretion?</li> <li>Can we have a mix of unsupported and supported clusters at our choice/discretion?</li> <li>How does Rancher track our license usage?</li> <li>Is Rancher Support only for production environments?</li> <li>Does my Rancher Prime Support subscription for production environment also covers testing and staging environments?</li> </ul>"},{"location":"kbs/000020432/#harverster-longhorn-rancher-desktop-opni-support","title":"Harverster, Longhorn, Rancher Desktop, Opni Support","text":"<ul> <li>What about support for Harvester, Longhorn, Rancher Desktop, Opni?</li> </ul>"},{"location":"kbs/000020432/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020434/","title":"[Rancher] I filed an RFE as a support case. What should I expect on how it will be followed up on?","text":"<p>This document (000020434) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020434/#resolution","title":"Resolution","text":"<p>Rancher Support will try to understand your request first and then qualify the request with information on business impact, time sensitivity, and criticality. It is not uncommon at all for there to be follow-up questions from Rancher Support on your request. This information will be most helpful to advocate for the request with Rancher Product Management.</p> <p>Unless there is a pressing urgency that has been understood and acknowledged, it could take up to a few weeks to triage the RFE (from the product backlog) through our release planning and identify the earliest release vehicle the item could be considered for (or committed to). Once the issue is triaged, it's at the Product Management's discretion to create a public Github for the tracking which can be shared with the customers.</p> <p>As there is progress and/or should there be specific questions from Engineering, Rancher Support will keep you updated via this case. As necessary, Rancher Support will also recommend and schedule/facilitate a direct web conference session for you with our product management to go over the RFE.</p> <p>For committed RFEs, Rancher Support will update you as the item gets closer to its general availability via a new Rancher release or keep you informed should there be any delays or proposed changes in the previously understood scope.</p>"},{"location":"kbs/000020434/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020437/","title":"[Rancher] Can Rancher Support validate our planned upgrade?","text":"<p>This document (000020437) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020437/#resolution","title":"Resolution","text":"<p>Rancher Support recommends Customers to notify them ahead of planned maintenance events for upgrade and migration. The notification can be made as a support case. At a minimum, this case should have information on:</p> <ul> <li>Current Rancher version</li> <li>Target Rancher version</li> <li>Reason for upgrade</li> <li>Upgrade date and maintenance time window</li> </ul> <p>But it would be best if Customer could provide the requested information in a document (please ask for the \" Upgrade Notification RFI Template\" in the support case). Based on the information provided, Rancher Support can provide any applicable advisories to the Customer for the planned event. Rancher Support may request Customer to run specific information gathering scripts. Data collected from these scripts will be used by Rancher Support to understand the Customer deployment and validate the upgrade path that is being considered.</p> <p>Note 1:\u00a0On all regular, planned upgrades, Rancher Support requests Customer to notify as early as possible. Notifications that are one (1) calendar week in advance provide reasonable time for Rancher Support to follow up with any applicable advisories.</p> <p>Note 2: A Supportability Review is suitable to be run in advance of such a planned activity if the customer can afford the time to run it. Please, consult the Rancher Supportability Review Guide for more information.</p>"},{"location":"kbs/000020437/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020437/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020438/","title":"[Rancher] We run a burst of high-profile events (e.g., big sale) that run as k8s workloads managed by Rancher. Can Rancher Support join our team remotely for the duration of these events?","text":"<p>This document (000020438) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020438/#environment","title":"Environment","text":"<p>Rancher - any version</p> <p>Longhorn - any version</p>"},{"location":"kbs/000020438/#resolution","title":"Resolution","text":"<p>Rancher Support will not join the Customer team remotely for the duration of the event. However, they will remain on standby, on high alert, to respond per SLA should there be an issue. Customer should open a new case if support is required or bump the severity of an existing one, following this procedure.</p> <p>Rancher Support recommends Customers to notify in advance, via a support ticket, all necessary information about such events. We also advise that you gather a full log collector output prior to the planned operations in case later troubleshooting or root cause analysis is required. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the event.</p>"},{"location":"kbs/000020438/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020440/","title":"[Rancher] Can Rancher Support join me as I do my upgrade?","text":"<p>This document (000020440) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020440/#resolution","title":"Resolution","text":"<p>As a general rule, Rancher Support will not join a Customer proactively for the purpose of performing upgrades of their environment. However, Customers are encouraged to notify Rancher Support of planned maintenance events for upgrade and migration. This will help increase the awareness of the Customer event with Rancher Support and help us ensure fastest response time should some assistance be needed during the maintenance window.</p> <p>Additionally, such notifications provide for an opportunity to validate the planned upgrade proactively rather than react to issues that could have been avoided.</p> <p>For more information, please, check the following article:\u00a0Can Rancher Support validate our planned upgrade?</p> <p>If customers experiences issues during an upgrade window, Rancher Support encourages them to open a case or bump the severity of an existing one to request immediate assistance.</p>"},{"location":"kbs/000020440/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020441/","title":"[Rancher] We need help validating our deployment design and operational readiness, can Rancher Support help?","text":"<p>This document (000020441) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020441/#resolution","title":"Resolution","text":"<p>Validating deployment design and operational readiness is an activity for SUSE Rancher Premium Support and/or Consulting teams. Requests coming in as a support case for this activity shall be routed to the leadership team of Rancher Premium Support and Consulting, for follow-up with the Customer.</p> <p>Topics such as the following, but not limited to, will be handled in a similar manner:</p> <ul> <li>Best Practice Guidance</li> <li>Deployment Review</li> <li>Design Assistance</li> <li>Co-development</li> <li>Migration Assistance</li> <li>Performance Tuning</li> <li>Security Assessment</li> <li>Solution Consulting</li> <li>Topical Training</li> <li>Technology Recommendation</li> </ul> <p>Typically, these are topics that are not specific to incidents and issue troubleshooting. They are more requests with strategic drivers that usually require the help and guidance of SUSE Rancher Premium Support and/or Consulting teams.</p> <p>Customers are also encouraged to get familiar with the Rancher Supportability Review Guide and request it in case it fits the needs of their business. Supportability Review is NOT a replacement of the activities outlined above.</p>"},{"location":"kbs/000020441/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020441/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020442/","title":"[Rancher] Can I run other, non-Rancher workloads on the same Kubernetes cluster that Rancher is installed on?","text":"<p>This document (000020442) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020442/#resolution","title":"Resolution","text":"<p>While technically possible, running other workloads or microservices in the same Kubernetes cluster that Rancher is installed on invalidates the Rancher Support SLA. So it is something that is recommended against.</p> <p>Any technical support that is offered for tickets stemming from this scenario shall only be on a best-effort basis.</p> <p>The customers should be aware that Rancher Support Engineers might ask them to separate the workloads as part of the troubleshooting.</p> <p>Note:</p> <p>Run Rancher on a Separate Cluster is also one of the top items called out in our docs page on Tips for Running Rancher.</p>"},{"location":"kbs/000020442/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020443/","title":"[Rancher] Could you help us understand the development and support status of RancherOS for 2020 and beyond?","text":"<p>This document (000020443) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020443/#resolution","title":"Resolution","text":""},{"location":"kbs/000020443/#development-status","title":"Development Status","text":""},{"location":"kbs/000020443/#edit-feb-2024-rancheros-1x-has-a-new-name-and-new-identity","title":"EDIT (FEB 2024):\u00a0RancherOS 1.x has a new name and new identity.","text":"<p>RancherOS was re-named in July 2022&lt; https://github.com/rancher/elemental/pull/161 &gt; and is now known as Elemental&lt; https://github.com/rancher/elemental &gt;.</p> <p>It's evolved, and is now based on SLE Micro&lt; https://www.suse.com/products/micro/ &gt; -&gt; into SLEMicro-Rancher&lt; https://github.com/rancher/elemental/issues/1008 &gt;.</p> <p>Because it\u2019s based on SLE Micro, that is derived from SLES, it benefits from all the Security Certifications &lt; https://www.suse.com/support/security/certifications/ &gt;, hardening&lt; https://documentation.suse.com/compliance/all/html/SLES-openscap/index.html#openscap-ssg-profiles-sle15 &gt; and hardware certifications&lt; https://www.suse.com/product-certification/yes-certified/faq/ &gt; of SLES.</p>"},{"location":"kbs/000020443/#edit-nov-2021-rancheros-1x-is-past-its-eol-milestone-and-no-longer-maintained","title":"EDIT (NOV 2021): RancherOS 1.x is past its EOL milestone and no longer maintained.","text":"<p>RancherOS 1.x is currently in a maintain-only-as-essential mode. That is to say, it is no longer being actively maintained at a code level other than addressing critical or security fixes. There are two significant reasons behind this product decision:</p> <p>1. Docker. The current industry requirements for a container runtime is very much evolving. Container runtimes like containerd and CRIO are now being actively considered as the default choice. RancherOS 1.x, which was specifically designed around using Docker engine only, unfortunately does not lend itself, in its current design, to this new evolving requirement.</p> <p>2. ISV Support. RancherOS was specifically designed as a minimalistic OS to support purpose-built containerized applications. It was not designed to be used as a general-purpose OS (such as CentOS or Ubuntu). As such, most ISVs have not certified their software to run on RancherOS, nor does RancherOS even contain the necessary components for many of these applications to run.</p>"},{"location":"kbs/000020443/#support-status","title":"Support Status","text":""},{"location":"kbs/000020443/#rancheros-1x-is-no-longer-commercially-supported","title":"RancherOS 1.x is no longer commercially supported.","text":"<p>Any assistance from Rancher Support on RancherOS topics, filed as support cases, is not SLA- bound.</p>"},{"location":"kbs/000020443/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020445/","title":"[Rancher] Does Rancher support migration from single node installation to high availability installation?","text":"<p>This document (000020445) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020445/#resolution","title":"Resolution","text":"<p>There is currently no validated path that is officially covered by Rancher Support for any Customer starting with a single node installation and wishing to migrate to a high availability installation at a later time. Whilst there is a Rancher blog post\u00a0that talks about one possible migration path, it is not covered by Rancher Support.</p> <p>Where scale and performance criteria are well understood to be critical, Customers are recommended to set up Rancher in a high availability configuration, right from the outset.</p> <p>Running a single node installation might lead to unrecoverable circumstances, and it is highly recommended that production clusters run in a high availability configuration.</p>"},{"location":"kbs/000020445/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020446/","title":"[Rancher] Does Rancher Support cover single node installations?","text":"<p>This document (000020446) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020446/#resolution","title":"Resolution","text":"<p>Rancher Support can cover single node installations only for support tickets that are NOT related to (a) scale and performance and (b) recovery of data and software (embedded etc.).</p> <p>Customer environments in need of scale and performance and meeting recovery criteria should be set up as high-availability installations.</p> <p>Refer to this Rancher docs page for single node installation versus high availability installation. Rancher recommends high-availability installations in production environments, where the customer's user base requires 24\u20447 access to running applications.</p> <p>Single-node installations will be supported on a best-effort basis.</p>"},{"location":"kbs/000020446/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020449/","title":"[Rancher] I plan to import my legacy k8s cluster into Rancher and having issues with my legacy cluster. Can Rancher Support help troubleshoot the issue?","text":"<p>This document (000020449) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020449/#resolution","title":"Resolution","text":"<p>Rancher Support is comprehensive for RKE, RKE2, and K3S clusters, provisioned through Rancher. In imported clusters, Rancher Support applies only to the extent of troubleshooting and root-causing. For issues in (legacy) clusters under consideration for import into Rancher, the Customer is advised to take it up with the party that is the provider of support for such clusters. Post-import, Rancher Support for such clusters is per response to this question, Is Rancher Support only for RKE-provisioned clusters?</p>"},{"location":"kbs/000020449/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020450/","title":"[Rancher] Is Rancher Support only for RKE-provisioned clusters?","text":"<p>This document (000020450) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020450/#resolution","title":"Resolution","text":"Update as of March 2024: Rancher Support now fully covers Rancher-provisioned or imported RKE2 and k3s clusters, too. Update:In November 2019, k3s graduated from being just a community project led by Rancher Labs to a fully conformant Kubernetes distribution that is supported commercially by Rancher Labs. View release notes for k3s v1.0.0 here . With the general availability of k3s, Rancher Support extends to k3s clusters. <p>Comprehensive Rancher Support, inclusive of Kubernetes and Docker, applies only to RKE-provisioned clusters for Rancher releases before 2.6.5. \u00a0As of the 2.6.6 release of Rancher, k3s, RKE2 are fully supported as well What this means is the following:</p> <p>In the RKE-provisioned clusters, Rancher can provide patch fixes as needed at the levels of Kubernetes and Docker.</p> <p>For clusters that are brought under the management of the Rancher control plane, as imported clusters, Rancher Support applies only to those Kubernetes versions published in the support matrix and to the extent of making sure Rancher control plane functionality works as published in Rancher docs. Issues root- caused to be inside these clusters will need to be taken up by Customer with the provider of support for these clusters.</p>"},{"location":"kbs/000020450/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020451/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Kubernetes?","text":"<p>This document (000020451) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020451/#resolution","title":"Resolution","text":"<p>Yes, should there be a critical need, Rancher can provide patch fixes to address issues root-caused in RKE/RKE2/K3S provisioned Kubernetes clusters. As a first option, Rancher will investigate if the fix is already available in a later version of Kubernetes. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later Kubernetes version that has the fix. Or validate and certify one of its existing versions to work with the later Kubernetes version that has the fix.</p> <p>Further, Rancher can submit a PR for the fix to Kubernetes for consideration to be accepted upstream. However, there is no guarantee that the PR will be accepted and merged into upstream Kubernetes.</p> <p>Rancher provides regular patch releases of RKE, RKE2 and K3S which aim to package all the upstream critical fixes.</p>"},{"location":"kbs/000020451/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020452/","title":"[Rancher] As part of a Rancher upgrade, do I need to upgrade my k8s cluster to a version listed in the support matrix for that Rancher version? If I do not, will the cluster continue to work fine as is? Will Rancher support this cluster?","text":"<p>This document (000020452) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020452/#resolution","title":"Resolution","text":"<p>It is recommended to move to the k8s versions listed in the matrix for a Rancher version, but not doing that should not break things. The most important is to stay within the Support Matrix.</p> <p>If you have a cluster running Kubernetes version 1.25, for example with Rancher version 2.7.x, and you'd like to go to, for instance, Rancher 2.8.2, you can stay at 1.25 until the Rancher version upgrade is completed. Rancher Support will continue to help should there be a need.</p> <p>That said, should there be an issue on that cluster that requires a fix in k8s v1.25, we may not be able to do that and would, at that point, require an upgrade to a higher-supported k8s version (for example, if a fix is available in 1.26).</p> <p>We always suggest to our customers to stick to the support matrix and run higher versions where possible - especially because of security fixes and other crucial features.</p>"},{"location":"kbs/000020452/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020453/","title":"What are the Kubernetes cloud providers supported by Rancher?","text":"<p>This document (000020453) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020453/#resolution","title":"Resolution","text":"<p>Rancher Support is currently limited to Kubernetes cloud providers for Amazon (EKS), Azure (AKS) and Google (GKE).</p> <p>For all other cloud providers which are not visible in Rancher UI, directly editing the YAML file via the custom option is the only way to pass configuration information. In this scenario, Rancher Support expects Customer to manage and troubleshoot the syntactic correctness of the YAML file, per provided by Kubernetes.</p>"},{"location":"kbs/000020453/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020454/","title":"What about support for Harvester, Longhorn, Rancher Desktop, Opni?","text":"<p>This document (000020454) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020454/#resolution","title":"Resolution","text":"<p>As of April 2024, Harvester and Longhorn are already\u00a0commercially supported products\u00a0by SUSE. They both require a separate subscription from the Rancher Prime subscription itself (an add-on). Customers can contact their account executives to learn about Longhorn and/or Harvester Support opportunities.</p> <p>As of April 2024, Rancher Desktop is still a community project, and SUSE does not provide official support subscription plans for it. Users are invited to work with the community, ask questions, and address issues without SLA commitment.</p> <p>As of April 2024, Kubewarden is an official Rancher UI extension that falls under the Rancher Prime Support subscription.</p> <p>As of April 2024, Opni project has been decommissioned.</p> Project GitHub Location Rancher Desktop https://github.com/rancher-sandbox/rancher-desktop/issues Kubewarden https://github.com/kubewarden"},{"location":"kbs/000020454/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020460/","title":"[Rancher] What language(s) is Rancher Support service offered in?","text":"<p>This document (000020460) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020460/#resolution","title":"Resolution","text":"<p>English is the official language in which support is delivered to global customers, with the exception of China, where it is in Chinese.</p> <p>If it is deemed helpful and necessary, SUSE Rancher Support will engage colleagues, from our\u00a0Premium Support, Consulting, and Customer Success teams, who are fluent in specific local languages for assistance on a case.</p> <p>Customers are invited to work with their account teams for engaging specific services on their behalf too.</p>"},{"location":"kbs/000020460/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020462/","title":"Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?","text":"<p>This document (000020462) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020462/#resolution","title":"Resolution","text":"<p>Yes. Prometheus and Grafana are the components natively used and supported by Rancher v2.2+ for monitoring and dashboards. Any issues root-caused in one of these two projects, as an included component of Rancher v2.2+, will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"kbs/000020462/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020463/","title":"Is support for Prometheus/Grafana available only with a valid Rancher support subscription?","text":"<p>This document (000020463) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020463/#resolution","title":"Resolution","text":"<p>Correct. To be more accurate, Rancher Support is only for the Prometheus/Grafana components that are natively embedded in Rancher 2.x for the functionality of monitoring and dashboards.</p> <p>In the exceptional scenario of any legacy support subscriptions that are limited to RKE-only (without Rancher 2.x), it does not include support of Prometheus/Grafana that are set up externally to monitor the RKE clusters.</p> <p>Refer this article for more details: Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?</p>"},{"location":"kbs/000020463/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020464/","title":"[Rancher] Does my Rancher Support cover Prometheus/Grafana deployments that did not originate from Rancher?","text":"<p>This document (000020464) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020464/#resolution","title":"Resolution","text":"<p>Support for Prometheus/Grafana is only for what is embedded in Rancher 2.x and natively used by it for the functionalities of monitoring and dashboards.</p> <p>Refer Will Rancher fix the issue and release a patch, if the problem is root-caused in Prometheus and Grafana?</p> <p>Where Prometheus/Grafana has been enabled from the chart in the \"Library\" catalog, refer Are the applications underlying the charts in the \"Library\" Catalog covered by Rancher Support?</p> <p>Prometheus/Grafana installed by all other means that did not originate from Rancher fall outside the scope of Rancher Support and are not covered by the terms of service of a Rancher Support subscription.</p>"},{"location":"kbs/000020464/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020465/","title":"Will Rancher fix the issue and release a patch, if the problem is root-caused in Jenkins?","text":"<p>This document (000020465) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020465/#resolution","title":"Resolution","text":"<p>Yes. Jenkins is the open-source component used and supported by Rancher 2.1+ as the native build engine for running pipelines. Any issues root-caused in Jenkins will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"kbs/000020465/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020466/","title":"[Rancher] What is the nature of Rancher support for log aggregation services such as Elasticsearch, Splunk, Kafka, Syslog, Fluentd?","text":"<p>This document (000020466) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020466/#resolution","title":"Resolution","text":"<p>Rancher supports its certified integrations with log aggregation systems such as Elasticsearch, Splunk, Kafka, Syslog, and Fluentd. What this means is the following:</p> <ul> <li>Rancher will help Customer troubleshoot the root cause for any issue related to one of these logging services.</li> <li>For issues root-caused to be in the integration to one of these services, Rancher will provide a fix to resolve such issues.</li> <li>For issues root-caused to be inside one of these logging services, Rancher will investigate if the fix is already available in a later version of the logging service. If it is, Rancher will provide a newer version of its product that is validated and certified to work with the later version that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the logging service, Rancher will advise Customer to contact the logging service vendor directly for issue resolution.</li> </ul>"},{"location":"kbs/000020466/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020467/","title":"[Rancher] Will Rancher fix problems root-caused to be in nginx?","text":"<p>This document (000020467) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020467/#resolution","title":"Resolution","text":"<p>Rancher will do one of the following, to help Customer resolve the issue root-caused to be in nginx:</p> <ul> <li>Should the issue have been resolved in a later version of nginx, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of nginx.</li> <li>If the issue is unresolved in any acceptable nginx product versions, Rancher will advise Customer to contact nginx directly for issue resolution. Should a new version or patch be provided by nginx, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>Note:</p> <p>Inclusion of nginx as a certified component in the Rancher support matrix, is based on Rancher's own testing and validation of nginx with its default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine tune the settings for scale and performance.</p>"},{"location":"kbs/000020467/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020468/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in one of the Rancher 2.x CNI plugin add-ons such as Weave, Cisco ACI, Cilium, and Calico?","text":"<p>This document (000020468) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020468/#resolution","title":"Resolution","text":"<p>No. However, Rancher will do one of the following, to help Customer resolve the issue root-caused to be in an add-on such as Weave, Cisco ACI, Cilium, and Calico:</p> <ul> <li>Should the issue have been resolved in a later version of the add-on component, Rancher will recommend Customer to upgrade to this version, along with the necessary upgrades to versions of Rancher and Kubernetes that has been certified for this later version of the add-on.</li> <li>If the issue is unresolved in any acceptable versions of the add-on, Rancher will advise the Customer to contact the add-on vendor directly for issue resolution. Should a new version or patch be provided by the add-on vendor, Rancher will validate its relevant product versions on the new version or patch and certify it as supported for Customer deployment.</li> </ul> <p>We invite all our customers to open a case if they are not confident in the root cause. Rancher Support will work on narrowing down the problem and giving the right recommendations, which might include addressing the upstream project.</p> <p>Note:</p> <p>Any inclusion of such add-ons as a certified component in the Rancher support matrix, is based on Rancher's own testing and validation of these add-ons with their default configuration settings. If the root cause of an issue is identified to be related to scale and performance, changes to default settings may be necessary. In this case, Rancher Support may recommend an engagement with Rancher Consulting or a partner to fine-tune the settings for scale and performance.</p>"},{"location":"kbs/000020468/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020470/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Flannel and Canal?","text":"<p>This document (000020470) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020470/#resolution","title":"Resolution","text":"<p>Yes. Any issues root-caused in one of these two projects will be supported fully, like how any Rancher product issue would be.</p> <p>Rancher Support and Engineering will always recommend running latest versions of the CNIs which are in accordance with the support matrix.</p>"},{"location":"kbs/000020470/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020471/","title":"[Rancher] Will Rancher fix the issue and release a patch, if the problem is root-caused in Istio?","text":"<p>This document (000020471) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020471/#resolution","title":"Resolution","text":"<p>Yes. Istio is the open-source component used and supported by Rancher 2.3+ for service mesh functionality. Any issues root-caused in Istio will be supported fully, like how any Rancher product issue would be.</p>"},{"location":"kbs/000020471/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020472/","title":"[Rancher] We came across a mention of Fluentd as a software Rancher provides Support SLA on. Could you clarify what that covers?","text":"<p>This document (000020472) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020472/#resolution","title":"Resolution","text":"<p>Fluentd is the open-source component used and supported by Rancher 2.x for the native log forwarder functionality. To the extent of this functionality, any issues root-caused in Fluentd will be supported fully, like how any Rancher product issue would be.</p> <p>In the context of Fluentd as a log aggregation system, Rancher Support is limited to ensuring its certified integration with Fluentd works as intended.</p>"},{"location":"kbs/000020472/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020474/","title":"Is Rancher software itself completely free?","text":"<p>This document (000020474) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020474/#resolution","title":"Resolution","text":"<p>Yes, Rancher (community release) itself is completely free and open-source. It's licensed under the Apache License 2.0, which allows you to use, modify, and distribute the software freely.</p> <p>Rancher Prime builds upon the free, open-source Rancher software by offering a set of features and benefits specifically designed for enterprise environments. This is an optional paid subscription that provides additional features like access to Rancher Support with guaranteed support response times, trusted registry, fortified security and seamless integration to other SUSE products.</p>"},{"location":"kbs/000020474/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020475/","title":"[Rancher] How would Rancher Support troubleshoot issues on systems where any external access is not allowed, for reasons of security and compliance?","text":"<p>This document (000020475) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020475/#resolution","title":"Resolution","text":"<p>\"How are you going to do the support for production systems, particularly for financial institutions that have to comply with PCI-DSS standards and that will not allow external access to the systems?\"</p> <p>Not just for customer systems that need to comply with restrictions but the following is applicable to any customer system that is covered by Rancher Support:</p> <p>A Rancher Support Engineer will never work on an issue on any customer system directly and with unmonitored access.</p> <p>Any troubleshooting will be done via a combination of the following:</p> <ul> <li>Log collection</li> <li>Over a screen share session and with the customer on a jump box</li> </ul> <p>Should there be very high-security scenarios where troubleshooting via the above is still not possible, Rancher Support can still help troubleshoot issues, but help can only be provided in a second-hand manner. That is to say, any response can only be provided to the extent of the sanitized information shared by the Customer, with Rancher Support, and in a back-and-forth request-response transaction that may not be very efficient.</p> <p>Where such scenarios are well known and access-based support is still sought, Customer is requested to inquire with their Rancher Account Executive or Customer Success Manager for other commercial models of engagement such as Premium Support\u00a0or via SUSE RGS.</p>"},{"location":"kbs/000020475/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020476/","title":"How (quickly) does Rancher respond to / resolve industry-reported vulnerabilities?","text":"<p>This document (000020476) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020476/#environment","title":"Environment","text":"<p>The following information applies to the following SUSE products</p> <ul> <li>Rancher</li> <li>RKE</li> <li>RKE2</li> <li>K3s</li> <li>Harvester</li> <li>Longhorn</li> <li>NeuVector</li> </ul>"},{"location":"kbs/000020476/#resolution","title":"Resolution","text":"<p>For industry-reported vulnerabilities in Rancher, RKE, RKE2, K3s, Harvester, Longhorn, NeuVector and upstream vulnerabilities in Kubernetes, Docker, and containerd, SUSE Rancher strives to adhere to industry standards and best practices. Due to the nature of upstream dependencies inherent to open-source software, the final delivery of patch releases may vary in timeline. We will prioritize our efforts and coordinate with upstream organizations and third-party entities according to the following guidelines:</p> <ul> <li>Critical: Immediate engagement to remediate the issue in code, and/or coordinate with upstream and/or third-party entities to deliver the remediation in the shortest timeline available. This includes creating an emergency release patch version when an existing one is not readily available.</li> <li>High: Prioritized engagement to align the delivery of the remediation with our next available release cycle. Emergency releases should only be needed unless the timing is such that the next available security release cycle is not in a reasonable timeline.</li> </ul>"},{"location":"kbs/000020476/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020477/","title":"Can we run Antivirus on our cluster nodes?","text":"<p>This document (000020477) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020477/#resolution","title":"Resolution","text":"<p>Antivirus in Kubernetes: A Balancing Act</p> <p>Running antivirus software on Kubernetes nodes introduces complexity. Traditional antivirus solutions may not be optimized for containerized environments like Kubernetes, and a lack of CNCF certification raises compatibility concerns. In some cases, Rancher has observed antivirus interfering with core functionalities like Docker, leading to issues.</p> <p>Resolving Third-Party Tool Conflicts:</p> <p>When troubleshooting issues in Kubernetes, consider if disabling antivirus software restores normal operation. This could indicate a conflict with essential system calls used by Docker or Kubernetes.</p> <p>Certified Configurations: The Foundation for Stability</p> <p>Published, certified configurations\u00a0in the product support matrices provide a baseline for supported components in Kubernetes. Deviating from these configurations might require reverting back to a certified setup to ensure compatibility and resolve issues.</p>"},{"location":"kbs/000020477/#additional-information","title":"Additional Information","text":"<p>faq</p>"},{"location":"kbs/000020477/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020478/","title":"Would Rancher Support provide assistance if we use components that are not listed in the Rancher support matrix?","text":"<p>This document (000020478) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020478/#resolution","title":"Resolution","text":"<p>Limited Support for Unlisted Open-Source Components:</p> <ul> <li>Rancher Support can help troubleshoot issues with open-source components not listed in the support matrix, but only up to the point where the problem lies with Rancher's interfaces interacting with that component.</li> <li>If the root cause is beyond Rancher's control (e.g., within the component itself), you'll need to work directly with the component's maintainers or seek commercial support.</li> </ul> <p>Ensuring Optimal Support:</p> <ul> <li>To receive the best possible support and avoid confusion, we recommend informing Rancher Support of any critical, unlisted components in your deployment. This allows them to understand your environment better and potentially offer more targeted assistance.</li> </ul>"},{"location":"kbs/000020478/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020479/","title":"Would Rancher Support provide assistance if we change the default configurations of one or more of these components listed in Rancher support matrix?","text":"<p>This document (000020479) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020479/#resolution","title":"Resolution","text":"<p>The configurations listed in the Rancher support matrix represent setups tested and verified for optimal performance. These configurations rely on the default settings of individual components.</p> <p>If you've customized your environment beyond these certified configurations, Rancher Support may recommend reverting to a supported setup to troubleshoot and resolve the issue you're facing. This ensures compatibility and helps isolate the root cause of the problem.</p>"},{"location":"kbs/000020479/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020480/","title":"[Rancher] What are the certified integrations with persistent volume plugins covered by Rancher Support?","text":"<p>This document (000020480) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020480/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Amazon EBS Disk Azure Disk</li> <li>Azure Filesystem</li> <li>Google Persistent Disk</li> <li>Longhorn</li> <li>Local Node Disk</li> <li>Local Node Path</li> <li>NFS Share</li> <li>VMware vSphere Volume</li> </ul> <p>Note:</p> <p>For any other persistent volume plugins from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.</p>"},{"location":"kbs/000020480/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020481/","title":"[Rancher] What are the certified integrations with storage class provisioners covered by Rancher Support?","text":"<p>This document (000020481) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020481/#resolution","title":"Resolution","text":"<p>In Rancher v2.2+, Rancher Support applies to the following certified integrations:</p> <ul> <li>Amazon EBS Disk</li> <li>Azure Disk</li> <li>Azure File</li> <li>Google Persistent Disk</li> <li>VMware vSphere Volume</li> <li>Longhorn</li> <li>Local</li> </ul> <p>Note:</p> <p>For any other storage class provisioners from the kubernetes ecosystem, Rancher Support is limited to the extent of ensuring they work together with Rancher to the extent they do directly with upstream kubernetes.</p>"},{"location":"kbs/000020481/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020482/","title":"What are the certified integrations with authentication providers covered by Rancher Support?","text":"<p>This document (000020482) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020482/#resolution","title":"Resolution","text":"<p>Within Rancher, Rancher Support applies to the following certified integrations:</p> <ul> <li>Microsoft Active Directory</li> <li>GitHub</li> <li>Microsoft Azure AD</li> <li>FreeIPA (LDAP)</li> <li>OpenLDAP (LDAP)</li> <li>Microsoft AD FS (SAML)</li> <li>PingIdentity (SAML)</li> <li>Keycloak (OIDC)</li> <li>Keycloak (SAML)</li> <li>Okta (SAML)</li> <li>Google OAuth</li> <li>Shibboleth</li> </ul>"},{"location":"kbs/000020482/#rancher-support-does-not-cover","title":"Rancher Support does not cover:","text":"<ul> <li>Switching between these external authentication providers.</li> <li>Migration from authentication scheme of one provider to another that would typically require maintaining existing user settings/preferences, access levels and privileges, user and group authorizations.</li> </ul>"},{"location":"kbs/000020482/#additional-information","title":"Additional Information","text":"<p>faq</p>"},{"location":"kbs/000020482/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020483/","title":"[Rancher] Could you clarify what you generally mean by a \"certified integration\" to another software system or service?","text":"<p>This document (000020483) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020483/#resolution","title":"Resolution","text":"<p>Rancher supports its certified integrations with various software systems and services for functionality ranging from User Authentication to Infrastructure. What this means is the following:</p> <ul> <li>Rancher has validated the integration for one or more of its product versions to work as designed with such systems and services.</li> <li>Rancher Support will help Customer troubleshoot the root cause for any issue related to one of these integrations.</li> <li>For issues root-caused to be in the integration component to one of these systems or services, Rancher will provide a fix to resolve such issues.</li> <li>For issues root-caused to be inside one of these systems or services, Rancher will investigate if the fix is already available in a later version of the system or service. If it is, Rancher will provide a newer version of its own product that is validated and certified to work with the later version of the system that has the fix. Or validate and certify one of its existing versions to work with the later version that has the fix. If no fix is available in any acceptable version of the system or service, Rancher will advise Customer to directly contact the vendor providing support for the system or service for issue resolution.</li> </ul>"},{"location":"kbs/000020483/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020491/","title":"Does Rancher Support covers OpenStack clusters?","text":"<p>This document (000020491) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020491/#resolution","title":"Resolution","text":"<p>While the OpenStack node driver isn't officially supported by Rancher, you can still use Rancher to manage Kubernetes clusters on your OpenStack infrastructure. This requires creating custom clusters within Rancher, which means manually configuring details instead of using the pre-built driver.</p> <p>Rancher Support can help troubleshoot issues related to Rancher itself, but their assistance won't extend to problems originating from the OpenStack platform. Their troubleshooting focuses on analyzing logs and collaborating through screen sharing sessions to identify the root cause. If the issue stems from OpenStack, you'll need to work directly with your OpenStack provider or maintainers for further assistance.</p>"},{"location":"kbs/000020491/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020493/","title":"I see node drivers tagged as \"Built-in\". What does that mean?","text":"<p>This document (000020493) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020493/#resolution","title":"Resolution","text":"<p>Built-in drivers are those that are included in a Rancher product distribution.</p>"},{"location":"kbs/000020493/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020494/","title":"I see some node drivers tagged as \"Active\" when I install Rancher. What does this mean?","text":"<p>This document (000020494) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020494/#resolution","title":"Resolution","text":"<p>Active drivers are those drivers that have been turned on. Only four Built-in node drivers are surfaced up in UI, as Active, in a default Rancher installation. It is only these five Built-in, Active node drivers that are validated and certified in any Rancher 2.x product version and consequently, published in the Rancher support matrix. These five Built-in, Active node drivers that are covered by Rancher Support are for:</p> <ul> <li>Digital Ocean</li> <li>AWS</li> <li>Azure</li> <li>vSphere - 6.5, 6.7, 7.0 update 2a</li> <li>Linode (supported only from Rancher v2.3 or higher)</li> </ul>"},{"location":"kbs/000020494/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020495/","title":"[Rancher] Does my support subscription to Rancher include support for Longhorn?","text":"<p>This document (000020495) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020495/#resolution","title":"Resolution","text":"<p>No, a subscription to Rancher Prime does not include support for Longhorn. With the general availability of Longhorn in June 2020, a separate commercial subscription to an add-on plan is needed to receive SLA-based support for Longhorn.</p> <p>Customers are invited to contact their account executives to learn about opportunities for Longhorn support.</p>"},{"location":"kbs/000020495/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020496/","title":"[Rancher] Can we have a mix of unsupported and supported nodes at our choice/discretion?","text":"<p>This document (000020496) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020496/#resolution","title":"Resolution","text":"<p>No. Within a supported Rancher environment, categorizing cluster nodes as \"supported'\" and \"unsupported\" at a team's own choice/discretion is not allowed.</p>"},{"location":"kbs/000020496/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020498/","title":"How does Rancher track our license usage?","text":"<p>This document (000020498) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020498/#resolution","title":"Resolution","text":"<p>Currently, Rancher relies on customers to report their usage and procure additional licenses if their usage has increased. From time to time, as part of support calls, a Rancher Support Engineer may request customers to run a simple command-line script on the Rancher Server that will generate high-level usage information:</p> <pre><code>wget -O - https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/systems-information/systems_summary.sh | sh - &gt; rancher_stats.txt\n</code></pre> <p>This is both to keep track of our customer's usage for license compliance as well as to offer advisories should customers be approaching any thresholds related to scale and performance.</p>"},{"location":"kbs/000020498/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020499/","title":"Is Rancher Prime support only for production environments?","text":"<p>This document (000020499) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020499/#resolution","title":"Resolution","text":"<p>Rancher Prime Support: Beyond Production</p> <p>Rancher Prime support isn't limited to production environments. You can purchase support for any Rancher installation, including Dev/Test, Staging, Demo, and more. This ensures Service Level Agreements (SLAs) are met across all your environments, keeping your development and testing pipelines running smoothly.</p> <p>For specific details on covering different environments, we recommend contacting your local Sales representative. They can help you tailor a support plan that meets your specific needs.</p>"},{"location":"kbs/000020499/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020500/","title":"Does my Rancher Prime Support subscription for production environment also covers testing and staging environments?","text":"<p>This document (000020500) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020500/#resolution","title":"Resolution","text":"<p>Your Rancher Prime subscription for the production environment typically doesn't extend to testing and staging environments. To receive support for those environments, you'll need to include them in your subscription.</p> <p>Consider discussing your needs with your Rancher sales representative. They can help you determine the best support coverage for your entire deployment, including production, testing, and staging environments.</p>"},{"location":"kbs/000020500/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020502/","title":"[Rancher] Could you illustrate the severity levels with subject lines of sample support cases?","text":"<p>This document (000020502) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020502/#resolution","title":"Resolution","text":"<p>Here are\u00a0some sample subject lines by severity level:</p> Severity Level Subject line of sample support cases Severity 1 1. Production cluster is down2. Apps in production cluster throwing '502 Bad Gateway' Error3. Enabled enhanced cluster monitoring and the cluster sporadically goes offline4. etcd not healthy in production rancher server5. Kubernetes ingress endpoint went unresponsive Severity 2 1. etcd restore failed for RKE-deployed cluster2. Microservices client encounters UnknownHostException in Production3. Network traffic routed over IPsec is super slow4. LDAP is intermittently failing to log in users in one of our environments5. DNS intermittent timeouts Severity 3 1. Webhook notifier not working as expected2. Need assistance with creating custom global roles to prevent cluster creation3. Problem adding new nodes to clusters4. Ingress timeout issue in Rancher-deployed cluster5. Bug: Unable to add AD groups to Rancher ACLs Severity 4 1. Ability to change UID and GID in docker containers on container creation.2. Feature Request: Show audit logs content in Rancher UI3. FYI: Scheduled upgrade XX/XX/XXXX XX:XXPM, request on-call assistance\u00a0as needed4. How can I disable the creation of the Prometheus-Operator CRDs on cluster creation5. Understand the impact of changing IPs of worker nodes in our k8s cluster"},{"location":"kbs/000020502/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020502/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020526/","title":"Security vulnerability: log4j remote code execution aka log4shell CVE-2021-44228","text":"<p>This document (000020526) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020526/#environment","title":"Environment","text":"<p>All products</p>"},{"location":"kbs/000020526/#situation","title":"Situation","text":"<p>A 0-day exploit in the log4j Java logging framework was found by Chen Zhaojun of Alibaba Cloud Security Team, which allowed remote attackers able to inject strings into log4j based Java logging to execute code by</p> <p>exploiting the default enabled JNDI bindings. This is possible without any preconditions, making it critical.</p>"},{"location":"kbs/000020526/#resolution","title":"Resolution","text":"<p>SUSE considers log4j versions 2.0 and newer as affected, log4j 1.2.x does not have the same critical vulnerability and is not considered affected by this CVE.</p> <p>SUSE Linux Enterprise products do not ship log4j 2.x.</p> <p>SUSE Manager does not ship log4j 2.x.</p> <p>SUSE Enterprise Storage does not ship log4j 2.x.</p> <p>SUSE Openstack Cloud embeds log4j2 in the \"storm\" component, which will receive updates.</p> <p>SUSE NeuVector product does not ship log4j 2.x.</p> <p>SUSE Rancher is not affected by this vulnerability. The Helm chart for Istio 1.5, provided by Rancher and which is currently deprecated, includes Zipkin and is vulnerable to Log4j. Customers are advised to upgrade to the recent Istio version provided in Cluster Explorer, which does not uses Zipkin and is not affect to the vulnerability.</p> <p>Please refer to the upstream guidance from log4j on fixing and mitigation measures if you deploy your Java Application stacks.</p>"},{"location":"kbs/000020526/#status","title":"Status","text":"<p>Security Alert</p>"},{"location":"kbs/000020526/#additional-information","title":"Additional Information","text":"<p>Additional information can be found here:</p> <ul> <li>https://suse.com/security/cve/CVE-2021-44228.html</li> <li>https://www.suse.com/c/suse-statement-on-log4j-log4shell-cve-2021-44228-vulnerability/</li> <li>https://logging.apache.org/log4j/2.x/security.html</li> </ul> <p>Note in regards to SUSE Manager Server:</p> <p>The CVE-search will use meta-data within a patch to display the needed information. As there is no patch needed (as SUSE is not effected), the CVE-search for CVE-2021-44228 will return a \"not found\".</p>"},{"location":"kbs/000020526/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020536/","title":"[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20","text":"<p>This document (000020536) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020536/#resolution","title":"Resolution","text":"Note:This\u00a0Rancher Labs operational advisory below was originally sent in December 2020.\u00a0 It has been published here to continue SUSE Rancher customer conversations around this topic via support cases and for sharing any relevant updates around it.For an update on this topic, please see\u00a0[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24. <p>Dear Rancher Customer,</p> <p>This is an operational advisory from Rancher Support related to the deprecation of dockershim in Kubernetes v1.20</p> <p>As announced on the official Kubernetes blog, the dockershim, which enables the use of the Docker Daemon as the container runtime in Kubernetes, will be deprecated with the upcoming Kubernetes v1.20 release.</p>"},{"location":"kbs/000020536/#what-is-dockershim","title":"What is dockershim?","text":"<p>The dockershim is built into Kubernetes to provide a Container Runtime Interface (CRI) compliant layer between the kubelet and the Docker Daemon. The shim is necessary because Docker Daemon is not CRI-compliant.</p>"},{"location":"kbs/000020536/#what-does-deprecation-of-dockershim-in-kubernetes-v120-mean","title":"What does deprecation of dockershim in Kubernetes v1.20 mean?","text":"<p>The dockershim will only be deprecated in Kubernetes v1.20, and will not yet be removed from the kubelet. As a result, no immediate action needs to be taken and Kubernetes clusters can continue to operate with the Docker Daemon container runtime in Kubernetes v1.20. The only change at this time will be a deprecation warning printed in the kubelet logs when running on Docker.</p>"},{"location":"kbs/000020536/#what-are-ranchers-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","title":"What are Rancher's plans to ensure on-going container runtime support in future Kubernetes releases?","text":"<p>We are working on our roadmap to ensure that all Rancher provisioned clusters will continue to operate on a CRI-compliant runtime.</p> <p>For existing RKE customers, users will continue to get Kubernetes updates until the shim is officially removed. The removal is currently targeted for late 2021 and will be supported with patches during the 12-month upstream maintenance window. Before the end of maintenance, we fully expect an upgrade path from RKE to RKE2.</p> <p>Looking forward, containerd is already the default runtime in both K3s and RKE2, so any removal of dockershim will have zero impact on future releases. As with RKE, organizations currently using K3s with the Docker runtime will continue to get Kubernetes updates until the shim is officially removed. The dockershim deprecation schedule is tracked by the upstream Kubernetes community in Kubernetes Enhancement Proposal (KEP) 1985,\u00a0Rancher will continue to keep you updated with related news on the roadmap from our product management, as well as information related to migration off of Docker.</p> <p>Thanks,</p> <p>Rancher Support Team</p>"},{"location":"kbs/000020536/#additional-information","title":"Additional Information","text":"<ul> <li>[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020536/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020538/","title":"[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24","text":"<p>This document (000020538) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020538/#resolution","title":"Resolution","text":"Note: This is a follow-up to the Rancher Operational Advisory that was sent on this topic in December 2020. <p>In the latest announcement\u00a0from the Kubernetes blog, it has been notified that dockershim removal has been planned in Kubernetes v1.24,\u00a0slated for release around April 2022.</p>"},{"location":"kbs/000020538/#what-are-suses-plans-to-ensure-on-going-container-runtime-support-in-future-kubernetes-releases","title":"What are SUSE's plans to ensure on-going container runtime support in future Kubernetes releases?","text":"<p>Starting with Kubernetes v1.21, RKE added support for CRI plugin cri-dockerd, see here\u00a0instructions to enable. All RKE clusters will need to leverage this CRI plugin before upgrading to Kubernetes v1.24. Future updates beyond Kubernetes v1.24 RKE will rely on the cri-dockerd shim.</p> <p>For more information on the dockershim removal schedule, you can check the upstream Kubernetes Enhancement Proposal (KEP) 2221.</p> <p>K3s and RKE2 are not impacted by the removal of dockershim and use the CRI plugin containerd. We expect to deliver a migration path from RKE to RKE2 in the future.</p> <p>SUSE will continue to keep you updated with related news on the roadmap from our product management</p>"},{"location":"kbs/000020538/#additional-information","title":"Additional Information","text":"<ul> <li>[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020538/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020543/","title":"[Rancher] Support Advisories","text":"<p>This document (000020543) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020543/#resolution","title":"Resolution","text":"<ul> <li>[Rancher] Operational Advisory, 20220405:\u00a0Rancher Kubernetes Distributions and Etcd 3.5 Updates</li> <li>[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5</li> <li>[Rancher] Operational Advisory, 20220113: Related to removal of dockershim in Kubernetes v1.24</li> <li>[Rancher] Operational Advisory, 20201210: Related to deprecation of dockershim in Kubernetes v1.20</li> <li>[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits</li> </ul>"},{"location":"kbs/000020543/#additional-information","title":"Additional Information","text":"<p>Rancher Support FAQs</p>"},{"location":"kbs/000020543/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020556/","title":"Rancher Prime Hosted FAQ","text":"<p>This document (000020556) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020556/#resolution","title":"Resolution","text":""},{"location":"kbs/000020556/#general","title":"General","text":"<ul> <li>What is Rancher Prime Hosted?</li> <li>What do I need to provide to get started on Rancher Prime Hosted?</li> <li>Do you have a whitepaper available for Rancher Prime Hosted?</li> <li>Can I move from self-managed Rancher to Rancher Prime Hosted?</li> <li>What if I no longer want Rancher Prime Hosted to manage my downstream clusters?</li> <li>Is it possible to have alpha, beta, or release candidate (RC) versions on Rancher Prime Hosted?</li> <li>Can Rancher Prime Hosted manage my on-premise clusters running on VMWare or bare metal servers?</li> <li>Is there any limit on the number of downstream clusters or nodes Rancher Prime Hosted can manage?</li> <li>Do I have access to the Rancher Prime Hosted \"local\" cluster in the management UI?</li> <li>Where is Rancher Prime Hosted hosted?</li> <li>Can I have more than one\u00a0Rancher Prime Hosted environment?</li> <li>What type of cluster is Rancher Prime Hosted running on?</li> <li>Does Rancher Prime Hosted provide downstream clusters?</li> <li>Can I move an existing cluster to Rancher Prime Hosted?</li> <li>How is Rancher Prime Hosted different from the open-source Rancher I can download for free?</li> </ul>"},{"location":"kbs/000020556/#maintenance-operations","title":"Maintenance &amp; Operations","text":"<ul> <li>Who creates user accounts in Rancher Prime Hosted?</li> <li>How is my Rancher Prime Hosted environment monitored?</li> <li>How often is maintenance performed on Rancher Prime Hosted?</li> <li>Can the admin password be reset if I\u2019m locked out of my Rancher Prime Hosted?</li> <li>Who upgrades Kubernetes on my\u00a0Rancher Prime Hosted downstream clusters?</li> <li>Does Rancher Prime Hosted offer a support SLA?</li> <li>How often are backups taken and retained on Rancher Prime Hosted?</li> </ul>"},{"location":"kbs/000020556/#upgrades-uptime","title":"Upgrades &amp; Uptime","text":"<ul> <li>What can be expected during a Rancher Prime Hosted upgrade?</li> <li>How often is Rancher Prime Hosted upgraded?</li> <li>How is uptime measured for my Rancher Prime Hosted?</li> <li>Does Rancher Prime Hosted offer an uptime SLA?</li> </ul>"},{"location":"kbs/000020556/#network-security-logging","title":"Network, Security, &amp; Logging","text":"<ul> <li>What are the networking requirements for using Rancher Prime Hosted?</li> <li>Can I use my own SSL/TLS certificates with Rancher Prime Hosted?</li> <li>How to connect your Rancher Prime Hosted network to your AWS transit gateway?</li> <li>How to make a VPN connection to your Rancher Prime Hosted network?</li> <li>Can I integrate Rancher Prime Hosted with my Active Directory, SAML, or LDAP-based directory service?</li> <li>Does Rancher Prime Hosted support multi-factor authentication (MFA)?</li> <li>Are API audit logs enabled in Rancher Prime Hosted?</li> <li>Can I get a copy of the Rancher API audit logs?</li> <li>What information is stored in Rancher Prime Hosted and where is it stored?</li> <li>Do SUSE employees have a login account for my Rancher Prime Hosted environment?</li> <li>Do SUSE employees have the credentials to my \u201cadmin\u201d account?</li> <li>Is Rancher Prime Hosted data encrypted at rest?</li> <li>Is Rancher Prime Hosted SOC2 compliant?</li> <li>What permissions does the admin account have in Rancher Prime Hosted?</li> </ul>"},{"location":"kbs/000020556/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020557/","title":"Do you have a whitepaper available for Rancher Hosted Prime?","text":"<p>This document (000020557) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020557/#resolution","title":"Resolution","text":"<p>Yes, there is a Rancher Hosted Prime architecture whitepaper that can be downloaded on SUSE's website. It can be found here - https://more.suse.com/fy21-global-web-landing-page-hosted-rancher-technical-guide</p>"},{"location":"kbs/000020557/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020558/","title":"What can be expected during a Rancher Hosted Prime upgrade?","text":"<p>This document (000020558) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020558/#resolution","title":"Resolution","text":"<p>There is minimal impact or disruption during a Rancher Hosted Prime upgrade. During the one-hour maintenance window you can expect a brief (usually 1-2 minutes) when your Rancher control plane UI/API is inaccessible. This does not impact the workloads on your managed downstream clusters, only your ability to make changes to these clusters. Immediately following the Rancher control plane upgrade, the cluster and node agents running in your managed downstream clusters will be upgraded and restarted. This also only takes a few minutes (unless you have very large clusters or very poor network speeds) and during this time Rancher will be unable to manage the cluster, but again the workloads running on the clusters should operate normally.</p>"},{"location":"kbs/000020558/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020559/","title":"What if I no longer want Rancher Hosted Prime to manage my downstream clusters?","text":"<p>This document (000020559) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020559/#resolution","title":"Resolution","text":"<p>If you decide you no longer want to continue using the Rancher Hosted Prime service, we can provide a one-time backup of your Rancher control plane which you can use to restore into a self-managed Rancher management server. Information on the restore process can be found in our documentation . To request a backup file, you can submit a support case on our support portal. After restoring Rancher, you will need to reconfigure your downstream clusters to point to the new server URL of your self-managed Rancher management server.</p>"},{"location":"kbs/000020559/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020561/","title":"How to connect your Rancher Hosted Prime network to your AWS transit gateway?","text":"<p>This document (000020561) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020561/#resolution","title":"Resolution","text":"<p>The following steps can be taken to connect your Rancher Hosted Prime network to an AWS transit gateway running in your AWS account.</p> <ol> <li>Make sure you have provided the Rancher Hosted Prime team with a CIDR that does not overlap with your existing infrastructure. If not, your Rancher Hosted Prime environment may need to be redeployed with the new CIDR. The CIDR must be a /25 block or larger. Using a /24 is normally preferred.</li> <li>If you haven't already, create a transit gateway in your AWS account. See Create a transit gateway.</li> <li>In the AWS console, go to Resource Access Manager (RAM) service.</li> <li>In RAM, click the orange button in the top right corner labeled \"Create a resource share\".</li> <li>For the name, use something descriptive that includes both your company name and \"Rancher Hosted Prime\". For example, \"Widget Corp transit gateway for\u00a0Rancher Hosted Prime\". For resource type, select Transit Gateways. Select the transit gateway you want to share. In Principals, check Allow external accounts and enter the AWS account number provided by the Rancher Hosted Prime team. Click the orange \"Create resource share\" in the bottom right corner.</li> <li>Let the Rancher Hosted Prime team know you have created the share. We will accept the share and make a request to attach the transit gateway to your Rancher Hosted Prime VPC.</li> <li>Accept the request to attach your transit gateway to the Rancher Hosted Prime VPC. To do this, go to the VPC service, click \"Transit Gateway Attachments\" in the navigation pane, select the transit gateway attachment, choose Actions -&gt; Accept.</li> <li>Provide the Rancher Hosted Prime team with a list of CIDRs you want to be routed through the transit gateway.</li> </ol> <p>See also Transit gateways and Transit gateway sharing considerations for more information.</p>"},{"location":"kbs/000020561/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020562/","title":"How to make a VPN connection to your Rancher Hosted Prime network?","text":"<p>This document (000020562) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020562/#resolution","title":"Resolution","text":"<p>It's normally preferred to connect Rancher Hosted Prime with your network using VPC peering through an AWS transit gateway. This is the most cost-effective, secure, and manageable solution. However, if this is not an option, a VPN connection can be established between your corporate network and Rancher Hosted Prime through an IPSec VPN tunnel. The following steps are required to set this up:</p> <ol> <li>Provide the Rancher Hosted Prime team with the following information about your VPN device:</li> <li>Public IP address for your VPN endpoint</li> <li>Routing option: a) static (no BGP support) or b) dynamic (BGP support)</li> <li>BGP ASN (only if dynamic routing)</li> <li>VPN device make and model used on-premise that we'll be connecting to.</li> <li>The Rancher Hosted Prime team will configure the VPN connection and provide configuration information based on the VPN device</li> <li>Customer will configure their VPN device to connect to Rancher Hosted Prime's\u00a0network.</li> </ol> <p>See also AWS Site-to-Site VPN User Guide.</p>"},{"location":"kbs/000020562/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020563/","title":"Can I get a copy of the Rancher API audit logs?","text":"<p>This document (000020563) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020563/#resolution","title":"Resolution","text":"<p>Yes, if you provide a log output configuration to your logging solution, such as AWS CloudWatch, Elasticsearch, Splunk, etc. we can stream Rancher API audit logs to you.</p>"},{"location":"kbs/000020563/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020630/","title":"[Rancher] Operational Advisory, 20201105: Related to Docker Hub rate limits","text":"<p>This document (000020630) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020630/#resolution","title":"Resolution","text":"Note: Below is the Rancher Customer email advisory sent in Nov 2020 on the topic of Docker Hub rate limits. <p>Dear Rancher Customer,</p> <p>As\u00a0announced by Docker Inc, the rate-limiting by Docker Hub is expected to progressively take effect beginning Nov 2.</p> <p>We have been engaged in direct conversations with many of you on the possible impact of this\u00a0rate-limiting and steps toward managing that.\u00a0 This operational advisory is a summary of those conversations.</p>"},{"location":"kbs/000020630/#do-the-rate-limits-apply-to-rancher-images-that-we-pull-anonymously","title":"Do the rate limits apply to Rancher images that we pull anonymously?","text":"<p>No.\u00a0 To ensure customers pulling Rancher resources are not affected by this, Rancher Labs has partnered with Docker, Inc. so that\u00a0pulls from the Rancher namespace on Docker Hub are exempt from these limits.</p> <p>If you run into any rate-limiting issues with images hosted in the Rancher namespace, please let us know.</p>"},{"location":"kbs/000020630/#what-about-images-that-are-outside-the-rancher-namespace-that-we-pull-anonymously","title":"What about images that are outside the Rancher namespace that we pull anonymously?","text":"<p>Yes. Rate limits do apply to the images that are outside of the Rancher namespace.</p>"},{"location":"kbs/000020630/#what-can-we-do-about-the-limits-on-images-outside-the-rancher-namespace","title":"What can we do about the limits on images outside the Rancher namespace?","text":"<p>We can introduce you to the right contact at Docker Inc should you be interested in procuring an exemption for your org based on something like an IP range.\u00a0 This is to derisk being limited on image pulls outside of the Rancher namespace.</p>"},{"location":"kbs/000020630/#what-other-practical-options-can-we-pursue","title":"What other practical options can we pursue?","text":"<p>Other options to mitigate this issue are:</p> <ul> <li>Moving from anonymous pulls to authenticated pulls on Docker Hub</li> <li>Copying resources to a private registry</li> </ul> <p>The viability of these options depends on the specific context of your environment.</p>"},{"location":"kbs/000020630/#are-there-any-plans-to-host-rancher-images-elsewhere-to-help-alleviate-the-potential-issues-caused-by-this","title":"Are there any plans to host Rancher images elsewhere to help alleviate the potential issues caused by this?","text":"<p>We are looking into alternates to Docker Hub. There have been recent offerings announced by\u00a0AWS and\u00a0GitHub. We are exploring them as well as other options and should have an update on this from our product management in the near future.</p> <p>Thanks,</p> <p>Rancher Support Team</p>"},{"location":"kbs/000020630/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020630/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020631/","title":"[Rancher] Product Lifecycle Advisory, 20220330: 2022 EOM/EOL dates for Rancher 2.4 and 2.5","text":"<p>This document (000020631) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020631/#resolution","title":"Resolution","text":"Note:This Rancher Customer advisory below was originally sent by email to subscribed customer users on March 30, 2022. <p>Dear SUSE Rancher user,</p> <p>We have been sharing important SUSE Rancher product lifecycle dates on our\u00a0Product Support Lifecycle page.</p> <p>To help you plan for any necessary upgrades for your deployments, we would like to bring to your attention information on some Rancher Manager product versions that are approaching (or have reached) their End of Maintenance (EOM)\u00a0and\u00a0End of Life (EOL)\u00a0milestones.</p> <p>EOM Dates</p> Rancher Version EOM Date v2.4.x July 30, 2021 v2.5.x January 5, 2022 <p>EOL Dates</p> Rancher Version EOL Date v2.4.x March 31, 2022 v2.5.x October 5, 2022"},{"location":"kbs/000020631/#what-does-the-above-mean","title":"What does the above mean?","text":"<p>After a product release reaches its EOM date, no further code-level maintenance will be provided, except for critical security-related fixes. Product will continue to be supported in a limited manner until it reaches EOL, in the form of:</p> <ul> <li>General troubleshooting of a specific issue to isolate potential causes</li> <li>Upgrade recommendation to an existing newer version of product</li> <li>Issue resolution limited to applying configuration changes and/or an upgrade recommendation to an existing newer version of product</li> </ul> <p>Once a product release reaches its EOL date, a Rancher user may continue to use the product within the terms of the product licensing agreement. However, Support Plan SLAs from SUSE Rancher do not apply to product versions that are past their EOL dates.</p> <p>Please review in detail the following resources to understand changes and prepare for your upgrade.</p> <ul> <li>Rancher Support Matrix</li> <li>Rancher 2.5.0 release notes</li> <li>Rancher 2.6.0 release notes</li> <li>Rancher Support Upgrade checklist</li> <li>Rancher Support FAQs</li> </ul> <p>In addition, please note that with the upcoming release of Kubernetes 1.24, scheduled for April 19th, dockershim removal from Kubernetes will be final. Please see the\u00a0Kubernetes blog and our\u00a0Rancher Support Advisory for more information.</p> <p>If you have any questions on this\u00a0advisory or would like assistance validating your upgrade path, simply contact your Customer Success Manager or open a new support case via the\u00a0SCC portal referencing this advisory.</p> <p>Thanks,</p> <p>SUSE Rancher Support Team</p>"},{"location":"kbs/000020631/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher Support Advisories</li> <li>Rancher Support FAQs</li> </ul>"},{"location":"kbs/000020631/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020672/","title":"How to set multiple rules in the Rancher2 Terraform Provider Role Template resource","text":"<p>This document (000020672) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020672/#situation","title":"Situation","text":"<p>Defining multiple rules within a single role template.</p>"},{"location":"kbs/000020672/#resolution","title":"Resolution","text":"<p>To define multiple rules for a single role template, set multiple rule blocks as per the following example:</p> <pre><code>resource \"rancher2_role_template\" \"foo\" {\n  name = \"foo\"\n  context = \"project\"\n  default_role = false\n  description = \"Terraform role template acceptance test\"\n  rules {\n    api_groups = [\"*\"]\n    resources = [\"secrets\"]\n    verbs = [\"create\"]\n  }\n  rules {\n    api_groups = [\"*\"]\n    resources = [\"namespaces\"]\n    verbs = [\"create\"]\n  }\n}\n</code></pre>"},{"location":"kbs/000020672/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020682/","title":"Azure AD API Removal","text":"<p>This document (000020682) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020682/#situation","title":"Situation","text":""},{"location":"kbs/000020682/#summary-of-changes","title":"Summary of Changes","text":"<p>Microsoft is ending support of the existing AzureAD Graph API before 2023. Accordingly, Rancher has updated our AzureAD auth provider to use the new Microsoft Graph API to access users and groups in Active Directory.</p>"},{"location":"kbs/000020682/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>Old</p> <ul> <li>ADAL is the authentication library we use to get access tokens to the deprecated Azure AD Graph API.</li> </ul> <p>New</p> <ul> <li>MSAL is the new authentication library we will instead use to get access tokens to the new Microsoft Graph API.</li> </ul>"},{"location":"kbs/000020682/#actions-required-of-users","title":"Actions Required of Users","text":"<ul> <li>New users of v2.6.x and v2.7.x will use the new Microsoft Graph API when they register Rancher with Azure AD. There will be no need for a transition.</li> <li>Existing users who have Azure AD as the auth provider will see an informational notification/banner that will urge them to upgrade Rancher's auth provider before the end of 2022. Beforehand, their app in Azure will need to have the necessary permissions for Rancher to be able to work with Users and Groups in AD. To upgrade, the UI will have a button to instruct the backend to use the new authentication/authorization flow without requiring Rancher admins to reconfigure the existing auth provider.</li> <li>AD admins must add the necessary Microsoft Graph permissions to their apps:</li> <li>In 2.6.X, Rancher needs User.Read.All and Group.Read.All - both must be Application (not Delegated) permissions.</li> <li> <p>In 2.7.X, Rancher needs permissions that allow the following actions:</p> <ul> <li>Get a user.</li> <li>List all users.</li> <li>List groups of which a given user is a member.</li> <li>Get a group.</li> <li>List all groups.</li> </ul> </li> </ul> <p>Here are a few examples of permission combinations that satisfy Rancher's needs:</p> <pre><code> 1. Directory.Read.All\n2. User.Read.All and GroupMember.Read.All\n3. User.Read.All and Group.Read.All\n</code></pre>"},{"location":"kbs/000020682/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>When you choose to upgrade the existing Azure AD auth provider configuration in Rancher, please keep in mind that all users' access tokens to the deprecated Azure AD Graph API will be deleted, since Rancher won't need them anymore because it won't be communicating with it.</p> <p>Instead, Rancher will store in a secret only one access token to the new Microsoft Graph API - that of the service principal associated with the App registration in Azure AD. This token is refreshed once an hour (not in the background, but when its use triggers a refresh).</p> <p>Additional migration instructions can be found at these links:</p> <p>For Rancher 2.6.x</p> <p>For Rancher 2.7.x</p>"},{"location":"kbs/000020682/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020684/","title":"Windows RKE1 EOL","text":"<p>This document (000020684) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020684/#situation","title":"Situation","text":""},{"location":"kbs/000020684/#summary-of-changes","title":"Summary of Changes","text":"<p>RKE1 Windows will move to EOL status on September 1, 2022, due to the deprecation of Docker EE support by Microsoft. Docker EE is the only supported container runtime for Windows nodes in RKE1.</p> <p>The impact on customers and internal development/QA efforts after September 1, 2022, is summarized below:</p> <p>- Rancher users should expect that they are unable to provision new RKE1 Windows clusters.</p> <p>- Rancher users should expect to have no available upgrade path for existing RKE1 Windows clusters.</p> <p>- Customers who are unable to move to RKE2 Windows would need to purchase a support contract for Mirantis Container Runtime, previously known as Docker Engine - Enterprise.</p> <p>- Rancher Support is unable to provide full-stack support for workloads deployed on the Mirantis Container Runtime.</p> <p>- We expect cloud providers to completely remove images that contain Docker EE for Windows Server.</p> <p>- Microsoft has indicated to us in an unofficial capacity that the current and only installation method for Docker EE on Windows Server, which is through the PSGallery, will be removed as part of the EOL of Docker EE.</p>"},{"location":"kbs/000020684/#details-of-old-vs-new","title":"Details of Old vs New","text":"<p>RKE2 Windows is the only path forward for Windows support in Rancher.</p> <p>RKE1 Windows Clusters: Each Linux node in an RKE1 Windows cluster, regardless of the role assigned to it, will have have a default taint that prevents workloads to be scheduled on it unless the workload has a toleration configured. This is a major design feature for RKE1 Windows clusters which were designed to only run Windows workloads.</p> <p>RKE2 Hybrid Clusters: Based on feedback and requests for hybrid workload support, RKE2 Windows was designed to support both Linux and Windows workloads by default. RKE2 scheduling relies on node selectors. This is a marked change from RKE1 as taints and tolerations were not incorporated into RKE2. Node selectors were a critical part of RKE1 Windows clusters, which makes for an easy migration of your workloads.</p>"},{"location":"kbs/000020684/#actions-required-of-users","title":"Actions Required of Users","text":"<p>Moving forward, Rancher customers will need to upgrade to Rancher v2.6.5+ (to have GA of RKE2 Windows provisioning available) and migrate their container workloads to run on RKE2 Hybrid clusters, which are built on the containerd runtime. These steps will be required for them to stay in compliance with the Rancher Support Matrix.</p>"},{"location":"kbs/000020684/#support-considerations-or-gotchas","title":"Support Considerations or Gotchas","text":"<p>It should be assumed that all existing functionality required for creating new RKE1 Windows clusters or upgrading existing RKE1 Windows clusters will cease functioning on this date. Rancher will be unable to publish any future versions of RKE1 with Windows Support for Docker EE. RKE2 is the only option for Windows customers who wish to stay in a supported configuration.</p> <p>Windows Server 2019 LTSC and Windows Server 2022 LTSC are the only supported versions of Windows for RKE2.</p> <p>Please, refer to the RKE2 support matrix for details:\u00a0https://www.suse.com/suse-rke2/support-matrix/all-supported-versions/rke2-v1-28/</p>"},{"location":"kbs/000020684/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020695/","title":"How to recover an RKE cluster when all control plane nodes have failed","text":"<p>This document (000020695) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020695/#environment","title":"Environment","text":"<p>A Rancher-provisioned RKE cluster</p>"},{"location":"kbs/000020695/#situation","title":"Situation","text":"<p>In a disaster recovery scenario, the control plane and etcd nodes managed by Rancher in a downstream cluster may no longer be available or functioning. The cluster can be rebuilt by adding control plane and etcd nodes again, followed by restoring from an available snapshot.</p>"},{"location":"kbs/000020695/#resolution","title":"Resolution","text":""},{"location":"kbs/000020695/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Nodes to add to the cluster with control plane and etcd roles with adequate resources</li> <li>An offline copy of a snapshot to be used as the recovery point, often stored in S3 or copied off node filesystems to a backup location</li> </ul> <p>Note: This article assumes that all control plane and etcd nodes are no longer functional and/or cannot be repaired via any other means, like a VM snapshot restore.</p>"},{"location":"kbs/000020695/#steps","title":"Steps","text":"<p>To recover the downstream cluster, any existing nodes with the control plane and/or etcd roles must be removed. Worker nodes can remain in the cluster, and these may continue to operate with running workloads.</p> <p>Please use the following steps as a guideline to recover the cluster, from this point the cluster that has experienced the disaster will be referred to as the downstream cluster.</p> <ol> <li>As a precaution, it's recommended to take a snapshot of the Rancher local cluster. Please see the documentation ( RKE, RKE2, K3s) for the appropriate way to take a snapshot for the Rancher installation.</li> </ol> <p>Alternatively the <code>rancher-backup</code> operator\u00a0can be used to backup all of the related objects for restoration.</p> <ol> <li>Delete all nodes with the control plane and/or etcd roles from the downstream cluster in the Rancher UI.</li> </ol> <p>The delete action can fail when the downstream cluster is in this condition, if nodes do not get removed, follow the below to remove it from the cluster:</p> <ol> <li>Click on the node and select <code>View in API</code>, click the delete button for the object</li> <li>If this does not succeed, using <code>kubectl</code> or the Cluster Explorer for the Rancher local cluster, edit the corresponding <code>nodes.management.cattle.io</code> object in the namespace that matches the downstream cluster ID to remove the <code>finalizers</code> field</li> </ol> <ol> <li>Add a clean node back to the cluster with the <code>all</code> role (control plane, etcd, worker). The IP address does not have to match any of the previous nodes. If the node has previously been used in a cluster, use the extended cleanup script steps to remove any previous configuration.</li> </ol> <p>The newly added node will fail to successfully register to the downstream cluster, it won't proceed past \"Waiting to register with Kubernetes\", this is normal.</p> <ol> <li>Copy the snapshot into place on the new node, under the <code>/opt/rke/etcd-snapshots</code> directory structure.</li> </ol> <p>The filename must match a snapshot name in the list of snapshots in the Rancher UI\u00a0for the downstream cluster, any snapshot should be usable, if the name is different, rename the file to match one of the known snapshots in the list.</p> <ol> <li> <p>Initiate a snapshot restore from Rancher UI\u00a0using the same snapshot name used in the previous step.</p> </li> <li> <p>Monitor the Rancher pod logs for progress.</p> </li> </ol> <p>To follow all pod logs at once, a kubeconfig for the Rancher local cluster can be used with this kubectl command:</p> <ul> <li><code>kubectl logs -n cattle-system -l app=rancher -f -c rancher</code></li> </ul> <ol> <li>Once the new node reaches the active state, check the cluster and add additional nodes by repeating step 3 when ready, the additional nodes can be added with only the control plane and etcd roles if desired.</li> </ol> <p>As a follow up, once all desired nodes are added and the cluster is healthy, the control plane and etcd node roles can be configured as needed. For example, if the <code>all</code> role is not needed, update the the node by removing and adding the node again in a rolling fashion .</p>"},{"location":"kbs/000020695/#additional-information","title":"Additional Information","text":"<ul> <li>Troubleshooting - Kubernetes Components</li> <li>Backing up a downstream cluster</li> </ul>"},{"location":"kbs/000020695/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020710/","title":"After Rancher 2.6.x upgrade, HTTP 403 Errors in Rancher UI","text":"<p>This document (000020710) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020710/#environment","title":"Environment","text":"<p>Several features of Rancher UI don't work and return HTTP 403 for some users after Rancher upgrade from 2.6.x :</p> <p>- Shell execution</p> <p>- Yaml editing</p>"},{"location":"kbs/000020710/#situation","title":"Situation","text":"<p>For some users, several Rancher features are not working and returning HTTP 403 (Forbidden)</p> <p>Rancher Trace log:</p> <pre><code>User-system-serviceaccount-cattle-impersonation-system-cattle-impersonation-u-vnds56pccy-cannot-impersonate-resource-users-in-API-group-at-the-cluster-scope-due-to-missing-clusterrolebinding\n</code></pre>"},{"location":"kbs/000020710/#resolution","title":"Resolution","text":"<p>1. Check RBAC Clusterroles and Clusterrolebindings of the affected user</p> <pre><code>## Clusterroles of the user\n$ kubectl get clusterrole | grep u-b3l74guter\n\n## Clusterrolebindings of the  user\n$ kubectl get clusterrolebinding | grep u-b3l74guter\n</code></pre> <p>2. From the previous output, the expected Clusterrole cattle-impersonation-u-xxxxxxxx is present, but the Clusterrolebinding is absent.</p> <p>3. Delete the cattle-impersonation-user-xxxx Clusterrole of the user</p> <pre><code>$ kubectl delete clusterrole\u00a0cattle-impersonation-u-b3l74guter\n</code></pre> <p>4. Trigger the recreation of the Clusterrole and Clusterrolebinding by browsing to a Rancher feature.</p> <p>e.g: open a Monitoring link in the cluster</p> <p>This action triggered the recreation of the Clusterrole and Clusterrolebinding</p>"},{"location":"kbs/000020710/#additional-information","title":"Additional Information","text":"<p>https://github.com/rancher/rancher/issues/33912</p>"},{"location":"kbs/000020710/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020712/","title":"Is it possible use mTLS for rancher agent connectivity?","text":"<p>This document (000020712) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020712/#resolution","title":"Resolution","text":"<p>It is not possible to configure mTLS authentication for the rancher-agent connectivity; however, the connection to Rancher is secured via TLS and the agents use a token to authenticate themselves to Rancher.</p>"},{"location":"kbs/000020712/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020713/","title":"DNS does not work with Weave CNI and Firewalld on RHEL 8 based OSs","text":"<p>This document (000020713) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020713/#environment","title":"Environment","text":"<p>Firewalld service blocks Rancher cluster DNS on Rhel8.</p> <p>Steps to reproduce:</p> <p>1. Install and setup RKE on RHEL 8</p> <p>2. CoreDNS is deployed as part of the RKE setup</p> <p>3. Start firewalld service on RHEL8 nodes.</p> <p>After starting firewalld service, k8s pod logs return connection error:</p> <pre><code>ent-041273.voicelab.local. A: read udp 172.21.0.19:58953-&gt;1.10.64.26:53: i/o timeout --------------\n</code></pre>"},{"location":"kbs/000020713/#situation","title":"Situation","text":"<p>The Internal Kubernetes DNS server (coredns) is blocked. Once firewalld is stopped, the Kubernetes DNS works.</p> <p>Firewalld blocks these ports that are required:</p> <p>-\u00a02379-2380/tcp</p> <p>-\u00a04789/udp</p> <p>-\u00a05000/tcp</p> <p>-\u00a06443/tcp</p> <p>-\u00a06783/tcp</p> <p>-\u00a06783-6784/udp</p> <p>-\u00a09100/tcp</p> <p>-\u00a010250/tcp</p> <p>-\u00a010257/tcp</p> <p>-\u00a010259/tcp</p>"},{"location":"kbs/000020713/#resolution","title":"Resolution","text":"<p>Stop firewalld on RHEL8 nodes, per requirements described in Rancher requirements.</p>"},{"location":"kbs/000020713/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020721/","title":"How to test Rancher RC/Alpha versions","text":"<p>This document (000020721) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020721/#environment","title":"Environment","text":"<p>Helm</p>"},{"location":"kbs/000020721/#situation","title":"Situation","text":"<p>By default, Helm only returns the final releases.</p> <p>This article provides details on how to install Rancher RC versions.</p>"},{"location":"kbs/000020721/#resolution","title":"Resolution","text":"<p>First, you need to add the Rancher 'latest' helm repository.</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n</code></pre> <p>Then you can list the current repositories on your configuration machine with</p> <pre><code>$ helm repo ls\nNAME            URL\ncoredns         https://coredns.github.io/helm\nrancher-charts  https://charts.rancher.io\nrancher-stable  https://releases.rancher.com/server-charts/stable\nrancher-latest  https://releases.rancher.com/server-charts/latest\n</code></pre> <p>You can list the final releases with</p> <pre><code>$ helm search repo \"rancher-latest\" --versions\nNAME                    CHART VERSION   APP VERSION DESCRIPTION\nrancher-latest/rancher  2.6.6           v2.6.6      Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.5           v2.6.5      Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.4           v2.6.4      Install Rancher Server to manage Kubernetes clu...\n[...]\nrancher-latest/rancher  2.0.4           v2.0.4      Install Rancher Server to manage Kubernetes clu...\n</code></pre> <p>To list the RC versions, you can use the --devel argument.</p> <pre><code>helm search repo \"rancher-latest\" --versions --devel\nNAME                    CHART VERSION       APP VERSION         DESCRIPTION\nrancher-latest/rancher  2.6.7-rc7           v2.6.7-rc7          Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.7-rc6           v2.6.7-rc6          Install Rancher Server to manage Kubernetes clu...\nrancher-latest/rancher  2.6.7-rc5           v2.6.7-rc5          Install Rancher Server to manage Kubernetes clu...\n[...]\nrancher-latest/rancher  2.0.4               v2.0.4              Install Rancher Server to manage Kubernetes clu...\n</code></pre> <p>The Rancher installation command line becomes</p> <pre><code>helm install rancher &lt;rancher-latest-repo&gt;\n  --devel\n  --version 2.6.7-rc1\n  --namespace cattle-system \\\n  --set hostname=rancher.my.org \\\n  --set replicas=3\n</code></pre>"},{"location":"kbs/000020721/#additional-information","title":"Additional Information","text":"<p>https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart</p>"},{"location":"kbs/000020721/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020724/","title":"How to set up ACE for HashiCorp Vault","text":"<p>This document (000020724) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020724/#environment","title":"Environment","text":"<p>Make sure you have a valid CA certificate;</p> <p>And a Fully Qualified Domain Name (FQDN)</p>"},{"location":"kbs/000020724/#resolution","title":"Resolution","text":"<ol> <li>First, we need to log into the Rancher UI.</li> <li>Next, navigate to the desired cluster and select \"Edit Config.\"</li> <li>Next, select \"Authorized Endpoint.\"</li> <li>Before moving forward, you must connect to your Kubernetes master node.</li> <li>From there, you will have to cd into /etc/kubernetes/ssl, copy kube-ca.pem file, navigate to the Rancher UI, and paste in the CA Certificate field.</li> <li>Next, verify the kube-ca.pem file.</li> <li>We won't be able to see the Network Load Balancer address</li> <li>Run this command to get the contents of your certificate:<ul> <li>openssl x509 -in kube-apiserver.pem -noout -text</li> </ul> </li> <li>Next, we need to add our ELB to our certificate, so select \"Edit as YAML\"</li> <li>Then click save</li> <li>Next, verify the kube-ca.pem file.</li> <li>Next, copy the . /kube/config file in the Rancher UI.</li> <li>Finally, we need to edit the last line (current-context: \"xxx-fqdn\")</li> <li>Then click save</li> </ol>"},{"location":"kbs/000020724/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020727/","title":"Rancher upgrade FAQ","text":"<p>This document (000020727) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020727/#environment","title":"Environment","text":"<p>As of writing this article, there are now two main branches that we will be working on. When you navigate to the rancher/rancher GitHub, you will see two branches: the latest branch and the stable branch. These two branches are now different, and when upgrading Rancher, we recommend using the stable branch as our team has not verified and certified the latest branch.</p>"},{"location":"kbs/000020727/#situation","title":"Situation","text":"<p>As we near the end of maintenance, end of life, and end of support for Rancher 2.6, we felt it pertinent to provide a one-stop FAQ page as customers begin to plan their upgrades.</p> <p>All upgrades should go from the latest to the latest. For instance, if you are on 2.6.10, you should go to 2.6.newest-stable and then to 2.7.newest-stable. The stop on 2.6.newest-stable should be about a week to ensure you can catch any issues before moving forward.</p> <p>After the upgrade to 2.7.newest-stable, you should then upgrade the underlying Kubernetes version to the latest supported version of the Rancher release. And again, the new Kubernetes version should be tested and verified for around one week. From there, progress to the next Rancher upgrade, test, and then Kubernetes.</p> <p>Many customers ask why we recommend one week. The 1-week recommendation is because this is often enough time for your clusters and app to be thoroughly tested and any issues flagged. We have seen customers who do less of a testing phase and only find issues when their cluster is being fully used and under normal \"strain.\"</p> <p>Here are some useful links for upgrades \u2014 see link (1) below for our team's general best practices around upgrade paths.</p> <p>During the course of your upgrade, see link (2) for how you and your team can bump the severity of this case if there is an impacting event during your upgrade. Doing so will notify our on-call engineer, who will engage as quickly as possible.</p> <p>Please review link (3) to verify that the Rancher and Kubernetes versions remain inline. It is best to ensure that any testing is done in a lower environment first so that you and your team can be more aware of any issues that the version jumps could have on your environment and applications.</p> <p>Lastly, for our team to better understand the updated environment, would you please run our system support script (4). This script will give our team a better understanding of the upgraded environment and the ability to call out items that may be out of our best practices.</p> <p>(1) https://www.suse.com/support/kb/doc/?id=000020061</p> <p>(2) https://www.suse.com/support/kb/doc/?id=000020296</p> <p>(3) https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/</p> <p>(4) https://www.suse.com/support/kb/doc/?id=000020192</p>"},{"location":"kbs/000020727/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020728/","title":"Collecte de journaux Linux Rancher v2.x","text":"<p>This document (000020728) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020728/#situation","title":"Situation","text":""},{"location":"kbs/000020728/#collecte-de-journaux-linux-rancher-v2x_1","title":"Collecte de journaux Linux Rancher v2.x","text":"<p>Les journaux peuvent \u00eatre collect\u00e9s \u00e0 partir d'un n\u0153ud Linux dans un cluster Rancher v2.x \u00e0 l'aide du script de collecte de journaux Rancher v2.x.</p> <p>Important:Ce script ne peut \u00eatre utilis\u00e9 que pour collecter des journaux \u00e0 partir de clusters provisionn\u00e9s par l'interface de ligne de commande Rancher Kubernetes Engine (RKE) , de clusters K3s, de clusters personnalis\u00e9s\u00a0provisionn\u00e9s par Rancher et de clusters provisionn\u00e9s par Rancher \u00e0 l'aide d'un pilote de n\u0153ud.</p> <p>Ce script n'est pas adapt\u00e9 \u00e0 la collecte de journaux \u00e0 partir de clusters de fournisseurs Kubernetes h\u00e9berg\u00e9s.</p> <p>Le script doit \u00eatre t\u00e9l\u00e9charg\u00e9 et ex\u00e9cut\u00e9 directement sur l'h\u00f4te en utilisant l'utilisateur root ou en utilisant sudo, comme suit:</p> <pre><code>wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s\n</code></pre> <p>Par d\u00e9faut, la sortie sera \u00e9crite dans /tmpdans un fichier tar gzipp\u00e9 nomm\u00e9 -.tar.gz"},{"location":"kbs/000020728/#choix","title":"Choix","text":"<p>Les indicateurs disponibles pouvant \u00eatre transmis au script se trouvent dans le script de collecteur de journaux Rancher v2.x README</p>"},{"location":"kbs/000020728/#disclaimer","title":"Disclaimer","text":"<p>Cette base de connaissances de support technique fournit un outil pr\u00e9cieux aux clients SUSE et autres parties int\u00e9ress\u00e9es par nos produits et solutions pour obtenir des informations, des id\u00e9es et apprendre r\u00e9ciproquement. Les documents sont fournis \u00e0 des fins d'information, personnelles ou non commerciales au sein de votre organisation et sont pr\u00e9sent\u00e9s \u00abEN L'\u00c9TAT \u00bb SANS GARANTIE D'AUCUNE SORTE.</p>"},{"location":"kbs/000020731/","title":"Tuning for nodes with a high number of CPUs allocated","text":"<p>This document (000020731) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020731/#environment","title":"Environment","text":"<p>An RKE2 or k3s cluster provisioned by Rancher or as a stand-alone cluster</p>"},{"location":"kbs/000020731/#situation","title":"Situation","text":"<p>Some components in a Kubernetes cluster apply a linear scaling mechanism, often based on the number of CPU cores allocated.</p> <p>For nodes that have a high number CPU cores allocated, the default scaling curve can be too steep and can introduce issues.</p> <p>Two common components are known to scale in this way: kube-proxy and ingress-nginx. However, additional workloads (such as nginx) may be deployed to the cluster and also need consideration.</p> <p>Adjusting the scaling for these components can be applied proactively or if there are indications of an issue.</p>"},{"location":"kbs/000020731/#resolution","title":"Resolution","text":""},{"location":"kbs/000020731/#kube-proxy","title":"kube-proxy","text":"<p>As explained in the Kubernetes GitHub issue here, the default scaling of the conntrack-max setting allocates 32K of memory per CPU core.</p> <p>On a node with a high number of CPU cores this can present events like the below in OS logs:</p> <pre><code>kernel: nf_conntrack: falling back to vmalloc.\n</code></pre> <p>This static default can present issues with contiguous memory being allocated for the conntrack table, or reach unnecessary levels of space allocated. When observed frequently, this has been associated with network instability.</p> <p>As a starting point, the suggestion is to halve this amount for a cluster with affected nodes:</p>"},{"location":"kbs/000020731/#rke2","title":"RKE2","text":"<p>Add the\u00a0kube-proxy-arg parameter to the cluster configuration.</p> <p>Provisioned by Rancher</p> <p>Click on Edit YAML in Cluster Management to make the change to a cluster, locate the machineGlobalConfig, and add the parameter, using the below as an example.</p> <pre><code>  rkeConfig:\n    machineGlobalConfig:\n        [ ... ]\n      kube-proxy-arg:\n        - conntrack-max-per-core=16384\n</code></pre> <p>Note: The change will be applied to all nodes in the cluster. The change can also be added only to certain nodes, by instead adding the parameter within a\u00a0machineSelectorConfig, more details are available in this documentation link</p> <p>Standalone cluster</p> <p>Edit the /etc/rancher/rke2/config.yaml file on each cluster node where the change is desired. Add the parameter and perform a restart of the appropriate rke2 service.</p> <pre><code>kube-proxy-arg:\n  - conntrack-max-per-core=16384\n</code></pre>"},{"location":"kbs/000020731/#k3s","title":"k3s","text":"<p>Add the kube-proxy-arg parameter to the cluster configuration.</p> <p>Provisioned by Rancher</p> <p>Click on Edit YAML in Cluster Management to make the change to a cluster, locate the machineGlobalConfig, and add the parameter, using the below as an example.</p> <pre><code>  rkeConfig:\n    machineGlobalConfig:\n        [ ... ]\n      kube-proxy-arg:\n        - conntrack-max-per-core=16384\n</code></pre> <p>Note: The change will be applied to all nodes in the cluster. The change can also be added only to certain nodes, by instead adding the parameter within a\u00a0machineSelectorConfig, more details are available in this documentation link</p> <p>Standalone cluster</p> <p>Edit the /etc/rancher/k3s/config.yaml file on each cluster node where the change is desired. Add the parameter and perform a restart of the appropriate k3s service.</p> <pre><code>kube-proxy-arg:\n  - conntrack-max-per-core=16384\n</code></pre>"},{"location":"kbs/000020731/#rke1","title":"RKE1","text":"<p>This can be done by editing the cluster as YAML, or the cluster.yml file when using the RKE CLI.</p> <pre><code>services:\n  kubeproxy:\n    extra_args:\n      conntrack-max-per-core: '16384'\n</code></pre> <p>Note: When using the RKE CLI, an rke up command will put the changes into effect.</p>"},{"location":"kbs/000020731/#ingress-nginx","title":"ingress-nginx","text":"<p>Modern versions of ingress-nginx perform detection to determine the\u00a0worker_processes based on the number of logical CPUs allocated to the node, or the CPU resource limit amount. By default CPU resource limits are not used and should generally be used only if unless necessary.</p> <p>On a node with a high number of CPUs allocated this can result in an undesirable number of PIDs and open files with the threads consumed (number of cores * 32 (default thread_pool size)).</p> <ul> <li>http://nginx.org/en/docs/ngx_core_module.html#worker_processes</li> <li>http://nginx.org/en/docs/ngx_core_module.html#thread_pool</li> </ul>"},{"location":"kbs/000020731/#rke2_1","title":"RKE2","text":"<p>Provisioned by Rancher</p> <p>Add a HelmChartConfig to adjust the ingress-nginx helm chart value, click on Edit Config &gt; Additional Manifest to add the HelmChartConfig in Cluster Management. An example of 8 worker_processes is used below.</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n\u00a0 name: rke2-ingress-nginx\n\u00a0 namespace: kube-system\nspec:\n\u00a0 valuesContent: |-\n\u00a0 \u00a0 controller:\n\u00a0 \u00a0 \u00a0 config:\n\u00a0 \u00a0 \u00a0 \u00a0 worker-processes: \"8\"\n</code></pre> <p>Standalone cluster</p> <p>Add the HelmChartConfig as a YAML file to the /var/lib/rancher/rke2/server/manifests directory on all rke2-server nodes.</p> <ul> <li>https://docs.rke2.io/networking/networking_services#nginx-ingress-controller</li> </ul>"},{"location":"kbs/000020731/#rke1_1","title":"RKE1","text":"<p>This can be adjusted by editing the cluster as YAML, or the cluster.yml file when using the RKE CLI. An example of 8 worker_processes is used below. For a nodes that may process a high amount of ingress traffic, you may wish to use a higher number.</p> <pre><code>ingress:\n  provider: nginx\n  options:\n    worker-processes: \"8\"\n</code></pre>"},{"location":"kbs/000020731/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020733/","title":"Reduce Memory and CPU footprint of Prometheus Monitoring Operator","text":"<p>This document (000020733) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020733/#environment","title":"Environment","text":"<p>rancher-monitoring chart: v100.1.3+up19.0.3</p>"},{"location":"kbs/000020733/#situation","title":"Situation","text":"<p>On a fresh install of rancher-monitoring, the Prometheus monitoring operator consumes high CPU and Memory resources without any custom Prometheus CRDs configured. You would notice the following messages on the Prometheus Operator Pod very frequently.</p> <pre><code>$ kubectl logs -n cattle-monitoring-system rancher-monitoring-operator-784c69bc54-dvndg -f\nlevel=info ts=2022-07-26T09:34:55.46606005Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\nlevel=info ts=2022-07-26T09:34:55.612269913Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:55.694485011Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:55.92009322Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\nlevel=info ts=2022-07-26T09:34:59.042606472Z caller=operator.go:747 component=alertmanageroperator msg=\"sync alertmanager\" key=cattle-monitoring-system/rancher-monitoring-alertmanager\nlevel=info ts=2022-07-26T09:34:59.043983987Z caller=operator.go:1224 component=prometheusoperator msg=\"sync prometheus\" key=cattle-monitoring-system/rancher-monitoring-prometheus\n</code></pre>"},{"location":"kbs/000020733/#resolution","title":"Resolution","text":"<p>Add SecretListWatchSelector to reduce memory and CPU footprint.</p> <pre><code>prometheusOperator:\n   secretFieldSelector: \"type!=helm.sh/release.v1\"\n</code></pre>"},{"location":"kbs/000020733/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020733/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020737/","title":"How to set up Alertmanager configs in Monitoring V2 in Rancher","text":"<p>This document (000020737) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020737/#environment","title":"Environment","text":"<p>Validated with Rancher Monitoring version v2 (v102.0.1+up40.1.2).</p> <p>It should work all all monitoring v2.</p>"},{"location":"kbs/000020737/#situation","title":"Situation","text":"<p>Before we get started:</p> <ul> <li>Admin permissions are needed</li> <li>Gather the required information for the alerting you'd like to set up</li> <li>Monitoring will need to be installed</li> </ul> <p>Please note that I will set up Slack alerts in this example</p> <ul> <li>To set up Slack, add and configure the Incoming webhooks app within your Slack environment</li> <li>Copy the Webhook URL and paste it into a blank notepad</li> </ul>"},{"location":"kbs/000020737/#resolution","title":"Resolution","text":"<ul> <li> <p>Create an Opaque Secret in the cattle-monitoring-system namespace</p> </li> <li> <p>Click on Projects/Namespaces</p> </li> <li>Select cattle-monitoring-system</li> <li>Then click on Secrets</li> <li>Then select Create</li> <li>Choose Opaque Secret</li> <li>Specify a key name (Insert name here for the secret)</li> <li>Then paste the Slack Webhook URL under Value</li> <li>Verify that the secret has been created successfully</li> <li> <p>Next, select the Monitoring tab on the left side</p> </li> <li> <p>Select Alerting</p> </li> <li>Then create a new AlertManagerConfig in the same namespace</li> <li>Add a name for this configuration</li> <li>Then select Create</li> <li>Once added, we will need to Edit Config</li> <li>From there, we can add a new Receiver(This is where we can specify which type of notifications to receive)</li> <li>For Slack, select Add Slack</li> <li>Under Secret with Slack Webhook URL, select the Secret name</li> <li>Then under the Key drop-down, we'll see the new, generated webhook secret key</li> <li>Next, specify the channel that the notifications will be sent to<ul> <li>(Optional) specify a proxy if applicable</li> </ul> </li> <li>Then select Create</li> <li>After creation, a Slack receiver will be shown, pointing to the webhook URL secret</li> <li>Next, edit the AlertManagerConfig again and point the base route of the config to the slack receiver</li> <li>Under the drop-down, our receiver will show up</li> <li>Select the Receiver</li> <li> <p>Specify Groupings and Matchers and set different intervals here</p> <ul> <li>For faster testing, change the default interval times to quicker times like 5 seconds, 10 seconds, 1 minute, etc.</li> <li>Then click Save</li> <li>Under status, now we see the resulting config in the Alertmanager section in the Rancher UI</li> <li>Then, if everything has been set up correctly, a notification should be sent to the desired Slack channel</li> </ul> </li> </ul>"},{"location":"kbs/000020737/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020750/","title":"Customize Helm Chart values for RKE2 default addons","text":"<p>This document (000020750) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020750/#environment","title":"Environment","text":"<p>Rancher 2.6/2.7/2.8</p> <p>RKE2 downstream cluster</p>"},{"location":"kbs/000020750/#situation","title":"Situation","text":"<p>By default, RKE2 installs multiple addons, including CoreDNS, Local-Storage, Nginx-Ingress, etc.:</p> <pre><code>kubectl -n kube-system get addons\n</code></pre> <p>Many of these addons are deployed from a Helm chart and represented within the cluster via a HelmChart custom resource:</p> <pre><code>kubectl -n kube-system get helmchart\n</code></pre> <p>These built-in addons deployed from a Helm chart can be customized with the use of a HelmChartConfig custom resource:</p> <pre><code>kubectl -n kube-system get helmchartconfig\n</code></pre> <p>This is where you can use the Helm chart values to change an addon's default installation.</p>"},{"location":"kbs/000020750/#resolution","title":"Resolution","text":"<p>To edit the values of a Helm chart, you must find the currently installed version. To do this navigate\u00a0to the RKE2 GitHub repository releases page to find the Packaged Component Versions (https://github.com/rancher/rke2/releases/) for the specific RKE release.</p> <p>For example, RKE2 1.23.10+rke2r1 uses ingress-nginx 4.1.0 (https://github.com/rancher/rke2/releases/tag/v1.23.10+rke2r1). Checking the values for this version of the ingress-nginx chart \u00a0within the ingress-nginx GitHub repository\u00a0you can determine the possible values</p> <p>(https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.1.0/charts/ingress-nginx/values.yaml).</p> <p>Thus, to add tolerations to the ingress-nginx controller, you can use the below manifest as an example. All of the values from the chart can be customised via this schema.</p> <pre><code>---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-ingress-nginx\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    controller:\n      tolerations:\n        - key: \"key\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n</code></pre> <p>After creating the HelmChartConfig manifest, you need to apply it via Rancher. To do so:</p> <p>1. Navigate to Cluster Management.</p> <p>2. On the selected cluster, click\u00a0Edit Config.</p> <p>3. Click on the Add-On Config tab and enter the manifest at the bottom in\u00a0Additional Manifest.</p> <p></p>"},{"location":"kbs/000020750/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020767/","title":"AKS Downstream Cluster not Available with Websockets failing","text":"<p>This document (000020767) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020767/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>AKS 1.22+</p>"},{"location":"kbs/000020767/#situation","title":"Situation","text":"<p>After upgrading AKS to version 1.22+ users may experience a situation where the Downstream clusters show as unavailable on Rancher.</p> <p>Testing the Websocket using these instructions will show the following error:</p> <pre><code>Bad Request\n{\"baseType\":\"error\",\"code\":\"ServerError\",\"message\":\"websocket: the client is not using the websocket protocol: 'upgrade' token not found in 'Connection' header\",\"status\":400,\"type\":\"error\"}\n</code></pre>"},{"location":"kbs/000020767/#resolution","title":"Resolution","text":"<p>Update the Kubernetes Ingress NGINX with the tag --set controller.watchIngressWithoutClass=true:</p> <pre><code>helm upgrade --install \\\n  ingress-nginx ingress-nginx/ingress-nginx \\\n  --namespace ingress-nginx \\\n  --set controller.service.type=LoadBalancer \\\n  --version 4.0.18 \\\n  --create-namespace \\\n  --set controller.watchIngressWithoutClass=true\n</code></pre> <p>Alternatively, on Rancher 2.6.7 onward, you can add the class name on the helm install/upgrade steps :</p> <pre><code>--set ingress.ingressClassName=nginx\n</code></pre>"},{"location":"kbs/000020767/#cause","title":"Cause","text":"<p>Kubernetes version 1.22 deprecated versions of the Ingress APIs in favor of the stable <code>networking.k8s.io/v1</code> API. That leads to this scenario, where we update the controller.watchIngressWithoutClass tag.</p>"},{"location":"kbs/000020767/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020786/","title":"Restore fails with metadata.deletionGracePeriodSeconds: Invalid value: 0: field is immutable","text":"<p>This document (000020786) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020786/#environment","title":"Environment","text":"<p>Rancher backup &lt;\u00a0 2.1.3</p>"},{"location":"kbs/000020786/#situation","title":"Situation","text":"<p>Restore of backup fails with error:</p> <pre><code>Error restoring namespaced resources [error restoring rancher-k3s-upgrader of type project.cattle.io/v3, Resource=apps: restoreResource: err updating resource App.project.cattle.io \"rancher-k3s-upgrader\" is invalid: metadata.deletionGracePeriodSeconds: Invalid value: 0: field is immutable]\n</code></pre>"},{"location":"kbs/000020786/#resolution","title":"Resolution","text":"<p>There are two ways to resolve this:</p> <p>1. Upgrade to Rancher-backup 2.1.3 or higher.</p> <p>2. Extract the backup archive, search for resources that have a deletingGracePeriod field. Remove these resources manually and package the archive again. You can either remove just the metadata field or remove the entire resource. It\u2019s a resource that\u2019s supposed to be deleted so will have no negative effects.</p>"},{"location":"kbs/000020786/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020788/","title":"How to clean the orphaned cluster objects from the deleted cluster namespaces.","text":"<p>This document (000020788) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020788/#environment","title":"Environment","text":"<p>Rancher 2.x</p>"},{"location":"kbs/000020788/#situation","title":"Situation","text":"<p>In some cases there may be orphaned cluster objects left behind after the in-proper deletion of a downstream cluster in Rancher. These orphaned objects could introduce a condition that causes the leader Rancher pod to enter a CrashLoop state.</p> <p>Examples of errors from the Rancher pod logs.</p> <pre><code>[ERROR] failed to call leader func: namespaces \"c-xxxxx\" not found\nfatal error: concurrent map read and map write\n</code></pre> <pre><code>[ERROR] error syncing \u2018c-xxxx/p-xxxx\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing\n[ERROR] error syncing \u2018c-xxxxx/p-xxxxx\u2019: handler mgmt-project-rbac-remove: failed to remove finalizer on controller.cattle.io/mgmt-project-rbac-remove, requeuing\n[ERROR] error syncing \u2018c-xxxxx/p-xxxxx\u2019: handler cluster-registration-token: clusters.management.cattle.io \"c-xxxxx\" not found, requeuing\n</code></pre>"},{"location":"kbs/000020788/#resolution","title":"Resolution","text":"<p>Find the objects under the deleted cluster namespaces and manually delete each objects. Make sure there are no such orphaned objects or namespaces left in the local cluster.</p> <p>1. Set a kubeconfig for the Rancher (local) management cluster to be used with the following steps</p> <p>2. Verify the Active downstream clusters</p> <pre><code>kubectl get clusters.management.cattle.io -o custom-columns=\"ID:.metadata.name,NAME:.spec.displayName,K8S_VERSION:.status.version.gitVersion,CREATED:.metadata.creationTimestamp,DELETED:.metadata.deletionTimestamp,LAST_READY:.status.conditions[?(@.type == 'Ready')].lastUpdateTime,READY:.status.conditions[?(@.type == 'Ready')].status\" --sort-by=.metadata.creationTimestamp\n</code></pre> <p>3. Cross verify with the Rancher pod logs to get the deleted downstream cluster namespace and collect the details. Compare with the active list of clusters versus the cluster namespaces.</p> <pre><code>kubectl logs -n cattle-system -l app=rancher -c rancher\n</code></pre> <pre><code>kubectl get ns -A |grep \"c-\"\n</code></pre> <p>4. If there is a cluster that is stuck deleting, this may not complete. In this case, the finalizer object can be removed from the cluster.management.cattle.io object. Please note the c-xxxxx needs to be replaced with the cluster ID that is stuck deleting.</p> <pre><code>kubectl patch clusters.management.cattle.io &lt;c-xxxxx&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre> <p>5. If there is a namespace for a cluster that no longer exists, get the orphaned object details under the deleted cluster namespace.</p> <pre><code>kubectl api-resources --verbs=list --namespaced -o name  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;c-xxxxx&gt;\n</code></pre> <p>6. Do the cleanup of orphaned objects.</p> <ul> <li>Create the cluster namespace which is deleted, ignore if the cluster namespace is present</li> </ul> <pre><code>kubectl create ns &lt;c-xxxxx&gt;\n</code></pre> <ul> <li>Check the objects detected (in step 5) if desired, each object should have a deletion timestamp if a finalizer is preventing the object from being deleted.</li> </ul> <pre><code>kubectl -n &lt;c-xxxxx&gt; get &lt;resource type&gt; &lt;name of object&gt; -o yaml\n</code></pre> <ul> <li>Remove the finalizer to unblock the deletion of the objects. The command needs to be run for each object.</li> </ul> <pre><code>kubectl -n &lt;c-xxxxx&gt; patch &lt;resource type&gt; &lt;name of object&gt; -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre> <ul> <li>Make sure there are no objects left in the namespace.</li> </ul> <pre><code>kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;c-xxxxx&gt;\n</code></pre> <ul> <li>Finally, delete the namespace.</li> </ul> <pre><code>kubectl delete ns &lt;c-xxxxx&gt;\n</code></pre>"},{"location":"kbs/000020788/#cause","title":"Cause","text":"<p>It is important to delete downstream clusters in a process to allow Rancher to delete clusters and clean nodes that are in an Active state.</p> <p>Downstream cluster deletion is ideally performed from the Rancher UI / API, where nodes are available and able to be gracefully removed. For example, where possible do not terminate nodes in the infrastructure before the deletion is completed.</p>"},{"location":"kbs/000020788/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020788/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020792/","title":"How to make a request with the Rancher2 Terraform Provider","text":"<p>This document (000020792) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020792/#environment","title":"Environment","text":"<ul> <li>Rancher v2.x</li> <li>An up-to-date version of Terraform</li> <li>An up-to-date version of the Rancher2 Terraform Provider</li> <li>A Rancher API Token for a user with Rancher admin permissions</li> </ul>"},{"location":"kbs/000020792/#situation","title":"Situation","text":"<p>Sometimes it is necessary to create a basic skeleton for beginning a task, like using the Rancher2 Terraform Provider to speak with the Rancher API.</p> <p>This represents a starting point to achieve a simple read-only task: \"query the cluster information for the local Rancher cluster\".</p>"},{"location":"kbs/000020792/#resolution","title":"Resolution","text":"<p>Terraform commands are very easy, below are the main options one may typically use.</p> <ul> <li>terraform init -- download needed files like the rancher2 terraform provider</li> <li>terraform plan -- compare the environment to the state file, plan is like a diff of any changes to be made</li> <li>terraform apply -- apply the planned changes</li> <li>terraform refresh -- update the state to match remote systems</li> <li>terraform output -- show output values from the main.tf plan</li> <li>terraform destroy -- clean up anything created by terraform</li> <li>terraform fmt -- spacing is important in HCL, terraform's language, use this command to format all spacing in the current working directory</li> </ul> <p>To get started, create a directory to hold all of the files. Terraform will examine the local file or files, and then populate a local terraform.tfstate data file which represents the most recent refresh of the information from the Rancher API.\u00a0 The files below can be separate or all together in a main.tf file.\u00a0 Separating plan files into individual pieces can make managing a larger project easier.</p> <p>Terraform will take actions required using variables supplied by the user or admin, or computed during the \"apply\" operation.\u00a0 As a typical rule of thumb for any provider, \"data\" sources are read operations while \"resource\" operations are write/create/change.</p> <p>Upon running \"terraform apply\" with the main.tf file below, terraform will contact the Rancher API, authenticate, request the cluster_info for the local Rancher cluster with ID \"local\" and store it into the terraform statefile, as well as output the cluster labels to the screen.\u00a0 The comments explain a potential name for each file, the only requirement that it ends with the file suffix \".tf\".</p> <pre><code>### tfvars.tf or environment.tf\n\n#  these outline the url speaking to, and the authorization token\n\nvariable \"api_url\" {\n  description = \"rancher api url\"\n  default     = \"https://urlto.rancher-fqdn.com\"\n}\n\nvariable \"token_key\" {\n  description = \"api key to use for tf\"\n  default     = \"token-nameid:jwt-long-hash-string\"\n}\n\n### providers.tf\n\n# use the variables from the earlier section to define the provider\n\nprovider \"rancher2\" {\n  api_url   = var.api_url\n  token_key = var.token_key\n  insecure  = true\n}\n\n### versions.tf\n\n# tell terraform what versions of providers and terraform itself, to expect\n\nterraform {\n  required_providers {\n    rancher2 = {\n      source  = \"rancher/rancher2\"\n      version = \"&gt;= 6.0.0\"\n    }\n  }\n  required_version = \"&gt;= 1.5.7\"\n}\n\n### main.tf\n\n## hard-coded example, read cluster info for local\n\ndata \"rancher2_cluster\" \"local\" {\n  name = \"local\"\n}\n\noutput \"cluster_labels\" {\n  value     = data.rancher2_cluster.local.labels\n}\n</code></pre>"},{"location":"kbs/000020792/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020792/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020803/","title":"Restore k3s from MySQL dump","text":"<p>This document (000020803) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020803/#environment","title":"Environment","text":"<p>k3s with an external database MySQL (this has been tested using Azure MySQL)</p>"},{"location":"kbs/000020803/#situation","title":"Situation","text":"<p>MySQL DB dump is available, and the cluster token from /var/lib/rancher/k3s/server/token</p>"},{"location":"kbs/000020803/#resolution","title":"Resolution","text":"<ol> <li>In the new MySQL instance create a database</li> <li>Here we set the Character set to latin1 and collation to latin1_swedish_ci, as the original DB</li> <li>we also chose the same name, as it is a new instance</li> <li>Restore the dump, you may use mysql db_name &lt; backup-file.sql</li> <li>on the first node start k3s with:</li> <li>curl -sfL https://get.k3s.io | sh -s - server --token  --datastore-endpoint=\"mysql://:@tcp(.mysql.database.azure.com:3306)/?tls=true\" <li>the token is retrieve from the failed cluster in /var/lib/rancher/k3s/server/token</li> <li>Remove the failed nodes running:</li> <li>k3s kubectl get nodes</li> <li>k3s kubectl delete nodes  <li>Join any additional node using the instructions from k3s documentation</li>"},{"location":"kbs/000020803/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020805/","title":"Rancher upgrade has failed with an error no matches for kind \"Issuer\" in version \"cert-manager.io/v1alpha2\"","text":"<p>This document (000020805) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020805/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p>"},{"location":"kbs/000020805/#situation","title":"Situation","text":"<p>Rancher upgrade is failing due to the deprecated apiVersion for the cert-manager CRD. This affects cert-manager upgrades from an earlier release, for example upgrading cert-manager from 0.12 to 1.7.1, which in turn has the potential to create a deprecated apiVersion within the existing Rancher release manifest.</p> <p>The relevant error message may appear as below and occurs when running the helm upgrade command to upgrade Rancher.</p> <pre><code>Error: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: resource mapping not found for name: \"rancher\" namespace: \"\" from \"\": no matches for kind \"Issuer\" in version \"cert-manager.io/v1alpha2\" ensure CRDs are installed first\n</code></pre>"},{"location":"kbs/000020805/#resolution","title":"Resolution","text":"<p>Follow the below steps to edit the latest Helm v3 config for Rancher, and replace cert-manager.io/v1alpha2 with cert-manager.io/v1.</p> <p>1. Execute the below command and locate the latest version of\u00a0sh.helm.release.v1.rancher.v*</p> <pre><code> kubectl get secrets -n cattle-system\n</code></pre> <p>2. Back up the object, this example assumes sh.helm.release.v1.rancher.v1 is the latest</p> <pre><code>kubectl get secret sh.helm.release.v1.rancher.v1 -n cattle-system -o yaml &gt; helm-rancher-config.yaml\n</code></pre> <p>3. Decode the data.release field and save the output to yaml (jq must be installed before executing the below steps)</p> <pre><code>kubectl get secrets sh.helm.release.v1.rancher.v1 -n cattle-system -o json | jq .data.release | tr -d '\"' | base64 -d | base64 -d | gzip -d &gt; helm-rancher-config-data-decoded.yaml\n</code></pre> <p>4. Change the apiVersion from v1/alpha2 to v1.</p> <pre><code>sed -e 's/cert-manager.io\\/v1alpha2/cert-manager.io\\/v1/' helm-rancher-config-data-decoded.yaml &gt; helm-rancher-config-data-decoded-replaced.yaml\n</code></pre> <p>5. Store the encoded data in a variable to reuse in the next step</p> <pre><code>releaseData=$(cat helm-rancher-config-data-decoded-replaced.yaml | gzip | base64 | base64 | tr -d \"\\n\")\n</code></pre> <p>6. Replace the release data</p> <pre><code>sed 's/^\\(\\s*release\\s*:\\s*\\).*/\\1'$releaseData'/' helm-rancher-config.yaml &gt; helm-rancher-config-final.yaml\n</code></pre> <p>7. Apply the yaml</p> <pre><code>kubectl apply -f helm-rancher-config-final.yaml -n cattle-system\n</code></pre>"},{"location":"kbs/000020805/#cause","title":"Cause","text":"<p>Old CRD's are not deleted properly after the upgrade of cert-manager, this may cause a deprecated apiVersion to be used in the Rancher release manifest.</p>"},{"location":"kbs/000020805/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020805/#additional-information","title":"Additional Information","text":"<p>The correct way of upgrading cert-manager is in the below link</p> <p>https://docs.ranchermanager.rancher.io/getting-started/installation-and-upgrade/resources/upgrade-cert-manager#option-a-upgrade-cert-manager-with-internet-access</p> <p>Below is a snippet of helm get manifest -n cattle-system rancher which uses old CRDs, and thus has deprecated apiVersions.</p> <pre><code>---\n# Source: rancher/templates/issuer-rancher.yaml\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: rancher\n  labels:\n    app: rancher\n    chart: rancher-2.6.6\n    heritage: Helm\n    release: rancher\nspec:\n  ca:\n    secretName: tls-rancher\n</code></pre> <p>As in the above, /v1apha2 is referenced, this version has been deprecated.</p> <p>Command to get the available apiVersion for cert-manager</p> <pre><code>kubectl get --raw /apis/cert-manager.io | jq .\n</code></pre>"},{"location":"kbs/000020805/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020811/","title":"How to set the metadata config using the CATTLE prefixed extra environment variable","text":"<p>This document (000020811) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020811/#environment","title":"Environment","text":"<p>Rancher v2.6.x, v2.7.x, v2.9.x and v2.10.x</p>"},{"location":"kbs/000020811/#situation","title":"Situation","text":"<p>Rancher integrates the RKE metadata feature\u00a0( <code>rke-metadata-config</code>) to automatically synchronize Kubernetes version metadata at regular intervals. This ensures that Rancher remains aligned with the latest Kubernetes patch versions without requiring a Rancher upgrade.</p> <p>The\u00a0rke-metadata-config\u00a0feature enhances flexibility by enabling the automatic synchronization of Kubernetes metadata, allowing users to provision clusters with the latest patch versions of Kubernetes without upgrading Rancher. However, upgrading to new minor versions still requires a Rancher upgrade to ensure full compatibility.</p> <p>If the\u00a0rke-metadata-config ``\u00a0value has been manually modified at any point, it will not auto-update during a Rancher upgrade, as per its design. This can lead to inconsistencies and potential disruptions in the managed RKE2 downstream clusters.</p> <p>In some air-gapped environments the RKE metadata setting can be modified to meet security and compliance concerns. In addition users will be able to manage these settings through the Rancher helm values file on installation and/or upgrades.</p>"},{"location":"kbs/000020811/#resolution","title":"Resolution","text":"<p>A) Method 1: Customizing the Metadata Settings via Rancher UI</p> <p>The metadata config setting can be found in the UI under Global Settings &gt; Settings &gt; rke-metadata-config.</p> <p>In an open environment (with internet access), the default metadata config for Rancher v2.5.x and 2.6.x is to pull/refresh the Kubernetes metadata via a JSON file in the Rancher Git repository.</p> <p>Note:\u00a0In air-gapped environments, this works differently, i.e. only those Kubernetes versions available and included at the time of the Rancher release will be selectable. To take advantage of later Kubernetes patch versions, the Kontainer Driver Metadata from the Kubernetes metadata repository will need to be mirrored in a location that is accessible to air-gapped installations of Rancher. The default rke-metadata-config URL can be modified to point to the local mirror. Updated system images will also be required.</p> <p>B) Method 2: Customizing the Metadata Settings via Helm</p> <p>Using Helm, the rke-metadata-config settings can be passed with --set, using the CATTLE_ prefixed extra environment variables.</p> <p>The example command shown below makes use of the current Rancher release values exported to a file, and the file is referenced during a Helm upgrade.</p> <pre><code>helm upgrade rancher rancher-stable/rancher   --namespace cattle-system  -f rancher-values.yaml --version &lt;version&gt; --set 'extraEnv[0].name=CATTLE_RKE_METADATA_CONFIG' --set 'extraEnv[0].value=\\{\\\"refresh-interval-minutes\\\":\\\"1450\"\\,\\\"url\"\\:\\\"https://releases.rancher.com/kontainer-driver-metadata/release-v2.6/data.json\\\"}'\n</code></pre> <p>Alternatively, it is also possible to supply the necessary arguments in the values file.</p> <pre><code>extraEnv:\n- name: CATTLE_RKE_METADATA_CONFIG\n  value: '{\"refresh-interval-minutes\":\"1450\",\"url\":\"https://releases.rancher.com/kontainer-driver-metadata/release-v2.6/data.json\"}'\n</code></pre> <p>Please see below example screenshot taken from the Global Settings &gt; Settings menu in the Rancher UI after applying the rke-metadata-config setting using Helm.</p> <p></p> <p>IMPORTANT NOTE:</p> <p>When upgrading Rancher to the next major/minor release (e.g 2.9.x or 210.x), please ensure that the RKE metadata setting matches the release in use, i.e. release-v2.9 or release-v2.10</p> <p>To ensure that Rancher uses the default\u00a0<code>rke-metadata-config</code> value \u2014 which is automatically updated during a Rancher upgrade\u2014\u00a0it is necessary to remove any manually set value in the Rancher local cluster. This helps prevent Kubernetes metadata (KDM) related issues during the upgrade process.</p> <p>To reset the configuration, you can use the UI or a valid\u00a0<code>kubeconfig</code>\u00a0for the Rancher cluster and execute the following command:</p> <pre><code>kubectl edit settings rke-metadata-config\n</code></pre> <p>``</p> <p>Then, set the\u00a0<code>value</code>\u00a0field to an empty string ( <code>\"\"</code>) to revert to the default configuration.</p>"},{"location":"kbs/000020811/#cause","title":"Cause","text":"<p>The\u00a0rke-metadata-config ``\u00a0value is unique to each Rancher version. If the Kubernetes metadata (KDM) version does not match the Rancher version, there is a risk of cluster failures due to inconsistencies between supported Kubernetes versions and system configurations.</p>"},{"location":"kbs/000020811/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020811/#additional-information","title":"Additional Information","text":"<p>Further reading</p> <p>Please refer to the following Rancher documentation pages to find out more information.</p> <ul> <li>Upgrading Kubernetes without Upgrading Rancher</li> <li>Setting Extra Environment Variables</li> <li>Air-Gapped Setups</li> </ul>"},{"location":"kbs/000020811/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020821/","title":"How to re-add a Master node to the RKE2 HA cluster once its removed from cluster.","text":"<p>This document (000020821) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020821/#environment","title":"Environment","text":"<p>RKE2</p>"},{"location":"kbs/000020821/#situation","title":"Situation","text":"<p>Failed to re-add one of the master nodes to the cluster after the node maintenance/OS repair</p>"},{"location":"kbs/000020821/#resolution","title":"Resolution","text":"<p>Once the Node is ready to rejoin the cluster after the repair, the below steps has to be performed on the node.</p> <p>1. Collect the token from the existing master node and adjust the config.yaml</p> <p>2. Make sure all the RKE2 processes are cleaned up on the deleted node.</p> <pre><code>cd /usr/local/bin\n./rke2-killall.sh\n</code></pre> <p>3. Run the command to do the db cleanup.</p> <pre><code>sudo rm -rf /var/lib/rancher/rke2/server/db\n</code></pre> <p>3. Start the RKE2 server using the binary.</p> <pre><code>sudo systemctl start rke2-server\n</code></pre>"},{"location":"kbs/000020821/#cause","title":"Cause","text":"<p>Due to the OS corruption, one of the Master node got removed from a running RKE2 cluster.</p>"},{"location":"kbs/000020821/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020821/#additional-information","title":"Additional Information","text":"<p>This fix can be applied on a RKE2 cluster which used the binary install method. It is not verified in the Rancher provisioned RKE2 clusters.</p> <p>The token can be retrieved from the running Master node if its not the pre-shared token.</p> <pre><code>cat /var/lib/rancher/rke2/server/node-token\n</code></pre> <p>The basic config.yaml looks like below</p> <pre><code>cat /etc/rancher/rke2/config.yaml\nserver: https://xxxxxxxsx:9345\ntoken: xxxxxxxxxx7e8068c9fe03ec::server:6cc0ffdd5a127be53031efea454xxxx\n</code></pre>"},{"location":"kbs/000020821/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020831/","title":"How to troubleshoot Overlay Network Connectivity issues","text":"<p>This document (000020831) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020831/#environment","title":"Environment","text":"<p>Rancher, Kubernetes</p>"},{"location":"kbs/000020831/#situation","title":"Situation","text":"<p>pod-to-pod communication not happening</p>"},{"location":"kbs/000020831/#resolution","title":"Resolution","text":"<p>Pod-to-Pod communication should depend on multiple factors. Mainly, network communication should be allowed between the nodes. The following checkpoints help us trace the root cause of the problem.</p> <ul> <li>Check that the ports for their overlay are open between nodes (if they have multiple subnets/VLANs/DCs); testing from just one node to nodes in the other network should be good enough,\u00a0e.g.,</li> </ul> <pre><code>`nc -uvz &lt;node IP&gt; 8472`\n</code></pre> <p>(if they use the canal, change the port as needed). Please refer to this article [https://rancher.com/docs/rancher/v2.6/en/installation/requirements/ports/#commonly-used-ports]</p> <ul> <li>Check the DNS from a test pod with suitable\u00a0tools (not busybox, it has nslookup issues), The `rancherlabs/swiss-army-knife` image is ideal\u00a0for this.</li> </ul> <p># Do this for all coredns pod IPs.</p> <pre><code>\u00a0 \u00a0`dig &lt;hostname&gt; @&lt;coredns pod IP&gt;`\n</code></pre> <p>#\u00a0\u00a0Use the same test pod to test their upstream nameservers (all 3, over a few retries),</p> <pre><code>   `dig &lt;hostname&gt; \u00a0@&lt;upstream ns IP&gt;`\n</code></pre> <p>Refer to this article [ https://docs.ranchermanager.rancher.io/v2.7/troubleshooting/other-troubleshooting-tips/dns ]</p> <p>[Note: \u00a0In an air-gap environment, Swiss-army-knife is not available. You can try a specific busy box image with network tools like busybox image v1.28.]</p> <ul> <li>Check whether the overlay network test is successful or not.</li> </ul> <p>The overlay network procedure tests the pod-to-pod connectivity between the nodes. Refer to this article. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0[https://docs.ranchermanager.rancher.io/v2.5/troubleshooting/other-troubleshooting-tips/networking#check-if-overlay-network-is-functioning-correctly]</p> <p>[Note: This overlay test performs the pod-to-pod communication using ICMP protocol, which means you will still see networking issues because TCP communication might be blocked even though the test passes. So you have to test with good network tools like NC and iperf.]</p> <ul> <li>Check the Infra VMS \u00a0known issues and ensure that overlay network ports are allowed at the switch level.</li> </ul> <p># \u00a0 , e.g., In the case of Vmware\u00a0vSphere version 6.7u2.</p> <p>1. \u00a0Change the VXLAN port to 8472 (when NSX is not used) or 4789 (when NSX is used)</p> <p>2. \u00a0Disable the VXLAN hardware offload feature on the VMXNET3 NIC (which the recent Linux driver version enabled by default.\u00a0 [https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer\u00a0PR 2766401 , https://github.com/projectcalico/calico/issues/4727\u00a0]</p>"},{"location":"kbs/000020831/#additional-information","title":"Additional Information","text":"<p>Reference Artiles&amp; Links:</p> <p>https://docs.vmware.com/en/VMware-vSphere/6.7/rn/esxi670-202111001.html -Refer\u00a0PR 2766401</p> <p>https://github.com/projectcalico/calico/issues/4727</p>"},{"location":"kbs/000020831/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020834/","title":"How to define additional static pods on an RKE2 cluster node","text":"<p>This document (000020834) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020834/#situation","title":"Situation","text":"<p>Is it possible to define additional static Pods to start on an RKE2 during RKE2 node initialization?</p>"},{"location":"kbs/000020834/#resolution","title":"Resolution","text":"<ul> <li>Yes, you can define additional static Pods on an RKE2 host by placing the manifests into the directory /var/lib/rancher/rke2/agent/pod-manifests/</li> <li>Any Pod manifests within this directory will be created by the kubelet, as static pods, during the RKE2 node agent startup.</li> </ul>"},{"location":"kbs/000020834/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020848/","title":"Rancher install fails in an airgap environment with an error: no matches for kind \"Issuer\" in version certmanager.k8s.io/v1alpha1","text":"<p>This document (000020848) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020848/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p>"},{"location":"kbs/000020848/#situation","title":"Situation","text":"<p>When installing Rancher in an airgap environment, an error is seen for the issuer CRD.</p> <p>The relevant error message may appear as below and occurs when running the kubectl -n cattle-system apply -R -f ./rancher command</p> <pre><code>clusterrolebinding.rbac.authorization.k8s.io/rancher unchanged\ndeployment.apps/rancher created\ningress.networking.k8s.io/rancher created\nservice/rancher created\nserviceaccount/rancher created\nerror: unable to recognize \"rancher/templates/issuer-rancher.yaml\": no matches for kind \"Issuer\" in version \"certmanager.k8s.io/v1alpha1\n</code></pre>"},{"location":"kbs/000020848/#resolution","title":"Resolution","text":"<p>While rendering the Rancher template, add to the helm template arguments to specify the exact Kubernetes version you are using with --kube-version.</p> <pre><code>helm template rancher ./rancher-2.6.6.tgz --output-dir . --no-hooks --namespace cattle-system --set hostname=rancherreg.support.rancher.space --set rancherImage=registry.example.com:443/rancher/rancher --set systemDefaultRegistry=registry.example.com:443\n--set useBundledSystemChart=true --set certmanager.version=1.7.1 --kube-version=1.22.9\n</code></pre> <p>Then deploy the Rancher manifest files</p> <pre><code>kubectl -n cattle-system apply -R -f ./rancher\n</code></pre>"},{"location":"kbs/000020848/#cause","title":"Cause","text":"<p>When the helm template runs, it does not interact with the cluster to determine available CRDs and apiVersions. This in turn means the logic does not select the correct apiVersion for the Issuer object in the rendered template, applying the manifest in this case presents the apiVersion error.</p> <p>Below is the snippet of issuer-rancher.yaml file which has the deprecated apiVersion</p> <pre><code>cat \"rancher/templates/issuer-rancher.yaml\"\n---\n# Source: rancher/templates/issuer-rancher.yaml\napiVersion: certmanager.k8s.io/v1alpha1\nkind: Issuer\nmetadata:\n  name: rancher\n  labels:\n    app: rancher\n    chart: rancher-2.6.6\n    heritage: Helm\n    release: rancher\nspec:\n  ca:\n    secretName: tls-rancher\n</code></pre>"},{"location":"kbs/000020848/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020848/#additional-information","title":"Additional Information","text":"<p>Helm chart - Issuer object</p>"},{"location":"kbs/000020848/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020895/","title":"Adding tolerations to components in the Rancher Logging chart","text":"<p>This document (000020895) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020895/#environment","title":"Environment","text":"<p>- SUSE Rancher 2.7.x</p> <p>- SUSE Rancher 2.8.x</p> <p>- Rancher Logging Chart</p> <p>- Taints defined on cluster nodes</p>"},{"location":"kbs/000020895/#situation","title":"Situation","text":"<p>The \"fluentbit\" pods are not be present on all nodes, and logs are not collected.</p> <p>When deploying the Rancher logging chart, Pods will not be scheduled where user-added taints\u00a0are present on cluster nodes if\u00a0tolerations are not set on the rancher-logging chart. If only some nodes within the cluster are tainted, logs will be missing from those nodes. If all nodes are tainted, no logs will be forwarded and Pods for Deployments within the Rancher Logging will fail to schedule with an error of the following format</p> <pre><code>Events:\nType Reason Age From Message\n---- ------ ---- ---- -------\nWarning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate\n</code></pre>"},{"location":"kbs/000020895/#resolution","title":"Resolution","text":"<p>If there are user-added\u00a0taints on nodes within the cluster, tolerations for these taints must be added in the rancher-logging chart via the tolerations value, alongside the default\u00a0cattle.io/os=linux\u00a0NoSchedule toleration.</p> <p>For example, if the taint with key=foo, value=bar and effect=NoSchedule is present on nodes within the cluster, the following tolerations should be defined in the values of the rancher-logging Chart:</p> <pre><code>tolerations:\n  - key: cattle.io/os\n    operator: \"Equal\"\n    value: \"linux\"\n    effect: NoSchedule\n\u00a0 - key: foo\n\u00a0   operator: \"Equal\"\n\u00a0   value: \"bar\"\n\u00a0   effect: NoSchedule\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020895/#cause","title":"Cause","text":"<p>Nodes with specific taints will deny the scheduling of any pod if no matching toleration is present on the workload.</p>"},{"location":"kbs/000020895/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020896/","title":"Unable to see Rancher GUI, getting 500, 502, 503","text":"<p>This document (000020896) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020896/#environment","title":"Environment","text":"<p>A Kubernetes cluster with Rancher installed.</p>"},{"location":"kbs/000020896/#situation","title":"Situation","text":"<p>When attempting to navigate to the Rancher UI, cannot see the UI and instead are receiving a 500, 502,503 error.</p>"},{"location":"kbs/000020896/#cause","title":"Cause","text":"<p>For HTTP, the 5XX errors (500-511) are server-side errors meaning that the service itself is not responding to queries. The most common errors seen when trying to navigate to Rancher and seeing a 5XX error are:</p> <ul> <li>500 Internal Server Error - Generic error that something is wrong with the server</li> <li>502 Bad Gateway - Upstream server provided an invalid response</li> <li>503 Service Unavailable - The server cannot handle the request</li> </ul> <p>Generally speaking, these errors are seen due to either a problem with the ingress or the Rancher pods themselves.</p> <p>A 5XX error can be corrected by trying to redeploy the Rancher pods, but we should first identified the root cause.</p> <p>To begin troubleshooting these errors, check that the Rancher pods are running.</p> <p>If they are all running, check that there are no errors in the Rancher logs.</p> <p>If everything with the Rancher pods seems healthy, check that your ingress controller pods are also running and not posting errors.</p> <p>To finish, check your LoadBalancer if any for any logs or any connectivity issues with Rancher nodes.</p>"},{"location":"kbs/000020896/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020904/","title":"Is it possible to change the CNI for a rke/rke2/k3s cluster or Rancher launched cluster post-provisioning?","text":"<p>This document (000020904) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020904/#environment","title":"Environment","text":"<p>RKE1/RKE2/K3s Clusters</p> <p>Any cluster provisioned by Rancher</p>"},{"location":"kbs/000020904/#situation","title":"Situation","text":"<p>The CNI\u00a0for a cluster can be specified in the cluster configuration\u00a0 when launching a Kubernetes cluster via both the CLI and Rancher v2.x. Is it possible to change the CNI after the cluster has been provisioned?</p>"},{"location":"kbs/000020904/#resolution","title":"Resolution","text":"<p>Changing the cluster CNI after the cluster has been provisioned is not supported and care should be taken to set these as required when first configuring the cluster.</p>"},{"location":"kbs/000020904/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020904/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020907/","title":"Cannot provision new RKE Cluster from Template: Unable to validate S3 backup target configuration","text":"<p>This document (000020907) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020907/#environment","title":"Environment","text":"<ul> <li>Suse Rancher 2.5.9</li> <li>Air-gapped environment</li> <li>1 or more RKE downstream clusters</li> <li>RKE template configured for the downstream clusters</li> </ul>"},{"location":"kbs/000020907/#situation","title":"Situation","text":""},{"location":"kbs/000020907/#the-provisioning-of-an-rke-downstream-cluster-fails-after-adding-extra_args-for-the-kubelet-service-to-a-new-rke-template-and-using-this-newly-created-template-to-provision-the-rke-cluster","title":"The provisioning of an RKE downstream cluster fails after adding extra_args for the kubelet service to a new RKE template and using this newly created template to provision the RKE cluster.","text":"<p>Error:</p> <pre><code>Error message while creating a new RKE cluster from the new RKE Template:\nUnable to validate S3 backup target configuration: Get\u00a0http://169.254.169.254/latest/meta-data/iam/security-credentials/\": dial tcp 169.254.169.254:80: i/o timeout\n</code></pre> <p>Configuration:</p> <pre><code>E.g. extra_args for the kubelet added to the RKE Template:\n\n    kubelet:\n      ...\n      extra_args:\n        ...\n        image-gc-high-threshold: '80'\n        image-gc-low-threshold: '75'\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020907/#resolution","title":"Resolution","text":"<p>Verify if the RKE Template has configurable questions:</p> <ul> <li>Check in\u00a0the RKE Template if there is any setting set as a configurable question for the end-user</li> <li>Go to Cluster Management =&gt; RKE1 Configuration =&gt; RKE Templates</li> <li>Select your desired RKE Template, and click on the 3-dot menu far on the right menu, select \"Clone revision\"</li> <li>Go to the \"Cluster Option Overrides\" section lower down on the cloned template, and uncheck any available options.\u00a0After removing these configurable questions,\u00a0\u00a0the value set in the template is used without user entry.</li> <li>Set a Name for the new Template and click on Save. Select the newly created template and set it as Default Template if this is going to be your default template.</li> </ul>"},{"location":"kbs/000020907/#cause","title":"Cause","text":"<p>Configurable questions on the RKE template would require the end-user to answer those questions to set up a new RKE cluster and won't allow a new RKE cluster to be deployed automatically.</p>"},{"location":"kbs/000020907/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020909/","title":"How to enable fluent-bit debug logging","text":"<p>This document (000020909) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020909/#environment","title":"Environment","text":"<p>Rancher with Rancher-Logging chart v2</p>"},{"location":"kbs/000020909/#situation","title":"Situation","text":"<p>When troubleshooting the fluent-bit log level might need to be increased to debug level.</p>"},{"location":"kbs/000020909/#resolution","title":"Resolution","text":"<p>Fluent-bit debug level logging can be changed by modifying the logLevel spec of the logging CRD</p> <p>The following steps will help enable or disable debug-level logging for fluent-bit:</p> <p>1.\u00a0Edit rancher logging CRD</p> <pre><code>&gt; kubectl edit logging rancher-logging-root\n</code></pre> <p>2. Add logLevel debug to the fluentbit spec</p> <pre><code>spec:\n  controlNamespace: cattle-logging-system\n  fluentbit:\n    image:\n      repository: rancher/mirrored-fluent-fluent-bit\n      tag: 1.9.3\n    logLevel: debug\n</code></pre> <p>3. Confirm log level is set to debug</p> <pre><code>&gt; kubectl get pods -n cattle-logging-system -l app.kubernetes.io/name=fluentbit\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0READY \u00a0 STATUS \u00a0 \u00a0RESTARTS \u00a0 AGE\nrancher-logging-root-fluentbit-gqq8l \u00a01/1 \u00a0 \u00a0 Running \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a019s\n\n&gt; kubectl logs --tail=10 -n cattle-logging-system rancher-logging-root-fluentbit-gqq8l\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1841\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=927\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1840\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=1840\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=927\n[2023/01/13 09:44:18] [debug] [input:tail:tail.0] inode=2594165 events: IN_MODIFY\n[2023/01/13 09:44:18] [debug] [input chunk] update output instances with new chunk size diff=913\n</code></pre> <p>Notes:</p> <ul> <li>To reset the logging level to the default info, re-run the same steps and change the logLevel to info.</li> <li>The logLevel value is not exposed via the rancher-logging chart and cannot be configured persistently. As a result, the\u00a0change\u00a0will be overwritten and logLevel reset to the default of info after a rancher-logging chart\u00a0upgrade.</li> </ul> <p>Reference:</p> <ul> <li>https://banzaicloud.com/docs/one-eye/logging-operator/configuration/crds/v1beta1/fluentbit_types/#fluentbitspec-loglevel</li> </ul>"},{"location":"kbs/000020909/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020910/","title":"Downstream clusters in unavailable state after upgrade from Rancher v2.5 at v2.5.16 or above to Rancher v2.6 below v2.6.7","text":"<p>This document (000020910) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020910/#environment","title":"Environment","text":"<p>Rancher v2.5 at or above patch release v2.5.16 upgraded to Rancher v2.6 below patch release v2.6.7</p>"},{"location":"kbs/000020910/#situation","title":"Situation","text":"<p>After the upgrade of a Rancher v2.5 environment running patch release v2.5.16 or above, to Rancher v2.6 below patch release v2.6.7 (e.g. an upgrade from Rancher v2.5.16 to v2.6.6) downstream clusters are in an unavailable state within Rancher.</p> <p>Rancher Pod logs contain error messages of the following format:</p> <pre><code>2022/12/17 08:47:18 [ERROR] error syncing 'c-ayhjd': handler cluster-deploy: cluster context c-ayhjd is unavaiblable, requeuing\n2022/12/17 08:47:25 [ERROR] error syncing '_all_': handler user-controllers-controller: failed to start user controllers for cluster c-ayhjd: ClusterUnavailable 503: cluster not found\n</code></pre>"},{"location":"kbs/000020910/#resolution","title":"Resolution","text":"<p>To resolve the issue it is necessary to set the status.serviceAccountToken field on the cluster.management.cattle.io object for each downstream cluster from the service account token secret for the cluster. This can be done with the following BASH one-liner, with a kubeconfig sourced for the Rancher local cluster:</p> <pre><code>for cluster in $(kubectl get clusters.management.cattle.io --field-selector metadata.name!=local -o custom-columns=NAME:.metadata.name --no-headers); do echo $cluster; kubectl patch -v=9 cluster.management.cattle.io $cluster --type=merge -p \"{\\\"status\\\":{\\\"serviceAccountToken\\\":\\\"`kubectl -n cattle-global-data get secret -o jsonpath=\\\"{.items[?(@.metadata.ownerReferences[0].name==\\\\\"$cluster\\\\\")].data.credential}\\\"|base64 -d`\\\"}}\"; done\n</code></pre> <p>Next, edit the cluster.management.cattle.io resource in the Rancher local cluster, for each downstream cluster, to set the status of the ServiceAccountMigrated condition from True to Unknown. This action is taken to ensure that on upgrade to Rancher v2.6.7+ the secretAccountToken field is again removed and migrated to a secret. With a kubeconfig sourced for the Rancher local cluster, get the cluster IDs for all downstream clusters:</p> <pre><code>kubectl get clusters.management.cattle.io --field-selector metadata.name!=local -o custom-columns=NAME:.metadata.name --no-headers\n</code></pre> <p>One at a time for each cluster ID listed execute `kubectl edit cluster.management.cattle.io ` locate the condition with the type ServiceAccountMigrated in the status.conditions array, and update the status from \"True\" to \"Unknown\" per the following example: <pre><code>[...]\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:57Z\"\n\u00a0 \u00a0 status: \"True\"\n\u00a0 \u00a0 type: Updated\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:51Z\"\n\u00a0 \u00a0 status: \"Unknown\"\n\u00a0 \u00a0 type: ServiceAccountMigrated\n\u00a0 - lastUpdateTime: \"2023-01-04T12:11:57Z\"\n\u00a0 \u00a0 status: \"True\"\n\u00a0 \u00a0 type: GlobalAdminsSynced\n\u00a0 - lastUpdateTime: \"2023-01-04T12:17:40Z\"\n\u00a0 [...]\n</code></pre> <p>Finally, take a copy of the service account token secrets and then remove these, as they are no longer used and fresh secrets will be created upon upgrade to Rancher v2.6.7+.</p> <p>With a kubeconfig for the Rancher local cluster sourced, first take a copy of the service account token secret manifests, with tthe following bash one-liner:</p> <pre><code>for secret in `kubectl -n cattle-global-data get secrets -o name | grep \"cluster-serviceaccounttoken-\"`; do kubectl -n cattle-global-data get $secret -o yaml &gt;&gt; cluster-serviceaccounttoken-secrets.yaml; echo \"---\" &gt;&gt; cluster-serviceaccounttoken-secrets.yaml; done\n</code></pre> <p>Then with the Rancher local cluster kubeconfig still sourced, delete the secrets:</p> <pre><code>for secret in `kubectl -n cattle-global-data get secrets -o name | grep \"cluster-serviceaccounttoken-\"`; do kubectl -n cattle-global-data delete $secret; done\n</code></pre>"},{"location":"kbs/000020910/#cause","title":"Cause","text":"<p>In order to address CVE-2021-36782 the service account token used by Rancher to connect to the Kubernetes API Server of a downstream cluster was moved from the status.serviceAccountToken field of the cluster.management.cattle.io resource to a secret referenced by the status.serviceAccountTokenSecret field. This fix was introduced to Rancher v2.6 in patch release v2.6.7 and above; and to Rancher v2.5 in patch release v2.5.16 and above. Rancher versions v2.5.0 - v2.5.15 and v2.6.0 - v2.6.6 inclusive use the status.serviceAccountToken field to store and retrieve the service account token for downstream clusters.</p> <p>As a result, where a Rancher environment is upgraded from Rancher v2.5 at v2.5.16 or above (containing the fix), to Rancher v2.6 below patch release v2.6.7 (which does not contain the fix), the status.serviceAccountToken field will be missing from the cluster.management.cattle.io resource and Rancher will be unable to connect to existing downstream clusters.</p>"},{"location":"kbs/000020910/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020917/","title":"How to fix Azure AD authenication errors when upgrading to new graph endpoint in Rancher v2.6.7+","text":"<p>This document (000020917) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020917/#environment","title":"Environment","text":"<p>If you are using Rancher v2.6.7 and above and wish to set up Azure AD initially, please review our documentation here and enable Azure AD as your primary authentication method.</p> <p>For those using Rancher version(s) v2.6.6 and below, please refer to the following messages under the v2.6.0-v2.6.6 section on our documentation page here.</p>"},{"location":"kbs/000020917/#situation","title":"Situation","text":"<p>Disclaimer:</p> <p>There are a few issues that you may encounter when upgrading to the latest Microsoft Graph endpoint when using Azure AD. This KB Article aims to address these particular errors and provide the best solution for each scenario.</p> <p>If you are experiencing one of the following error messages, please continue to the corresponding numerical value.</p> <ol> <li><code>server error while authenticating: missing required permissions from Microsoft Graph: need Group.Read.All, User.Read.All</code></li> <li><code>\"refusing to set principal on user that is already bound to another user\"</code></li> <li>``` Error:AADSTS9000411: The request is not properly formatted.</li> </ol> <pre><code>4. ```\nError during login \"AADSTS901002: The 'resource' request parameter is not supported\"\n</code></pre>"},{"location":"kbs/000020917/#resolution","title":"Resolution","text":"<ol> <li> <p>If you are receiving this error, you likely have the incorrect type of permissions in the Azure console. When setting up Azure AD, you will need Application Permissions, NOT Delegated Permissions. For more information, please review our documentation here.</p> </li> <li> <p>If you are receiving this error in the Rancher UI, likely, you are using a different user that initially set up Azure AD to make modifications. For example, suppose you are logged in as an Azure AD user and try to disable/re-enable the authorization provider. In that case, it is likely, the local Rancher admin had initially set up the authentication provider and is bound to that admin user. So when trying to re-enable Azure AD as a local user and missing the correct permissions, you'll likely run into this error. Rancher will be aware of this user, and there is a link between the Azure Ad user and the Rancher user. There are a few solutions to this:</p> </li> <li> <p>In the Users &amp; Authentication section in the Rancher UI, as a local admin, you can grant the Azure AD user Configure Authentication and Manage Users permissions. Doing this should allow the Azure AD user to make changes to the authentication provider and should be able to re-enable it.</p> </li> <li> <p>Another way you can fix this issue is by enabling Azure AD with an Azure AD user unknown to Rancher.</p> </li> <li> <p>If you are receiving this Error: AADSTS9000411: The request is not properly formatted. The parameter 'response_type' is duplicated; Rancher is likely trying to send multiple requests simultaneously. You will want to verify that you are logging into Rancher with the correct URL; the URL will be https://rancher_url/dashboard.</p> </li> <li> <p>If you are receiving this error Error during login \"AADSTS901002: The 'resource' request parameter is not supported\" and are running on Rancher 2.6.10 or below, the error is likely due to Conditional access policies set up in your organization. To fix this issue, upgrade to Rancher 2.6.11 or above in the 2.6.x version, or to 2.7.0 or above in the 2.7.x versions. You will also need to change the Azure AD application permissions from user.read.all and group.read.all permissions to <code>directory.read.all.</code></p> </li> </ol>"},{"location":"kbs/000020917/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020946/","title":"How to configure rancher-logging to send to a file","text":"<p>This document (000020946) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020946/#environment","title":"Environment","text":"<p>Rancher 2.9.x+</p> <p>Logging Operator V2</p>"},{"location":"kbs/000020946/#situation","title":"Situation","text":"<p>The rancher-logging operator includes an option to send logs to a file. While this approach is not ideal for production environments where logs should be shipped to a centralized location for storage and further processing. It can be a valuable tool for testing purposes to ensure that the operator is properly configured.</p>"},{"location":"kbs/000020946/#resolution","title":"Resolution","text":"<p>Below is a simple way to configure logs to be sent to a file on the node. An output YAML would look like this:</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: Output\nmetadata:\n  name: fileoutput\n  namespace: default\nspec:\n  file:\n    path: /tmp/${tag}\n</code></pre> <p>Note the ${tag} on the path, required by Fluentd.</p> <p>In this case, the logs will be available on the /tmp folder of the Pod rancher-logging-root-fluentd-0</p> <p>If logs are not written to the /tmp folder, open a case with us to help you.</p>"},{"location":"kbs/000020946/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020949/","title":"RKE2 cluster provisioning in Rancher with profile: cis-1.6, requires parameter protect-kernel-defaults to true","text":"<p>This document (000020949) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020949/#environment","title":"Environment","text":"<p>Rancher 2.6</p>"},{"location":"kbs/000020949/#situation","title":"Situation","text":"<p>When provisioning a new custom RKE2 cluster with Worker CIS Profile 1.6 from Rancher UI, if\u00a0 the parameter\u00a0 \"protect-kernel-defaults\"\u00a0 is not set to \"true\", the RKE2 server will exit with error:</p> <pre><code>RKE2 server error log\n\n#journalctl -fu rke2-server\nStarting Rancher Kubernetes Engine v2 (server)...\nsh[26475]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service\nsh[26475]: /bin/sh: 1: /usr/bin/systemctl: not found\nrke2[26486]: time=\"2023-01-23T12:11:54Z\" level=fatal msg=\"--protect-kernel-defaults must be true when using --profile=cis-1.6\"\nJsystemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE\n systemd[1]: rke2-server.service: Failed with result 'exit-code'\n</code></pre>"},{"location":"kbs/000020949/#resolution","title":"Resolution","text":""},{"location":"kbs/000020949/#how-to-set-flag-protect-kernel-defaults","title":"How to set flag\u00a0<code>protect-kernel-defaults?</code>","text":"<p>When provisioning the cluster, the \"protect-kernel-default\" can be set in the\u00a0\u00a0Advanced\u00a0section under Cluster Configuration.</p> <ol> <li>Click\u00a0\u2630 &gt; Cluster Management</li> <li>On the\u00a0Clusters\u00a0page, click\u00a0Create</li> <li>Toggle the switch to\u00a0RKE2/K3s</li> <li>Custom</li> <li>Cluster Configuration ==&gt; Advanced</li> <li>Click the checkbox</li> </ol> <pre><code>Raise error if kernel parameters are different than the expected kubelet defaults\n</code></pre>"},{"location":"kbs/000020949/#cause","title":"Cause","text":"<p>When\u00a0 RKE2 starts with the \"profile\" flag set to cis-1.6, \" <code>protect-kernel-defaults\"</code>\u00a0is exposed as a configuration flag for RKE2. This flag has to be set to \"true\" when provisioning the cluster.</p>"},{"location":"kbs/000020949/#additional-information","title":"Additional Information","text":"<p>RKE2 is designed to be \"hardened by default\" and pass the majority of the Kubernetes CIS controls without modification. There are a few notable exceptions to this that require manual intervention to fully pass the CIS Benchmark.</p> <p>CIS Hardening Guide</p> <ul> <li> <p>Host-level requirements</p> </li> <li> <p>RKE2 CIS v1.6 Benchmark - Self-Assessment Guide - Rancher v2.6</p> </li> </ul>"},{"location":"kbs/000020949/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020950/","title":"Failed to handling tunnel request from remote address x.x.x.x:42412 (X-Forwarded-For: x.x.x.x): response 400: cluster not found","text":"<p>This document (000020950) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020950/#environment","title":"Environment","text":"<ul> <li>Rancher v2.x</li> </ul>"},{"location":"kbs/000020950/#situation","title":"Situation","text":"<p>There are a lot of errors observed in the Rancher pods:</p> <pre><code>[ERROR] Failed to handling tunnel request from remote address x.x.x.x:42412 (X-Forwarded-For: x.x.x.x): response 400: cluster not found\n</code></pre>"},{"location":"kbs/000020950/#resolution","title":"Resolution","text":"<p>The error message \" Failed to handling tunnel request from remote address x.x.x.x:42412 (X-Forwarded-For: x.x.x.x): response 400: cluster not found\"\u00a0 indicates that on some hosts\u00a0from now-deleted clusters, there are Rancher agent containers/Pods that are still running and attempting to connect to Rancher.</p> <p>To locate these instances and stop the running containers or hosts to prevent these messages:</p> <p>Suppose your load-balancer is performing Layer-7 load-balancing and setting the X-Forwarded-For header itself. In that case, you could enable use-forwarded-headers on the ingress-nginx controller in the Rancher local cluster ( https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-forwarded-headers).</p> <p>This would pass through the X-Forwarded-For header from the loadbalancer, enabling you to identify the hosts from which these requests originate within the Rancher Pod logs.</p>"},{"location":"kbs/000020950/#cause","title":"Cause","text":"<p>Remaining nodes from deleted clusters trying to connect to Rancher.</p>"},{"location":"kbs/000020950/#additional-information","title":"Additional Information","text":"<p>The reason for seeing\u00a0 IPs in the range 10.42.x.x in the remote address and your load-balancer IPs in the X-Forwarded-For on the logs is that:</p> <ol> <li>the remote address\u00a0is the ingress-nginx Pod IP</li> <li>the X-Forwarded-For address is the load-balancer forwarding the requests to the cluster.</li> </ol>"},{"location":"kbs/000020950/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020951/","title":"Adding missing cAdvisor labels to k8s 1.24+","text":"<p>This document (000020951) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020951/#environment","title":"Environment","text":"<p>RKE1 on k8s 1.24+, Rancher 2.6.8+ or 2.7.0+</p>"},{"location":"kbs/000020951/#situation","title":"Situation","text":"<p>Monitoring V2 dashboards are not displaying CPU/Memory usage.</p>"},{"location":"kbs/000020951/#resolution","title":"Resolution","text":"<p>Rancher Engineering team has put in place a workaround that consists of the following:</p> <ul> <li>Creating a cAdvisor standalone instance</li> <li>Creating ServiceMonitor with some relabeling</li> <li> <p>Disabling\u00a0kubelet.serviceMonitor.cAdvisor in the rancher-monitoring chart</p> </li> <li> <p>Example standalone cAdvisor instance and ServiceMonitor YAML:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\nrules:\n- apiGroups:\n  - policy\n  resourceNames:\n  - cadvisor\n  resources:\n  - podsecuritypolicies\n  verbs:\n  - use\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cadvisor\nsubjects:\n- kind: ServiceAccount\n  name: cadvisor\n  namespace: kube-system\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: cadvisor\n      name: cadvisor\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: \"\"\n      labels:\n        app: cadvisor\n        name: cadvisor\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - args:\n        - --housekeeping_interval=10s\n        - --max_housekeeping_interval=15s\n        - --event_storage_event_limit=default=0\n        - --event_storage_age_limit=default=0\n        - --enable_metrics=app,cpu,disk,diskIO,memory,network,process\n        - --docker_only\n        - --store_container_labels=false\n        - --whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace\n        image: gcr.io/cadvisor/cadvisor:v0.45.0\n        name: cadvisor\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 800m\n            memory: 2000Mi\n          requests:\n            cpu: 400m\n            memory: 400Mi\n        volumeMounts:\n        - mountPath: /rootfs\n          name: rootfs\n          readOnly: true\n        - mountPath: /var/run\n          name: var-run\n          readOnly: true\n        - mountPath: /sys\n          name: sys\n          readOnly: true\n        - mountPath: /var/lib/docker\n          name: docker\n          readOnly: true\n        - mountPath: /dev/disk\n          name: disk\n          readOnly: true\n      priorityClassName: system-node-critical\n      serviceAccountName: cadvisor\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - key: node-role.kubernetes.io/controlplane\n        value: \"true\"\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/etcd\n        value: \"true\"\n        effect: NoExecute\n      volumes:\n      - hostPath:\n          path: /\n        name: rootfs\n      - hostPath:\n          path: /var/run\n        name: var-run\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /var/lib/docker\n        name: docker\n      - hostPath:\n          path: /dev/disk\n        name: disk\n---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\nspec:\n  allowedHostPaths:\n  - pathPrefix: /\n  - pathPrefix: /var/run\n  - pathPrefix: /sys\n  - pathPrefix: /var/lib/docker\n  - pathPrefix: /dev/disk\n  fsGroup:\n    rule: RunAsAny\n  runAsUser:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  volumes:\n  - '*'\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cadvisor\n  labels:\n    app: cadvisor\n  namespace: kube-system\nspec:\n  selector:\n    app: cadvisor\n  ports:\n  - name: cadvisor\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: kube-system\nspec:\n  endpoints:\n  - metricRelabelings:\n    - sourceLabels:\n      - container_label_io_kubernetes_pod_name\n      targetLabel: pod\n    - sourceLabels:\n      - container_label_io_kubernetes_container_name\n      targetLabel: container\n    - sourceLabels:\n      - container_label_io_kubernetes_pod_namespace\n      targetLabel: namespace\n    - action: labeldrop\n      regex: container_label_io_kubernetes_pod_name\n    - action: labeldrop\n      regex: container_label_io_kubernetes_container_name\n    - action: labeldrop\n      regex: container_label_io_kubernetes_pod_namespace\n    port: cadvisor\n    relabelings:\n    - sourceLabels:\n      - __meta_kubernetes_pod_node_name\n      targetLabel: node\n    - sourceLabels:\n      - __metrics_path__\n      targetLabel: metrics_path\n      replacement: /metrics/cadvisor\n    - sourceLabels:\n      - job\n      targetLabel: job\n      replacement: kubelet\n  namespaceSelector:\n    matchNames:\n    - kube-system\n  selector:\n    matchLabels:\n      app: cadvisor\n</code></pre> <ol> <li>On the Rancher Monitoring chart, change the kubelet.serviceMonitor.cAdvisor to false</li> </ol> <pre><code>kubelet:\n  serviceMonitor:\n    cAdvisor: false\n</code></pre>"},{"location":"kbs/000020951/#cause","title":"Cause","text":"<p>This is caused by the changes on k8s 1.24, by the removal of the Dockershim</p>"},{"location":"kbs/000020951/#status","title":"Status","text":"<p>Reported to Engineering</p>"},{"location":"kbs/000020951/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020952/","title":"Interacting with the Rancher API using cURL","text":"<p>This document (000020952) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020952/#environment","title":"Environment","text":"<p>Rancher 2.6.X and above.</p>"},{"location":"kbs/000020952/#situation","title":"Situation","text":"<p>How can I interact with the Rancher API endpoint using cURL?</p>"},{"location":"kbs/000020952/#resolution","title":"Resolution","text":""},{"location":"kbs/000020952/#interacting-with-the-rancher-api-using-curl_1","title":"Interacting with the Rancher API using cURL","text":"<p>The Rancher API is an application programming interface that allows users to interact with their Rancher infrastructure without manually logging into their servers. This article will explain how to use cURL to interact with the Rancher API and provide an example command.</p>"},{"location":"kbs/000020952/#what-is-curl","title":"What is cURL?","text":"<p>cURL is a command-line tool that transfers data from one system to another. It supports various protocols, including HTTP, HTTPS, FTP, and FTPS. cURL is commonly used for making API requests and interacting with web services.</p>"},{"location":"kbs/000020952/#using-curl-to-interact-with-the-rancher-api","title":"Using cURL to Interact with the Rancher API","text":"<p>To interact with the Rancher API, you need to use cURL to make an HTTP request. The request must include the authentication token, the URL of the Rancher API endpoint, and any other parameters specific to your request.</p> <p>The root of the Rancher API endpoint is:\u00a0https://YOUR_RANCHER_URL/v3</p> <p>For example, if you want to list all the users in your Rancher infrastructure, you would use the following cURL command:</p> <pre><code>curl -u \"$access_key:$secret_key\" -X GET \"https://YOUR_RANCHER_URL/v3/users\"\n</code></pre> <p>The variables in this command, should be replaced with your Access Key and Secret Key. These can be created in the\u00a0Account and API Keys section of the Rancher UI.</p>"},{"location":"kbs/000020952/#conclusion","title":"Conclusion","text":"<p>Using cURL to interact with the Rancher API is a convenient way to automate tasks and manage infrastructure. With cURL, you can quickly and easily interact with the Rancher API to list namespaces, create resources, deploy applications, and more.</p>"},{"location":"kbs/000020952/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000020952/#disclaimer","title":"Disclaimer","text":"<p>This document serves as a disclaimer related to the Rancher API endpoint. Interacting directly with this endpoint is not officially supported. Managing Kubernetes clusters is best done using Kubectl or the UI.</p> <p>The API should only be used for testing or debugging and not for production-level changes. If you want a supported CLI utility to interact with the Rancher API, please check out Rancher CLI.</p> <p>You may need to use the insecure option in your cURL command if you're Rancher UI doe not have a trusted cert.</p> <p><code>curl --insecure -u \"$access_key:$secret_key\" -X GET \"https://YOUR_RANCHER_URL/v3/users\"</code></p>"},{"location":"kbs/000020952/#disclaimer_1","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020954/","title":"Testing Connectivity Between Kubernetes Pods with Iperf3","text":"<p>This document (000020954) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020954/#environment","title":"Environment","text":"<ul> <li>A running Kubernetes cluster</li> <li>The kubectl command-line tool installed on your local machine</li> </ul>"},{"location":"kbs/000020954/#situation","title":"Situation","text":"<p>In this article, we will show you how to use iperf3 to test the connectivity between Kubernetes pods in your cluster. We will use a Deployment to ensure that iperf3 pods are running on each node in the cluster.</p>"},{"location":"kbs/000020954/#resolution","title":"Resolution","text":""},{"location":"kbs/000020954/#step-1-deploy-iperf3-as-a-daemonset","title":"Step 1: Deploy Iperf3 as a DaemonSet","text":"<p>To deploy iperf3 as a Deployment on your Kubernetes cluster, you will need to create a YAML file with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: iperf3-ds\nspec:\n  selector:\n    matchLabels:\n      app: iperf3\n  template:\n    metadata:\n      labels:\n        app: iperf3\n    spec:\n      containers:\n      - name: iperf3\n        image: leodotcloud/swiss-army-knife\n        ports:\n        - containerPort: 5201\n</code></pre> <pre><code>This YAML file creates a Deployment named iperf3-ds of the iperf3 container.\n</code></pre> <p>You can then create the DaemonSet by running the following command:</p> <pre><code>kubectl apply -f iperf3-ds.yaml\n</code></pre> <pre><code>Step 2: Verify the DaemonSet\n</code></pre> <p>Once the DaemonSet is up and running, you can use the following command to check that iperf3 pods are running on all nodes in the cluster:</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>You should see output similar to the following:\n</code></pre> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\niperf3-ds-5b7f6f5c8-5wcw5   1/1     Running   0          36s\niperf3-ds-5b7f6f5c8-6q4q4   1/1     Running   0          36s\niperf3-ds-5b7f6f5c8-8wnw7   1/1     Running   0          36s\n</code></pre> <pre><code>Step 3: Test Connectivity Between Pods\n</code></pre> <p>To test the connectivity between pods, you will need to use the pod's IP addresses. You can use the following command to get the IP addresses of the pods:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <pre><code>You should see output similar to the following:\n</code></pre> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE    IP            NODE\niperf3-ds-5b7f6f5c8-5wcw5   1/1     Running   0          36s   10.1.0.1      node1\niperf3-ds-5b7f6f5c8-6q4q4   1/1     Running   0          36s   10.1.0.2      node2\niperf3-ds-5b7f6f5c8-8wnw7   1/1     Running   0          36s   10.1.0.3      node3\n</code></pre> <pre><code>Once you have the IP addresses of the pods you want to test, you can use the iperf3 command to test the connectivity.\n\nFirst, choose a pod to run in server mode:\n</code></pre> <pre><code>kubectl exec -it &lt;pod-name&gt; -- iperf3 -s -p 12345\n</code></pre> <pre><code>Second, choose a pod to run in client mode:\n</code></pre> <pre><code>kubectl exec -it &lt;pod-name&gt; --\u00a0iperf3 -c &lt;server pod IP address&gt; -p 12345\n</code></pre> <pre><code>The iperf3 command will output the network performance statistics, including the bandwidth, packet loss, and jitter.\n\nThis is an example output from a test cluster; please note that due to the various layers of networking involved, there is a performance impact that is expected:\n</code></pre> <pre><code>Connecting to host 10.42.0.36, port 12345\n[  4] local 10.42.0.37 port 45568 connected to 10.42.0.36 port 12345\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  2.23 GBytes  19.1 Gbits/sec  300   2.88 MBytes\n[  4]   1.00-2.00   sec  2.49 GBytes  21.4 Gbits/sec  737   1.58 MBytes\n[  4]   2.00-3.00   sec  2.42 GBytes  20.8 Gbits/sec  524   2.33 MBytes\n[  4]   3.00-4.00   sec  2.17 GBytes  18.6 Gbits/sec  248   2.41 MBytes\n[  4]   4.00-5.00   sec  2.25 GBytes  19.3 Gbits/sec  151   2.45 MBytes\n[  4]   5.00-6.00   sec  2.36 GBytes  20.2 Gbits/sec   73   3.00 MBytes\n[  4]   6.00-7.00   sec  2.40 GBytes  20.6 Gbits/sec  181   2.84 MBytes\n[  4]   7.00-8.00   sec  2.29 GBytes  19.7 Gbits/sec   73   2.64 MBytes\n[  4]   8.00-9.00   sec  2.35 GBytes  20.2 Gbits/sec  110   2.44 MBytes\n[  4]   9.00-10.00  sec  2.27 GBytes  19.5 Gbits/sec  167   2.43 MBytes\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-10.00  sec  23.2 GBytes  19.9 Gbits/sec  2564             sender\n[  4]   0.00-10.00  sec  23.2 GBytes  19.9 Gbits/sec                  receiver\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000020954/#cause","title":"Cause","text":"<p>In this article, we have shown you how to use iperf3 to test connectivity between Kubernetes pods in your cluster. We have used a DaemonSet to ensure that iperf3 pods are running on each node in the cluster. We hope that this article has helped demonstrate how to use iperf3 for testing connectivity between pods in your Kubernetes cluster.</p>"},{"location":"kbs/000020954/#additional-information","title":"Additional Information","text":"<p>https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</p>"},{"location":"kbs/000020954/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020971/","title":"Enable CSR signing on an RKE cluster so certificates are issued","text":"<p>This document (000020971) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020971/#situation","title":"Situation","text":"<p>When creating a private key, a CertificateSigningRequest, and approving the CSR. You may notice in the output that the CSR is Approved but not Issued. For example, you may see the following:</p> <pre><code>kubectl get csr\nNAME                  AGE   REQUESTOR   CONDITION\nmy-csr                18m   admin       Approved\n</code></pre> <p>But you actually expect to see the following:</p> <pre><code>kubectl get csr\nNAME                  AGE   REQUESTOR   CONDITION\nmy-csr                18m   admin       Approved,Issued\n</code></pre>"},{"location":"kbs/000020971/#resolution","title":"Resolution","text":"<p>You will need to provide the following flags for the\u00a0<code>kube-controller-manager:</code> <code>--cluster-signing-cert-file</code> and <code>--cluster-signing-key-file</code></p> <p>RKE1</p> <p>In order to do this from the Rancher UI:</p> <ol> <li>Go to Cluster Management</li> <li>Select the 3-dot menu next to the desired cluster and click Edit Config</li> <li>Click the Edit as YAML button</li> <li>Under the rancher_kubernetes_engine_config.services section, replace</li> </ol> <pre><code>kube-controller: {}\n</code></pre> <pre><code>with\n</code></pre> <pre><code>kube-controller:\n     extra_args:\n       cluster-signing-cert-file: /etc/kubernetes/ssl/kube-ca.pem\n       cluster-signing-key-file: /etc/kubernetes/ssl/kube-ca-key.pem\n</code></pre> <ol> <li>Click the Save button at the bottom of the screen</li> <li>Once the cluster finishes reconciling, you should be able to go through the steps again and have the certificate issued</li> </ol> <p>If this is on a cluster managed using rke up, you will have to put these values in the cluster.yml file and run rke up</p> <p>RKE2</p> <p>In order to do this from the Rancher UI:</p> <ol> <li>Go to Cluster Management</li> <li>Select the 3-dot menu next to the desired cluster and click Edit Config</li> <li>Go to the Advanced setting under cluster config</li> <li>Add the following additional Controller Manager Args</li> </ol> <pre><code>cluster-signing-cert-file=/etc/kubernetes/ssl/kube-ca.pem\ncluster-signing-key-file=/etc/kubernetes/ssl/kube-ca-key.pem\n</code></pre> <ol> <li>Click the Save button at the bottom of the screen</li> <li>Once the cluster finishes reconciling, you should be able to go through the steps again and have the certificate issued</li> </ol> <p>If this is on a standalone RKE2 cluster, you would need to add the argument in the config.yaml and restart the RKE2 service</p>"},{"location":"kbs/000020971/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020972/","title":"How to create multiple IPPool for Calico CNI in rke2 cluster","text":"<p>This document (000020972) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020972/#environment","title":"Environment","text":"<p>Rancher 2.6.x and rancher 2.7.x</p> <p>rke2 cluster with calico as the CNI</p>"},{"location":"kbs/000020972/#situation","title":"Situation","text":"<p>There are several reasons why you might want to create multiple IP pools in Calico:</p> <ol> <li>Resource allocation: You might want to allocate a specific set of IP addresses to a certain group of containers or pods, to ensure that they have access to the resources they need. By defining a separate IP pool for each group, you can control the allocation of resources and ensure that each group has enough IP addresses to meet its needs.</li> <li>Security: You might want to create multiple IP pools to enforce different security policies for different groups of containers or pods. For example, you might want to assign a separate IP pool to a group of containers that require stricter security controls, such as those that process sensitive data.</li> <li>Flexibility: Multiple IP pools can provide more flexibility in network design, enabling different types of workloads to be separated or grouped together as needed. For example, you might create separate IP pools for frontend and backend services, or for testing and production environments.</li> </ol>"},{"location":"kbs/000020972/#resolution","title":"Resolution","text":"<p>Below is the manifest for creating the IPPool with the name \"new-pool\". Use the same yaml by changing the name and IP range for a different IPPool</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: IPPool\nmetadata:\n  name: new-pool\nspec:\n  cidr: 10.168.0.0/16\n  blockSize: 24\n  ipipMode: Never\n  natOutgoing: true\n  vxlanMode: Always\n</code></pre> <p>Note: vxlanMode is used above to provide a complete example. If vxlan is not in use, switch in the relevant routing method, for example, ipipMode.</p> <p>From here, you can allocate IPs per IPPool based on node topology.</p> <p>Alternatively, use the annotation below to allocate IPs from a specific IPPool in a workload.</p> <pre><code>annotations:\n        \"cni.projectcalico.org/ipv4pools\": \"[\\\"new-pool\\\"]\"\n</code></pre> <p>For example:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        app: nginx\n      annotations:\n        \"cni.projectcalico.org/ipv4pools\": \"[\\\"new-pool\\\"]\"\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n</code></pre> <p>If you do not want to allocate IPs from an IPPool, you can disable the IPPool without removing it:</p> <pre><code>apiVersion: projectcalico.org/v3\nkind: IPPool\nmetadata:\n  name: new-pool\nspec:\n  cidr: 10.168.0.0/16\n  blockSize: 24\n  ipipMode: Never\n  natOutgoing: true\n  vxlanMode: Always\n  disabled: true\n</code></pre>"},{"location":"kbs/000020972/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020972/#additional-information","title":"Additional Information","text":"<p>https://projectcalico.docs.tigera.io/archive/v3.24/networking/migrate-pools</p> <p>https://docs.tigera.io/calico/3.25/operations/install-apiserver</p>"},{"location":"kbs/000020972/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020976/","title":"How to configure the logging app to use Custom PVC volume for Fluentd buffers","text":"<p>This document (000020976) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020976/#environment","title":"Environment","text":"<p>Rancher with Rancher-Logging chart v2</p>"},{"location":"kbs/000020976/#resolution","title":"Resolution","text":"<ol> <li>Go to Apps &gt; Charts, then Select Logging and click Install/Update</li> <li>Select the project to install the logging app and select \"Customize Helm Options before install\", and click Next</li> <li> <p>Select Edit YAML tab and modify the following:</p> </li> <li> <p>Search for disablePvc and change that from true to false</p> </li> </ol> <pre><code>disablePvc: false\n</code></pre> <ul> <li>Search for the bufferStorageVolume section and change it to the following (Modify the STORAGE_CLASS_NAME with your environement value):</li> </ul> <pre><code>\u00a0 bufferStorageVolume:\n\u00a0 \u00a0 pvc:\n\u00a0 \u00a0 \u00a0 spec:\n\u00a0 \u00a0 \u00a0 \u00a0 accessModes:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - ReadWriteOnce\n\u00a0 \u00a0 \u00a0 \u00a0 resources:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storage: 40Gi\n\u00a0 \u00a0 \u00a0 \u00a0 storageClassName: \"&lt;STORAGE_CLASS_NAME&gt;\"\n\u00a0 \u00a0 \u00a0 \u00a0 volumeMode: Filesystem\n</code></pre> <ol> <li>Modify other fields if required, and finally, Click on Next, then Install/Update.</li> </ol> <p>If the logging app is already installed, then the PVC will need to be created manually and associated with the fluentd StatefulSet, as per the following steps:</p> <ol> <li>From UI go to Storage &gt; PersistentVolumeClaims, and Create PVC to be used for the fluentd pod (PVC need to be on the namespace cattle-logging-system):</li> </ol> <pre><code>&gt; kubectl get pvc -n cattle-logging-system\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0STATUS \u00a0 VOLUME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CAPACITY \u00a0 ACCESS MODES \u00a0 STORAGECLASS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0AGE\nrancher-logging-root-fluentd-buffer \u00a0 Bound \u00a0 \u00a0pvc-4b78c686-7b68-4f7d-a40f-32c0a5470831 \u00a0 40Gi \u00a0 \u00a0 \u00a0 RWO \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0rancher-storage-class \u00a0 10s\n</code></pre> <ol> <li> <p>From UI, go to Workload &gt; StatefulSets, then\u00a0Edit the fluentd StatefulSet YAML ( rancher-logging-root-fluentd) and update the claimName for the rancher-logging-root-fluentd-buffer volume with the PVC name.</p> </li> <li> <p>Change the following two lines:</p> </li> </ol> <pre><code>\u00a0 \u00a0 \u00a0 - emptyDir: {}\n\u00a0 \u00a0 \u00a0 \u00a0 name: rancher-logging-root-fluentd-buffer\n</code></pre> <ul> <li>To the following:</li> </ul> <pre><code>\u00a0 \u00a0 \u00a0 - name: rancher-logging-root-fluentd-buffer\n\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: \"rancher-logging-root-fluentd-buffer\"\n</code></pre> <ol> <li>Finally, click on save, and a new fluentd pod will be scheduled with to use PV for buffer volume.</li> </ol> <p>To confirm if fluentd is using PV for buffer volume, you can run the following command and review the mount and volumes output sections:</p> <pre><code>kubectl describe pod rancher-logging-root-fluentd-0 -n cattle-logging-system\n</code></pre> <ul> <li>If fluentd is using PV for buffer volume, the output will be:</li> </ul> <pre><code>\u00a0 rancher-logging-root-fluentd-buffer:\n\u00a0 \u00a0 Type: \u00a0 \u00a0 \u00a0 PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n\u00a0 \u00a0 ClaimName: \u00a0rancher-logging-root-fluentd-buffer\n\u00a0 \u00a0 ReadOnly: \u00a0 false\n</code></pre> <ul> <li>If fluentd is using EmptyDir for buffer volume, the output will be:</li> </ul> <pre><code>\u00a0 rancher-logging-root-fluentd-buffer:\n\u00a0 \u00a0 Type: \u00a0 \u00a0 \u00a0 EmptyDir (a temporary directory that shares a pod's lifetime)\n\u00a0 \u00a0 Medium:\n\u00a0 \u00a0 SizeLimit: \u00a0&lt;unset&gt;\n</code></pre> <p>Reference:</p> <ul> <li>https://banzaicloud.com/docs/one-eye/logging-operator/logging-infrastructure/fluentd/</li> </ul>"},{"location":"kbs/000020976/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020985/","title":"LDAP error when logging into the Rancher UI after a Rancher upgrade","text":"<p>This document (000020985) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020985/#situation","title":"Situation","text":"<p>After upgrading Rancher to version 2.6.9 or higher, the following error is seen after logging into the Rancher UI:</p> <pre><code>ldap error Error creating ssl connection: LDAP Result Code 200 \"Network Error\": tls: server selected unsupported protocol version 301\n</code></pre>"},{"location":"kbs/000020985/#resolution","title":"Resolution","text":"<p>Upgrade the version of TLS that the LDAP server uses to at least 1.2.</p>"},{"location":"kbs/000020985/#cause","title":"Cause","text":"<p>This problem happens when the LDAP server used for authentication with Rancher uses a version of TLS lower than 1.2.</p> <p>Rancher 2.6.9 uses Go 1.19 where the crypto/tls library that is used requires a minimum version of TLS 1.2 for LDAP connections.</p>"},{"location":"kbs/000020985/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020988/","title":"How to troubleshoot rancher-logging","text":"<p>This document (000020988) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020988/#situation","title":"Situation","text":"<p>After installing the rancher-logging app and creating your resources, flows(clusterflows)/outputs(clusteroutputs), how do you troubleshoot if you are not seeing your logs at the final destination?</p>"},{"location":"kbs/000020988/#resolution","title":"Resolution","text":"<p>The rancher-logging app troubleshooting can be divided into three phases as below:</p> <ol> <li>User resources review: The first part is to ensure there is no error for the resources you created and that they are active.</li> </ol> <pre><code>1. ClusterOutputs and Outputs review. They should be active with no errors. Please correct the errors if any\n\nkubectl get clusteroutput -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0 \u00a0      ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-c-output \u00a0 true\nkubectl get output -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0 \u00a0    ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-output \u00a0 true\n\n2. Clusterflows and flows review. They should be active with no errors. Please correct the errors if any\n\nkubectl get clusterflow -o wide -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0 \u00a0    ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-c-flow \u00a0 true\nkubectl get flow -o wide -A\nNAMESPACE \u00a0 NAME \u00a0 \u00a0   \u00a0ACTIVE \u00a0 PROBLEMS\ntest-ns \u00a0 \u00a0 test-flow \u00a0 true\n</code></pre> <ol> <li>FluentD and Fluentbit pods review: The fluentbit is a daemonset and should be running on each node, while you should have at least one FluentD pod running as the fluentbit pods will collect logs from each node and forward them\u00a0to the FluentD pod to be sent to their final destination</li> </ol> <pre><code>Your output could look different depending on what type Kubernetes cluster you have. However, you should have a fluentbit pod on each node and at least one FluentD pod\n\nkubectl get pods -n cattle-logging-system\nNAME                                                READY   STATUS      RESTARTS   AGE\nrancher-logging-655578478b-7k46r                    1/1     Running     0          89s\nrancher-logging-k3s-journald-aggregator-957gz       1/1     Running     0          89s\nrancher-logging-root-fluentbit-lczl5                1/1     Running     0          70s\nrancher-logging-root-fluentd-0                      2/2     Running     0          70s\nrancher-logging-root-fluentd-configcheck-ac2d4553   0/1     Completed   0          84s\n</code></pre> <ol> <li>Logs review: At this step, you review to ensure no errors in the logs for the FluentD or Fluentbit pods. For Fluentbit, you will probably need to review each if you suspect the logs are not being collected from fluentbit.</li> </ol> <p>``` 1. kubectl exec rancher-logging-root-fluentd-0 -n cattle-logging-system -- cat /fluentd/log/out  # logging-operator version +up3.17.10 and earlier    kubectl logs rancher-logging-root-fluentd-0 -n cattle-logging-system #  logging-operator version +up4.4.0 and later</p> <p>This will dump the logs out of the fluentd container</p> <ol> <li>kubectl -n cattle-logging-system logs rancher-logging-root-bit-lczl5</li> </ol> <p>You should run it against each fluentbit pod if you suspect that the issue is on fluentbit ```</p>"},{"location":"kbs/000020988/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020992/","title":"Collecting file logs with Host Tailer on the Logging Operator","text":"<p>This document (000020992) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020992/#environment","title":"Environment","text":"<p>SUSE Rancher 2.6.9</p> <p>SUSE Rancher 2.7.x</p>"},{"location":"kbs/000020992/#situation","title":"Situation","text":"<p>Pre-requisites</p> <ul> <li>Rancher Logging operator</li> </ul> <p>To retrieve logs not written to the stdout (e.g. kube-audit logs), one can use the Banzai\u00a0\u00a0Host Tailer\u00a0CRD provided by\u00a0https://banzaicloud.com/.\u00a0\u00a0From the documentation: \" HostTailer\u2019s main goal is to tail custom files and transmit their changes to stdout\" <code>.</code>\u00a0This way, the logging-operator can process them. Example usage is\u00a0here. Similarly, you can use the\u00a0file-tailer\u00a0if you know the log file name.</p>"},{"location":"kbs/000020992/#resolution","title":"Resolution","text":"<p>1. Create a HostTailer to retrieve the audit logs</p> <pre><code>apiVersion: logging-extensions.banzaicloud.io/v1alpha1\nkind: HostTailer\nmetadata:\n  name: kubeaudit-hosttailer-sample\nspec:\n  fileTailers:\n    - name: kube-audit\n      path: /var/log/kube-audit/audit-log.json\n      disabled: false\n</code></pre> <p>Note: It will be created in the \"default\" namespace if none is specified.</p> <p>1.2 Verify the HostTailer object</p> <pre><code>kubectl get hosttailer -n  default\n</code></pre> <p>1.3\u00a0 Verify\u00a0 the daemonset created by the HostTailer</p> <pre><code>kubectl get daemonsets -n default\n</code></pre> <p>1.4\u00a0 Verify the HostTailer pod labels</p> <pre><code>$ kubectl get pods --show-labels -n default | grep -i hosttailer\nkubeaudit-hosttailer-sample-host-tailer-n8h8q\u00a0\u00a0 1/1\u00a0\u00a0\u00a0\u00a0 Running\u00a0\u00a0 0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9d\u00a0\u00a0\u00a0 app.kubernetes.io/instance=kubeaudit-hosttailer-sample-host-tailer,app.kubernetes.io/name=host-tailer\n</code></pre> <p>The label \u201capp.kubernetes.io/name=host-tailer\u201d will be used in the ClusterFlow object.</p> <p>2. Modify or update if there is a ClusterFlow in place.</p> <pre><code>  match:\n  - select:\n      labels:\n        app.kubernetes.io/name: host-tailer\n</code></pre> <p>Otherwise, create a new ClusterFlow and ClusterOutput following these examples.</p> <p>2.1 Create ClusterFlow</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterFlow\nmetadata:\n  name: flowhostfile\n  namespace: cattle-logging-system\nspec:\n  globalOutputRefs:\n  - output-test-hostfile\n  match:\n  - select:\n      labels:\n        app.kubernetes.io/name: host-tailer\n</code></pre> <p>For testing purposes, the ClusterOutput will output on a file on the Fluentd container.</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterOutput\nmetadata:\n  name: output-test-hostfile\n  namespace: cattle-logging-system\nspec:\n  file:\n    path: /tmp/${tag}\n</code></pre> <p>Note the ${tag} on the path, required by Fluentd. In this case, the logs will be available on the /tmp folder of the pod rancher-logging-root-fluentd-0 in the cattle-logging-system namespace.</p>"},{"location":"kbs/000020992/#cause","title":"Cause","text":"<p>The underlying issue is that\u00a0kube-audit logs are not written to stdout so the logging operator cant process them. This is why the Banzai\u00a0\u00a0Host Tailer\u00a0CRD is needed to tail custom files and transmit their changes to stdout.</p>"},{"location":"kbs/000020992/#additional-information","title":"Additional Information","text":"<p>Rancher Integration with Logging Services</p> <ul> <li>https://ranchermanager.docs.rancher.com/v2.7/pages-for-subheaders/logging</li> </ul> <p>Flow and Outputs</p> <ul> <li>https://ranchermanager.docs.rancher.com/v2.7/integrations-in-rancher/logging/custom-resource-configuration/flows-and-clusterflows</li> <li>https://ranchermanager.docs.rancher.com/v2.7/integrations-in-rancher/logging/custom-resource-configuration/outputs-and-clusteroutputs</li> </ul> <p>RKE2 Kube-audit logs</p> <ul> <li>https://ranchermanager.docs.rancher.com/how-to-guides/advanced-user-guides/enable-api-audit-log-in-downstream-clusters</li> </ul>"},{"location":"kbs/000020992/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020994/","title":"How to Restore the fleet-local namespace if it gets deleted accidently","text":"<p>This document (000020994) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020994/#environment","title":"Environment","text":"<p>Rancher 2.x</p>"},{"location":"kbs/000020994/#situation","title":"Situation","text":"<p>The fleet-local namespace in the local cluster is showing a Pending state in the Rancher dashboard, or a Terminating state with kubectl.</p>"},{"location":"kbs/000020994/#resolution","title":"Resolution","text":"<p>Prepare to clean up the related helm chart, to let Rancher reinstall it again.</p> <p>Requirements:</p> <ul> <li>A kubeconfig for the Rancher (local) management cluster</li> <li>The helm CLI</li> </ul> <p>If you don't have a kubeconfig for the local cluster, the steps here can be used to retrieve one from the control plane node in the cluster.</p> <p>1. Check that the kubeconfig is set for the correct cluster</p> <pre><code>export KUBECONFIG=/path/to/kubeconfig.yaml\nkubectl get nodes\n</code></pre> <p>2. Delete the fleet chart using helm</p> <pre><code>helm uninstall\u00a0fleet -n\u00a0cattle-fleet-system\n</code></pre> <p>3. Perform a rollout restart of the Rancher deployment, this will trigger the recreation of fleet-local and its associated contents</p> <pre><code>kubectl rollout restart deploy/rancher -n cattle-system\n</code></pre> <p>Note, this may cause a small interruption for users as the websocket tunnel to Rancher will be re-established.</p> <p>4. Check the fleet/fleet-crd apps are installed and active</p>"},{"location":"kbs/000020994/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020994/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020995/","title":"Kubernetes upgrade from v1.21.x to 1.22.x failing with error \"does not recognize SELinux label\"","text":"<p>This document (000020995) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020995/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>K8S 1.21.x</p> <p>RKE1 with\u00a0SELinux is enabled</p>"},{"location":"kbs/000020995/#situation","title":"Situation","text":"<p>The RKE1 kubernetes version upgrade from v1.21.x to v1.22.x / latest from the Rancher UI is failing. The Rancher UI reporting the error below.</p> <pre><code>[[selinux] Host [xx.xx.xx.xx] does not recognize SELinux label [label=type:rke_container_t]. This is required for Kubernetes version [&gt;=1.22.0-rancher0]. Please install rancher-selinux RPM package and try again]\n</code></pre>"},{"location":"kbs/000020995/#resolution","title":"Resolution","text":"<p>Make sure that the SELinux rpm is installed and updated to the latest version. The latest version is now\u00a0rancher-selinux-0.2-1.el7.noarch</p> <p>To verify\u00a0the current version for Redhat based systems.</p> <pre><code>rpm -qa | grep rancher-selinux\n</code></pre> <p>The OS repository for installing / upgrading the package ( Example )</p> <pre><code>vi /etc/yum.repos.d/rancher.repo\n[rancher]\nname=Rancher\nbaseurl=https://rpm.rancher.io/rancher/production/centos/7/noarch\nenabled=1\ngpgcheck=1\ngpgkey=https://rpm.rancher.io/public.key\n\nyum -y install rancher-selinux\n\nTo upgrade if already installed the old version\n\nyum update rancher-selinux\n</code></pre>"},{"location":"kbs/000020995/#cause","title":"Cause","text":"<p>The package\u00a0rancher-selinux-0.2-1.el7.noarch must be installed on all nodes if SELinux is enabled at OS level. The older\u00a0rancher-selinux version are also not supported with kubernetes 1.22.x onwards.</p>"},{"location":"kbs/000020995/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000020995/#additional-information","title":"Additional Information","text":"<p>GH issue :\u00a0https://github.com/rancher/rancher/issues/36509</p> <p>Rancher documentation : here</p>"},{"location":"kbs/000020995/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020998/","title":"How to enable View in API","text":"<p>This document (000020998) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020998/#situation","title":"Situation","text":"<p>By default, the View in API is disabled in the rancher UI</p> <p>This will allow you to navigate the API with your\u00a0 Web Browser.</p>"},{"location":"kbs/000020998/#resolution","title":"Resolution","text":"<p>Follow the steps below to enable it:</p> <ol> <li>Click on your user avatar in the upper right corner.</li> <li>Click on Preferences.</li> <li>Under the Advanced section, check the Enable Developer Tools &amp; Features. This will enable the View In API for the resources.</li> </ol>"},{"location":"kbs/000020998/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000020999/","title":"Secrets in downstream clusters is repeatedly overwritten","text":"<p>This document (000020999) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000020999/#environment","title":"Environment","text":"<p>Long running setups</p> <p>Cluster that used old Project secrets feature</p> <p>Upgraded from Rancher versions &lt;2.6</p>"},{"location":"kbs/000020999/#situation","title":"Situation","text":"<p>A secret is replaced with a new one with the same name.</p> <p>When \"kubectl rollout restart deployment -n cattle-system rancher\" is run in the management cluster the secret is replaced with the old one.</p> <p>Rancher also overwrites the secret every few hours, on its own.</p>"},{"location":"kbs/000020999/#resolution","title":"Resolution","text":"<p>Check for a project secret having been configured for the project in the past.</p> <p>Navigate to the old UI (put /g behind the URL) and check for project secrets in the project(s) of the affected cluster.</p>"},{"location":"kbs/000020999/#cause","title":"Cause","text":"<p>\"Project Secrets\" was a legacy feature from older Rancher &lt;2.6 versions.</p>"},{"location":"kbs/000020999/#additional-information","title":"Additional Information","text":"<p>https://ranchermanager.docs.rancher.com/v2.6/how-to-guides/new-user-guides/kubernetes-resources-setup/secrets#creating-secrets-in-projects</p>"},{"location":"kbs/000020999/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021006/","title":"Cattle Cluster Agent flapping with runtime error: slice bounds out of range [:3] with capacity 0","text":"<p>This document (000021006) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021006/#environment","title":"Environment","text":"<p>This bug seems to affect all current Rancher versions as of March 2023.</p> <p>The issue is now fixed since 2.7.5, please upgrade to the latest available version.</p>"},{"location":"kbs/000021006/#situation","title":"Situation","text":"<p>A downstream cluster flaps between Active and Unavailable. The cattle-cluster-agent logs show errors like the following:</p> <pre><code>0308 15:04:59.702627 55 runtime.go:78] Observed a panic: runtime.boundsError{x:3, y:0, signed:true, code:0x2} (runtime error: slice bounds out of range [:3] with capacity 0)\ngoroutine 3160 [running]:\nk8s.io/apimachinery/pkg/util/runtime.logPanic({0x3cee760, 0xc00bef0660})\n/go/pkg/mod/k8s.io/apimachinery@v0.23.3/pkg/util/runtime/runtime.go:74 +0x7d\nk8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xfffffffe})\n/go/pkg/mod/k8s.io/apimachinery@v0.23.3/pkg/util/runtime/runtime.go:48 +0x75\npanic({0x3cee760, 0xc00bef0660})\n/usr/lib64/go/1.17/src/runtime/panic.go:1038 +0x215\ngithub.com/rancher/rancher/pkg/catalogv2/helm.decodeHelm3({0x0, 0xc9d32d})\n/go/src/github.com/rancher/rancher/pkg/catalogv2/helm/helm3.go:124 +0x1b1\ngithub.com/rancher/rancher/pkg/catalogv2/helm.fromHelm3Data({0x0, 0xc004d60540}, 0x3fc4c34)\n/go/src/github.com/rancher/rancher/pkg/catalogv2/helm/helm3.go:23 +0x25\ngithub.com/rancher/rancher/pkg/catalogv2/helm.ToRelease({0x47b3a20, 0xc004d60540}, 0x6c696877206e6564)\n/go/src/github.com/rancher/rancher/pkg/catalogv2/helm/release.go:74 +0x3eb\ngithub.com/rancher/rancher/pkg/controllers/dashboard/helm.(*appHandler).OnSecretChange(0xc00baa1950, {0xc005b81080, 0x2d}, 0xc004d60540)\n/go/src/github.com/rancher/rancher/pkg/controllers/dashboard/helm/apps.go:170 +0xa5\n</code></pre>"},{"location":"kbs/000021006/#resolution","title":"Resolution","text":"<p>This error seems to be caused by bad Helm release data on the downstream cluster. The first check should be if any releases have no release data stored. The below command will list all Helm release secrets when run against the downstream cluster. If the data column shows 0, that release secret has no release data.</p> <pre><code>kubectl get secrets -A | grep helm.sh/release.v1\n</code></pre> <p>All secrets with no release data on the downstream cluster need to be deleted to allow cattle-cluster-agent to start properly.</p> <pre><code>kubectl delete secrets -n &lt;NAMESPACE&gt; &lt;SECRET_NAME&gt;\n</code></pre> <p>Once the bad Helm release secrets are removed, cattle-cluster-agent pods should successfully start. If desired, the current cattle-cluster-agent pods in a CrashLoopBackOff can be deleted to speed up this process.</p>"},{"location":"kbs/000021006/#cause","title":"Cause","text":"<p>Occasionally, Helm release secrets are improperly stored, causing the release data to not be present. cattle-cluster-agent checks the Helm data everytime it starts, but will fail if there is no release data to check</p>"},{"location":"kbs/000021006/#additional-information","title":"Additional Information","text":"<p>Reported in GitHub issue 35971:\u00a0https://github.com/rancher/rancher/issues/35971</p> <p>~~Per the GitHub issue, this is scheduled for the 2023-Q2 releases~~</p> <p>The issue is now fixed since 2.7.5</p>"},{"location":"kbs/000021006/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021018/","title":"How to change the default helm values for the Rancher Logging operator","text":"<p>This document (000021018) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021018/#environment","title":"Environment","text":"<p>Rancher v2.7+</p>"},{"location":"kbs/000021018/#situation","title":"Situation","text":"<p>When you install the Rancher Logging operator through the Apps feature within Rancher, how do you change the default helm values, for example, to set additional tolerations for logging workloads?</p>"},{"location":"kbs/000021018/#resolution","title":"Resolution","text":"<p>To configure the Rancher Logging operator chart, click\u00a0Edit YAML, during the chart installation/upgrade, and add the desired values,\u00a0per the Rancher documentation.</p>"},{"location":"kbs/000021018/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021022/","title":"How to collect kube-api audit logs with rancher-logging for an RKE/RKE2/K3S cluster","text":"<p>This document (000021022) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021022/#environment","title":"Environment","text":"<p>RKE/RKE2/K3S</p>"},{"location":"kbs/000021022/#situation","title":"Situation","text":"<p>kube-api server audit logs are usually placed in a different directory than the one configured for rancher-logging when collecting</p>"},{"location":"kbs/000021022/#resolution","title":"Resolution","text":"<p>By configuring the following, you can enable the kube-api server audit logs collection from rancher-logging helm charts. The rancher-logging helm chart has it disabled by default:</p> <p>RKE:</p> <pre><code>kubeAudit:\n    auditFilename: 'audit-log.json'\n    enabled: enabled\n    fluentbit:\n      logTag: kube-audit\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          value: 'true'\n        - effect: NoExecute\n          key: node-role.kubernetes.io/etcd\n          value: 'true'\n    pathPrefix: '/var/log/kube-audit'\n</code></pre> <p>RKE2:</p> <pre><code>kubeAudit:\n    auditFilename: 'audit.log'\n    enabled: enabled\n    fluentbit:\n      logTag: kube-audit\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          value: 'true'\n        - effect: NoExecute\n          key: node-role.kubernetes.io/etcd\n          value: 'true'\n    pathPrefix: '/var/lib/rancher/rke2/server/logs'\n</code></pre> <p>k3s:</p> <pre><code>kubeAudit:\n    auditFilename: 'audit.log'\n    enabled: enabled\n    fluentbit:\n      logTag: kube-audit\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/controlplane\n          value: 'true'\n        - effect: NoExecute\n          key: node-role.kubernetes.io/etcd\n          value: 'true'\n    pathPrefix: '/var/lib/rancher/k3s/server/logs'\n</code></pre>"},{"location":"kbs/000021022/#cause","title":"Cause","text":"<p>The kube-api server audit logs aren't collected by rancher-logging as they are placed outside of the directory parsed by the logging operator by default</p>"},{"location":"kbs/000021022/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021029/","title":"How to stream k3s journal logs to Cloudwatch on Rancher","text":"<p>This document (000021029) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021029/#environment","title":"Environment","text":"<p>Rancher 2.6.x\u00a0 and above</p>"},{"location":"kbs/000021029/#situation","title":"Situation","text":"<p>Send the k3s journal logs to AWS CloudWatch using Rancher's v2 logging integration</p>"},{"location":"kbs/000021029/#resolution","title":"Resolution","text":""},{"location":"kbs/000021029/#requirements","title":"Requirements:","text":"<ul> <li>Gathering k3s journal logs from each node in the cluster.</li> <li>Parsing\u00a0the logs to forward only the required fields.</li> <li>Forwarding the parsed data to cloudwatch.</li> </ul> <p>Rancher uses this\u00a0logging operator\u00a0that comes with the below CRDS:</p> <ul> <li>flow</li> <li>clusterFlow</li> <li>output</li> <li>clusterOutput</li> </ul> <p>You can read more about them\u00a0here.</p> <p>We will be using clusterFlow and clusterOutput as they are not namespaced. The clusterFlow CRD defines a logging flow for Fluentd with filters and outputs. Using this, we can define and apply filters to select only the desired data. Once parsed, data will be forwarded to the clusterOutput object. The clusterOutput CRD defines where to send the data. It supports several\u00a0plugins, but we will use Cloudwatch. You can read the spec\u00a0here.</p> <p>Now we have clusterFlow to parse the data and clusterOutput to define the destination of data. We need a way to get the journal logs from the nodes.</p> <p>HostTailer\u00a0CRD is provided by\u00a0https://kube-logging.dev\u00a0and is supported on the Rancher. From the doc,\u00a0<code>HostTailer\u2019s main goal is to tail custom files and transmit their changes to stdout.</code> This way, the logging-operator can process them.\u00a0Similarly, you can use the\u00a0file-tailer\u00a0if you know the log file name.</p> <p>The difference between the two is host-tailer looks at specific systemd service logs like k3s.service logs, while for file-tailer, you need to specify the exact location of the log file like /var/log/nginx/access.log.</p> <p>Here is the YAML to get the systemd journal logs from each host. This will create a daemonset. Pods will fetch the logs from the journal log files of the specified service name and output them to stdout.</p> <pre><code>apiVersion: logging-extensions.banzaicloud.io/v1alpha1\nkind: HostTailer\nmetadata:\n  name: k3s-systemd-tailer\n  namespace: cattle-logging-system\nspec:\n  systemdTailers:\n    - name: k3s-systemd-tailer\n      maxEntries: 100\n      path: /run/log/journal/\n      systemdFilter: k3s.service\n</code></pre> <pre><code>\n</code></pre> <p>The log output will then be fed to clusterFlow, which parses the logs.</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterFlow\nmetadata:\n  name: host-tailer-flow\n  namespace: cattle-logging-system\nspec:\n  filters:\n    - parser:\n        key_name: message\n        reserve_time: true\n        parse:\n          type: json\n    - record_transformer:\n        remove_keys: _CMDLINE,_BOOT_ID,_MACHINE_ID,PRIORITY,SYSLOG_FACILITY,_UID,_GID,_SELINUX_CONTEXT,_SYSTEMD_SLICE,_CAP_EFFECTIVE,_TRANSPORT,_SYSTEMD_CGROUP,_SYSTEMD_INVOCATION_ID,_STREAM_ID,SYSLOG_IDENTIFIER,_COMM,_EXE\n  match:\n    - select:\n        labels:\n          app.kubernetes.io/name: host-tailer\n  globalOutputRefs:\n    - host-logging-cloudwatch\n</code></pre> <pre><code>\n</code></pre> <p>Here we are matching the app name to the name of the host-tailer daemonset, which is host-tailer. Once matched, we parse them using the\u00a0parser\u00a0plugin. We only need the\u00a0message\u00a0field from the logs, so\u00a0key_name\u00a0is specified as the\u00a0message, and the\u00a0parse\u00a0type is set to\u00a0json. After this, we remove unwanted fields from the message field using the remove_keys spec from the\u00a0record_transformer\u00a0plugin.</p> <p>The globalOutputRefs is set to the name of the clusterOutput.</p> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: ClusterOutput\nmetadata:\n  name: host-logging-cloudwatch\n  namespace: cattle-logging-system\nspec:\n  cloudwatch:\n    auto_create_stream: true\n    format:\n      type: json\n    buffer:\n      timekey: 30s\n      timekey_use_utc: true\n      timekey_wait: 30s\n    log_group_name: hosted-group\n    log_stream_name: host-logs\n    region: us-west-2\n</code></pre> <pre><code>\n</code></pre> <p>In the clusterOutput spec, we use cloudwatch and define log_group_name, log_stream_name, and region.</p>"},{"location":"kbs/000021029/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021036/","title":"rancher-logging: how to control fluentd logs flushing frequency","text":"<p>This document (000021036) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021036/#situation","title":"Situation","text":"<p>By default fluentd in rancher-logging flushes logs every 11 minutes as it uses timekey for the range of logs which is set to 10 minutes and timekey_wait set to 1 minute.</p> <pre><code> &lt;buffer tag,time&gt;\n      @type file\n      chunk_limit_size 8MB\n      path /buffers/clusterflow:cattle-logging-system:test-es-flow-1:clusteroutput:cattle-logging-system:test-es-1.*.buffer\n      retry_forever true\n      timekey 10m\n      timekey_wait 1m\n    &lt;/buffer&gt;\n</code></pre>"},{"location":"kbs/000021036/#resolution","title":"Resolution","text":"<p>You have two parameters to manage how often fluentd should flush logs. The timekey specifies the time range of logs that should be grouped in chunks. With timekey of 10m, the chunk will contain logs within a 10 minutes time range.</p> <p>Example:\u00a0timekey 10m: <code>[\"12:00:00\", ..., \"12:09:59\"]</code>, <code>[\"12:10:00\", ..., \"12:19:59\"]</code></p> <p>The <code>timekey_wait</code>\u00a0 configures the flush delay for events. Below is an example to illustrate this</p> <p>timekey 10m</p> <p>-------------------------------------------------------</p> <p>time range for chunk | timekey_wait | actual flush time</p> <p>12:00:00 - 12:09:59\u00a0 | 60s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 12:11:00</p> <p>The two parameters are to be specified at the output/clusteroutput level to manage the logs flushing frequency from fluentd. At the creation, you have a section called Output Buffer\u00a0( <code>Logging &gt; Output &gt; Create &gt; Output Buffer</code>\u00a0or\u00a0<code>Logging &gt; ClusterOutput &gt; Create &gt; Output Buffer</code>) where this can be changed, or if you are using kubectl command line, then you can update the buffer as shown in the example below.</p> <pre><code>&lt;buffer tag,time&gt;\n      @type file\n      chunk_limit_size 8MB\n      path /buffers/clusterflow:cattle-logging-system:test-es-flow-1:clusteroutput:cattle-logging-system:test-es-1.*.buffer\n      retry_forever true\n      timekey 3m\n      timekey_wait 1m\n    &lt;/buffer&gt;\n</code></pre>"},{"location":"kbs/000021036/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021045/","title":"How to set container log rotation with RKE2","text":"<p>This document (000021045) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021045/#environment","title":"Environment","text":"<p>Container Log rotation with RKE2</p>"},{"location":"kbs/000021045/#situation","title":"Situation","text":"<p>In a Kubernetes cluster running an alternative container runtime, such as containerd, instead of Docker, the kubelet manages container logs. The kubelet default values in relation to log rotation can be found in the upstream\u00a0kubelet | Kubernetes\u00a0documentation,-%2D%2Dcontainer%2Druntime%20string). These values can be adjusted by adding options to the kubelet process.</p>"},{"location":"kbs/000021045/#resolution","title":"Resolution","text":"<p>Two kubelet options need to be added to RKE2 config file, /etc/rancher/rke2/config.yaml</p> <pre><code>kubelet-arg:\n  - \"container-log-max-files=5\"\n  - \"container-log-max-size=10Mi\"\n</code></pre> <p>Note\u00a0please adjust the values to suit your needs, for demonstration purposes the above commands used 5 log files of 10MB, allowing for 50MB of total space to be retained per container.</p>"},{"location":"kbs/000021045/#additional-information","title":"Additional Information","text":"<p>Here is a reference to K3s (which have a similar way to see using CLIt) for setting through binary command.( RKE2 CLI example page)</p>"},{"location":"kbs/000021045/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021053/","title":"Upgrading Your Rancher Downstream Clusters to Kubernetes v1.25","text":"<p>This document (000021053) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021053/#environment","title":"Environment","text":"<p>A Rancher-provisioned RKE, RKE2, or K3s cluster, which you wish to upgrade to Kubernetes v1.25</p>"},{"location":"kbs/000021053/#situation","title":"Situation","text":"<p>The PodSecurityPolicy API, initially deprecated in Kubernetes v1.21, was entirely removed i n Kubernetes v1.25. Because the API was removed, you cannot create, edit or query PodSecurityPolicy resources in a Kubernetes v1.25 cluster. Also, because its admission controller was removed, your clusters can no longer enforce any PodSecurityPolicy rules that were created in Kubernetes v1.24 and prior. Therefore, you must either migrate your workload security to the new Pod Security Admission controller, a supplemental policy engine or to a combination of both.</p> <p>This article discusses how to prepare your Rancher downstream clusters to Kubernetes v1.25, including how to use Rancher-specific mechanisms to apply a Pod Security Admission configuration and how to remove PodSecurityPolicies from Rancher-maintained workloads you have installed in your downstream clusters. It is structured as a semi-tutorial. Run these steps in a non-production environment to get acquainted with the process, and then determine what your production environment requires.</p>"},{"location":"kbs/000021053/#resolution","title":"Resolution","text":""},{"location":"kbs/000021053/#requirements","title":"Requirements","text":"<p>Before you upgrade your clusters to Kubernetes v1.25, make sure that:</p> <ul> <li> <p>You are running Rancher v2.7.2 or higher.</p> </li> <li> <p>Your downstream clusters are running Kubernetes v1.24.</p> </li> <li> <p>You have gone over the steps outlined in the Migrating from PodSecurityPolicies to the Built-In Pod Security Admission Controller Kubernetes documentation to map your PodSecurityPolicies into a Pod Security Admission configuration or into Pod Security Standards labels.</p> </li> <li> <p>You have evaluated whether Pod Security Admission is suitable for your needs. At the end of this article, there are some linked resources on supplemental policy engines you may wish to add to your cluster if Pod Security Admission is not enough for your use case.</p> </li> </ul>"},{"location":"kbs/000021053/#configuring-the-pod-security-admission-controller","title":"Configuring the Pod Security Admission controller","text":"<p>The first step is to configure the new Pod Security Admission controller. In Kubernetes v1.24, this admission controller is already enabled by default in the kube-apiserver. There are a few ways this admission controller can be configured. One of the options is to deploy a cluster-wide AdmissionConfiguration via kube-apiserver command-line arguments. As of version 2.7.2, Rancher provides two out-of-the-box AdmissionConfigurations you can use, or you can also create your own Pod Security Admission Configuration via the Rancher user interface. These features will be covered in greater detail below.</p>"},{"location":"kbs/000021053/#review-the-pod-security-admission-configuration-presets","title":"Review the Pod Security Admission Configuration presets","text":"<p>To check out the Pod Security Admission Configuration presets shipped with Rancher, navigate to Cluster Management -&gt; Advanced -&gt; Pod Security Admissions while logged in to Rancher as an administrator. You will see two available presets: rancher-privileged and rancher-restricted. The rancher-restricted sets the enforce, warn, and audit values to restricted and includes some namespace exemptions to allow Rancher to work properly in your cluster without further intervention. The rancher-privileged preset is equivalent to the sample AdmissionConfiguration available in Kubernetes documentation and does not provide any specific security assurances. You can read more about Pod Security Admission and Pod Security Standards on the Rancher Docs.</p>"},{"location":"kbs/000021053/#optional-create-a-custom-pod-security-admission-configuration","title":"(Optional) Create a custom Pod Security Admission configuration","text":"<p>You can also create your own Pod Security Admission configuration. To do so, navigate to Cluster Management -&gt; Advanced -&gt; Pod Security Admissions, and click Create:</p> <p></p> <p></p> <p>Choose a name for your new Pod Security Admission Configuration and the rules you would like to apply for enforcing, auditing and warning. You can also specify exemptions if any are applicable for your use case. After filling out the form, make sure to click Create.</p>"},{"location":"kbs/000021053/#configuring-your-cluster-to-use-a-pod-security-admission-configuration","title":"Configuring your cluster to use a Pod Security Admission Configuration","text":"<p>You can leverage the Rancher user interface to apply a cluster-wide Pod Security Admission Configuration. In your Cluster Management tab in Rancher Manager, select the hamburger menu for the cluster you would like to configure, then select the Edit Config option:</p> <p></p> <p>Follow the cluster-specific steps to enable it:</p> <p></p> <p></p> <ul> <li>RKE: Navigate to Advanced Options, then select your preferred option in the Pod Security Admission Configuration Template field. </li> <li>RKE2 or K3S: Navigate to the Cluster Configuration panel. In the Basics pane, under the Security section, select your preferred option in the Pod Security Admission Configuration Template field. </li> </ul> <p>After finishing your configuration, make sure to save and test it.</p>"},{"location":"kbs/000021053/#configuring-individual-namespaces-manually","title":"Configuring individual namespaces manually","text":"<p>For configuring namespaces individually with the Pod Security Standards labels, follow the Kubernetes documentation on the topic. Note that when a Pod Security Admission Configuration is combined with Pod Security Standards labels, any Pod Security Standards labels applied to resources marked as exempt in the Pod Security Admission Configuration will be ignored by the admission controller.</p>"},{"location":"kbs/000021053/#removing-podsecuritypolicies","title":"Removing PodSecurityPolicies","text":"<p>This section assumes you have already mapped your PodSecurityPolicies into a Pod Security Admission Configuration and Pod Security Standards namespace labels and that your cluster conforms to the requirements. If you have not done so already, go over the Kubernetes documentation on migrating from PodSecurityPolicies to the built-in PodSecurity Admission Controller.</p> <p>You should not remove PodSecurityPolicies manually. Deleting PodSecurityPolicies that were added by a Helm chart via kubectl delete will not remove their references from the Helm release and can lead to situations where a Helm chart cannot be upgraded or even removed. To learn more about how to prevent this situation, continue reading this section.</p>"},{"location":"kbs/000021053/#upgrade-your-apps-marketplace-charts-to-remove-podsecuritypolicies","title":"Upgrade your Apps &amp; Marketplace charts to remove PodSecurityPolicies","text":"<p>Rancher-maintained workloads that previously installed PodSecurityPolicies have a new version with the format v102.x.y that allows you to remove those resources. Notable changes include:</p> <ul> <li> <p>The creation of a new PodSecurityPolicy switch: global.cattle.psp.enabled. The previous PodSecurityPolicy switches have been superseded by this new one.</p> </li> <li> <p>The addition of a cluster capability verification for PodSecurityPolicies before a chart installation. If you try to install these new charts into your Kubernetes v1.25 cluster with the PodSecurityPolicy switch turned on, you will see an error message asking you to disable PodSecurityPolicies before proceeding.</p> </li> </ul> <p>For a smooth upgrade to Kubernetes v1.25, you must remove PodSecurityPolicy resources that were deployed with Helm. To do so, upgrade each of your charts\u2019 installations to the newest version v102.0.0, making sure to set the PodSecurityPolicy switch global.cattle.psp.enabled value to false.</p>"},{"location":"kbs/000021053/#verify-all-workloads-in-the-cluster-have-been-migrated-to-pod-security-admission","title":"Verify all workloads in the cluster have been migrated to Pod Security Admission","text":"<p>Verify that other workloads you have in your cluster have also been migrated from PodSecurityPolicies to Pod Security Admission and Standards. You can check which PodSecurityPolicies still exist in your cluster by running kubectl get podsecuritypolicies. Note that the presence of a PodSecurityPolicy resource in a cluster does not mean that there are workloads using it.</p> <p>To check which PodSecurityPolicies are still in use, you can run the command below. Be aware that this strategy may miss workloads that are not currently running, such as CronJobs, workloads that are currently scaled to zero or other workloads that have not rolled out:</p> <pre><code>kubectl get pods --all-namespaces \\\n  --output jsonpath=\"{.items[*].metadata.annotations.kubernetes\\.io\\/psp}\" \\\n  | tr \" \" \"\\n\" | sort -u\n</code></pre> <p>To learn more about this strategy for checking for PodSecurityPolicies in use, see section 3.a. Identify an appropriate Pod Security level of the PodSecurityPolicy migration documentation for Kubernetes.</p>"},{"location":"kbs/000021053/#upgrade-your-cluster-to-kubernetes-v125","title":"Upgrade your cluster to Kubernetes v1.25","text":"<p>After the previous steps are complete and there are no leftover PodSecurityPolicies in your cluster, it is time to upgrade your cluster to Kubernetes v1.25. For your downstream clusters, you can do so via the user interface.</p> <p>To upgrade your clusters, navigate to Cluster Management. In the Clusters page, click the hamburger menu for the cluster you wish to upgrade to Kubernetes v1.25, then select the Edit Config option. Change the Kubernetes version according to your cluster type:</p> <ul> <li> <p>RKE: navigate to Cluster Options -&gt;\u00a0Kubernetes options. In the Kubernetes Version field, select the v1.25 patch you would like to upgrade to.</p> </li> <li> <p>RKE2 or K3S: navigate to the Cluster Configuration panel and select the Basics pane. Under the Basics section, find the Kubernetes Version field. Select the v1.25 patch you would like to upgrade to.</p> </li> </ul> <p>Save the selected configuration. You will see the status for your cluster transition from Active to Upgrading. The upgrade may take some time and will be reflected in the Cluster Management list view once it is completed, as your cluster status will be once again Active.</p>"},{"location":"kbs/000021053/#my-use-case-requires-fine-grained-policies","title":"My use case requires fine-grained policies","text":"<p>Because Pod Security Admission and Pod Security Standards are not as granular as PodSecurityPolicies and do not provide mutating features to ensure pods comply with the rules, you may wish to complement the functionality by having a separate admission controller installed in your cluster. On the Kubernetes landscape, there are admission controllers that offer mutating admissions and more granular validation functionalities, such as Kubewarden, Kyverno, Neuvector and OPA Gatekeeper, to cite some names. You can find links to documentation on these supplemental policy engines at the end of this article.</p>"},{"location":"kbs/000021053/#troubleshooting","title":"Troubleshooting","text":""},{"location":"kbs/000021053/#i-forgot-to-check-my-workloads-for-lingering-podsecuritypolicies","title":"I forgot to check my workloads for lingering PodSecurityPolicies","text":"<p>After the upgrade to Kubernetes v1.25, if you forgot to check for lingering PodSecurityPolicies (or other APIs that were discontinued), you may notice that some Helm releases cannot be upgraded or uninstalled. If this happens, you can use the helm-mapkubeapis plugin to restore your release to a working state. This plugin reads Helm release data and replaces superseded APIs with their new versions or removes resources that refer to APIs that were completely removed from Kubernetes.</p> <p>Note that Helm plugins are installed locally in the machine where the commands are run. Therefore, please make sure to run the installation steps in the same machine where you intend to run the cleanup steps.</p> <p>Install helm-mapkubeapis</p> <ol> <li>In your terminal of choice, make sure that Helm is installed by running helm version. You should see an output similar to the following:</li> </ol> <pre><code>version.BuildInfo{Version:\"v3.10.2\", GitCommit:\"50f003e5ee8704ec937a756c646870227d7c8b58\", GitTreeState:\"clean\", GoVersion:\"go1.18.8\"}\n</code></pre> <ol> <li>Install the helm-mapkubeapis plugin:</li> </ol> <pre><code>helm plugin install https://github.com/helm/helm-mapkubeapis\n</code></pre> <p>Output should be similar to:</p> <pre><code>Downloading and installing helm-mapkubeapis v0.4.1 \u2026\nhttps://github.com/helm/helm-mapkubeapis/releases/download/v0.4.1/helm-mapkubeapis_0.4.1_darwin_amd64.tar.gz\nInstalled plugin: mapkubeapis\n</code></pre> <ol> <li>Check that the plugin was installed correctly:</li> </ol> <pre><code>helm mapkubeapis --help\n</code></pre> <p>Output should be similar to:</p> <pre><code>Map release deprecated or removed Kubernetes APIs in-place\nUsage:\n   \u00a0\u00a0\u00a0 mapkubeapis [flags] RELEASE\n\nFlags:\n   \u00a0\u00a0\u00a0 --dry-run\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 simulate a command\n   \u00a0\u00a0\u00a0 -h, --help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 help for mapkubeapis\n   \u00a0\u00a0\u00a0 --kube-context string\u00a0\u00a0 name of the kubeconfig context to use\n   \u00a0\u00a0\u00a0 --kubeconfig string\u00a0\u00a0\u00a0\u00a0 path to the kubeconfig file\n   \u00a0\u00a0\u00a0 --mapfile string\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 path to the API mapping file\n   \u00a0\u00a0\u00a0 --namespace string\u00a0\u00a0\u00a0\u00a0\u00a0 namespace scope of the release\n</code></pre> <p>Make sure that the installed version of helm-mapkubeapis is v0.4.1 or later, as earlier versions do not support removal of resources.</p> <p>Clean up releases with lingering PodSecurityPolicies</p> <ol> <li>Open your terminal of choice and make sure the target cluster is accessible by running</li> </ol> <pre><code>kubectl cluster-info\n</code></pre> <ol> <li>List all the releases installed in your cluster by running</li> </ol> <pre><code>helm list --all-namespaces\n</code></pre> <ol> <li>Perform a dry run for each one of the releases you wish to clean up with helm</li> </ol> <pre><code>    helm mapkubeapis --dry-run &lt;release-name&gt; --namespace &lt;namespace-name&gt;\n</code></pre> <p>The output will inform you what resources will be replaced or removed.</p> <ol> <li>After reviewing the changes, perform a full run with helm</li> </ol> <pre><code>helm mapkubeapis &lt;release-name&gt; --namespace &lt;namespace-name&gt;\n</code></pre> <p>Upgrade your workloads to a Kubernetes v1.25 supported version</p> <p>After cleaning up any broken releases, you will need to upgrade your workloads to a version that is supported in Kubernetes v1.25. This is a necessary step that should not be skipped, as cleaned up releases are not guaranteed to work or have the security required in Kubernetes v1.25.</p> <p>For Rancher-maintained workloads, follow the steps outlined in the Removing PodSecurityPolicies section of this article. For other workloads, refer to the vendor documentation.</p>"},{"location":"kbs/000021053/#next-steps","title":"Next steps","text":"<ul> <li> <p>Learn more about the Pod Security Admission controller and Pod Security Standards.</p> </li> <li> <p>Learn more about supplemental policy engines: Kubewarden, Kyverno, NeuVector, and OPA Gatekeeper.</p> </li> <li> <p>Check out how to migrate your PodSecurityPolicies to Kubewarden policies.</p> </li> <li> <p>Learn more about how to use helm-mapkubeapis.</p> </li> </ul>"},{"location":"kbs/000021053/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021064/","title":"Fleet cluster in Wait Applied state is missing the fleet-agent","text":"<p>This document (000021064) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021064/#environment","title":"Environment","text":"<p>Rancher 2.6/2.7/2.8</p> <p>At least one downstream cluster.</p>"},{"location":"kbs/000021064/#situation","title":"Situation","text":"<p>On the Rancher UI, when exploring the Continuous Delivery &gt; Clusters section, there may be clusters that are in the Wait Applied state.</p> <p>This is likely because the fleet-agent deployment is missing in the cluster.</p> <p>To confirm, explore the cluster and go to Workloads &gt; Deployments and check to see if there is a fleet-agent deployment under the cattle-fleet-system namespace.</p>"},{"location":"kbs/000021064/#resolution","title":"Resolution","text":"<p>If the fleet-agent deployment is missing, it can be recovered by doing a Force Update on the clusters in question in the Continuous Delivery &gt; Clusters section of the Rancher UI.</p> <p>If this solution does not resolve the issue, please reach out to our support team for thorough diagnosis and assistance.</p>"},{"location":"kbs/000021064/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021065/","title":"How to remove Kubernetes namespaces stuck in a Terminating state","text":"<p>This document (000021065) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021065/#environment","title":"Environment","text":"<p>A Kubernetes cluster with a Namespace stuck in a Terminating state</p>"},{"location":"kbs/000021065/#situation","title":"Situation","text":"<p>When attempting to delete a Namespace within a Kubernetes cluster, the Namespace is stuck in a Terminating state and is not being successfully removed.</p> <p>Some online guides for this situation instruct users to remove the kubernetes finalizer from affected Namespaces, to force their deletion from the cluster. However, the kubernetes finalizer should not be manually removed from a Namespace. Whilst its removal will permit the deletion of the Namespace resource, it may leave orphaned resources within it, which will be accessible again via the Kubernetes API if a Namespace with the same name is created again, and may cause other cluster issues.</p> <p>This article details how to correctly identify the issue preventing deletion of a Namespace and how to safely remedy this.</p>"},{"location":"kbs/000021065/#resolution","title":"Resolution","text":"<p>A Namespace stuck in a Terminating state is most frequently caused by one of two issues:</p> <ol> <li>An unavailable APIService, e.g. the v1beta1.custom.metrics.k8s.io APIService was created within the cluster, but the workload backing the service that implements it has subsequently been deleted.</li> <li>A resource present within the Namespace, which has a finalizer that can no longer be satisfied, i.e. a custom resource controlled by an operator, with a finalizer referencing the operator, but where the operator has already been removed from the cluster.</li> </ol> <p>To identify and resolve these issues, follow the instructions below.</p> <p>Check if an APIService is unavailable</p> <ol> <li>Check if any APIService is unavailable by running\u00a0<code>kubectl get apiservice | grep False</code>, per the following example:</li> </ol> <pre><code>kubectl get apiservices | grep False\nv1beta1.custom.metrics.k8s.io          cattle-monitoring-system/rancher-monitoring-prometheus-adapter   False (ServiceNotFound)   55m\n</code></pre> <ol> <li>If an unavailable APIService is identified, such as v1beta1.custom.metrics.k8s.io in the example above, then take steps to resolve this:</li> <li>If the workload serving the API has been removed from the cluster, and the APIService is no longer expected to work, then delete it with <code>kubectl delete apiservice &lt;name&gt;</code>, e.g. <code>kubectl delete apiservice v1beta1.custom.metrics.k8s.io</code></li> <li>If the APIService is expected to work, then investigate why the Kubernetes Service and/or workload backing it are not working, and fix them to resolve the unavailable APIService.</li> </ol> <p>Identify and remove those resources, within the Namespace, which have a finalizer that cannot be satisfied</p> <ol> <li>Find all resources that still exist where  is the Namespace stuck in a terminating state: <pre><code>kubectl api-resources --verbs=list --namespaced -o name | xargs -I {} sh -c 'echo \"Checking for {} resources\"; kubectl get {} -n &lt;namespace&gt;;'\n</code></pre> <ol> <li>Remove the finalizers from any resources within the Namespace identified from the output of the command above:</li> </ol> <pre><code>kubectl -n &lt;namespace&gt; patch &lt;kind&gt; &lt;name&gt; -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\n</code></pre>"},{"location":"kbs/000021065/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021078/","title":"failed etcd snapshot with StorageError invalid object message","text":"<p>This document (000021078) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021078/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>Rancher 2.7.x</p> <p>Downstream RKE2 cluster</p> <p>Kubernetes 1.22 and above</p>"},{"location":"kbs/000021078/#situation","title":"Situation","text":"<p>During the manual or recurring snapshot process you're seeing in Rancher logs the following error messages:</p> <pre><code>23/05/04 14:45:01 [INFO] [snapshotbackpopulate] rkecluster fleet-default/xxxx-yyyy-wwww: processing configmap kube-system/rke2-etcd-snapshots\n2023/05/04 14:45:02 [ERROR] error syncing 'kube-system/rke2-etcd-snapshots': handler snapshotbackpopulate: rkecluster fleet-default/xxxx-yyyy-wwww: error while setting status missing=true on etcd snapshot /: Operation cannot be fulfilled on etcdsnapshots.rke.cattle.io \"xxxx-yyyy-wwww-etcd-snapshot-clstr-k8s-xxxx-yyyy-wwww-0102b\": StorageError: invalid object, Code: 4, Key: /registry/rke.cattle.io/etcdsnapshots/fleet-default/xxxx-yyyy-wwww-etcd-snapshot-clstr-k8s-xxxx-yyyy-wwww-0102b, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: fzzzzzz-1111-2222-3333-000000000000, UID in object meta: , requeuing\n\n2023/05/04 14:47:01 [INFO] [snapshotbackpopulate] rkecluster fleet-default/xxxx-yyyy-wwww: processing configmap kube-system/rke2-etcd-snapshots\n2023/05/04 14:47:10 [ERROR] rkecluster fleet-default/xxxx-yyyy-wwww: error while creating s3 etcd snapshot fleet-default/clstr-k8s-xxxx-yyyy-wwww-.-s3: ETCDSnapshot.rke.cattle.io \"clstr-k8s-xxxx-yyyy-wwww-.-s3\" is invalid: metadata.name: Invalid value: \"clstr-k8s-xxxx-yyyy-wwww-.-s3\": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')\n</code></pre> <p>This message is indicating that snapshot is in process but failing due to possibly:</p> <p>- issue with the S3-compatible storage hosting solution</p> <p>- not respecting the limit name length of objects that Kubernetes is allowed to (RFC 1123)</p> <p>- corrupted snapshot with invalid/malformed name hosted on S3 bucket but Rancher (snapshot controller) keeps adding them to the configmap</p>"},{"location":"kbs/000021078/#resolution","title":"Resolution","text":"<p>If it's complaining about the name length, then please double check the naming convention in your environment and change it to respect the requirement. Refer to RFC 1123 Kubernetes.</p> <p>If it's complaining about snapshot itself, you may want to check the S3 bucket to see if there are still snapshots in the specified folder with the invalid file name.</p> <p>The snapshot controller on the rke2 side will keep re-adding those snapshots to the confimap as long as the files exist in the bucket or on disk on the nodes.</p> <p>Removing those corrupted / malformed snapshots from the bucket and/or nodes is useful in this case.</p> <p>At some point Rancher can get stuck finishing the process even if the other parameters (S3 config, snapshot names etc...) are fine.</p> <p>For that purpose, a small and safe edit can get around and let Rancher proceed properly like:</p> <p>On Rancher UI, go to Cluster Management</p> <p>Select the downstream cluster having the issue</p> <p>Edit the cluster configuration and refer to ETCD</p> <p>Change Folder Name in the S3 bucket configuration</p> <p>Save and exit then wait for the change to finish applying</p> <p>Edit the cluster again the same way</p> <p>Change the folder back to its original name</p> <p>Save and exit</p> <p>This should re-instruct Rancher to use the right folder name again properly.</p> <p>It's also good to restart the Rancher deployment for better responsiveness.</p> <p>If the above doesn't have any effect, try the following steps</p> <ul> <li>Save\u00a0copies of etcd snapshots in another folder as a precaution.</li> <li>Reduce\u00a0the etcd snapshots retention to 10 snapshots (instead of 40) on the downstream cluster and disable\u00a0S3 backups temporarily.</li> <li>Edite\u00a0the 'rke2-etcd-snapshots' ConfigMap on 'kube-system' on the downstream cluster and empty\u00a0it out of its data (only keeping the manifest metadata)</li> </ul> <p>&gt; 'kubectl edit ConfigMap -n kube-system rke2-etcd-snapshots'.</p> <ul> <li>After saving the edits above, Fleet triggered all of the snapshots it missed (this is likely because it had the old snapshot jobs in a queue).</li> <li>Change\u00a0the snapshot schedule to every 5 minutes to allow it to apply its retention settings and clean up the snapshots. This should work\u00a0after waiting for the 5 minutes period.</li> <li>After that, clean the on-demand snapshots since they do not get cleaned up automatically by the retention settings.</li> </ul> <p>To do so, since the Delete option in the UI is not available, delete them on the local filesystem of each node. After around 15 to 20 minutes (maybe less depending on the environment), Rancher should reconcile the changes, and the old on-demand snapshots should be removed from the UI.</p> <ul> <li>Then re-enable\u00a0S3 snapshots and check\u00a0if new snapshots are being taken there.</li> <li>Finally, set back the schedule of snapshots as it was\u00a0before (5 hours, 1 day etc...).</li> </ul> <p>Note: the above steps are tested and validated on various clusters.</p>"},{"location":"kbs/000021078/#cause","title":"Cause","text":"<p>This can happen due to a couple of reasons such as:</p> <p>Failure on an ETCD restore from backup</p> <p>Failure during the upgrade process</p> <p>Corrupted old/previous snapshots</p>"},{"location":"kbs/000021078/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021083/","title":"How to define routes and matchers in AlertmanagerConfigs","text":"<p>This document (000021083) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021083/#environment","title":"Environment","text":"<p>Rancher 2.x</p> <p>Rancher Monitoring enabled</p>"},{"location":"kbs/000021083/#situation","title":"Situation","text":"<p>Using the wrong syntax can stop alerts being sent correctly.</p>"},{"location":"kbs/000021083/#resolution","title":"Resolution","text":"<p>In a case where we want to :</p> <p>*\u00a0Catch\u00a0all instances where alertname = foo</p> <p>\\ Send them to the\u00a0receiver \"myReceiver\"* the correct syntax would be:</p> <pre><code>routes:\n  - matchers:\n\u00a0   - name: alertname\n\u00a0     value: foo\n\u00a0 \u00a0   receiver: 'myReceiver'\n\u00a0 \u00a0 \u00a0 regex: false\n</code></pre>"},{"location":"kbs/000021083/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021084/","title":"What information is stored from external Auth providers like Azure AD?","text":"<p>This document (000021084) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021084/#environment","title":"Environment","text":"<p>Rancher v2.x with an external auth provider configured</p>"},{"location":"kbs/000021084/#situation","title":"Situation","text":"<p>When using external auth providers a frequently asked question is what information does rancher import from the auth provider and where does it store it.</p>"},{"location":"kbs/000021084/#resolution","title":"Resolution","text":"<p>Rancher imports the UUIDs of users and groups.</p> <p>A local user is made at first login and mapped to the external UUID.</p> <p>The Azure AD example here:</p> <pre><code>Principal Ids:\n\n azuread_user://a913127f-02ec-4820-b5c8-7e240a4e63d0\n\n local://u-qmxsorn2uz\n</code></pre> <p>Further that users group membership is mapped in userattributes:</p> <pre><code>  Azuread:\n    Principalid:\n      azuread_user://6a5547cc-5e77-4187-8114-420a43fbda8a\n.....\nGroup Principals:\n  Azuread:\n    Items:\n      Display Name:  Rancher\n      Member Of:     true\n      Metadata:\n        Creation Timestamp:  &lt;nil&gt;\n        Name:                azuread_group://ac3dc906-cdf7-4357-997e-931e6b783a1c\n      Principal Type:        group\n      Provider:              azuread\n      Display Name:          TestProject\n      Member Of:             true\n      Metadata:\n        Creation Timestamp:  &lt;nil&gt;\n        Name:                azuread_group://4c21bffd-c79e-4420-8070-a2609417842f\n      Principal Type:        group\n      Provider:              azuread\n</code></pre> <pre><code>\n</code></pre> <p>Finally the group UUIDs are also used to map to (global)rolebindings:</p> <pre><code>Group Principal Name: azuread_group://4c21bffd-c79e-4420-8070-a2609417842f\n</code></pre>"},{"location":"kbs/000021084/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021088/","title":"Get the list of available Kubernetes versions for RKE","text":"<p>This document (000021088) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021088/#environment","title":"Environment","text":"<ul> <li>RKE 1.6+</li> <li>Rancher v2.9.x+</li> </ul>"},{"location":"kbs/000021088/#situation","title":"Situation","text":"<p>One wants to install an RKE cluster without the default version of RKE or the Kubernetes version is not listed on the Rancher UI</p>"},{"location":"kbs/000021088/#resolution","title":"Resolution","text":"<p>1. For RKE CLI:</p> <p>Through the RKE CLI, one might get the available versions of Kubernetes for that release:</p> <pre><code>$ rke config --list-version --all\nv1.30.2-rancher1-1\nv1.28.11-rancher1-1\nv1.29.6-rancher1-1\nv1.27.15-rancher1-1\n\n$ rke version\nrke version\nINFO[0000] Running RKE version: v1.6.0\n</code></pre> <p>Without the flag --all it will display the default version of the Kubernetes to be installed if the version is not specified on the cluster.yml. Documentation on how to do it can be found here</p> <p>2. For RKE Kubernetes versions available on Rancher, check the RKE k8s System Images resource:</p> <pre><code>kubectl get rkek8ssystemimages.management.cattle.io -n cattle-global-data\n</code></pre>"},{"location":"kbs/000021088/#additional-information","title":"Additional Information","text":"<p>To get the system images available, e.g. core-dns or calico, use:</p> <pre><code># for the current k8s version\n$ rke config --system-images\nINFO[0000] Generating images list for version [v1.30.2-rancher1-1]:\nrancher/mirrored-coreos-etcd:v3.5.12\nrancher/rke-tools:v0.1.100\nrancher/mirrored-k8s-dns-kube-dns:1.23.0\nrancher/mirrored-k8s-dns-dnsmasq-nanny:1.23.0\nrancher/mirrored-k8s-dns-sidecar:1.23.0\nrancher/mirrored-cluster-proportional-autoscaler:v1.8.9\nrancher/mirrored-coredns-coredns:1.11.1\nrancher/mirrored-k8s-dns-node-cache:1.23.0\nrancher/hyperkube:v1.30.2-rancher1\nrancher/mirrored-flannel-flannel:v0.25.1\nrancher/flannel-cni:v1.4.1-rancher1\nrancher/mirrored-calico-node:v3.28.0\nrancher/calico-cni:v3.28.0-rancher1\nrancher/mirrored-calico-kube-controllers:v3.28.0\nrancher/mirrored-calico-ctl:v3.28.0\nrancher/mirrored-calico-pod2daemon-flexvol:v3.28.0\nrancher/mirrored-pause:3.7\nrancher/nginx-ingress-controller:nginx-1.10.1-rancher1\nrancher/mirrored-nginx-ingress-controller-defaultbackend:1.5-rancher1\nrancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.4.1\nrancher/mirrored-metrics-server:v0.7.1\nnoiro/cnideploy:6.0.4.2.81c2369\nnoiro/aci-containers-host:6.0.4.2.81c2369\nnoiro/opflex:6.0.4.2.81c2369\nnoiro/openvswitch:6.0.4.2.81c2369\nnoiro/aci-containers-controller:6.0.4.2.81c2369\n\n# for a specific k8s version\n$ rke config --system-images --version v1.24.13-rancher2-1\n\n# for all available k8s on the rke distribution\n$ rke config --system-images --all\n</code></pre> <p>Note: the information is also available on the RKE release page: https://github.com/rancher/rke/releases/</p>"},{"location":"kbs/000021088/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021095/","title":"Exposing the ingress-nginx controller with a LoadBalancer service in RKE1 and RKE2 Kubernetes Clusters","text":"<p>This document (000021095) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021095/#environment","title":"Environment","text":"<p>An RKE or RKE2 cluster deployed with the bundled ingress-nginx ingress controller</p>"},{"location":"kbs/000021095/#situation","title":"Situation","text":"<p>This knowledge base article provides instructions on configuring the ingress controller in Rancher RKE1 and RKE2 Kubernetes clusters to be exposed through a LoadBalancer service instead of the default host ports 80 and 443 on worker nodes. This is particularly useful when running a Kubernetes cluster on a cloud provider that supports automatic configuration and management of LoadBalancer services through Kubernetes's cloud provider integration.</p>"},{"location":"kbs/000021095/#resolution","title":"Resolution","text":""},{"location":"kbs/000021095/#exposing-the-ingress-controller-with-a-loadbalancer-service-in","title":"Exposing the Ingress Controller with a LoadBalancer Service in:","text":""},{"location":"kbs/000021095/#rke1","title":"RKE1:","text":"<p>In RKE1, it is not possible to directly configure the ingress-nginx controller with a LoadBalancer service through the ingress options in the cluster configuration. However, you can manually create a LoadBalancer service as shown below:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n\u202f name: ingress-nginx-lb\n\u202f namespace: ingress-nginx\nspec:\n\u202f ports:\n\u202f\u202f\u202f - name: http\n\u202f\u202f\u202f\u202f\u202f port: 80\n\u202f\u202f\u202f\u202f\u202f protocol: TCP\n\u202f\u202f\u202f\u202f\u202f targetPort: 80\n\u202f\u202f\u202f - name: https\n\u202f\u202f\u202f\u202f\u202f port: 443\n\u202f\u202f\u202f\u202f\u202f protocol: TCP\n\u202f\u202f\u202f\u202f\u202f targetPort: 443\n\u202f selector:\n\u202f\u202f\u202f app: ingress-nginx\n\u202f type: LoadBalancer\n</code></pre> <p>This will create a LoadBalancer service named \"ingress-nginx-lb\" in the \"ingress-nginx\" namespace, exposing ports 80 and 443.</p> <p>This LoadBalancer service manifest can be added to the cluster via the user addons configuration, as documented at https://rke.docs.rancher.com/config-options/add-ons/user-defined-add-ons, to manage and deploy it alongside the cluster components/upgrades.</p> <p>RKE2:</p> <p>In RKE2, the ingress-nginx controller is managed through a Helm chart, allowing configuration changes using a HelmChartConfig resource. For more information see the following - https://docs.rke2.io/helm#customizing-packaged-components-with-helmchartconfig. Here is an example below of how to achieve this configuration.</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n\u202f name: rke2-ingress-nginx\n\u202f namespace: kube-system\nspec:\n\u202f valuesContent: |-\n\u202f\u202f\u202f controller:\n      hostPort:\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f enabled: false\n\u202f\u202f\u202f\u202f\u202f service:\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f enabled: true\n\u202f\u202f\u202f\u202f\u202f\u202f\u202f type: LoadBalancer\n</code></pre> <p>This configuration sets the \"type\" of the nginx ingress controller service to LoadBalancer, allowing it to be exposed through a cloud LoadBalancer.</p> <p>For standalone RKE2 clusters, this HelmChartConfig manifest can be defined within the manifests directory on server nodes, as documented at https://docs.rke2.io/helm#customizing-packaged-components-with-helmchartconfig.</p> <p>For Rancher-provisioned RKE2 clusters, this HelmChartConfig manifest can be defined within the cluster configuration under 'Additional Manifest'.</p>"},{"location":"kbs/000021095/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021114/","title":"Recovering cluster.yml and cluster.rkestate files from kubeconfig - RKE clusters","text":"<p>This document (000021114) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021114/#environment","title":"Environment","text":""},{"location":"kbs/000021114/#important-this-script-is-specifically-intended-for-use-with-clusters-created-using-the-rke-cli-do-not-utilize-it-against-any-other-clusters-eg-a-rancher-created-downstream-cluster","title":"Important: This script is specifically intended for use with clusters created using the RKE CLI.  Do not utilize it against any other clusters (e.g. a Rancher-created downstream cluster).","text":""},{"location":"kbs/000021114/#situation","title":"Situation","text":"<p>During the installation of an RKE Kubernetes cluster, two essential files are created locally in the working directory where you invoke the RKE CLI:</p> <ul> <li>cluster.yml: also recognized as the\u00a0Cluster Configuration File, this file\u00a0is\u00a0referenced by\u00a0RKE\u00a0to determine what nodes will be in the cluster and how to deploy Kubernetes.</li> <li>cluster.rkestate: the\u00a0Kubernetes Cluster State file, which\u00a0contains the credentials for full access to the cluster.</li> </ul> <p>These files are needed to maintain, troubleshoot and upgrade your cluster and, therefore, should always be preserved in a secure location. However, if something unforeseen happens, and these files are lost, it is possible to recover them from the cluster itself, per the steps below.</p>"},{"location":"kbs/000021114/#resolution","title":"Resolution","text":"<p>To recover these two files, it's possible to use the following script. Please note that you will need to fulfill these prerequisites:</p> <ul> <li>Access to the kubectl command line tool, with the kubeconfig file correctly configured to access the cluster.</li> <li>jq\u00a0command-line JSON processor installed.</li> <li>yq\u00a0command-line YAML, JSON, and XML processor installed.</li> </ul> <p>For RKE binary versions &lt; 1.4.19 , 1.5.10 or 1.6.0:</p> <p>On RKE versions prior to 1.4.19, 1.5.10 or 1.6.10, the Kubernetes Cluster state file (cluster.rkestate) and cluster.yml files are stored as a configmap, under the kube-system namespace.</p> <pre><code>#!/bin/bash\necho \"Building cluster_recovery.yml...\"\necho \"Working on Nodes...\"\necho 'nodes:' &gt; cluster_recovery.yml\nkubectl -n kube-system get configmap full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .desiredState.rkeConfig.nodes | yq -P | sed 's/^/  /' | \\\nsed -e 's/internalAddress/internal_address/g' | \\\nsed -e 's/hostnameOverride/hostname_override/g' | \\\nsed -e 's/sshKeyPath/ssh_key_path/g' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on services...\"\necho 'services:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get configmap full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .desiredState.rkeConfig.services | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on network...\"\necho 'network:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get configmap full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .desiredState.rkeConfig.network | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on authentication...\"\necho 'authentication:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get configmap full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .desiredState.rkeConfig.authentication | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on systemImages...\"\necho 'system_images:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get configmap full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r .desiredState.rkeConfig.systemImages | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Building cluster_recovery.rkestate...\"\nkubectl -n kube-system get configmap full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | jq -r . &gt; cluster_recovery.rkestate\n</code></pre> <p>For RKE binary versions\u00a0 1.4.19+ , 1.5.10+ or 1.6.0+:</p> <p>On RKE 1.4.19, 1.5.10 and 1.6.0 or higher, the Kubernetes Cluster state file (cluster.rkestate) and cluster.yml files are stored as a secret, under the kube-system namespace.</p> <pre><code>#!/bin/bash\necho \"Building cluster_recovery.yml...\"\necho \"Working on Nodes...\"\necho 'nodes:' &gt; cluster_recovery.yml\nkubectl -n kube-system get secret full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .desiredState.rkeConfig.nodes | yq -P | sed 's/^/  /' | \\\nsed -e 's/internalAddress/internal_address/g' | \\\nsed -e 's/hostnameOverride/hostname_override/g' | \\\nsed -e 's/sshKeyPath/ssh_key_path/g' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on services...\"\necho 'services:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get secret full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .desiredState.rkeConfig.services | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on network...\"\necho 'network:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get secret full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .desiredState.rkeConfig.network | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on authentication...\"\necho 'authentication:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get secret full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .desiredState.rkeConfig.authentication | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Working on systemImages...\"\necho 'system_images:' &gt;&gt; cluster_recovery.yml\nkubectl -n kube-system get secret full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r .desiredState.rkeConfig.systemImages | yq -P | sed 's/^/  /' &gt;&gt; cluster_recovery.yml\necho \"\" &gt;&gt; cluster_recovery.yml\n\necho \"Building cluster_recovery.rkestate...\"\nkubectl -n kube-system get secret full-cluster-state -o json | jq -r .data.\\\"full-cluster-state\\\" | base64 -d | jq -r . &gt; cluster_recovery.rkestate\n</code></pre> <p>Once the execution is completed, you will find two files in the folder of execution: cluster_recovery.yml and cluster_recovery.rkestate, which correspond to a copy of the Cluster Configuration File and the Kubernetes Cluster State file.</p> <p>After the recovery is done, please back up these files in a secure location to avoid future loss.</p>"},{"location":"kbs/000021114/#cause","title":"Cause","text":"<p>This change has been implemented to mitigate the\u00a0CVE-2023-32191.\u00a0 More information is available at https://github.com/rancher/rke/security/advisories/GHSA-6gr4-52w6-vmqx</p>"},{"location":"kbs/000021114/#additional-information","title":"Additional Information","text":"<p>Original source:</p> <p>https://gist.github.com/mattmattox/d32b3fea4820075c08c6cc2f6d736702</p>"},{"location":"kbs/000021114/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021115/","title":"How to forward Kubernetes events with the rancher-logging chart","text":"<p>This document (000021115) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021115/#environment","title":"Environment","text":"<ul> <li>Rancher v2.5+</li> <li>A Rancher-managed cluster with rancher-logging installed</li> </ul>"},{"location":"kbs/000021115/#situation","title":"Situation","text":"<p>This article details how to configure Kubernetes event forwarding with rancher-logging.</p>"},{"location":"kbs/000021115/#resolution","title":"Resolution","text":"<p>Rancher logging supports forwarding Kubernetes events, via the\u00a0logging-operator Event Tailer extension. The steps below detail how to enable the Event Tailer extension, and forward the events.</p> <ol> <li>Create the EvenTailer resource: The EventTailer resource creates a Pod that listens for Kubernetes events and writes them to stdout, enabling their collection and forwarding by the logging-operator's fluentbit and fluentd processes.</li> </ol> <pre><code>apiVersion: logging-extensions.banzaicloud.io/v1alpha1\nkind: EventTailer\nmetadata:\n     name: cluster\nspec:\n     controlNamespace: cattle-logging-system\n</code></pre> <ol> <li>Create a Flow resource: A Flow is a namespaced resource that uses filters and selectors to route log messages to the appropriate output. In this instance the Kubernetes event logs generated by the cluster EventTailer above will be routed to the cluster-event-output.</li> </ol> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: Flow\nmetadata:\n     name: eventtailer-flow\n     namespace: cattle-logging-system\nspec:\n     filters:\n  - tag_normaliser: {}\nlocalOutputRefs:\n  - cluster-event-output\nmatch:\n  - select:\n      labels:\n        app.kubernetes.io/instance: cluster-event-tailer\n</code></pre> <ol> <li>Create an Output resource: An\u00a0Output is a namespaced resource that defines where the log messages are sent. In this instance, the logs are sent to an Elasticsearch instance. Configure the Output to match your logging endpoint, per the Rancher Logging documentation.</li> </ol> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: Output\nmetadata:\n  name: cluster-event-output\n  namespace: cattle-logging-system\nspec:\n  elasticsearch:\n    host:\"IP or elastic hostname\"\n    port: 9200\n    scheme: http\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000021115/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021157/","title":"How to create a read-only custom Global Role for Fleet","text":"<p>This document (000021157) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021157/#environment","title":"Environment","text":"<p>Rancher v2.6+</p>"},{"location":"kbs/000021157/#situation","title":"Situation","text":"<p>To permit a user read-only access to Fleet resources a custom Global Role\u00a0can be created with the required grants.</p> <p>In this role you need to grant the get, list and watch verbs on the following resources:</p> Resources API Group clusters fleet.cattle.io gitrepos fleet.cattle.io bundles fleet.cattle.io clustergroups fleet.cattle.io fleetworkspaces management.cattle.io"},{"location":"kbs/000021157/#resolution","title":"Resolution","text":"<p>1.\u00a0 Create the custom Global Role</p> <p>Navigate within the Rancher UI to Users &amp; Authentication\u00a0&gt; Role Templates and click Create Global Role.\u00a0Create a new role granting list, and get and watch the resources listed in the above table.</p> <p></p> <p>In the Rancher UI, under Users &amp; Authentication &gt; Role Templates &gt; Global, a new Global Role appears:</p> <p></p> <p>2. Assign this role to individual users or groups</p> <p>After creating this custom Global Role, you can then assign it to individual users\u00a0or groups. When a non-admin user assigned this role accesses Rancher, the user will not be able to edit any Fleet resources but will be able to view these within the Continuous Delivery UI.</p> <p></p>"},{"location":"kbs/000021157/#additional-information","title":"Additional Information","text":"<p>Custom Roles</p> <ul> <li>https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/authentication-permissions-and-global-configuration/manage-role-based-access-control-rbac/custom-roles#creating-a-custom-role</li> </ul>"},{"location":"kbs/000021157/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021185/","title":"How to disable autoscaler for rke2-coredns","text":"<p>This document (000021185) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021185/#environment","title":"Environment","text":"<p>A Rancher-provisioned or standalone RKE2 cluster</p>"},{"location":"kbs/000021185/#situation","title":"Situation","text":"<p>You wish to disable the autoscaling feature of coredns in an RKE2 cluster</p>"},{"location":"kbs/000021185/#resolution","title":"Resolution","text":"<p>Details on how to customize the rke2-coredns HelmChart via a HelmChartConfig can be found in the How to customize rke2-coredns\u00a0article.</p> <p>Follow the process in that article to configure the HelmChartConfig below:</p> <pre><code>---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-coredns\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    autoscaler:\n      enabled: false\n</code></pre> <p>After deploying the HelmChartConfig, you can confirm that the autoscaler pod is not present with kubectl:</p> <pre><code>kubectl -n kube-system get pods | grep autoscaler\n</code></pre>"},{"location":"kbs/000021185/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021198/","title":"Rancher 2.7.6 login is disabled while migration is running","text":"<p>This document (000021198) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021198/#environment","title":"Environment","text":"<p>Rancher 2.7.5 being upgraded to 2.7.6 with AD authentication enabled</p>"},{"location":"kbs/000021198/#situation","title":"Situation","text":"<p>Immediately following the upgrade from Rancher 2.7.5 to 2.7.6, users are unable to login to Rancher and are met with the following error message:</p> <p></p>"},{"location":"kbs/000021198/#cause","title":"Cause","text":"<p>This is caused by a code change meant to revert the 2.7.5 AD bug. Login is locked while the migration is running so that users do not accidentally interfere with the migration and to prevent duplicate users from being created, blocking legitimate user access.</p> <p>Depending on the number of users and corresponding rolebindings, this process can take sometime to resolve itself.</p> <p>When the migration script begins, we expect to see\u00a0<code>[migrate-ad-user] beginning ad-guid unmigration</code> in the Rancher logs. When it ends, we expect to see\u00a0<code>[migrate-ad-user] end of ad-guid unmigration</code> in the Rancher logs.</p>"},{"location":"kbs/000021198/#additional-information","title":"Additional Information","text":"<p>More information about this change can be found in the Rancher 2.7.6 Release Notes:\u00a0https://github.com/rancher/rancher/releases/tag/v2.7.6#:~:text=Active%20Directory%20Authentication%20Fixes</p>"},{"location":"kbs/000021198/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021206/","title":"Configuring Rancher Backup to use GCP Cloud Storage as Remote Storage Location","text":"<p>This document (000021206) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021206/#environment","title":"Environment","text":"<p>A Rancher v2.5+ instance</p>"},{"location":"kbs/000021206/#situation","title":"Situation","text":"<p>Configuration of Google Cloud Storage as the S3 backend to store Rancher Backup Operator\u00a0backups.</p>"},{"location":"kbs/000021206/#resolution","title":"Resolution","text":"<p>To configure the Rancher Backup Operator with Google Cloud Storage, perform the following steps:</p> <ol> <li> <p>Create a new bucket in\u00a0Google Cloud Storage</p> </li> <li> <p>Create a GCP serviceaccount in\u00a0IAM &amp; Admin</p> </li> <li> <p>Give the GCP serviceaccount permissions to read, write, and delete objects in the bucket. The serviceaccount will require the\u00a0<code>roles/storage.objectAdmin</code>\u00a0role to read, write, and delete objects in the bucket.</p> </li> <li> <p>Navigate to your\u00a0buckets in cloud storage\u00a0and select your newly created bucket.</p> </li> <li> <p>Go to the cloud storage\u2019s settings menu and navigate to the\u00a0interoperability tab</p> </li> <li> <p>Scroll down to\u00a0Service account HMAC\u00a0and press\u00a0<code>+ CREATE A KEY FOR A SERVICE ACCOUNT</code></p> </li> <li> <p>Select the GCP serviceaccount you created earlier and press\u00a0<code>CREATE KEY</code></p> </li> <li> <p>Save the\u00a0Access Key\u00a0and\u00a0Secret.</p> </li> <li> <p>Create the credentials secret using the following documentation:\u00a0https://ranchermanager.docs.rancher.com/reference-guides/backup-restore-configuration/backup-configuration#example-credentialsecret</p> </li> <li> <p>Install the Backup Operator with a default S3 storage location. The S3 storage location\u00a0yaml looks like this:</p> </li> </ol> <pre><code>     credentialSecretName: s3-creds\n     credentialSecretNamespace: default\n     bucketName: rancher-backups\n     region: us\n     endpoint: storage.googleapis.com\n</code></pre>"},{"location":"kbs/000021206/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021213/","title":"Rancher upgrades fails with \"resource mapping not found for name\" error due to deprecated api version for a resource","text":"<p>This document (000021213) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021213/#situation","title":"Situation","text":"<p>Rancher upgrade is failing due to the deprecated\u00a0api version of a resource.</p> <p>The relevant error message below may appear when running the\u00a0helm upgrade\u00a0command to upgrade Rancher.</p> <p>Examples:</p> <pre><code>helm upgrade rancher rancher-stable/rancher --namespace cattle-system  -f values.yaml --version=2.X.X\nError: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: resource mapping not found for name: \"rancher\" namespace: \"\" from \"\": no matches for kind \"Issuer\" in version \"cert-manager.io/v1alpha2\"\" ensure CRDs are installed first\n</code></pre> <pre><code>helm upgrade rancher rancher-stable/rancher --namespace cattle-system -f values.yaml \u00a0 --version=2.X.X\nError: UPGRADE FAILED: unable to build kubernetes objects from current release manifest: resource mapping not found for name: \"rancher\" namespace: \"\" from \"\": no matches for kind \"Ingress\" in version \"networking.k8s.io/v1beta1\"\n</code></pre>"},{"location":"kbs/000021213/#resolution","title":"Resolution","text":"<p>The helm-mapkubeapis plugin can be used to restore your release to a working state. This plugin reads Helm release data and replaces superseded APIs with their new versions or removes resources that refer to APIs that were completely removed from Kubernetes.</p> <p>Note that Helm plugins are installed locally in the machine where the commands are run. Therefore, please make sure to run the installation steps on the same machine where you intend to run the cleanup steps.</p> <p>Install helm-mapkubeapis</p> <ol> <li> <p>Ensure\u00a0that Helm is installed by running `helm version`</p> <p>You should see an output similar to the following:</p> </li> </ol> <pre><code>version.BuildInfo{Version:\"v3.10.2\", GitCommit:\"50f003e5ee8704ec937a756c646870227d7c8b58\", GitTreeState:\"clean\", GoVersion:\"go1.18.8\"}\n</code></pre> <pre><code>\n</code></pre> <ol> <li>Install the helm-mapkubeapis plugin:</li> </ol> <pre><code>helm plugin install https://github.com/helm/helm-mapkubeapis\n</code></pre> <p>Output should be similar to:</p> <pre><code>Downloading and installing helm-mapkubeapis v0.4.1 ...\nhttps://github.com/helm/helm-mapkubeapis/releases/download/v0.4.1/helm-mapkubeapis_0.4.1_linux_amd64.tar.gz\nInstalled plugin: mapkubeapis\n</code></pre> <p>Make sure that the installed version of helm-mapkubeapis is v0.4.1 or later, as earlier versions do not support the removal of resources.</p> <ol> <li>Check that the plugin was installed correctly:</li> </ol> <pre><code>helm mapkubeapis --help\n</code></pre> <p>Output should be similar to:</p> <pre><code>Map release deprecated or removed Kubernetes APIs in-place\nUsage:\n   \u00a0\u00a0\u00a0 mapkubeapis [flags] RELEASE\n\nFlags:\n   \u00a0\u00a0\u00a0 --dry-run\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 simulate a command\n   \u00a0\u00a0\u00a0 -h, --help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 help for mapkubeapis\n   \u00a0\u00a0\u00a0 --kube-context string\u00a0\u00a0 name of the kubeconfig context to use\n   \u00a0\u00a0\u00a0 --kubeconfig string\u00a0\u00a0\u00a0\u00a0 path to the kubeconfig file\n   \u00a0\u00a0\u00a0 --mapfile string\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 path to the API mapping file\n   \u00a0\u00a0\u00a0 --namespace string\u00a0\u00a0\u00a0\u00a0\u00a0 namespace scope of the release\n</code></pre> <pre><code>\n</code></pre> <p>Clean up releases</p> <ol> <li>Open your terminal of choice and make sure the Rancher management\u00a0upstream\u00a0cluster is accessible by running:</li> </ol> <pre><code>kubectl cluster-info\n</code></pre> <ol> <li>Perform a dry run for the Rancher release in the cattle-system namespace to check the changes:</li> </ol> <pre><code>helm mapkubeapis --dry-run rancher  --namespace cattle-system\n</code></pre> <p>Output should be similar to:</p> <pre><code>NOTE: This is in dry-run mode, the following actions will not be executed.\nRun without --dry-run to take the actions described below:\nRelease 'rancher' will be checked for deprecated or removed Kubernetes APIs and will be updated if necessary to supported API versions.\nGet release 'rancher' latest version.\nCheck release 'rancher' for deprecated or removed APIs...\nFound 1 instances of deprecated or removed Kubernetes API:\n\"apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nNOTE: This is in dry-run mode, the following actions will not be executed.\nRun without --dry-run to take the actions described below:\nRelease 'rancher' will be checked for deprecated or removed Kubernetes APIs and will be updated if necessary to supported API versions.\nGet release 'rancher' latest version.\nCheck release 'rancher' for deprecated or removed APIs...\nFound 1 instances of deprecated or removed Kubernetes API:\n\"apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\n</code></pre> <ol> <li>Perform a full run with helm:</li> </ol> <pre><code>helm mapkubeapis rancher --namespace cattle-system\n</code></pre> <pre><code>Output should be similar to:\n</code></pre> <pre><code>        Release 'rancher' will be checked for deprecated or removed Kubernetes APIs and will be updated if necessary to supported API versions.\nGet release 'rancher' latest version.\nCheck release 'rancher' for deprecated or removed APIs...\nFound 1 instances of deprecated or removed Kubernetes API:\n\"apiVersion: networking.k8s.io/v1beta1\nkind: Ingress\n\"\nSupported API equivalent:\n\"apiVersion: networking.k8s.io/v1\nkind: Ingress\n\"\nFinished checking release 'rancher' for deprecated or removed APIs.\nDeprecated or removed APIs exist, updating release: rancher.\nSet status of release version 'rancher.v1' to 'superseded'.\nRelease version 'rancher.v1' updated successfully.\nAdd release version 'rancher.v2' with updated supported APIs.\nRelease version 'rancher.v2' added successfully.\nRelease 'rancher' with deprecated or removed APIs updated successfully to new version.\nMap of release 'rancher' deprecated or removed APIs to supported versions, completed successfully.\n</code></pre> <pre><code>Re-run the helm upgrade command after all deprecated or superseded APIs are removed.\n</code></pre>"},{"location":"kbs/000021213/#additional-information","title":"Additional Information","text":"<p>How to check apiVersions for a release</p> <p>Command to get the available\u00a0apiVersion\u00a0for cert-manager</p> <pre><code>kubectl get --raw /apis/cert-manager.io | jq .\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000021213/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021220/","title":"[Rancher] What is \"cattle-impersonation-system\" for?","text":"<p>This document (000021220) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021220/#situation","title":"Situation","text":""},{"location":"kbs/000021220/#what-is-cattle-impersonation-system","title":"What is cattle-impersonation-system?\u200b","text":"<p>It is part of the\u00a0Rancher\u00a0Authentication\u00a0Proxy\u200b:</p> <ul> <li> <p>Cattle-impersonation-system is a\u00a0critical part of how Rancher has implemented\u00a0authentication.\u200b</p> </li> <li> <p>A subject must explicitly have the \u201cimpersonate\u201d privilege\u00a0to\u00a0be able to perform\u00a0impersonation of other\u00a0users. \u200b</p> </li> <li> <p>One of the key features that Rancher adds to Kubernetes is centralized user\u00a0authentication.\u00a0This feature allows users to use one set of credentials to authenticate\u00a0with any of your\u00a0Kubernetes\u00a0clusters.\u00a0\u00a0\u200b</p> </li> <li> <p>This centralized user authentication is accomplished using the Rancher authentication\u00a0proxy, installed along with the rest of Rancher. This proxy authenticates your users\u00a0and\u00a0forwards their requests to your Kubernetes clusters using a service account.</p> </li> </ul>"},{"location":"kbs/000021220/#why-do-we-need-cattle-impersonation-system","title":"Why do we need cattle-impersonation-system\u200b?","text":""},{"location":"kbs/000021220/#doesnt-kubernetes-come-with-user-authentication","title":"Doesn't Kubernetes come with user authentication?","text":"<p>The short answer is yes, but\u00a0Rancher is a multi-cluster Kubernetes management tool to deploy and run clusters\u00a0anywhere and on any provider from one central point.\u200b\u00a0Rancher is sitting \"in front\" of these multiple Kubernetes\u00a0clusters. Because Kubernetes has a lot of support for authentication, each of these clusters can come with its own authentication strategy.</p> <p>By default, Kubernetes clusters are accessed with a\u00a0kubeconfig\u00a0file, and the\u00a0kubeconfig\u00a0file contains full access\u00a0to\u00a0the\u00a0cluster.\u200b\u00a0With Rancher, this file is not required for cluster API communication because it uses the\u00a0authentication proxy mechanism. The Rancher Manager server connects to the\u00a0Kubernetes API server on a downstream user cluster by using a service account to communicate with the Kubernetes clusters, which\u00a0provides an\u00a0identity for processes that\u00a0run in\u00a0pods.\u200b</p>"},{"location":"kbs/000021220/#what-about-users-in-kubernetes","title":"What about\u00a0users\u00a0in Kubernetes?\u200b","text":"<p>Kubernetes does not have full support for the user and group entities\u200b</p> <ul> <li> <p>It does have\u00a0service accounts\u00a0as\u00a0users\u200b</p> </li> <li> <p>But\u00a0for\u00a0normal\u00a0users, Kubernetes documents that they should be managed externally by external\u00a0identity providers.\u200b</p> </li> <li> <p>Thus, there is no API support for\u00a0a\u00a0user\u00a0object and its group membership\u00a0( link)\u200b</p> </li> <li> <p>If you want to manage multiple Kubernetes clusters and users across these clusters, you need to manage them externally.</p> </li> </ul>"},{"location":"kbs/000021220/#how-does-rancher-do-user-authentication-in-this-case","title":"How does Rancher do\u00a0user authentication in this case?\u200b","text":"<p>Rancher has designed and developed its authentication framework and extended\u00a0Kubernetes\u00a0to support the user and group object:\u200b</p> <p>To achieve this, it uses the\u00a0user impersonation of Kubernetes.\u200b User Impersonation:\u200b</p> <ul> <li> <p>The ability for one user (or service account) to act as another\u200b</p> </li> <li> <p>The user or service account should have \"impersonate\" permissions granted\u200b</p> </li> <li> <p>An important key to Rancher's authentication system: on every Kubernetes API call, the\u00a0authentication\u00a0proxy authenticates the caller. It sets the proper\u00a0Kubernetes\u00a0impersonation headers before forwarding the\u00a0call to Kubernetes\u00a0masters.\u200b</p> </li> </ul>"},{"location":"kbs/000021220/#rancher-authentication-proxy","title":"\u200bRancher authentication proxy\u200b","text":"<p>In the central authentication model, Rancher\u00a0is\u00a0sitting in \"front\" of all Kubernetes clusters, acts\u00a0as a central authentication proxy, and thus\u00a0facilitates the authentication for multiple\u00a0clusters.\u200b</p> <p>A Rancher admin will configure and enable authentication\u00a0against one of the integrations available for Rancher(e.g.\u00a0Active Directory,\u00a0OpenLdap, etc.) once, centrally on the Rancher\u00a0server and manage it for all clusters.\u00a0\u200b</p> <p>Users authenticate with Rancher, and Rancher redirects the\u00a0user request to the specific Kubernetes cluster using\u00a0standard bearer tokens and user impersonation to act as\u00a0that\u00a0user.\u200b</p> <p></p>"},{"location":"kbs/000021220/#resources-for-cattle-impersonation-system","title":"Resources for\u00a0cattle-impersonation-system\u200b","text":"<p>Four resources are created to handle impersonation:\u200b</p> <ol> <li> <p>namespace: cattle-impersonation-system\u200b</p> </li> <li> <p>service account: cattle-impersonation-system/cattle-impersonation-\u200b <p>Behind the scenes,\u00a0Kubernetes\u00a0also creates an account token that Rancher uses for\u00a0authentication: cattle-impersonation-system/cattle-impersonation--token-\u200b <li> <p>cluster role: cattle-impersonation-\u200b <li> <p>cluster role binding: cattle-impersonation-"},{"location":"kbs/000021220/#internal-implementation","title":"Internal implementation","text":"<p>Creating the impersonation resources works in two parts.\u00a0\u00a0\u200b</p>"},{"location":"kbs/000021220/#part-1","title":"Part 1\u200b","text":"<ul> <li>When a user creates a cluster or is added as a\u00a0cluster\u00a0owner/member/other (or added to a\u00a0project on that cluster), a\u00a0ClusterRoleTemplateBinding\u00a0or a\u00a0ProjectRoleTemplateBinding\u00a0is created\u00a0for that user on the\u00a0cluster.\u200b</li> <li>When the\u00a0ClusterRoleTemplateBinding\u00a0or a\u00a0ProjectRoleTemplateBinding\u00a0exists, Rancher creates\u00a0the\u00a0namespace\u00a0(if it does not already exist), the\u00a0service account, the\u00a0cluster role binding, and\u00a0the\u00a0cluster role\u00a0with rules based on the\u00a0information it has at that time. The rules will include the ability\u00a0to\u00a0impersonate the user's ID and the groups that Rancher knows about.\u200b</li> </ul>"},{"location":"kbs/000021220/#part-2","title":"Part 2\u200b","text":"<ul> <li> <p>When a user makes a request on the downstream cluster, using either the cluster explorer,\u00a0kubectl\u00a0with a downloaded\u00a0kubeconfig, or curl with the /k8s/clusters proxy API, Rancher\u00a0intercepts the request and checks to make sure the resources are up to date,\u00a0that they exist,\u00a0and that the rules are up to date for the user.\u200b</p> </li> <li> <p>If the user is logging in from an external auth provider, Rancher may not know during part 1 what\u00a0extra attributes the user has, as these are provided in the token in the request. At this point,\u00a0Rancher will update the cluster role to include these extra attributes included in the request, such\u00a0as the principal ID and username.\u200b</p> </li> <li> <p>All relevant resources (namespace, service account, secret,\u00a0clusterrole\u00a0and\u00a0clusterrolebinding)\u00a0on the downstream cluster are watched and cached on the Rancher server, so checking the\u00a0resources should have minimal performance overhead.</p> </li> </ul>"},{"location":"kbs/000021220/#request-diagram","title":"Request Diagram","text":""},{"location":"kbs/000021220/#example-in-this-diagram-a-user-named-bob-wants-to-see-all-pods-running-on-a-downstream-user-cluster-called-user-cluster","title":"Example:\u200b        In this diagram, a user named Bob wants to see all pods running on a\u00a0downstream user\u00a0cluster called User Cluster. \u200b","text":"<ol> <li> <p>From within Rancher, he can run a\u00a0kubectl\u00a0command to see the\u00a0pods. Bob is\u00a0authenticated through Rancher\u2019s authentication proxy.\u200b</p> </li> <li> <p>The authentication proxy forwards all Kubernetes API calls to\u00a0downstream clusters.\u00a0It integrates with authentication services like\u00a0local authentication, Active Directory,\u00a0and GitHub. On every\u00a0Kubernetes API call, the authentication proxy authenticates\u00a0the\u00a0caller and sets the proper Kubernetes impersonation headers\u00a0before\u00a0forwarding the call to Kubernetes masters.\u200b</p> </li> <li> <p>Rancher communicates with Kubernetes clusters using a\u00a0service\u00a0account ,\u00a0which provides an identity for processes that run in a\u00a0pod.\u200b</p> </li> <li> <p>By default, Rancher generates a\u00a0kubeconfig file\u00a0that contains\u00a0credentials for\u00a0proxying through the Rancher server to connect to\u00a0the Kubernetes API server on a\u00a0downstream user cluster. The\u00a0kubeconfig\u00a0file (kube_config_rancher-cluster.yml)\u00a0contains full\u00a0access to the cluster.</p> </li> </ol>"},{"location":"kbs/000021220/#qa","title":"Q&amp;A","text":""},{"location":"kbs/000021220/#when-is-the-clusterrole-supposed-to-change","title":"When is the\u00a0ClusterRole\u00a0supposed to change?\u00a0\u200b","text":"<ul> <li> <p>It often changes on the first login to add\u00a0userextras/principalid\u00a0and\u00a0userextras/username. A\u00a0future enhancement to the way user extra attributes are handled in Rancher will make this less\u00a0necessary in the future.\u200b</p> </li> <li> <p>It will change if the user adds an auth provider or changes auth providers. (NOTE: Rancher does\u00a0not support changing auth providers\u00a0at the moment).\u200b</p> </li> <li> <p>It will change if the user's groups in the auth provider change.\u200b</p> </li> <li> <p>It may change depending on how the user makes the request - UI,\u00a0kubectl, or curl - sometimes\u00a0the token includes different user extra attributes.</p> </li> </ul>"},{"location":"kbs/000021220/#when-will-cleanup-happen","title":"When will Cleanup happen?\u200b","text":"<ul> <li>The service account,\u00a0clusterrole\u00a0and\u00a0clusterrolebinding\u00a0are cleaned up when the\u00a0ClusterRoleTemplateBinding\u00a0or\u00a0ProjectRoleTemplateBinding\u00a0are deleted (when the user is\u00a0removed from the project or cluster).</li> </ul>"},{"location":"kbs/000021220/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021242/","title":"Rancher Supportability Review - Guide","text":"<p>This document (000021242) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021242/#situation","title":"Situation","text":"<p>Supportability Review is a feature entitlement of Rancher Prime. It provides proactive support to customers\u00a0 with the reassurance that their system design/solution is based on a sound foundation, is supportable in principle and complies with SUSE\u2019s current recommended practice.</p> <p>These reviews check to see how SUSE customer's cluster(s) align to best practices, support matrix, and helps to ensure that SUSE customers are set up for success.</p> <p>Your Deliverables:</p> <ul> <li>In-depth Report: Receive a comprehensive PDF report that provides clear feedback on your entire Rancher deployment, including all clusters.</li> <li>Actionable Insights: A concise executive summary will highlight any potential issues that could impact your support experience, along with identification of any major design flaws. We'll also provide recommendations for improvement, where applicable.</li> <li>Expert Consultation: Schedule a meeting to discuss the report in detail with one of our Rancher Support Experts. They'll be happy to answer any questions you may have.</li> </ul> <p>To get started with the supportability review for your Rancher environment, simply open a Support Case in SCC (Subject: Supportability Review) and our team will be glad to work with you and your team.</p> <p>For more detailed information, please download the following document:</p> <p>Rancher Supportability Review Guide Download</p>"},{"location":"kbs/000021247/","title":"How to use a Rancher hotfix image","text":"<p>This document (000021247) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021247/#environment","title":"Environment","text":"<p>SUSE Rancher 2.9.x+</p>"},{"location":"kbs/000021247/#situation","title":"Situation","text":"<p>SUSE Rancher support and engineering might propose a hotfix image, which contains a potential fix for a specific issue, for troubleshooting purposes.</p> <p>Hotfix images usually follow the naming convention: \" <code>v&lt;X.Y.Z&gt;-hotfix-&lt;IDENTIFIER</code> &gt;\"</p> <p>The Rancher version should be respected (for Rancher v2.9.3, the debug image should be v2.9.3-hotfix-XYZ).</p> <p>SUSE Support will give information about the content of a hotfix.</p> <p>Disclaimers:</p> <p>Please, use a hotfix build only when instructed by SUSE Support/Engineering personnel. Hotfix builds are meant to help troubleshoot specific problems and they are not supported for general use.</p> <p>From an Engineering standpoint we are confident that hotfix builds will not introduce regressions, as changes target a small code region, pass reviews and a selection of automated tests as well. Nevertheless, hotfix builds do not go through full Rancher QA processes, so please be ready to revert to the original version in case unexpected behavior is noticed (instructions are provided below).</p> <p>Important Upgrade Notice: This hotfix includes changes that may not be present in all subsequent Rancher releases. Therefore, to ensure a smooth upgrade process after applying this hotfix, please contact SUSE Support for guidance before upgrading to any later Rancher version. SUSE Support will indicate which versions are safe to upgrade to.</p> <p>Backups: As a precaution, it's recommended to take a snapshot of the Rancher local cluster. Please see the documentation ( RKE, RKE2, K3s) for the appropriate way to take a snapshot for the Rancher installation.</p> <p>Alternatively the rancher-backup operator\u00a0can be used to backup all of the related objects for restoration.</p>"},{"location":"kbs/000021247/#resolution","title":"Resolution","text":""},{"location":"kbs/000021247/#installation-instructions","title":"Installation instructions","text":""},{"location":"kbs/000021247/#method-1-editing-the-deployment","title":"Method 1, editing the deployment","text":"<ul> <li>from a machine with access to the upstream cluster, edit the <code>rancher</code> deployment (to change the <code>rancher</code> image to the hotfix version):</li> </ul> <pre><code>export TAG=v2.9.3-hotfix-XXX\nkubectl set image -n cattle-system deployment/rancher rancher=registry.rancher.com/rancher/rancher:$TAG\n</code></pre> <p>Note: <code>rancher-agent</code> images will be propagated downstream automatically, usually within minutes.</p> <p>Note: customers using air-gap setups or not allowing images being pulled from external registries should transfer or whitelist the <code>rancher</code> and <code>rancher-agent</code> images appropriately for the procedure to work.</p>"},{"location":"kbs/000021247/#uninstallation-instructions-restoring-the-original-rancher-version","title":"Uninstallation instructions (restoring the original Rancher version)","text":"<ul> <li>repeat instructions above, but setting the original version tag:</li> </ul> <pre><code>export TAG=v2.9.3\nkubectl set image -n cattle-system deployment/rancher rancher=registry.rancher.com/rancher/rancher:$TAG\n</code></pre>"},{"location":"kbs/000021247/#method-2-using-helm","title":"Method 2, using helm","text":"<pre><code>export TAG=v2.9.3-hotfix-XXX\nhelm get values rancher -n cattle-system -o yaml &gt; values.yaml\nhelm upgrade rancher rancher-&lt;CHART-REPO&gt;/rancher --namespace cattle-system -f values.yaml --set rancherImageTag=$TAG\n</code></pre> <p>Note: <code>rancher-agent</code> images will be propagated downstream automatically, usually within minutes.</p> <p>Note: customers using air-gap setups or not allowing images being pulled from external registries should transfer or whitelist the <code>rancher</code> and <code>rancher-agent</code> images appropriately for the procedure to work.</p>"},{"location":"kbs/000021247/#uninstallation-instructions-restoring-the-original-rancher-version_1","title":"Uninstallation instructions (restoring the original Rancher version)","text":"<ul> <li>repeat instructions above, but setting the original version tag</li> </ul> <pre><code>export TAG=v2.9.3\nhelm get values rancher -n cattle-system -o yaml &gt; values.yaml\nhelm upgrade rancher rancher-&lt;CHART-REPO&gt;/rancher --namespace cattle-system -f values.yaml --set rancherImageTag=$TAG\n</code></pre>"},{"location":"kbs/000021247/#follow-up-instruction","title":"Follow-up instruction","text":"<p>Please report if changes help with symptoms.</p> <p>Please contact SUSE again before upgrading to a newer Rancher version.</p>"},{"location":"kbs/000021247/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021248/","title":"How to enable user session timeout in rancher UI ?","text":"<p>This document (000021248) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021248/#environment","title":"Environment","text":"<p>- Rancher 2.6.x, 2.7.x and Rancher 2.8.x</p>"},{"location":"kbs/000021248/#resolution","title":"Resolution","text":"<p>- User session timeout is configured in rancher with property \" auth-user-session-ttl-minutes\",\u00a0and the default value is 16hrs. However, this property is configurable from Rancher UI -&gt; Global Setting -&gt; Settings.</p> <p>- As a result, the users are automatically logged out of Rancher. If not, then Once the changes are done, make sure to logout and login again to the Rancher UI.</p> <p></p>"},{"location":"kbs/000021248/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021250/","title":"Getting x509 error when adding http repository to downstream cluster in Rancher","text":"<p>This document (000021250) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021250/#environment","title":"Environment","text":"<p>- Rancher 2.6.x, 2.7.x and Rancher 2.8.x</p>"},{"location":"kbs/000021250/#situation","title":"Situation","text":"<p>-\u00a0 The ' http' type repository is in ' downloading' status with the error message \" Get :x509:certificate signed by unknown authority\"."},{"location":"kbs/000021250/#resolution","title":"Resolution","text":"<p>- The error will occur if using the certificate signed by a private CA on the repository. So, as a solution, we need to add the CA certificate to the HTTP-based repo.</p> <p>- Follow the below steps to add the custom CA certificate to HTTP based repo:</p> <p>Steps :</p> <p>A) Get the \"caBundle\" key: The caBundle key is a base64 encoded DER certificate, and you can get it using the command below.</p> <pre><code>openssl x509 -outform der -in ca.pem | base64 -w0\n\nNote :  Make sure to replace the ca.pem certificate in the above command\n</code></pre> <pre><code>B)  Go to Rancher UI -&gt; select the downstream cluster -&gt;  edit the rancher-repo and\u00a0 \"Edit YAML\" and add the resulting value from above steps, in 'caBundle' section below :\n</code></pre> <pre><code>spec:\n  forceUpdate: \"\"\n  url: https://[url]\n  caBundle: \"&lt;add_value_here&gt;\"\n</code></pre> <pre><code>C) (Optional) If you do not want to add the custom CA and want to ingore/bypass the error, then add 'insecureSkipTLSVerify:true' flag in the clusterepo specification like below :\n</code></pre> <pre><code>spec:\n  clientSecret: null\n  forceUpdate: \"2023-08-10T05:42:22Z\"\n  insecureSkipTLSVerify: true              &lt;&lt; Note here\n  url: &lt;URL&gt;\n</code></pre>"},{"location":"kbs/000021250/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021253/","title":"Rancher kubectl shell pod tries to pull an image from external dockerhub instead of pulling from private registry","text":"<p>This document (000021253) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021253/#environment","title":"Environment","text":"<p>Rancher 2.6.x, 2.7.x and 2.8.x</p>"},{"location":"kbs/000021253/#situation","title":"Situation","text":"<p>- In the Rancher airgap environment, all the images including rancher-shell should get pulled from the default private registry, but there are some situations where few images are getting pulled from the external docker hub instead of the default private registry.</p>"},{"location":"kbs/000021253/#resolution","title":"Resolution","text":"<p>- Verify the rancher \"system-default-registry\" propertyis correctly configured using the below command. If it's not, then follow the document [1] and re-configure the registry with the Rancher server.</p> <pre><code>&gt; kubectl get settings.management.cattle.io | grep -i system-default-registry\nsystem-default-registry  &lt;registery URL here&gt;\n</code></pre> <p>- If the registry is correctly configured, then verify the ' shell-image' and correct the repo value accordingly using below commands :</p> <pre><code>a) kubectl edit settings shell-image\n\nb) modify the value section :\n\n apiVersion: management.cattle.io/v3\n  customized: false\n  default: rancher/shell:v0.1.19\n  kind: Setting\n  metadata:\n    creationTimestamp: \"2023-02-12T05:45:41Z\"\n    generation: 2\n    name: shell-image\n    resourceVersion: \"53446115\"\n    uid: 39391805-a73a-45b1-b0c4-c336f6326c12\n  source: \"\"\n  value: \"&lt;add_value_here&gt;\"\n</code></pre> <p>[1]\u00a0https://ranchermanager.docs.rancher.com/v2.6/getting-started/installation-and-upgrade/installation-references/helm-chart-options#advanced-options</p>"},{"location":"kbs/000021253/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021260/","title":"Logging stack failing with PKey error","text":"<p>This document (000021260) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021260/#environment","title":"Environment","text":"<p>Rancher-logging is installed and configured with Kafka as an output</p>"},{"location":"kbs/000021260/#situation","title":"Situation","text":"<p>A Clusterflow and an Outpuut or ClusterOutput for rancher-logging are configured to send logs to a Kafka server, fluentd is failing with the following error:</p> <pre><code>[error]: fluent/log.rb:372:error: unexpected error error_class=OpenSSL::PKey::PKeyError error=\"Could not parse PKey: no start line\"\n</code></pre>"},{"location":"kbs/000021260/#resolution","title":"Resolution","text":"<p>If you don't have client authentication enabled, the root CA is sufficient to connect to Kafka brokers. Removing invalid values and missing keys should resolve the issue</p> <pre><code>    ssl_ca_cert:\n      mountFrom:\n        secretKeyRef:\n          key: tls.crt\n          name: root-ca\n</code></pre>"},{"location":"kbs/000021260/#cause","title":"Cause","text":"<p>Invalid client certificate/key is configured on the Output or ClusterOutput for Kafka, while the client certificate secret name is kept blank</p> <pre><code>    ssl_ca_cert:\n      mountFrom:\n        secretKeyRef:\n          key: tls.crt\n          name: root-ca\n    ssl_client_cert:\n      mountFrom:\n        secretKeyRef:\n          key: tls.crt\n          name: certs\n    ssl_client_cert_chain:\n      mountFrom:\n        secretKeyRef:\n          key: ''\n    ssl_client_cert_key:\n      mountFrom:\n        secretKeyRef:\n          key: tls.key\n          name: certs\n</code></pre>"},{"location":"kbs/000021260/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021260/#additional-information","title":"Additional Information","text":"<p>Logging outputs</p> <p>Kafka plugin configuration</p>"},{"location":"kbs/000021260/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021261/","title":"Ingress controller failing in RKE1 with port conflict when using port 8443","text":"<p>This document (000021261) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021261/#environment","title":"Environment","text":"<p>RKE1</p>"},{"location":"kbs/000021261/#situation","title":"Situation","text":"<p>When deploying the ingress controller on RKE1 with custom configuration as below</p> <pre><code>ingress:\n    extra_args:\n      http-port: '8080'\n      https-port: '8443'\n    http_port: 8080\n    https_port: 8443\n</code></pre> <p>Ingress controller deployment is failing with the following error</p> <pre><code>\u201c2023/08/22 12:42:08 [emerg] 20#20: bind() to 0.0.0.0:8443 failed (98: Address in use)\u201d\n</code></pre>"},{"location":"kbs/000021261/#resolution","title":"Resolution","text":"<p>Either keep the default port used by the ingress-nginx container by removing the extra_args.https-port, or use a different port like 7443 which will avoid the conflict</p> <pre><code>  ingress:\n    http_port: 8080\n    https_port: 8443\n</code></pre> <p>However, you may use any port for the\u00a0ingress.https_port as it denotes the hostPorts field for the ingress pods, the port used for external access</p>"},{"location":"kbs/000021261/#cause","title":"Cause","text":"<p>Port 8443 is configured with extra_args.https-port which conflicts with the internal port 8443 bound for the validating webhook also provided by the ingress-nginx pod</p>"},{"location":"kbs/000021261/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021261/#additional-information","title":"Additional Information","text":"<p>RKE ingress network options</p>"},{"location":"kbs/000021261/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021262/","title":"Logging stack fails to send to splunk with error \"Token disabled\"","text":"<p>This document (000021262) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021262/#environment","title":"Environment","text":"<p>Rancher-logging chart in RKE/RKE2,\u00a0 is installed and configured to send logs to Splunk</p>"},{"location":"kbs/000021262/#situation","title":"Situation","text":"<p>Getting the following error in fluentd logs:</p> <pre><code>#0 [clusterflow:cattle-logging-system:testflow:clusteroutput:cattle-logging-system:testoutput] Fluent::Plugin::SplunkHecOutput: Failed POST to https://X.X.X.X:8088/services/collector, response: {\"text\":\"Token disabled\",\"code\":1}\n</code></pre> <p>Note: fluentd does not log to stdout, but instead to /fluentd/log/out\u00a0within the container's filesystem.</p>"},{"location":"kbs/000021262/#resolution","title":"Resolution","text":"<p>Re-check the Splunk token to see whether it's correct. You can go to the below path in Splunk admin GUI and confirm whether the token is valid and enabled. You can enable it if it's disabled.</p> <pre><code>Settings &gt;&gt; Data Inputs &gt;&gt; HTTP Event Collector &gt;&gt; Global Settings &gt;&gt; All Tokens &gt;&gt; Enabled\n</code></pre>"},{"location":"kbs/000021262/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021262/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021270/","title":"Cgroup \"Misc controller\" introduce instability in Kubelet","text":"<p>This document (000021270) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021270/#environment","title":"Environment","text":"<p>Kubernetes version build with runc library &lt; 1.1.6</p> <p>A node running Cgroup V1</p> <p>You can check the Runc library version of your Kubernetes version in the go.mod file in the Kubernetes repo.</p> <p>Eg: See the below example for Kubernetes 1.24.6 using runc 1.1.1</p> <p>https://github.com/kubernetes/kubernetes/blob/v1.24.6/go.mod#L66</p>"},{"location":"kbs/000021270/#situation","title":"Situation","text":"<p>The Kubelet on the node report the following error message</p> <pre><code>kubelet_getters.go:300] \"Path does not exist\" path=\"/var/lib/kubelet/pods/&lt;ID_NUMBER&gt;/volumes\"\n</code></pre> <p>These lines fill up the kubelet log ending up in gigabyte of logs.</p> <p>This can also cause high CPU usage.</p>"},{"location":"kbs/000021270/#resolution","title":"Resolution","text":"<p>To fix the issue you need to either :</p> <ul> <li>Update the node to use Cgroup V2</li> </ul> <p>OR</p> <ul> <li>Upgrade\u00a0to a Kubernetes version using runc libraries &gt;= 1.1.6</li> </ul> <p>It is recommended to upgrade your Kubernetes version.</p> <p>Doing a change at the Kernel level may introduce unkowns to the stability of your system.</p>"},{"location":"kbs/000021270/#cause","title":"Cause","text":"<p>The issue comes from the integration of the so called\u00a0Misc controller \u00a0in Kernel 5.13.</p> <p>Problem comes from a discrepancy between the code creating the \"Misc\" Cgroup and the code cleaning the Cgroup that doesn't handle the \"Misc\" Cgroup, leaving it behind.</p> <ul> <li>runc\u00a0integrated a\u00a0fix\u00a0in\u00a01.1.6,\u00a0HOWEVER</li> <li>kubelet\u00a0depends on\u00a0runc's cgroup libraries</li> <li>In order to clean up pods using the new \"Misc\" controller, runc\u00a0cgroup library need to be updated to be aware of it.</li> <li>So even if your system run 1.1.6 but kubelet is not build with these library, the problem occurs.</li> </ul> <p>Ex:</p> <p>Kubernetes 1.24.8 uses runc library 1.1.1</p> <p>https://github.com/kubernetes/kubernetes/blob/v1.24.8/go.mod#L66</p> <p>The first kubernetes release with runc library 1.1.6 in k8s 1.24.x is\u00a0K8s 1.24.14</p> <p>https://github.com/kubernetes/kubernetes/blob/v1.24.14/go.mod#L59</p> <p>with the help of this PR</p> <p>https://github.com/kubernetes/kubernetes/pull/117892</p> <p>The following comment list all PR for all impacted Kubernetes minor releases</p> <p>https://github.com/opencontainers/runc/issues/3849#issuecomment-1544519250</p>"},{"location":"kbs/000021270/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021272/","title":"etcd snaphots failing to save in RKE2 &lt;v1.25.15, &lt;1.26.10, &lt;1.27.7 and &lt;1.28.3 due to large configmap error","text":"<p>This document (000021272) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021272/#environment","title":"Environment","text":"<p>RKE2 &lt;v1.25.15, &lt;1.26.10, &lt;1.27.7 and &lt;1.28.3</p>"},{"location":"kbs/000021272/#situation","title":"Situation","text":"<p>At some point snapshots may start failing to complete. Viewing the logs in\u00a0<code>rke2-server.service</code>\u00a0should show:</p> <pre><code>level=error msg=\"failed to save local snapshot data to configmap: ConfigMap \\\"rke2-etcd-snapshots\\\" is invalid: []: Too long: must have at most 1048576 bytes\"\n</code></pre>"},{"location":"kbs/000021272/#resolution","title":"Resolution","text":"<p>This issue has been fixed in v1.28.3 and has been backported to 1.25.15, 1.26.10, and v1.27.7.</p> <p>If an upgrade is not possible, the following steps can be taken to manually clean the config map:</p> <ul> <li>Save copies of local etcd snapshots to another folder as a precaution.</li> <li>Reduce the etcd snapshots retention on the downstream cluster configuration and disable\u00a0S3 backups temporarily.</li> <li>Edit the 'rke2-etcd-snapshots' ConfigMap in the 'kube-system' namespace on the downstream cluster and remove the values beneath the data field:</li> </ul> <pre><code>kubectl edit ConfigMap -n kube-system rke2-etcd-snapshots\n</code></pre> <ul> <li>After saving the edits above, Fleet should trigger\u00a0all of the snapshots it missed.</li> <li>Change the snapshot schedule to every 5 minutes to allow it to apply its retention settings and clean up the snapshots. This will happen after waiting for the 5-minute period.</li> <li>Clean\u00a0the on-demand snapshots since they are not automatically cleaned by the retention settings. To do this, delete them on each node's local filesystem. After a few minutes, Rancher will reconcile\u00a0the changes, and the old on-demand snapshots will be\u00a0removed from the UI.</li> <li>Re-enable\u00a0S3 snapshots and verify\u00a0if new snapshots are being saved there.</li> <li>Update the cluster configuration to re-enable the original cron schedule and retention settings.</li> </ul>"},{"location":"kbs/000021272/#cause","title":"Cause","text":"<p>If the number of etcd nodes and snapshot retention count is too high, the rke2-etcd-snapshots configmap will grow too large and eventually the rke2-server process will be unable to save the configmap as it has grown over 1MB.</p> <p>This issue was tracked in https://github.com/rancher/rke2/issues/4495</p>"},{"location":"kbs/000021272/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021273/","title":"How to use a Fleet hotfix image","text":"<p>This document (000021273) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021273/#environment","title":"Environment","text":"<p>SUSE Rancher 2.9.x</p>"},{"location":"kbs/000021273/#situation","title":"Situation","text":"<p>SUSE Rancher support and engineering might propose a hotfix image, which contains a potential fix for a specific issue, for troubleshooting purposes.</p> <p>Hotfix images usually follow the naming convention: \" <code>v&lt;X.Y.Z&gt;-hotfix-&lt;IDENTIFIER</code> &gt;\"</p> <p>The Rancher version should be respected (for Rancher v2.9.3, the debug image should be v2.9.3-hotfix-XYZ).</p> <p>SUSE Support will give information about the content of a hotfix.</p> <p>Disclaimers: Please, use a hotfix build only when instructed by SUSE Support/Engineering personnel. Hotfix builds are meant to help troubleshoot specific problems and they are not supported for general use.</p> <p>From an Engineering standpoint we are confident that hotfix builds will not introduce regressions, as changes target a small code region, pass reviews and a selection of automated tests as well. Nevertheless, hotfix builds do not go through full Rancher QA processes, so please be ready to revert to the original version in case unexpected behavior is noticed (instructions are provided below).</p> <p>Important Upgrade Notice: This hotfix includes changes that may not be present in all subsequent Rancher releases. Therefore, to ensure a smooth upgrade process after applying this hotfix, please contact SUSE Support for guidance before upgrading to any later Rancher version. SUSE Support will indicate which versions are safe to upgrade to.</p> <p>Backups: As a precaution, it's recommended to take a snapshot of the Rancher local cluster. Please see the documentation ( RKE, RKE2, K3s) for the appropriate way to take a snapshot for the Rancher installation.Alternatively the rancher-backup operator\u00a0can be used to backup all of the related objects for restoration.</p>"},{"location":"kbs/000021273/#resolution","title":"Resolution","text":""},{"location":"kbs/000021273/#installation-instructions","title":"Installation instructions","text":"<ul> <li> <p>from a machine with access to the upstream cluster, edit the <code>fleet</code> deployment (to change the <code>fleet</code> image to the hotfix version):</p> </li> <li> <p>Edit the <code>fleet-controller</code> ConfigMap (to change the <code>fleet-agent</code> image to the test version):</p> </li> </ul> <pre><code>$ export TAG=v0.10.3-hotfix-ch-2-1e8d071a\n$ kubectl get configmap -n cattle-fleet-system fleet-controller -o yaml | sed -E \"s#rancher/fleet-agent:v[0-9A-Za-z\\.\\-]+#rancher/fleet-agent:$TAG#g\" | kubectl apply -f -\n</code></pre> <ul> <li>Edit the <code>fleet-controller</code> deployment (to change the <code>fleet-controller</code> image to the test version):</li> </ul> <pre><code>$ export TAG=v0.10.3-hotfix-ch-2-1e8d071a\n$ kubectl set image -n cattle-fleet-system deployment/fleet-controller \"*=registry.suse.com/rancher/fleet:$TAG\"\n</code></pre> <p>Note: for air-gap setups or environments not allowing images to be pulled from SUSE Registry, one should transfer or whitelist the fleet-controller and fleet-agent images appropriately for the procedure to work.</p>"},{"location":"kbs/000021273/#uninstallation-instructions-restoring-the-original-fleet-version","title":"Uninstallation instructions (restoring the original Fleet version)","text":"<pre><code>$ export TAG=v0.10.3\n$ kubectl get configmap -n cattle-fleet-system fleet-controller -o yaml | sed -E \"s#rancher/fleet-agent:v[0-9A-Za-z\\.\\-]+#rancher/fleet-agent:$TAG#g\" | kubectl apply -f -\n$ kubectl set image -n cattle-fleet-system deployment/fleet-controller \"*=registry.suse.com/rancher/fleet:$TAG\"\n</code></pre>"},{"location":"kbs/000021273/#follow-up-instructions","title":"Follow-up instructions","text":"<p>Please report if changes help with symptoms.</p> <p>Please contact SUSE again before upgrading to a newer Rancher version.</p>"},{"location":"kbs/000021273/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021284/","title":"kube-proxy not upgraded on some nodes during RKE2 version upgrade in RKE2 &lt;1.27.12, &lt;1.28.8 and &lt;1.29.4","text":"<p>This document (000021284) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021284/#environment","title":"Environment","text":"<p>An RKE2 cluster with a version &lt;1.27.12, &lt;1.28.8 and &lt;1.29.4</p>"},{"location":"kbs/000021284/#situation","title":"Situation","text":"<p>As a result of a bug affecting RKE2 clusters with a version &lt;1.27.12, &lt;1.28.8 and &lt;1.29.4, during an upgrade the kube-proxy containers are not correctly upgraded and remain on the pre-upgrade version. Errors of the following format are observed in the kubelet logs:</p> <pre><code>\"Unable to attach or mount volumes for pod; skipping pod\" err=\"unmounted volumes=[file0 file1 file2 file3], unattached volumes=[file0 file1 file2 file3]: timed out waiting for the condition\" pod=\"kube-system/kube-proxy-test-00865632-02\"\n</code></pre>"},{"location":"kbs/000021284/#resolution","title":"Resolution","text":"<p>The issue can be resolved by upgrading the affected RKE2 cluster to 1.27.12, 1.28.8, 1.29.4 or above.</p> <p>A workaround is also available for affected nodes:</p> <ol> <li>Open an SSH shell on the affected node</li> <li>Move the kube-proxy.yaml manifest out of the static pod folder. You might need to change the path if you are using a non-default static pod folder:</li> </ol> <pre><code>mv /var/lib/rancher/rke2/agent/pod-manifests/kube-proxy.yaml /var/lib/rancher/rke2/agent/kube-proxy.yaml_backup\n</code></pre> <ol> <li>Restart the rke2-agent service:</li> </ol> <pre><code>systemctl restart rke2-agent\n</code></pre>"},{"location":"kbs/000021284/#cause","title":"Cause","text":"<p>This bug was tracked in https://github.com/rancher/rke2/issues/4864 and fixed in RKE2 1.27.12, 1.28.8, 1.29.4 and above.</p>"},{"location":"kbs/000021284/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021286/","title":"Inability to attach/detach vSphere CNS block volumes","text":"<p>This document (000021286) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021286/#environment","title":"Environment","text":"<ul> <li>Rancher 2.6 / 2.7</li> <li>RKE1/RKE2</li> <li>Kubernetes v1.19+</li> <li>vSphere 6.7 U3+ or vSphere 7.0+</li> <li>Vsphere cloud provider:</li> <li>Vsphere CPI: rancher-vsphere-cpi:100.3.0+up1.2.1+</li> <li>Vsphere CSI: rancher-vsphere-csi:100.3.0+up2.5.1-rancher1+</li> </ul>"},{"location":"kbs/000021286/#situation","title":"Situation","text":"<p>Inability to detach/attach CNS block volumes:</p> <p>Customers can create CNS block volumes in the RKE1/RKE2 cluster using the Vsphere CSI.</p> <p>However, when scaling down a workload (deployment,statefulset), the block volume does not get detached automatically from the nodes. Scaling up the workload, the following error appears in the cluster events:</p> <pre><code>rpc error: code = Internal desc = queryVolume failed for volumeID: \"5db7cc3c-62b9-427d-823b-87729fcef771\" with err=ServerFaultCode: NoPermission\n</code></pre>"},{"location":"kbs/000021286/#resolution","title":"Resolution","text":"<p>This error indicates the user is missing the permission \" Cns.Searchable\" at the root vCenter level and Datastore level.</p> <p>To grant the user account the Cns.Searchable permission in vSphere, see the following documentation:</p> <p>https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/launch-kubernetes-with-rancher/use-new-nodes-in-an-infra-provider/vsphere/create-credentials</p>"},{"location":"kbs/000021286/#cause","title":"Cause","text":"<p>The user account within vSphere must be granted the following permissions:</p> <p>https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-C04F1605-D158-4B65-810F-6F5B109BCDEC.html</p> <p></p>"},{"location":"kbs/000021286/#additional-information","title":"Additional Information","text":"<p>vSphere CNS Block volumes:</p> <p>Cloud Native Storage (CNS) integrates vSphere and Kubernetes and offers capabilities to create and manage container volumes in vSphere environment. CNS consists of the two components, CNS component in vCenter Server and a vSphere volume driver in Kubernetes, called vSphere Container Storage Plug-in.</p> <p>vSphere Cloud Provider Interface (CPI):</p> <p>Is responsible for running all the platform-specific control loops that were previously run in core Kubernetes components like the KCM and the kubelet, but have been moved out-of-tree to allow cloud and infrastructure providers to implement integrations that can be developed, built, and released independent of Kubernetes core</p> <p>vSphere Container Storage Interface (CSI):</p> <p>It is a specification designed to enable persistent storage volume management on Container Orchestrators (COs) such as Kubernetes. The specification allows storage systems to integrate with containerized workloads running on Kubernetes. Using CSI, storage providers, such as VMware, can write and deploy plugins for storage systems in Kubernetes without a need to modify any core Kubernetes code.</p> <p>CSI allows volume plugins to be installed on Kubernetes clusters as extensions. Once a CSI-compatible volume driver is deployed on a Kubernetes cluster, users can use the CSI to provision, attach, mount, and format the volumes exposed by the CSI driver.</p> <p>The CSI driver for vSphere is <code>csi.vsphere.vmware.com</code>.</p>"},{"location":"kbs/000021286/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021299/","title":"Security Group behaviour in Rancher-provisioned EKS clusters","text":"<p>This document (000021299) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021299/#environment","title":"Environment","text":"<p>- Rancher v2.6.7+</p> <p>- Rancher-provisioned EKS clusters with\u00a0user-specified AWS Security Group configuration</p>"},{"location":"kbs/000021299/#situation","title":"Situation","text":"<p>- Provision an EKS cluster from Rancher, adding additional\u00a0user-specified AWS Security Groups to the cluster configuration</p>"},{"location":"kbs/000021299/#resolution","title":"Resolution","text":"<p>The following is applied to the AWS Security Group configuration when provisioning an EKS cluster from Rancher.</p> <p>1. If a user-specified Security Group is not set in the EKS cluster configuration within Rancher:</p> <p>- The default Security Group is applied at the cluster level</p> <p>- The default Security Group is applied to nodes in nodegroups without a Launch Template containing a Security Group configuration</p> <p>- On any nodes in nodegroups with a Launch Template containing a Security Group configuration, the default Security Group is replaced by the Security Group configuration from the Launch Template</p> <p>2. If a user-specified Security Group is set in the EKS cluster configuration within Rancher:</p> <p>- The default Security Group and the user-specified Security Group are applied at the cluster level</p> <p>- The default Security Group is applied to nodes in nodegroups without a Launch Template containing a Security Group configuration</p> <p>- On any nodes in nodegroups with a Launch Template containing a Security Group configuration, the default Security Group is replaced by the Security Group configuration from the Launch Template</p> <p>As a result of a bug, in Rancher v2.6.4 - v2.6.6, if any user-specified Security Groups were applied to the cluster, only these user-specified groups were applied to nodegroups without a Launch Template containing a Security Group configuration, potentially breaking communication between nodes and the cluster controlplane, as detailed in https://github.com/rancher/rancher/issues/38014. Any user on an affected version should upgrade to a later Rancher release.</p>"},{"location":"kbs/000021299/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021301/","title":"RKE2 OS Upgrade Process","text":"<p>This document (000021301) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021301/#environment","title":"Environment","text":"<p>A Rancher-provisioned or standalone RKE2 cluster</p>"},{"location":"kbs/000021301/#situation","title":"Situation","text":"<p>You need to upgrade or patch the operating system of your RKE2 cluster nodes.</p>"},{"location":"kbs/000021301/#resolution","title":"Resolution","text":"<p>Below are the required steps to upgrade or patch the underlying operating system of nodes within an RKE2 cluster.</p> <p>After upgrading each node, whether a worker or controlplane/etcd node, you should check that the node and cluster are healthy before upgrading the next node.</p> <p>The steps to upgrade or patch the operating system are as follows:</p> <ol> <li>During the OS upgrade, don't stop the rke2-server or rke2-agent services</li> <li>Drain worker nodes before upgrading them</li> <li>Wait for the pods to be drained and scheduled to other nodes before upgrading the drained worker</li> <li>Perform the upgrade in a rolling fashion:</li> <li>Start by upgrading a single controlplane/etcd node at a time</li> <li>Upgrade one or more workers at a time (depending upon the capacity within the cluster to reschedule workloads\u00a0to other nodes)</li> <li>Reboot the node after the upgrade is completed</li> <li>Un-cordon the node</li> </ol> <p>Draining nodes during the operating system upgrade:</p> <p>Controlplane/etcd nodes should not be running with the worker role, and so should not be running any user workloads. The rancher cluster-agent scheduled on these nodes runs as a Deployment of two replicas on separate nodes. As a result, you do not need to drain controlplane/etcd nodes during upgrade.</p> <p>Special care if Longhorn is installed:</p> <p>If Longhorn is installed on the cluster, please take a look at the upgrade requirements in the Longhorn documentation related to the version you are using, in particular the section\u00a0concerning draining nodes:\u00a0https://longhorn.io/docs/1.6.2/maintenance/maintenance/#updating-the-node-os-or-container-runtime</p>"},{"location":"kbs/000021301/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021302/","title":"Rancher pods in CrashLoopBackOff state with log messages \"panic: indexer conflict: map[byPod:{}]\" and \"namespaces not found\"","text":"<p>This document (000021302) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021302/#environment","title":"Environment","text":"<ul> <li>Rancher v2.6+</li> <li>A cluster Namespace in the Rancher local cluster that has been forcibly removed by removing the Kubernetes finalizer</li> </ul>"},{"location":"kbs/000021302/#situation","title":"Situation","text":"<p>The Rancher UI is not accessible and the Rancher pods are in a\u00a0CrashLoopBackOff state due to a panic, with the log message \"panic: indexer conflict: map[byPod:{}]\".</p> <p>Rancher pods logs contain resource and namespace not found errors related to the forcibly deleted cluster Namespace:</p> <pre><code>[ERROR] error syncing 'c-g4k52/m-c6n6g': handler node-controller: namespaces \"c-g4k52 \" not found, handler node-controller-sync: namespaces \"c-g4k52 \" not found\n[ERROR] error syncing 'c-g4k52/m-6j6pk': handler node-controller: namespaces \"c-g4k52 \" not found, handler node-controller-sync: namespaces \"c-g4k52 \" not found,\n[ERROR] error syncing 'c-g4k52/m-zx6qg': handler node-controller: namespaces \"c-g4k52 \" not found, handler node-controller-sync: namespaces \"c-g4k52 \" not found,\n</code></pre> <pre><code>[ERROR] error syncing 'c-g4k52/creator-cluster-owner': handler mgmt-auth-crtb-controller: clusters.management.cattle.io \"c-g4k52 \" not found, requeuing\n[ERROR] error syncing 'p-d6hvn/creator-project-owner': handler mgmt-auth-prtb-controller: clusters.management.cattle.io \"c-g4k52 \" not found, requeuing\n[ERROR] error syncing 'p-ch66s/creator-project-owner': handler mgmt-auth-prtb-controller: clusters.management.cattle.io \"c-g4k52 \" not found, requeuing\n</code></pre> <pre><code> [ERROR] error syncing 'c-g4k52/c-g4k52-fleet-default-owner': handler mgmt-auth-crtb-controller: failed to remove finalizer on controller.cattle.io/mgmt-auth-crtb-controller, requeuing\n [ERROR] error syncing 'c-g4k52/c-g4k52-rl-f62qq': handler etcdbackup-controller: failed to remove finalizer on controller.cattle.io/etcdbackup-controller, requeuing\n</code></pre>"},{"location":"kbs/000021302/#resolution","title":"Resolution","text":"<p>Remove all of the orphaned resources in the now-deleted cluster Namespace by following the process documented at\u00a0https://www.suse.com/support/kb/doc/?id=000020788</p> <p>In this example, the problematic deleted cluster identified from the Rancher logs has the cluster ID and Namespace\u00a0c-g4k52.</p>"},{"location":"kbs/000021302/#cause","title":"Cause","text":"<p>Orphaned resources within Namespaces for a now-deleted cluster exist as a result of the forced removal of the Kubernetes Namespace, due to the manual removal of the Kubernetes finalizer from the Namespace when it is stuck in a terminating state. The Kubernetes finalizer should never be removed from a Namespace and the following procedure can be used to remove a Namespace stuck in a terminating state:\u00a0https://www.suse.com/support/kb/doc/?id=000021065</p>"},{"location":"kbs/000021302/#additional-information","title":"Additional Information","text":"<ul> <li>Remove namespace stuck in terminating state: https://www.suse.com/support/kb/doc/?id=000021065</li> <li>How to clean orphaned cluster objects: https://www.suse.com/support/kb/doc/?id=000020788</li> </ul>"},{"location":"kbs/000021302/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021303/","title":"What is the correct amount of etcd nodes for my cluster?","text":"<p>This document (000021303) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021303/#environment","title":"Environment","text":"<p>Rancher management (local) cluster running with RKE or RKE2.</p> <p>The same\u00a0tips can be used for managed downstream clusters.</p>"},{"location":"kbs/000021303/#situation","title":"Situation","text":"<p>When performing the installation/configuration of a cluster, you may wonder how many etcd nodes you should setup to ensure a good performance of the cluster.</p>"},{"location":"kbs/000021303/#resolution","title":"Resolution","text":"<p>Three etcd nodes are generally sufficient for smaller and medium clusters, and five etcd nodes for large clusters.</p> <p>One thing that we must consider is that etcd employs a quorum mechanism. This requires the presence of over half of the replicas to be operational before it can authorize any modifications to the database.</p> <p>Therefore, in the case of a two-node etcd cluster, it won\u2019t be sufficient to have only one healthy etcd node; both nodes must be functional. We always recommend having at least three etcd nodes. As per the functioning logic of etcd, it is recommended to always work with an odd number of primary nodes.</p> <p>Adding more etcd nodes may also be possible, but we also need to consider replication performance, not only availability. Scaling up members will reduce performance. Our recommendation would be to add the necessary worker nodes and see how the cluster performs. If you notice high pressure after adding all the worker nodes, then you can go ahead and add two more etcd nodes. Always check the rule of having an odd number of etcd nodes.</p> <p>See the below table for a better understanding:</p> Nodes with\u00a0<code>etcd</code>\u00a0role Majority Failure Tolerance 1 1 0 2 2 0 3 2 1 4 3 1 5 3 2 6 4 2 7 4 3 8 5 3 9 5 4"},{"location":"kbs/000021303/#additional-information","title":"Additional Information","text":"<p>Please read more about this topic here:</p> <ul> <li>https://ranchermanager.docs.rancher.com/reference-guides/kubernetes-concepts</li> <li>https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/kubernetes-clusters-in-rancher-setup/checklist-for-production-ready-clusters/recommended-cluster-architecture</li> <li>https://etcd.io/docs/v3.4/faq/#what-is-failure-tolerance</li> <li>https://ranchermanager.docs.rancher.com/how-to-guides/advanced-user-guides/tune-etcd-for-large-installs</li> </ul>"},{"location":"kbs/000021303/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021304/","title":"Recommendation against the use of the --atomic flag for Rancher helm chart upgrades","text":"<p>This document (000021304) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021304/#environment","title":"Environment","text":"<p>A Rancher v2.x instance installed via Helm chart</p>"},{"location":"kbs/000021304/#situation","title":"Situation","text":"<p>Rancher upgrades using the Helm CLI</p>"},{"location":"kbs/000021304/#resolution","title":"Resolution","text":"<p>Do not use the <code>--atomic</code> flag for Rancher upgrades using Helm</p>"},{"location":"kbs/000021304/#cause","title":"Cause","text":"<p>The helm <code>--atomic</code> flag, causes helm to rollback any changes made in case of a failed helm chart upgrade.</p> <p>Hypothetically, if the Rancher Deployment portion of the upgrade succeeds and new Rancher Pods start running, and making configuration state changes to the Rancher custom resources, but the helm upgrade then fails on another resource within the chart for any reason, a helm rollback would be automatically performed if the\u00a0<code>--atomic</code>\u00a0flag is used.</p> <p>Rancher Helm chart version rollbacks, without a restore of Rancher state, are not supported and this may result in a broken state, with partially upgraded Rancher state, but with the older Rancher version running.</p> <p>For Rancher rollbacks in the case of a failed Rancher upgrade you should refer to the Rancher rollback documentation, using the Rancher backup operator, or a cluster-level restore, using the appropriate restore method for your local Rancher cluster (per the respective RKE, RKE2 and K3s documentation).</p>"},{"location":"kbs/000021304/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021309/","title":"How to set the maximum allowed size of the client request body for an Ingress with ingress-nginx","text":"<p>This document (000021309) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021309/#environment","title":"Environment","text":"<p>A Kubernetes cluster with the ingress-nginx ingress controller</p>"},{"location":"kbs/000021309/#situation","title":"Situation","text":"<p>The ingress-nginx controller in Kubernetes may return a 413 response code (Request Entity Too Large) when attempting to upload or send large files. This error occurs because ingress-nginx imposes a default limit of 1MB on the request body size.</p> <pre><code>192.168.1.10 - - [22/Aug/2025:15:10:25 +0000] \"POST /upload HTTP/1.1\" 413 583 \"-\" \"curl/7.68.0\" 10485760 0.000 [-] - - - - a1b2c3d4-e5f6-7890-1234-567890abcdef\n</code></pre>"},{"location":"kbs/000021309/#resolution","title":"Resolution","text":"<p>The allowed maximum size of the client request body can be set at either the individual Ingress level, or of the default value for all Ingresses can be set in the ingress-nginx controller ConfigMap.</p> <p>Updating the maximum request body size at the Ingress level</p> <p>Adjust the <code>proxy-body-size</code> setting using the <code>nginx.ingress.kubernetes.io/proxy-body-size</code> annotation in the Ingress resource. This setting determines the maximum allowed size of the request body for requests to this individual Ingress. For example, set the following annotation on the Ingress to set a maximum allowed size of 4096MB.</p> <pre><code>  nginx.ingress.kubernetes.io/proxy-body-size: \"4096m\"\n</code></pre> <p>Updating the default request maximum body size in the ingress-nginx configuration</p> <p>Adjust the\u00a0<code>proxy-body-size</code> in the ingress-nginx controller's ConfigMap.</p> <p>For RKE2 clusters this configuration can be defined by creating a\u00a0HelmChartConfig resource in the cluster, to customise the values of the deployed rke2-ingress-nginx chart. In the following example, the maximum allowed size of the client request body is set to 5MB.</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-ingress-nginx\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    controller:\n      config:\n        proxy-body-size: 5m\n</code></pre> <p>For standalone RKE2 clusters, this HelmChartConfig can be added via a manifest in the /var/lib/rancher/rke2/server/manifests directory on a server node. For Rancher-provisioned RKE2 clusters, the HelmChartConfig can be added via the Additional Manifest form of the Edit Config interface within Cluster Management.</p>"},{"location":"kbs/000021309/#cause","title":"Cause","text":"<p>The ingress-nginx controller imposes a limit on the size of the client request body it will accept, with a default of 1MB. When a client attempts to send a request with a body larger than this limit, ingress-nginx responds with a 413 Request Entity Too Large response code.</p>"},{"location":"kbs/000021309/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021309/#additional-information","title":"Additional Information","text":"<ul> <li>ingress-nginx documentation on defining a custom max body size</li> </ul>"},{"location":"kbs/000021309/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021310/","title":"How to collect Rancher Diagnostic Package","text":"<p>This document (000021310) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021310/#environment","title":"Environment","text":"<p>Rancher v2.7.x+</p>"},{"location":"kbs/000021310/#situation","title":"Situation","text":"<p>During troubleshooting with Rancher Support, you may be requested to provide a Rancher Diagnostic Package. A Rancher Diagnostic Package contains performance-related data, including the count of resources within clusters managed by a Rancher instance, and the time taken to retrieve these, in order to aid issue investigation by Rancher Support and Engineering.</p>"},{"location":"kbs/000021310/#resolution","title":"Resolution","text":"<p>From the Hamburger menu in the top-left of the Rancher UI click About -&gt; Diagnostics -&gt; Download Diagnostics Package</p> <ul> <li>About -&gt; Diagnostics:</li> </ul> <p></p> <ul> <li>Download Diagnostic Package:</li> </ul> <p></p> <ul> <li>Click Generate Response Times and Download Diagnostic Package:</li> </ul> <p></p> <ul> <li>The generated Rancher Diagnostic Package will be downloaded in your browser, which you can then share with Rancher Support via your support case.</li> </ul>"},{"location":"kbs/000021310/#additional-information","title":"Additional Information","text":"<p>If you are interested in analysing the data yourself you can run a few jq commands against the json data in the resulting diagostic package.</p> <p>You will need jq installed, a JSON processor detailed here:\u00a0https://jqlang.github.io/jq/</p> <p>You can then run the following selection of common jq queries on the diagnostic data.</p> <p>Top 10 resources in the local cluster:</p> <pre><code>jq '.resourceCounts[-1].counts[0:10][] | \"\\(.resource): \\(.count)\"' &lt; *rancher-diagnostic-data.json\n</code></pre> <p>Total resources in the local cluster:</p> <pre><code>jq '[.resourceCounts[-1].counts[].count] | add' &lt; \u00a0*rancher-diagnostic-data.json\n</code></pre> <p>CRD count in the local cluster:</p> <pre><code>jq '.resourceCounts[-1].counts[] | select(.resource == \"apiextensions.k8s.io.customresourcedefinition\") | \"\\(.resource): \\(.count)\"' &lt; \u00a0*rancher-diagnostic-data.json\n</code></pre> <p>Secrets across upstream and all downstream clusters:</p> <pre><code>jq '[.resourceCounts[].counts[] | select(.resource == \"secret\") | .count] | add '\u00a0&lt; \u00a0*rancher-diagnostic-data.json\n</code></pre> <p>Total Rancher users:</p> <pre><code>jq '.resourceCounts[-1].counts[] | select(.resource == \"management.cattle.io.user\")' \u00a0&lt; *rancher-diagnostic-data.json\n</code></pre> <p>Total downstream cluster count:</p> <pre><code>jq '[.resourceCounts[].id] | length - 1' &lt; \u00a0*rancher-diagnostic-data.json\n</code></pre>"},{"location":"kbs/000021310/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021314/","title":"Change max-worker-connections for ingress controller","text":"<p>This document (000021314) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021314/#environment","title":"Environment","text":"<p>RKE1, RKE2 clusters</p>"},{"location":"kbs/000021314/#situation","title":"Situation","text":"<p>Customise the\u00a0max-worker-connections for the ingress controller deployed in RKE1 or RKE2 downstream clusters (default is 16384).</p>"},{"location":"kbs/000021314/#resolution","title":"Resolution","text":""},{"location":"kbs/000021314/#rke1","title":"RKE1:","text":"<p>Edit Config of the RKE1 downstream cluster &gt;&gt; Edit as Yaml &gt;&gt; Add the below `addons:` section under `rancher_kubernetes_engine_config`</p> <pre><code>rancher_kubernetes_engine_config:\n  addon_job_timeout: 45\n  addons: |-\n    ---\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: ingress-nginx-controller\n      namespace: ingress-nginx\n    data:\n      max-worker-connections: \"8192\"\n</code></pre>"},{"location":"kbs/000021314/#rke2","title":"RKE2:","text":"<p>Edit config of the RKE2 downstream cluster&gt;&gt; Go to Add-on Config tab &gt;&gt; Additional Manifests section &gt;&gt; Paste the contents below:</p> <pre><code>---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-ingress-nginx\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    controller:\n      config:\n        max-worker-connections: 8192\n</code></pre> <p>Similar manifests can be used for upstream RKE1 and RKE2 clusters as well.</p>"},{"location":"kbs/000021314/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021314/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021315/","title":"'Internal error occurred: failed calling webhook \"validate.nginx.ingress.kubernetes.io\"' error when upgrading Rancher immediately after performing a local cluster Kubernetes upgrade","text":"<p>This document (000021315) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021315/#environment","title":"Environment","text":"<p>A Rancher v2.x instance running in an RKE or RKE2 cluster</p>"},{"location":"kbs/000021315/#situation","title":"Situation","text":"<p>The\u00a0Rancher helm chart upgrade is launched immediately after upgrading the underlying Kubernetes cluster and fails with an error of the following format:</p> <pre><code>Internal error occurred: failed calling webhook \"validate.nginx.ingress.kubernetes.io\": Post \"https://rke2-ingress-nginx-controller-admission.kube-system.svc:443/networking/v1/ingresses?time=10s\": no endpoint available for service \"rke2-ingress-nginx-controller-admission\"\n</code></pre>"},{"location":"kbs/000021315/#resolution","title":"Resolution","text":"<p>Before running a Rancher upgrade, check that ingress-nginx Pods in the local cluster are running. If the Rancher upgrade task is executed in an automated fashion, a delay should be introduced between the cluster upgrade and the Rancher helm upgrade.</p> <p>The commands below will enable you to validate that the ingress-nginx pods are running before performing the Rancher helm upgrade. The flag \"timeout\" ensures the command exits with an error on a timeout, rather than waiting indefinitely, e.g. --timeout=2m.</p> <p>RKE1</p> <pre><code>kubectl rollout status daemonset -n ingress-nginx nginx-ingress-controller --timeout=x\n\nkubectl wait --for=condition=Available -n ingress-nginx nginx-ingress-controller\n</code></pre> <p>RKE2</p> <pre><code>kubectl rollout status daemonset -n kube-system rke2-ingress-nginx-controller\u00a0 --timeout=x\n\nkubectl wait --for=condition=Available -n kube-system rke2-ingress-nginx-controller\n</code></pre>"},{"location":"kbs/000021315/#cause","title":"Cause","text":"<p>The 'helm upgrade --install' task for Rancher is invoked before the ingress-nginx pods in the cluster are running, following the cluster upgrade. As a result, the ingress-nginx validating webhook cannot be satisfied, resulting in errors when helm attempts to patch the Rancher ingress resource.</p>"},{"location":"kbs/000021315/#additional-information","title":"Additional Information","text":"<ul> <li>Rancher upgrade checklist:\u00a0https://www.suse.com/es-es/support/kb/doc/?id=000020061</li> <li>Rancher upgrade:\u00a0https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/upgrades#steps-to-upgrade-rancher</li> <li>How to install or upgrade to a specific Rancher v2.x version:\u00a0https://www.suse.com/es-es/support/kb/doc/?id=000020217</li> </ul>"},{"location":"kbs/000021315/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021317/","title":"How to test rancher-logging Flows and Outputs with local file output","text":"<p>This document (000021317) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021317/#environment","title":"Environment","text":"<ul> <li>A Rancher v2.5+ managed Kubernetes clusters</li> <li>rancher-logging installed in the cluster</li> </ul>"},{"location":"kbs/000021317/#situation","title":"Situation","text":"<p>When configuring rancher-logging on a cluster, it may be necessary to troubleshoot Flows and Outputs. This guide provides a way to test these without any third-party tools involved (e.g. an external logging destination such as Elasticsearch).</p>"},{"location":"kbs/000021317/#resolution","title":"Resolution","text":"<ol> <li>The first step will be to create an Output on the cluster-logging-system namespace. You can do so by exploring the cluster within the Rancher UI,\u00a0navigating to\u00a0Logging &gt; Outputs in the resources menu on the left side,\u00a0and then clicking\u00a0Create(alternatively, you can\u00a0apply the manifest\u00a0via the kubectl CLI).\u00a0The following example manifest will create a basic Output that you can customize later. It will send the resulting logs to the /tmp/logs/ folder on the FluentD\u00a0pod (rancher-logging-root-fluentd-0) in the cluster-logging-system Namespace:</li> </ol> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: Output\nmetadata:\n    name: test-output\n   \u00a0namespace: cattle-logging-system\nspec:\n    file:\n       path: /tmp/logs/${tag}/%Y/%m/%d/%H.%M\n       buffer:\n         timekey: 1m\n         timekey_wait: 10s\n         timekey_use_utc: true\n</code></pre> <ol> <li>The next step is to\u00a0create a Flow that targets the pod\u00a0created in step 3, using a label selector, and routes logs to the Output created in step 1.\u00a0Again, this can be done from the UI\u00a0by clicking on the\u00a0Logging &gt; Flows\u00a0tab and clicking Create(or via kubectl):</li> </ol> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: Flow\nmetadata:\n     name: test-flow\n     namespace: cattle-logging-system\nspec:\n     localOutputRefs:\n    - test-output\nmatch:\n    - select:\n        labels:\n          logging-test: 'true'\n</code></pre> <ol> <li>Next, a dummy pod will be deployed to generate log messages\u00a0with which to test the Flow and Output. In this example the\u00a0swiss-army-knife debug image created by Rancher Support is used, but you can replace this as required by your testing.\u00a0You can create this Pod via the Import YAML\u00a0function at the top-right of Rancher the UI or via the kubectl CLI:</li> </ol> <p>```    apiVersion: v1    kind: Pod    metadata:      name: test-logging-pod      namespace: cattle-logging-system    \u00a0 labels:    \u00a0 \u00a0 logging-test: \"true\"    spec:      containers:   - name: sak     image: rancherlabs/swiss-army-knife:latest</p> <p><pre><code>4. With the Output, Flow, and Pod in place, you can now test the setup by executing a shell in\u00a0the test pod and sending any\u00a0desired test log messages to STDOUT (before doing this, be sure that the fluentd-configcheck pod in the cattle-logging-system Namespace is in status Completed):\n</code></pre> $ kubectl -n cattle-logging-system exec -it test-logging-pod -- /bin/bash</p>"},{"location":"kbs/000021317/#echo-hello-world-from-rancher-logging-proc1fd1","title":"echo 'Hello World from Rancher Logging!' &gt; /proc/1/fd/1","text":"<pre><code>5. To view the results, you have to go to the /tmp/logs folder that will be created in the Fluentd\u00a0pod (you might have to wait up to a few minutes for the log file to be populated, due to container log scraping and buffering by Fluent Bit). Inside the /tmp/logs directory will be a directory for the test Pod, and the log file will be in a child directory under the day, month and year:\n6. ```\n$ kubectl -n cattle-logging-system exec -it rancher-logging-root-fluentd-0 -- /bin/sh\n\n$ ls -lrt /tmp/logs\ntotal 4\ndrwxr-xr-x \u00a0 \u00a03 fluent \u00a0 fluent \u00a0 \u00a0 \u00a0 \u00a04096 Feb 28 15:06 kubernetes.var.log.containers.test-logging-pod_cattle-logging-system_sak-8611b0ee3869dc245a4921aac63242901b104f77c86c7d58a32caafafe59aaaa.log\n\n$ cat 15.05_0.log\n2024-02-28T15:05:50+00:00 \u00a0 \u00a0 \u00a0 kubernetes.var.log.containers.test-logging-pod_cattle-logging-system_sak-8611b0ee3869dc245a4921aac63242901b104f77c86c7d58a32caafafe59aaaa.log \u00a0 {\"log\":\"Hello World from Rancher Logging!\\n\",\"stream\":\"stdout\",\"time\":\"2024-02-28T15:05:50.766535534Z\",\"kubernetes\":{\"pod_name\":\"test-logging-pod\",\"namespace_name\":\"cattle-logging-system\",\"pod_id\":\"1d16a19e-f565-447b-b9f0-39bdf559eaf6\",\"labels\".....\n</code></pre> <ol> <li>You can now freely change your Output and Flow configuration to test and troubleshoot configurations, filters and\u00a0parsers etc.\u00a0The results will appear in the /tmp/logs/ folder of the Fluentd\u00a0pod.</li> </ol>"},{"location":"kbs/000021317/#additional-information","title":"Additional Information","text":"<p>Provided you followed the steps above, you can now change the created Output and Flow to test any custom configurations that you need. You should be able to see the results right away and quickly spot any problems. Remember that you can use the following KB to guide you in troubleshooting issues:\u00a0How to troubleshoot rancher-logging | Support | SUSE</p>"},{"location":"kbs/000021317/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021328/","title":"Misconfigured Kubewarden mutating policies together with 3rd party Kubernetes Controllers can get stuck in an infinite loop","text":"<p>This document (000021328) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021328/#environment","title":"Environment","text":"<ul> <li>Rancher v2.7+</li> <li>Kubewarden</li> </ul>"},{"location":"kbs/000021328/#situation","title":"Situation","text":"<p>A Kubernetes resource, mutated by a Kubewarden mutating policy, is stuck in a reconciliation loop, as it is updated by both the Kubewarden policy and another controller</p>"},{"location":"kbs/000021328/#resolution","title":"Resolution","text":"<p>The solution, as described in the Kubewarden documentation, is to perform the mutation within the mutating policy against:</p> <ol> <li>The lower type of resource (e.g: Pod).</li> <li>The highest type of resource (e.g: Deployment). Note: this could still lead to loops if a controller is managing those resources. For example controllers of GitOps solutions (like fleet, flux, argo, ...) or other 3rd party controllers that translate their own CRDs into Deployment objects.</li> </ol>"},{"location":"kbs/000021328/#cause","title":"Cause","text":"<p>Per the Kubewarden documentation:</p> <p>\"Mutating policies return requests that proceed through the Kubernetes API. If there are other Kubernetes Controllers that listen for those same resources, they may mutate them back in a follow-up request. This could lead to an infinite feedback loop of mutations.\"</p>"},{"location":"kbs/000021328/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021332/","title":"Rancher Monitoring: Known Issues with NFS Persistent Storage for Prometheus","text":"<p>This document (000021332) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021332/#environment","title":"Environment","text":"<ul> <li>Rancher v2.x, from v2.5 and above</li> <li>rancher-monitoring deployed with a Prometheus persistent storage configuration using NFS</li> </ul>"},{"location":"kbs/000021332/#situation","title":"Situation","text":"<p>The Prometheus server is unstable, with storage-related errors in the Prometheus Pod logs:</p> <pre><code>2022-05-26T07:15:19.251601279Z level=error ts=2022-05-26T07:15:19.251Z caller=db.go:745 component=tsdb msg=\"compaction failed\" err=\"plan compaction: open /prometheus/01F8Q3MT8CJXMNMPKW7G86M6M5/meta.json: no such file or directory\"\n</code></pre> <p>and/or a warning about NFS storage not being supported:</p> <pre><code>2022-05-25T10:15:40.601740700Z level=warn ts=2022-05-25T10:15:40.601Z caller=main.go:756 fs_type=NFS_SUPER_MAGIC msg=\"This filesystem is not supported and may lead to data corruption and data loss. Please carefully read https://prometheus.io/docs/prometheus/latest/storage/ to learn more about supported filesystems.\"\n</code></pre>"},{"location":"kbs/000021332/#resolution","title":"Resolution","text":"<p>As described in the\u00a0Prometheus docs, NFS is not a supported filesystem for Prometheus database storage:</p> <pre><code>CAUTION: Non-POSIX compliant filesystems are not supported for Prometheus' local storage as unrecoverable corruptions may happen. NFS filesystems (including AWS's EFS) are not supported. NFS could be POSIX-compliant, but most implementations are not. It is strongly recommended to use a local filesystem for reliability.\n</code></pre> <p>You should therefore remove the persistent storage configuration using NFS in the rancher-monitoring chart options or switch to a supported persistent storage type, such as a block storage volume provisioned (e.g. AWS EBS, Longhorn, vSphere volumes).</p>"},{"location":"kbs/000021332/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021334/","title":"EMEA Support Business Hours for Rancher Prime","text":"<p>This document (000021334) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021334/#situation","title":"Situation","text":"<p>Customers who have purchased a Standard Support Subscription for Rancher Prime, i.e., they have 12x5 coverage, are entitled to receive support during business hours as defined in the SUSE Support Handbook.</p> <p>This document outlines the EMEA region holiday occasions when Standard Support Subscriptions will be serviced on the following business day.</p>"},{"location":"kbs/000021334/#resolution","title":"Resolution","text":"<p>EMEA Support Business Hours as defined in the SUSE Support Handbook:</p> <p>6 AM to 6 PM Central European Time - Monday to Friday</p> <p>Business Hours exclude the following Public holidays for the region:</p> Holiday/Occasion Date Day Good Friday (Easter Friday) 18th April 2025 Friday Easter Monday 21st April 2025 Monday Labor Day 1st May 2025 Thursday Christmas 25th December 2025 Thursday Christmas (Boxing Day) 26th December 2025 Friday New Year 1st January 2026 Thursday"},{"location":"kbs/000021334/#additional-information","title":"Additional Information","text":"<p>More information on the Support and Maintenance for Rancher Prime can be found here:</p> <p>https://www.suse.com/support/rancher-prime/</p> <p>SUSE Support Handbook:</p> <p>https:/ https://www.suse.com/support/handbook/</p> <p>Note: Customers with 24x7 subscriptions are not affected by the information in the current document.</p>"},{"location":"kbs/000021334/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021363/","title":"CVE-2024-21626 Runc","text":"<p>This document (000021363) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021363/#situation","title":"Situation","text":"<p>Upstream information from NIST</p> <p>The Problem:</p> <ul> <li>runc is a CLI tool for spawning and running containers on Linux according to the OCI specification. In runc 1.1.11 and earlier, due to an internal file descriptor leak, an attacker could cause a newly-spawned container process (from runc exec) to have a working directory in the host filesystem namespace, allowing for a container escape by giving access to the host filesystem (\"attack 2\"). The same attack could be used by a malicious image to allow a container process to gain access to the host filesystem through runc run (\"attack 1\"). Variants of attacks 1 and 2 could be also be used to overwrite semi-arbitrary host binaries, allowing for complete container escapes (\"attack 3a\" and \"attack 3b\"). runc 1.1.12 includes patches for this issue.</li> </ul> <p>For more on this CVE, check out this GitHub advisory from OpenContainers:</p> <ul> <li>https://github.com/opencontainers/runc/security/advisories/GHSA-xr7r-f8xq-vfvv</li> </ul>"},{"location":"kbs/000021363/#resolution","title":"Resolution","text":"<p>The solution:</p> <p>This CVE has been patched in runc versions &gt;=1.1.12:</p> <ul> <li>https://github.com/opencontainers/runc/releases/tag/v1.1.12</li> </ul> <p>Patched RKE, RKE2, K3s, and Rancher versions:</p> <p>You can also upgrade to Rancher 2.8.2 as it includes the patches for CVE-2024-21626, as mentioned in the release notes, specifically under Security Fixes:</p> <ul> <li>https://github.com/rancher/rancher/releases/tag/v2.8.2</li> </ul> <p>For RKE users, the releases for patched Kubernetes versions include:</p> <ul> <li> <p>=v1.5.5</p> </li> <li> <p>=v1.4.14</p> </li> </ul> <p>For RKE2 users, the patched Kubernetes versions include:</p> <ul> <li> <p>=v1.26.13+rke2r1</p> </li> <li> <p>=v1.27.10+rke2r1</p> </li> <li> <p>=v1.28.6+rke2r1</p> </li> <li> <p>=v1.29.1+rke2r1</p> </li> </ul> <p>For K3s users, the patched Kubernetes versions include:</p> <ul> <li> <p>=v1.26.13+k3s2</p> </li> <li> <p>=v1.27.10+k3s2</p> </li> <li> <p>=v1.28.6+k3s2</p> </li> <li> <p>=v1.29.1+k3s2</p> </li> </ul> <p>For users of Embedded K3s (Used when running rancher/rancher in a single Docker container(install) - Not recommended for production use)</p> <ul> <li>There will be a future release including the necessary patches (TBD)</li> <li>Please note that this does NOT affect Rancher itself, only if you have spun up an instance of Rancher in a single Docker container</li> <li>For reference</li> </ul> <p>For Air-Gapped users:</p> <ul> <li>There will be a future release specifically for these environments so they can benefit from the KDM (Kontainer driver metadata) upgrades (TDB)</li> <li>For more information on what KDM is, please review this GitHub repo:</li> <li>https://github.com/rancher/kontainer-driver-metadata</li> </ul>"},{"location":"kbs/000021363/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021365/","title":"What is the support scope for the Kubernetes versions of a Rancher release?","text":"<p>This document (000021365) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021365/#environment","title":"Environment","text":"<p>A Rancher-managed RKE, RKE2 or k3s Kubernetes cluster</p>"},{"location":"kbs/000021365/#situation","title":"Situation","text":"<p>When configuring a Rancher local/upstream cluster, or a downstream cluster, you may be concerned about the support scope of those clusters based on their Kubernetes version. This is, the date until you are completely covered by SUSE support, in the case of having an active support entitlement.</p>"},{"location":"kbs/000021365/#resolution","title":"Resolution","text":"<p>As long as the configured version of RKE/RKE2/k3s is included under the Support Matrix\u00a0for the running Rancher version, and that version of Rancher that is still under support, per the SUSE Product Lifecycle, it is covered by SUSE support. Then,\u00a0the critical EOL date to take into account is always the Rancher one, and also to verify that the RKE/RKE2/k3s version is under the Support Matrix.</p>"},{"location":"kbs/000021365/#cause","title":"Cause","text":"<p>The support scope may be\u00a0misunderstood when checking our\u00a0Product Support Lifecycle page for RKE/RKE/k3s. That page should only be consulted when using RKE/RKE2/k3s standalone, without a Rancher installation.</p>"},{"location":"kbs/000021365/#additional-information","title":"Additional Information","text":"<p>It is also important to highlight that fixes may not be backported, so in the event of a Kubernetes bug, the recommended mitigation may be an upgrade to a later Kubernetes release. However, we will still assist you in troubleshooting an issue if the cluster is covered by support, per the above.</p> <p>Besides, upstream Kubernetes fixes are to be incorporated by RKE/RKE2/K3S at the Engineering's team discretion.</p> <p>Note: Please, open a support case if you have any questions around versions and supportability.</p>"},{"location":"kbs/000021365/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021366/","title":"\"dial tcp [::1]:6443: connect: network is unreachable\" errors in kubernetes controlplane components when IPv6 is disabled on the loopback interface","text":"<p>This document (000021366) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021366/#environment","title":"Environment","text":"<ul> <li>Rancher v2.6+</li> <li>A Rancher RKE2 cluster</li> <li>IPv6 networking disabled on the loopback interface</li> </ul>"},{"location":"kbs/000021366/#situation","title":"Situation","text":"<p>Provisioning of a downstream RKE2 cluster with IPv6 networking enabled fails. In the kube-controller-manager container, messages about the lack of connectivity to the API can be seen:</p> <pre><code>dial tcp [::1]:6443: connect: network is unreachable\n</code></pre>"},{"location":"kbs/000021366/#resolution","title":"Resolution","text":"<p>With IPv6 networking enabled on the host, the OS will default to the resolution of localhost to ::1 over 127.0.0.1 (/etc/hosts has localhost resolving to both 127.0.0.1 and ::1).</p> <p>If the host node is started with IPv6 networking enabled, but this is then disabled on the loopback interface, localhost resolution will default to ::1 (which remains in the /etc/hosts file), but the address will not be available on the loopback interface.</p> <p>To resolve this enable IPv6 on the loopback interface:</p> <pre><code>sudo sysctl -w net.ipv6.conf.lo.disable_ipv6=0\n</code></pre> <p>This change will not persist node reboots, and you should also update any custom configuration you have defined in /etc/sysctl.conf or /etc/sysctl.d/*.conf files to ensure IPv6 networking is enabled on the loopback interface.</p>"},{"location":"kbs/000021366/#additional-information","title":"Additional Information","text":"<p>Additional discussion can be found in:</p> <ul> <li>kubernetes/kubernetes#99268 (comment)</li> <li>kubernetes/kubernetes#95706 (comment)</li> </ul>"},{"location":"kbs/000021366/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021371/","title":"Rancher-provisioned RKE2 clusters stuck \"Waiting for kube-controller-manager/kube-scheduler probes\" due to certificate expiration in Rancher &lt; v2.7.5","text":"<p>This document (000021371) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021371/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher &lt; v2.7.5</li> <li>A Rancher-provisioned RKE2 cluster</li> </ul>"},{"location":"kbs/000021371/#situation","title":"Situation","text":"<p>This article explains a problem where a Rancher-provisioned RKE2 cluster gets stuck in a \"Waiting for probes: kube-controller-manager, kube-scheduler\" state, when running Rancher &lt; v2.7.5. This happens because the certificates used by the\u00a0<code>kube-controller-manager</code>\u00a0and\u00a0<code>kube-scheduler</code>\u00a0components expire.</p>"},{"location":"kbs/000021371/#resolution","title":"Resolution","text":"<p>This issue is fixed in Rancher version 2.7.5. Starting with this version, the\u00a0<code>kube-controller-manager</code>\u00a0and\u00a0<code>kube-scheduler</code>\u00a0certificates in Rancher-managed RKE2 clusters are now part of the certificate rotation. When you use\u00a0Rancher's Rotate Certificates feature, these certificates will also be renewed, which stops the problem described earlier.</p> <p>Workaround</p> <p>For versions older than 2.7.5, as well as in exceptional cases, the suggested workaround would be as follows:</p> <ol> <li>Stop the RKE2 server:</li> <li><code>systemctl stop rke2-server</code></li> <li>Remove the .crt and .key file in the respective tls directories:</li> <li><code>rm /var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.{crt,key}</code></li> <li><code>rm /var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.{crt,key}</code></li> <li>Perform a certificate rotation:</li> <li><code>rke2 certificate rotate</code></li> <li>Restart RKE2 server:\u00a0``</li> <li><code>systemctl start rke2-server</code></li> </ol>"},{"location":"kbs/000021371/#cause","title":"Cause","text":"<p>RKE2's regular certificate rotation doesn't manage the certificates for the\u00a0<code>kube-controller-manager</code>\u00a0and\u00a0<code>kube-scheduler</code> in Rancher-provisioned RKE2 clusters. Because of an issue in Rancher versions before v2.7.5, these certificates were not automatically renewed when other cluster certificates were rotated. If these certificates expire, it can stop communication between these cluster components and the <code>kube-apiserver</code>, leading to the \"Waiting for probes\" state.</p>"},{"location":"kbs/000021371/#additional-information","title":"Additional Information","text":"<p>https://github.com/rancher/rancher/issues/41125</p>"},{"location":"kbs/000021371/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021372/","title":"Is it possible to modify the name and ID of an RKE, RKE2 or K3s cluster?","text":"<p>This document (000021372) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021372/#environment","title":"Environment","text":"<p>A Rancher-provisioned RKE, RKE2 or K3s cluster</p>"},{"location":"kbs/000021372/#situation","title":"Situation","text":"<p>Once a cluster is created, you may want to modify its name or identifier manually.</p> <ul> <li>Cluster ID: identifier of the cluster, composed of an alphanumeric string automatically generated, difficult to memorize and differentiate by humans. E.g.:\u00a0c-m-bxg4g77t</li> <li>Cluster name: a string that will recognize a cluster in a more human-friendly manner, e.g.\u00a0cluster-prod</li> </ul>"},{"location":"kbs/000021372/#resolution","title":"Resolution","text":""},{"location":"kbs/000021372/#cluster-id","title":"Cluster ID:","text":"<p>The cluster ID aims to identify a cluster among several components tied to the cluster referring to this ID. Therefore, it is not possible to change a cluster ID manually, as it would also require manually changing all the previous references to that identifier in all the related components in Rancher or the Rancher Kubernetes Engine, which could cause inconsistency if any reference is forgotten. This statement is valid for RKE, RKE2, and k3s clusters.</p>"},{"location":"kbs/000021372/#cluster-name","title":"Cluster name:","text":""},{"location":"kbs/000021372/#rke","title":"RKE:","text":"<p>For Rancher-provisioned RKE clusters, the cluster name can easily be modified using the Rancher UI ( Cluster Management -&gt; Edit Config -&gt; Cluster Name).</p>"},{"location":"kbs/000021372/#rke2k3s","title":"RKE2/k3s:","text":"<p>Contrary to RKE, for Rancher-provisioned RKE2 and k3s clusters, the cluster name is not a field stored on the cluster specification internally but is the name of the cluster custom resource itself that Rancher uses to represent the cluster. As resource names are immutable within Kubernetes (without destroying and recreating the resource), it is not possible to rename RKE2 and K3s clusters within this current design.</p>"},{"location":"kbs/000021372/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021373/","title":"How to set cipher-suites for etcd in RKE2","text":"<p>This document (000021373) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021373/#environment","title":"Environment","text":"<ul> <li>Rancher v2.7+</li> <li>A standalone or Rancher-provisioned RKE2 cluster</li> </ul>"},{"location":"kbs/000021373/#situation","title":"Situation","text":"<p>This article details how to customise the TLS cipher suites used by etcd in an RKE2 cluster</p>"},{"location":"kbs/000021373/#resolution","title":"Resolution","text":""},{"location":"kbs/000021373/#rancher-provisioned-rke2-clusters","title":"Rancher-provisioned RKE2 clusters:","text":"<ol> <li>Navigate to\u00a0Cluster Management within the Rancher UI</li> <li>Click\u00a0Edit Config for the relevant RKE2 cluster</li> <li>Click\u00a0Edit as YAML at the bottom of the page</li> <li>Add a machineSelectorConfig block to set the desired cipher-suites via the etcd-arg field on etcd nodes, per the following example:</li> </ol> <pre><code>spec:\n     [...]\n     rkeConfig:\n       [...]\n       machineSelectorConfig\n      - config:\n          etcd-arg: \"cipher-suites=[TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384]\"\n        matchLabels:\n          rke.cattle.io/etcd-role: 'true'\n[...]\n</code></pre> <ol> <li>Click\u00a0Save to apply the change</li> </ol>"},{"location":"kbs/000021373/#standalone-rke2-clusters","title":"Standalone RKE2 clusters:","text":"<p>Repeat the following process on each server node in the RKE2 cluster:</p> <ol> <li>Add the etcd-arg with the desired cipher-suites to the RKE2 configuration file at /etc/rancher/rke2/config.yaml file and save it, per the following example:</li> </ol> <pre><code>etcd-arg: \"cipher-suites=[TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384]\"\n</code></pre> <ol> <li>Restart the rke2-server service to apply the change:</li> </ol> <pre><code>systemctl restart rke2-server\n</code></pre> <ol> <li>Verify the change. The new configuration will be populated in the etcd configuration file.</li> </ol> <p>```    root@susenode01:~# cat /var/lib/rancher/rke2/server/db/etcd/config    advertise-client-urls: (redacted)    cipher-suites: - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 client-transport-security: \u00a0 cert-file: /var/lib/rancher/rke2/server/tls/etcd/server-client.crt \u00a0 client-cert-auth: true \u00a0 key-file: /var/lib/rancher/rke2/server/tls/etcd/server-client.key \u00a0 trusted-ca-file: /var/lib/rancher/rke2/server/tls/etcd/server-ca.crt data-dir: /var/lib/rancher/rke2/server/db/etcd ...(omitted)</p> <p>```</p>"},{"location":"kbs/000021373/#additional-information","title":"Additional Information","text":"<p>RKE2 Server Configuration Reference</p> <ul> <li>Flags: https://docs.rke2.io/reference/server_config#flags</li> </ul>"},{"location":"kbs/000021373/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021379/","title":"Is Rancher Prime Hosted SOC2 compliant?","text":"<p>This document (000021379) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021379/#resolution","title":"Resolution","text":"<p>Yes, as of October 2023, Rancher Prime Hosted is SOC2, type 2 compliant. A list of all SUSE certifications can be found at https://www.suse.com/support/security/certifications/</p>"},{"location":"kbs/000021379/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021383/","title":"How to change the downstream Rancher Webhook port on a Host Network configuration","text":"<p>This document (000021383) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021383/#environment","title":"Environment","text":"<p>Rancher 2.7.2+</p> <p>Downstream (Rancher-managed) cluster with Host Network enabled on the webhook</p>"},{"location":"kbs/000021383/#situation","title":"Situation","text":"<p>When exposing the Rancher Webhook on a downstream (Rancher-managed) cluster to the Host Network (for example: https://ranchermanager.docs.rancher.com/v2.7/reference-guides/rancher-webhook#eks-cluster-with-calico-cni), another service may already be using port 9443/tcp on the Host Network and the Rancher Webhook needs to have its port changed to avoid port conflicts.</p>"},{"location":"kbs/000021383/#resolution","title":"Resolution","text":"<p>Create a YAML file named 'rancher-config-portchange.yaml' with the following contents (replace the port '9444' with any other desired port number if needed):</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n\u00a0 name: rancher-config\n\u00a0 namespace: cattle-system\n\u00a0 labels:\n\u00a0 \u00a0 app.kubernetes.io/part-of: \"rancher\"\ndata:\n\u00a0 rancher-webhook: |\n\u00a0 \u00a0 \u00a0 port: 9444\n\u00a0 \u00a0 \u00a0 global:\n\u00a0 \u00a0 \u00a0 \u00a0 hostNetwork: true\n</code></pre> <p>Then, apply the manifest above with the following command on the affected cluster:</p> <pre><code>kubectl apply -f rancher-config-portchange.yaml\n</code></pre>"},{"location":"kbs/000021383/#cause","title":"Cause","text":"<p>On Rancher 2.7.2 and above, a dedicated webhook is deployed on clusters managed by Rancher (also known as downstream clusters). In scenarios where the default port of 9443 can conflict, for example, when using the Host Network and having another pod active on the same port (9443/tcp), the Rancher webhook would benefit from having its port changed to avoid any conflicts.</p> <p>A practical example of such a scenario would be to use EKS with a custom CNI, which requires exposing the webhook on the Host Network for it to work properly: https://ranchermanager.docs.rancher.com/v2.7/reference-guides/rancher-webhook#eks-cluster-with-calico-cni.</p>"},{"location":"kbs/000021383/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021385/","title":"system-upgrade-controller failing with \"exec /opt/rancher-system-agent-suc/run.sh: argument list too long\"","text":"<p>This document (000021385) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021385/#environment","title":"Environment","text":"<ul> <li>Rancher v2.6+</li> <li>A Rancher-managed RKE2 or K3s cluster</li> </ul>"},{"location":"kbs/000021385/#situation","title":"Situation","text":"<p>The system-upgrade-controller Pods in a Rancher-managed RKE2 or K3s cluster fail with the error \"exec /opt/rancher-system-agent-suc/run.sh: argument list too long\"</p>"},{"location":"kbs/000021385/#resolution","title":"Resolution","text":"<p>The Rancher TLS certificate may be signed by a private CA or by intermediate certificate(s) signed by a public root CA. In this instance, the tls-ca secret generated in the Rancher local cluster should contain the root CA, followed by any required intermediate CA certificates.</p> <p>If the secret contains additional certificates that are not required, it might be too large, and the system-upgrade-controller Pods will fail.</p> <p>To solve this issue, you must recreate the tls-ca secret containing only the Rancher root, and any required intermediate, CA certificates. To do so, you can follow this documentation:\u00a0https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/resources/add-tls-secrets#using-a-private-ca-signed-certificate</p> <p>Then, perform a restart of the Rancher deployment:</p> <pre><code>kubectl rollout restart deploy/rancher -n cattle-system\n</code></pre> <p>Afterwards, the system-upgrade-controller Job Pods should be able to restart successfully.</p>"},{"location":"kbs/000021385/#cause","title":"Cause","text":"<p>This issue is caused by the tls-ca secret in the cattle-system Namespace of the Rancher local cluster. The secret is\u00a0passed down to the system-upgrade-controller Job Pods as an environment variable (via the stv-aggregation secret in the cattle-system Namespace of the downstream clusters). If the environment variable is too large, it will result in the \"exec /opt/rancher-system-agent-suc/run.sh: argument list too long\" error.</p>"},{"location":"kbs/000021385/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021399/","title":"Failed ETCD snapshot restoration leads the cluster into stuck \"paused\" state","text":"<p>This document (000021399) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021399/#environment","title":"Environment","text":"<p>Rancher Server 2.7.6 and above</p>"},{"location":"kbs/000021399/#situation","title":"Situation","text":"<p>In some cases, the downstream cluster can get into a broken state which requires a Disaster Recovery process to bring it back to its active state.</p> <p>At some point, the DR process does not finish properly and hangs up indefinitely which leads\u00a0the cluster into what is called a \"paused\" state.</p> <p>This symptom can be seen by checking the\u00a0<code>clusters.cluster.x-k8s.io</code>\u00a0object in the\u00a0<code>fleet-default</code>\u00a0namespace from the local (upstream) cluster.</p> <pre><code>kubectl get clusters.cluster.x-k8s.io &lt;CLUSTER_NAME&gt; -n fleet-default -o yaml\n</code></pre> <p>In the yaml output,\u00a0you should see the\u00a0<code>.spec.paused</code>\u00a0field being set to true.</p>"},{"location":"kbs/000021399/#resolution","title":"Resolution","text":"<p>To unblock this situation, the following steps are recommended to perform:</p> <p>- edit the\u00a0<code>clusters.cluster.x-k8s.io</code>\u00a0object in the\u00a0<code>fleet-default</code>\u00a0namespace from the local (upstream) cluster</p> <pre><code>kubectl edit clusters.cluster.x-k8s.io &lt;CLUSTER_NAME&gt; -n fleet-default -o yaml\n</code></pre> <p>- refer to the\u00a0<code>.spec.paused</code>\u00a0field being set to false</p> <p>- save the file and exit</p> <p>The above steps will instruct Rancher to unpause the cluster or unblock the stuck situation to continue doing the restore process.</p> <p>The recommended approach would be performing the DR process again after the edit is made.</p> <p>Right after this, please refer to Rancher Manager backup and restore docs\u00a0here\u00a0to continue the DR process depending on the distribution in use (RKE/RKE2/K3S).</p>"},{"location":"kbs/000021399/#cause","title":"Cause","text":"<p>an unforeseen incident (network, OS failure etc...) led the cluster into a broken state.</p> <p>an outage that made all Control Plane nodes completely unavailable.</p>"},{"location":"kbs/000021399/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021400/","title":"Add caBundle private CA to chart repository","text":"<p>This document (000021400) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021400/#environment","title":"Environment","text":"<p>Rancher 2.6.x</p> <p>Rancher 2.7.x</p> <p>Rancher 2.8.x</p>"},{"location":"kbs/000021400/#situation","title":"Situation","text":"<p>There are some cases where customers and users have their own custom charts and they would prefer to add them and use them in Rancher Server.</p> <p>The UI has a section called \"Repositories\" where they\u00a0simply add the repository in question and then it will become available in the Charts tab under the given name of that repository.</p> <p>While adding the repository an error related to the invalidity of the certificate can appear like the following</p> <pre><code>\"fatal: unable to access 'https://url.chart.repo/some-path/repo.git/': SSL certificate problem: unable to get local issuer certificate\"\n</code></pre> <p>or</p> <pre><code>Get \"https://docker.repo.local/chart/repo/index.yaml\": x509: certificate signed by unknown authority\n</code></pre>"},{"location":"kbs/000021400/#resolution","title":"Resolution","text":"<p>The custom CA certificate needs to be added to the\u00a0ClusterRepo manifest, under the spec.caBundle field.</p> <p>The documentation reference for that matter can be found here.</p>"},{"location":"kbs/000021400/#cause","title":"Cause","text":"<p>missing CA certificate when adding a custom or private chart repository.</p>"},{"location":"kbs/000021400/#additional-information","title":"Additional Information","text":"<p>- In the Apps v1 feature, the management of catalogs was handled centrally within the Rancher process itself. As such, all catalogs were downloaded by Rancher running in the local cluster; and, where additional trusted CAs were added to Rancher, catalogs served by an endpoint with a custom CA certificate were trusted.</p> <p>- In the Apps v2 feature, the management of cluster repositories is handled within each cluster, by Rancher in the local cluster, but by the cattle-cluster-agent in downstream clusters. It is, therefore, necessary to set the CA in the ClusterRepo definition, as the additional trusted CAs are only trusted by the Rancher process itself in the local cluster, and not by the cattle-cluster-agent which downloads the chart repositories within a downstream cluster.</p>"},{"location":"kbs/000021400/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021401/","title":"How to set calico IP_AUTODETECTION_METHOD","text":"<p>This document (000021401) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021401/#environment","title":"Environment","text":"<ul> <li>A Rancher-provisioned or standalone RKE or RKE2 cluster</li> <li>Kubernetes cluster nodes with multiple network interfaces</li> <li>Calico is the Kubernetes cluster CNI plugin</li> </ul>"},{"location":"kbs/000021401/#situation","title":"Situation","text":"<p>When Calico has multiple interfaces, by default, it will choose the first-found interface for the vxlan traffic. To change, this behavior, the Calico auto detection method can be configured to point to a specific network interface. More information on the Calico auto detection methods can be found in the\u00a0Calico documentation.</p>"},{"location":"kbs/000021401/#resolution","title":"Resolution","text":"<p>The configuration differs based on the Kubernetes distribution. In the examples below the configured interface for Calico is ens192.</p>"},{"location":"kbs/000021401/#rke","title":"RKE","text":"<p>In RKE, you can specify the desired host network interface that Calico will use by creating the following configmap under the kube-system namespace.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kubernetes-services-endpoint\n  namespace: kube-system\ndata:\n  IP_AUTODETECTION_METHOD: interface=ens192\n</code></pre>"},{"location":"kbs/000021401/#rke2","title":"RKE2","text":""},{"location":"kbs/000021401/#rancher-provisioned-rke2-clusters","title":"Rancher-provisioned RKE2 clusters","text":"<p>In a Rancher-provisioned RKE2 cluster you can specify the desired host network interface that Calico will use via the cluster add-on configuration:</p> <ol> <li>Navigate to Cluster Management</li> <li>Select the desired RKE2 Cluster</li> <li>Edit Config</li> <li>Add-On Config (under Cluster Configuration).</li> </ol> <p>Here is an example Add-On config with the host network interface set to ens192</p> <pre><code>affinity: {}\napiServer:\n\u00a0 enabled: false\ncalicoctl:\n\u00a0 image: rancher/mirrored-calico-ctl\n\u00a0 tag: v3.28.1\n[...]\ninstallation:\n\u00a0 calicoNetwork:\n \u00a0 \u00a0nodeAddressAutodetectionV4:\n       interface: ens192\n</code></pre>"},{"location":"kbs/000021401/#standalone-rke2-clusters","title":"Standalone RKE2 clusters","text":"<p>In a standalone RKE2 cluster, you can specify the desired host network interface that Calico will use via a HelmChartConfig resource ( https://docs.rke2.io/helm#customizing-packaged-components-with-helmchartconfig)).</p> <p>The HelmChartConfig below can used for this purpose by creating /var/lib/rancher/rke2/server/manifests/rke2-calico-config.yaml on one of the server nodes:</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-calico\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    installation:\n      calicoNetwork:\n        nodeAddressAutodetectionV4:\n          interface: ens192\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000021401/#additional-information","title":"Additional Information","text":"<p>Calico auto detection method documentation</p>"},{"location":"kbs/000021401/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021405/","title":"What are the definitions in the new Prime Release Cycle?","text":"<p>This document (000021405) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021405/#environment","title":"Environment","text":"<p>SUSE Rancher Prime</p> <p>SUSE Rancher Suite</p> <p>SUSE Virtualization</p> <p>SUSE Kubernetes</p> <p>SUSE Private Registry</p>"},{"location":"kbs/000021405/#situation","title":"Situation","text":"<p>Prime introduces a new release cycle to reinforce the value of the support subscriptions. The changes will include alignment with upstream Kubernetes releases, a consistent roadmap rolling forward and a predictable release schedule.</p> <ul> <li>The new release cycle is taking effect starting August 2024.</li> <li>EoM to EoL becomes, at this stage,\u00a0the 12-months maintenance support window, giving us a total of 18 months (to align with the new release cycle - 6 months of full support + 12 months of maintenance support).</li> <li>Release schedule - every ~4 months for Rancher Prime</li> <li>New features are expected to be delivered only upon a new minor release, i.e. X.Y.0 (example 2.9.0, 2.10.0, etc.)</li> <li>Since Rancher is currently releasing a new patch version every 4-6 weeks, these patch releases are expected to include critical bug fixes, CVEs and other fixes evaluated at the discretion of the Rancher Engineering team depending on the stage of the lifecycle.</li> </ul>"},{"location":"kbs/000021405/#resolution","title":"Resolution","text":"<p>Here is the terminology that will be populated across the different documentation SUSE resources once the new Prime release lifecycle takes effect.</p> Community Release The latest to-be-released is made available. Stability validation window of ~1 month before releasing to the Prime registry (for minor releases where minor is x.y.z -&gt; y) Prime Release Available only for paying customers with active subscriptions through the Prime registry. Full Support (6 months) Urgent and selected high-priority bug fixes will be released during the full support window, and all other patches (non-urgent, enhancements, new capabilities) will be released via the regular release schedule. Maintenance Support (12 months) During this period, Critical and Important Security Advisories (CVEs) will be released via patches. Other bug fixes may be released at SUSE's discretion but should not be expected. <p>Here is a graphical representation to better understand the new release lifecycle.</p> <p></p>"},{"location":"kbs/000021405/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021426/","title":"How can a project member create a namespace via command line (kubectl)","text":"<p>This document (000021426) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021426/#environment","title":"Environment","text":"<p>Rancher Version: v2.7.x and above</p>"},{"location":"kbs/000021426/#situation","title":"Situation","text":"<p>In a project setting, a collaborating member can seamlessly establish a namespace via the user interface for workload deployment. For proponents of GitOps and users predominantly utilizing command-line interfaces (CLIs), this approach proves beneficial for deploying a project's namespace efficiently.</p>"},{"location":"kbs/000021426/#resolution","title":"Resolution","text":"<p>To efficiently configure user access and namespace deployment within a project environment, the following steps should be undertaken:</p> <ul> <li>Create a user profile with ANY user permissions (User-Base, Administrator, Restricted Administrator, Standard User) and assign the role of \u2018Project Member\u2019 within the Project through the Rancher user interface</li> <li>Login to the Rancher user interface\u00a0as the new user.</li> <li>Initiate the generation of a\u00a0<code>KUBECONFIG</code>\u00a0file via the user interface linked to the respective user's profile.</li> <li>Develop a namespace manifest incorporating the specific annotation: \" field.cattle.io/projectId: c-m-zk2csxxp:p-bsr6h\", where \"c-m-zk2csxxp\" represents the ClusterID for the targeted cluster and \"p-bsr6h\" signifies the projectID.</li> </ul> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: demo\n  annotations:\n    field.cattle.io/projectId: c-m-cbkjbt2m:p-872c7\n</code></pre> <ul> <li>Configure the system environment variable, KUBECONFIG, to point to the newly generated kubeconfig file from Command Line Interface of your choice.</li> <li>Execute the\u00a0kubectl create command to instantiate the namespace creation process.</li> <li>Validate the operation's success by verifying the newly created namespace within the designated project through the Rancher user interface.</li> </ul>"},{"location":"kbs/000021426/#additional-information","title":"Additional Information","text":"<ul> <li>Ensure to use the kubectl create command and not the kubectl apply command.\u00a0When you are doing apply, it is trying to do a \"get\" to see if the namespace exists. Since, the namespace is not present, the operation would fail with the below error message when you use the apply command:</li> </ul> <pre><code>kubectl apply -f ns.yml\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=namespaces\", GroupVersionKind: \"/v1, Kind=Namespace\"\nName: \"demo\", Namespace: \"\"\nfrom server for: \"ns.yml\": namespaces \"demo\" is forbidden: User \"u-v26xr\" cannot get resource \"namespaces\" in API group \"\" in the namespace \"demo\"\n</code></pre>"},{"location":"kbs/000021426/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021440/","title":"Is it possible to use cluster-scoped Rancher API tokens with the Rancher CLI or Terraform provider?","text":"<p>This document (000021440) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021440/#environment","title":"Environment","text":"<ul> <li>A Rancher cluster, where you have created a cluster-scoped API token, as explained here.</li> <li>Use of\u00a0the cluster-scoped token to interact with Rancher using the Rancher CLI or Terraform provider.</li> </ul>"},{"location":"kbs/000021440/#situation","title":"Situation","text":"<p>If you try to use cluster-scoped Rancher API tokens in the Rancher CLI or Terraform provider, you may see the following error message with an authentication failure:</p> <p><code>FATA[0000]\u00a0Bad response statusCode\u00a0[401]. Status\u00a0[401 Unauthorized]. Body:\u00a0[message=Unauthorized 401: must authenticate]\u00a0from [https://&lt;RANCHER-URL&gt;/v3</code></p>"},{"location":"kbs/000021440/#resolution","title":"Resolution","text":"<p>By design, interaction with the Rancher CLI or Terraform provider does not work with cluster-scoped API tokens. Therefore, if you see the previous error message trying to use the Rancher CLI or the Terraform provided with a cluster-scoped API token, you should switch to an un-scoped API token.</p>"},{"location":"kbs/000021440/#cause","title":"Cause","text":"<p>These tokens are only intended for use with the Rancher /v3 API\u00a0endpoint.</p>"},{"location":"kbs/000021440/#additional-information","title":"Additional Information","text":"<p>https://github.com/rancher/rancher/issues/18639#issuecomment-575952955</p>"},{"location":"kbs/000021440/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021441/","title":"The Repositories of Air-gapped Downstream Clusters in Rancher v2.8.3. fail to pull charts.","text":"<p>This document (000021441) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021441/#environment","title":"Environment","text":"<p>SUSE Rancher v.2.8.3</p>"},{"location":"kbs/000021441/#situation","title":"Situation","text":"<p>There is a bug affecting Rancher v2.8.3 Downstream clusters in AirGapped environments. The Repositories try to pull from \" git.rancher.io\", when they should be using the bundledSystemCharts (assuming the system-catalog setting's value is \"bundled\" in the Upstream cluster). This action fails, as the AirGapped clusters do not have connection to the exterior.</p>"},{"location":"kbs/000021441/#resolution","title":"Resolution","text":"<p>You can check if you are hitting this issue by running the following command in the Upstream cluster:</p> <pre><code>kubectl get settings.management.cattle.io system-catalog\n</code></pre> <p>If the value is set to \"bundled\", run the same command on the Downstream Cluster. If the value is empty, that is a confirmation that you hit the issue.</p> <p>There is a manual workaround available to update the system-catalog setting for Downstream clusters.</p> <ul> <li>On Rancher Manager, navigate to the Downstream clusters\u00a0with the issue.</li> <li>Go to Workloads -&gt; Pods\u00a0\u00a0(with All Namespaces selected in the upper-right dropdown menu).</li> <li>Find the\u00a0cattle-cluster-agent-######## pod (it is in the cattle-system namespace) -&gt; click on the 3 vertical dots to the right -&gt;\u00a0Execute Shell</li> <li>Inside this shell, execute:</li> </ul> <pre><code>kubectl edit settings.management.cattle.io\u00a0system-catalog\n</code></pre> <ul> <li>Change the value to \" bundled\".</li> </ul> <p>The Repositories should work normally after the change is done and a few minutes pass or you Refresh the repositories.</p>"},{"location":"kbs/000021441/#cause","title":"Cause","text":"<p>The \" useBundledSystemChart=true\" setting is not being properly\u00a0shared from upstream to downstream clusters in Rancher v2.8.3. This will cause Downstream Clusters not to contain the \"bundled\" value for their system-catalog setting. As such, they will try to pull the charts from git.rancher.io. In the case of AirGapped clusters, this will fail, and the fetch action will throw a \"Context Deadline Exceeded\" error.</p>"},{"location":"kbs/000021441/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021459/","title":"How to configure RKE2 Windows nodes to run behind an authenticated proxy","text":"<p>This document (000021459) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021459/#environment","title":"Environment","text":"<ul> <li>Downstream RKE2 cluster configured with Windows machines as worker nodes.</li> <li>The Windows nodes work behind a proxy with authentication for internet access.</li> </ul>"},{"location":"kbs/000021459/#situation","title":"Situation","text":"<p>You have followed the recommended steps to register a new Windows node in a cluster:</p> <ul> <li> <p>You can add Windows hosts to the cluster by creating or editing the cluster and choosing the\u00a0Windows\u00a0option.</p> </li> <li> <p>In the upper left corner, click\u00a0\u2630 &gt; Cluster Management.</p> </li> <li>Go to the cluster that you created and click\u00a0\u22ee &gt; Edit Config.</li> <li>Scroll down to\u00a0Node Operating System. Choose\u00a0Windows. Note: You will see that the\u00a0worker\u00a0role is the only available role.</li> <li>Copy the command displayed on the screen to your clipboard.</li> <li> <p>Log in to your Windows host using your preferred tool, such as\u00a0Microsoft Remote Desktop. Run the command copied to your clipboard in the\u00a0Command Prompt (CMD).</p> </li> <li> <p>However, as your Windows node runs under a proxy, the registration command would get stuck with the following error message:</p> </li> <li> <p>```     Error writing proxy setting. (87) The parameter is incorrect.</p> <p>Current WinHTTP proxy settings:</p> <pre><code>Direct access (no proxy server)\n</code></pre> <p><code>- Likewise, a proxy error configuration can be confirmed by checking the 'rancher-wins' logs:   -</code> Get-EventLog -LogName Application -Source 'rancher-wins' -Newest 100 | format-table -Property TimeGenerated, ReplacementStrings -Wrap</p> <p>. . . {error executing instruction 0: Get \"https://index.docker.io/v2/\": dial tcp: lookup                      index.docker.io: getaddrinfow: This is usually a temporary error during hostname resolution and                      means that the local server did not receive a response from an authoritative server.: failed to                      get image index.docker.io/rancher/system-agent-installer-rke2:v1.27.12-rke2r1} ```</p> </li> </ul>"},{"location":"kbs/000021459/#resolution","title":"Resolution","text":"<p>In addition to the configuration of Proxy Settings for the Windows Server, under Internet Settings -&gt; Local LAN Settings -&gt; proxy Server Settings, you need to configure authentication for the proxy to solve the issue. This can be configured using the Windows Server tools:</p> <ol> <li>Open the start menu and type 'credential manager'.</li> <li>In the new window, select 'Add a Windows Credential'.</li> <li>Add the proxy URL, username, and password.</li> <li>Restart the Windows node.</li> </ol> <p>Once the authenticated proxy is configured from Windows settings, the node should be able to access the internet via the proxy. The node registration should finish as expected after this proxy configuration.</p>"},{"location":"kbs/000021459/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021462/","title":"RKE2 vSphere cluster provisioning failing, with failing kube-apiserver healthchecks due to inability to resolve localhost","text":"<p>This document (000021462) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021462/#environment","title":"Environment","text":"<p>Rancher 2.7.x</p> <p>Rancher 2.8.x</p>"},{"location":"kbs/000021462/#situation","title":"Situation","text":"<p>Symptoms</p> <ul> <li>High number restarts\u00a0 of the core pods</li> </ul> <pre><code>NAMESPACE      NAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0   READY \u00a0 STATUS     RESTARTS\ncattle-fleet-system fleet-agent-cc8c97f97-bvx78               1/1    Running      185\ncattle-system cattle-cluster-agent-b1460cbd-8ct5c             1/1    Running      115\ncattle-system cattle-cluster-agent-b1460cbd-l2l8l             1/1    Running      168\nkube-system kube-apiserver-cluster-suse-cp-f777105c-2qgvh     0/1    Running      314\nkube-system kube-controller-manager-cluster-suse-cp-5c-2qgvh  1/1    Running      491\nkube-system cloud-controller-manager-cluster-suse-cp-5c-2qgvh 1/1    Running      501\n</code></pre> <ul> <li>The apiserver pod flaps between ready and not ready status</li> </ul> <pre><code>NAMESPACE              NAME \u00a0 \u00a0  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0            READY \u00a0 STATUS     RESTARTS\n\nkube-system kube-apiserver-cluster-suse-cp-f777105c-2qgvh     0/1   Running    314\n</code></pre> <ul> <li>The kubelet logs register failing probes against the kube-apiserver.</li> </ul>"},{"location":"kbs/000021462/#resolution","title":"Resolution","text":"<p>1) Enable kubelet debug logging</p> <ol> <li> <p>Click\u00a0\u2630 &gt; Cluster Management.</p> </li> <li> <p>Go to the cluster you want to configure and click\u00a0\u22ee &gt; Edit Config.</p> </li> <li> <p>Advanced &gt; Additional Kubelet Args &gt; Add v=9 under For all machines, use Kubelet args</p> </li> </ol> <p></p> <p>2) Replicate the livenenessProbe and check the kubelet logs</p> <pre><code>   2.1 Open an SSH session to a master node\n   2.2 Execute the command to simulate livenessProbe for kube-apiserver in the cluster\n</code></pre> <pre><code>/var/lib/rancher/rke2/bin/crictl --runtime-endpoint unix:///run/k3s/containerd/containerd.sock exec $(/var/lib/rancher/rke2/bin/crictl --runtime-endpoint unix:///run/k3s/containerd/containerd.sock ps | grep kube-apiserver | awk '{print $1}') kubectl get --server=https://localhost:6443/\u00a0--client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt --client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key --certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt --raw=/livez\n</code></pre> <pre><code>   2.3 Open another SSH sessionn to the same master node and check the logs\n</code></pre> <pre><code>tail -f /var/lib/rancher/rke2/agent/logs/kubelet.log | grep kube-apiserver\n</code></pre> <pre><code> Check the DNS resolution and DNS lookups.\n</code></pre>"},{"location":"kbs/000021462/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021469/","title":"ManagedChart generation is X, but latest observed generation is Y","text":"<p>This document (000021469) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021469/#environment","title":"Environment","text":"<p>- Rancher v2.6+</p> <p>- A Rancher-managed RKE2 cluster</p>"},{"location":"kbs/000021469/#situation","title":"Situation","text":"<p>After upgrading the Kubernetes version for a downstream RKE2 cluster, there is a message in the Rancher UI:</p> <p>\" ManagedChart generation is X, but latest observed generation is Y\".</p>"},{"location":"kbs/000021469/#resolution","title":"Resolution","text":"<p>To resolve the issue:</p> <ol> <li>Delete the managed-system-upgrade-controller ManagedChart resource for the affected downstream cluster\u00a0from the Rancher local cluster (replacing  with the downstream cluster name): <pre><code>kubectl -n fleet-default delete ManagedChart &lt;cluster-name&gt;-managed-system-upgrade-controller\n</code></pre> <ol> <li>After that, Rancher will automatically recreate it.</li> </ol>"},{"location":"kbs/000021469/#cause","title":"Cause","text":"<p>This could happen if a user accidentally deleted the \"managed-system-upgrade-controller\" bundle within Fleet.</p> <p>Deleting the\u00a0managed-system-upgrade-controller bundle for the cluster within the fleet-default workspace resets the \" observedGeneration\" of the\u00a0managed-system-upgrade-controller ManagedChart.</p> <p>This leads to a mismatch with the generation, and an error of the format\u00a0\" ManagedChart generation is X, but latest observed generation is Y\".</p>"},{"location":"kbs/000021469/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021513/","title":"RKE End of Life - what, when, why?","text":"<p>This document (000021513) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021513/#environment","title":"Environment","text":"<p>RKE 1.6.0</p>"},{"location":"kbs/000021513/#situation","title":"Situation","text":"<p>What is changing?</p> <p>As part of our ongoing efforts to ensure our technology solutions remain at the forefront of industry standards, we need to inform our customers about an important update regarding the Rancher Kubernetes Engine (RKE).</p> <p>RKE was first released on 25th January 2018. Now, 6 Years and 625 releases later, we\u2019re beginning the last chapter in the journey of RKE, and for many users a new chapter using RKE2 and K3S.</p> <p>RKE has been a trusted solution, but the evolving landscape of container orchestration requires enhanced security, scalability and simplicity</p> <p>With the release of\u00a0Rancher Kubernetes Engine (RKE)\u00a01.6.0, we are informing customers that RKE will be maintained for two more versions after 1.6.0, targeting the release window of July 2025. Therefore, please note that RKE will reach End of life 31st, July 2025 meaning RKE customers must re-platform to RKE2 or K3S.</p>"},{"location":"kbs/000021513/#resolution","title":"Resolution","text":"<p>Transition to RKE2 or K3S</p> <p>RKE uses Docker, which is no longer maintained as a container runtime by Kubernetes. This is critical as Docker\u2019s removal from the Kubernetes project (as of release 1.24) necessitates a shift. RKE2 adopts containerd, providing a more secure and efficient environment. This change not only enhances the stability of container workloads but also aligns with the latest industry standards for container management. RKE2 and K3S offer a more secure, efficient, and future-proof environment for your Kubernetes needs.</p> <p>RKE2 benefits over RKE</p> <ul> <li>Modern Architecture: RKE uses Docker, which is no longer maintained as a container runtime. RKE2 adopts containers, providing a more secure and efficient environment.</li> <li>Enhanced Security: RKE2 is designed with security at its core, offering defaults and configurations that help clusters pass CIS Kubernetes Benchmarks with minimal intervention.</li> <li>Compliance: RKE2 supports FIPS 140-2 compliance and minimizes CVEs through comprehensive scanning with Trivy during the build process.</li> <li>Future-Proof: Combining the strengths of RKE and K3S, RKE2 provides a robust and scalable platform that meets modern requirements.</li> </ul>"},{"location":"kbs/000021513/#additional-information","title":"Additional Information","text":"<p>Replatforming and support</p> <p>SUSE understand that transitioning to a new platform can be challenging. The replatforming overview will provide enough guidance in many cases, but for any questions or assistance, please contact us at: replatform@suse.com\u00a0or speak directly to your SUSE Account Team (AE, PE, CSM, etc.)</p> <p>In addition, SUSE has both Consulting and Training services that are available to provide more detailed, personalized assistance. The replatforming overview guide contains a detailed overview for successful replacement of RKE with K3s or RKE2.</p> <p>If you find yourself needing extra time to make this replatforming effort happen you have 2 options. If you are an existing customer please reach out to your account team. \u00a0If you are a community user who needs a little extra coverage you could use , the live chat https://www.suse.com/contact/contact-us-live-chat\u00a0or request a sales call\u00a0https://www.suse.com/contact/</p> <p>RKE2 Replatforming FAQ</p> <p>Can I do an in-place upgrade?</p> <p>No, an in-place upgrade was deemed problematic due to the nature of the architectural differences, as well as going against the cloud-native ethos of \u201ctreating your clusters as cattle rather than pets\u201d.</p> <p>What does replatforming mean?</p> <p>Replatforming means that you create a new RKE2 cluster, and migrate the workloads from your existing RKE cluster to the new one.</p> <p>Where do I find the documentation?</p> <p>We have published the following official guide here. It is also part of the following article: RKE to RKE2 replatforming - instructions and FAQs</p> <p>Is there tooling that will help me?</p> <p>Currently, a tool called cattle-drive has been developed to help migrate Rancher-required objects from one downstream cluster to another.</p> <p>What services SUSE can offer in regards to the replatforming?</p> <p>SUSE Professional Services are ready to offer customers practical help with their re-platforming exercise, including the types of Consulting engagements listed below. Replatforming provides a great opportunity to revisit previous design and configuration decisions and improve on aspects such as platform security, scalability and automation.</p> <ul> <li>Replatform Planning Service (including Discovery, Requirements Analysis &amp; recommendations for implementation approach)</li> <li>Design or Design &amp; Build Services for the target RKE2 environment</li> <li>Automation Consulting Service</li> <li>Rancher Prime Readiness Services:</li> <li>Architectural Review Service</li> <li>Platform Readiness Assessment Service</li> <li>Security Assessment Service</li> </ul> <p>Please contact your local Services Engagement Manager (SEM) for more information.</p> <p>Additionally, SUSE provides a range of Training Services for customers who wish to obtain or update their in-house skills in the relevant technology areas, including deployment and administration of Kubernetes and/or Rancher Manager Server. Please refer to https://www.suse.com/training/ or ask your local SEM for more information.</p> <p>When can I call support? How will support help me?</p> <p>Support can be called upon to assist in understanding and troubleshooting the official replatforming documentation that our product team has provided, as well as pointing customers towards cattle-drive or any other tools that are built to assist our customers in the event they are doing the replatforming on their own.</p> <p>Support can not directly assist customers in their replatforming process, this work should be carefully and clearly worked out with our Services team on a customer by customer basis.</p>"},{"location":"kbs/000021513/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021518/","title":"RKE to RKE2 replatforming - instructions and FAQs","text":"<p>This document (000021518) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021518/#environment","title":"Environment","text":"<p>SUSE Rancher</p> <p>RKE - all supported versions</p> <p>RKE2 - all supported versions</p>"},{"location":"kbs/000021518/#situation","title":"Situation","text":"<p>This document provides a guide on how to replatform your environment following the RKE EoL announcement.</p> <p>With the release of Rancher Kubernetes Engine (RKE)\u00a01.6.0, we are informing customers that RKE will be maintained for two more versions after 1.6.0, targeting the release window of July 2025. Therefore, please note that\u00a0RKE will reach End of life 31st, July 2025 meaning RKE customers must replatform to RKE2 or K3S.</p> <p>For more information on the RKE sunset, please, consult the following article: RKE End of Life - what, when, why?</p>"},{"location":"kbs/000021518/#resolution","title":"Resolution","text":"<p>The replatforming overview guide contains a detailed overview for successful replacement of RKE with K3s or RKE2.</p> <p>For any questions or assistance, please contact us at:\u00a0replatform@suse.com\u00a0or speak directly to your SUSE Account Team (AE, PE, CSM, etc.)</p> <p>In addition, SUSE has both Consulting and Training services\u00a0that are available to provide more detailed, personalized assistance.</p> <p>RKE to RKE2 Replatforming Guide.pdf</p>"},{"location":"kbs/000021518/#additional-information","title":"Additional Information","text":"<p>RKE2 Replatforming FAQ</p> <p>Can I do an in-place upgrade?</p> <p>No, an in-place upgrade was deemed problematic due to the nature of the architectural differences, as well as going against the cloud-native ethos of \u201ctreating your clusters as cattle rather than pets\u201d.</p> <p>What does replatforming mean?</p> <p>Replatforming means that you create a new RKE2 cluster, and migrate the workloads from your existing RKE cluster to the new one.</p> <p>Is there tooling that will help me?</p> <p>Currently, a tool called\u00a0cattle-drive\u00a0has been developed to help migrate Rancher-required objects from one downstream cluster to another.</p> <p>When can I call support? How will support help me?</p> <p>Support can be called upon to assist in understanding and troubleshooting the official replatforming documentation that our product team has provided, as well as pointing customers towards cattle-drive or any other tools that are built to assist our customers in the event they are doing the replatforming on their own.</p> <p>Support can not directly assist customers in their replatforming process, this work should be carefully and clearly worked out with our Services team on a customer by customer basis.</p>"},{"location":"kbs/000021518/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021521/","title":"Performance considerations for a large number of CRDs in Rancher-managed clusters","text":"<p>This document (000021521) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021521/#environment","title":"Environment","text":"<p>SUSE Rancher 2.7.x+</p>"},{"location":"kbs/000021521/#situation","title":"Situation","text":"<p>Under the Practical Considerations documentation, it is mentioned we recommend not having above 100 CRDs in downstream clusters.</p>"},{"location":"kbs/000021521/#resolution","title":"Resolution","text":"<p>If performance issues are seen in the Rancher UI (such as the loading time taking several seconds) with a number of CRDs above 100 on a downstream cluster, it may be prudent to check the number of objects that each CRD has in order to verify if there is any concern. If so, consider cleaning unneeded objects that relate to CRDs with the highest object counts, or review distributing these to other clusters.</p> <p>You can check the number of objects by CRDs in the Rancher UI. To do so, click on the Hamburger menu in the top-left of the Rancher UI, then click\u00a0About\u00a0-&gt;\u00a0Diagnostics. You will see a\u00a0Resource Counts by Cluster\u00a0section which can be expanded on each managed cluster to see the number of objects on each CRD.</p>"},{"location":"kbs/000021521/#cause","title":"Cause","text":"<p>A high number of CRDs increases the chances for introducing latency when interacting with etcd. For example, if a CRD has a high amount of total objects, an API request for the objects will be chunked into batches, increasing the response time to retrieve the request data from etcd. If the additional CRDs only have a small number of objects each, the impact may not be observable, however, if they contain thousands of objects, that could be a concern.</p>"},{"location":"kbs/000021521/#additional-information","title":"Additional Information","text":"<p>Performance in Kubernetes is multi-dimensional,\u00a0so it is important to consider the needs of workloads sharing the same cluster, and the growth of these over time. When planning for multiple large-scale workloads that have unique resource needs, it is recommended to consider splitting loosely-coupled workloads into separate clusters so they can scale without contradicting the needs of others.</p>"},{"location":"kbs/000021521/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021538/","title":"Rancher UI does not show the Rancher Prime logo after an upgrade","text":"<p>This document (000021538) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021538/#environment","title":"Environment","text":"<p>Rancher Prime</p>"},{"location":"kbs/000021538/#situation","title":"Situation","text":"<p>The Rancher UI does not show the Rancher Prime logo after an upgrade.</p> <ol> <li>Rancher pod is using the correct image: registry.rancher.com/rancher/rancher:v2.x.x</li> <li>Verified with different browsers and logged-in users.</li> </ol>"},{"location":"kbs/000021538/#resolution","title":"Resolution","text":"<p>The upgrade process might have reset some branding configurations. Verify the branding settings in Rancher to ensure that the Rancher Prime logo is configured correctly. If there were any custom UI changes or branding applied previously, the upgrade might have overwritten these customizations. Check if the branding settings need to be re-applied.</p> <p>If you feel the upgrade got completed and only the rancher prime logo does not change on the rancher UI, you can follow this process.</p> <p>This can be fixed either by using kubectl command or by using Rancher UI settings.</p> <p>1. Using kubectl method.</p> <p>&gt; Get the current UI brand value</p> <pre><code>kubectl get settings.management.cattle.io ui-brand\n</code></pre> <p>Update the UI brand value as suse using the below command.</p> <pre><code>kubectl patch settings.management.cattle.io ui-brand --type merge -p '{\"default\":\"suse\"}'\n</code></pre> <p>2. Using Rancher UI</p> <p>Global settings &gt; settings &gt; ui-brand &gt; make sure the value is set as suse</p> <p></p>"},{"location":"kbs/000021538/#cause","title":"Cause","text":"<p>If the Rancher UI does not show the Rancher Prime logo after an upgrade, the upgrade process might have reset some branding configurations.</p>"},{"location":"kbs/000021538/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021538/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021547/","title":"How to monitor Canal or Calico in RKE2 Downstream Clusters","text":"<p>This document (000021547) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021547/#environment","title":"Environment","text":"<ul> <li>Rancher v2.x.</li> <li>A Rancher-managed RKE2 cluster with Calico or Canal CNI.</li> <li>Rancher-monitoring installed.</li> </ul>"},{"location":"kbs/000021547/#situation","title":"Situation","text":"<p>Canal and Calico offer metrics that can be exposed and consumed by rancher-monitoring. This article shows how to do it.</p>"},{"location":"kbs/000021547/#resolution","title":"Resolution","text":""},{"location":"kbs/000021547/#prerequisite-for-calico","title":"Prerequisite for Calico","text":"<p>Calico's RKE2 addon does not export Prometheus metrics by default.</p> <p>To enable this option in the rke2-calico addon, you can go to Cluster Management &gt; Select the cluster &gt; Edit Config &gt; Add-on: Calico. And add the following options in the corresponding \"felixConfiguration\" and \"installation\" sections:</p> <pre><code>felixConfiguration:\n  prometheusMetricsEnabled: true\ninstallation:\n  typhaMetricsPort: 9093\n</code></pre> <p>If your cluster's CNI is Canal, it already exports Prometheus metrics by default, as the following option is enabled in the rke2-canal addon:</p> <pre><code>    calico:\n      felixPrometheusMetricsEnabled: true\n</code></pre> <p>After this prerequisite is fulfilled, there are two steps involved to gather these metrics in Prometheus:</p>"},{"location":"kbs/000021547/#create-the-services","title":"Create the Services","text":""},{"location":"kbs/000021547/#canal","title":"Canal","text":"<p>Only one service is needed to be able to extract the metrics from the canal pods through the 9091 metrics port:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: calico-felix-metrics\n  namespace: kube-system\n  labels:\n    k8s-app: calico-felix\nspec:\n  clusterIP: None\n  ports:\n  - port: 9091\n    protocol: TCP\n    name: metrics-port\n  selector:\n    k8s-app: canal\n</code></pre>"},{"location":"kbs/000021547/#calico","title":"Calico","text":"<p>Two services are needed. One to export the calico-felix metrics and another one for the calico-typha metrics. The headless service to export metrics from the calico-kube-controllers is there by default so there is no need to create it.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: calico-felix\n  name: calico-felix-metrics\n  namespace: calico-system\nspec:\n  clusterIP: None\n  ports:\n  - name: metrics-port\n    port: 9091\n  selector:\n    k8s-app: calico-node\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: calico-typha\n  name: calico-typha-metrics-headless-svc\n  namespace: calico-system\nspec:\n  clusterIP: None\n  ports:\n  - name: metrics-port\n    port: 9093\n  selector:\n    k8s-app: calico-typha\n</code></pre> <p>After the service YAMLs are applied, you can proceed to the next step:</p>"},{"location":"kbs/000021547/#create-the-servicemonitors","title":"Create the ServiceMonitors","text":""},{"location":"kbs/000021547/#canal_1","title":"Canal","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: rancher-monitoring-canal\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-felix\n  endpoints:\n  - port: metrics-port\n    relabelings:\n    - sourceLabels:\n      - __meta_kubernetes_endpoint_node_name\n      targetLabel: instance\n</code></pre>"},{"location":"kbs/000021547/#calico_1","title":"Calico","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: rancher-monitoring-calico\n  namespace: calico-system\nspec:\n  endpoints:\n  - port: metrics-port\n    relabelings:\n    - sourceLabels:\n      - __meta_kubernetes_endpoint_node_name\n      targetLabel: instance\n  selector:\n    matchExpressions:\n    - key: k8s-app\n      operator: In\n      values: [ \"calico-felix\", \"calico-typha\",\"calico-kube-controllers\" ]\n</code></pre> <p>After both the Service and ServiceMonitors are applied, the target will appear in Prometheus, and the metrics will be scraped. Be aware that it might take a minute for Prometheus to see the new target and scrape the metrics.</p>"},{"location":"kbs/000021547/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021553/","title":"Large volume of cattle impersonation token created for the same service account on downstream cluster","text":"<p>This document (000021553) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021553/#environment","title":"Environment","text":"<p>Rancher v2.7.x; v2.8.x &lt; v2.8.8; v2.9.x &lt; v2.9.2</p>"},{"location":"kbs/000021553/#situation","title":"Situation","text":"<p>A large volume of cattle impersonation tokens are created for the same service account on a downstream cluster resulting in strange intermediate issues with user permissions and logins. The issue may also result in significant growth in memory usage by Rancher Pods.</p>"},{"location":"kbs/000021553/#resolution","title":"Resolution","text":"<p>This is a known issue and is addressed in Rancher versions v2.8.8, and v2.9.2.</p> <p>NOTE: This will not be back-ported to v2.7, as it is end of maintenance since March 2024 already.</p> <p>For those unable to upgrade right now there is a a job manifest available to deploy and clean up the secrets here. This must be executed on each downstream cluster where the issue (a large number of impersonation tokens for the same service account) is present.</p> <p>Instructions from the README:</p> <ol> <li>Modify the image defined in resources.yaml to point to your private registry kubectl image, or leave as the default Docker Hub-hosted image bitnami/kubectl:latest</li> <li>Apply resources.yaml using</li> </ol> <pre><code>kubectl apply -f resources.yaml\n</code></pre> <p>or the equivalent using\u00a0fleet/argo etc. 3. Monitor\u00a0the jobs until\u00a0completion 4. Clean up the resources using</p> <pre><code>kubectl delete -f resources.yaml\n</code></pre>"},{"location":"kbs/000021553/#additional-information","title":"Additional Information","text":"<ul> <li>Github issue:\u00a0https://github.com/rancher/rancher/issues/44347</li> <li>Clean up job README:\u00a0https://github.com/ryanelliottsmith/cleanup-impersonation-secrets/blob/main/README.md</li> </ul>"},{"location":"kbs/000021553/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021557/","title":"How to remove worker role from a node managed by Rancher","text":"<p>This document (000021557) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021557/#environment","title":"Environment","text":"<p>Rancher v2.8.x and above. This is applicable for RKE2 and K3S Rancher provisioned clusters.</p>"},{"location":"kbs/000021557/#situation","title":"Situation","text":"<ul> <li>There could be scenarios that a node pool was accidentally created with ALL roles and the cluster was provisioned but the intention was to have only Controlplane and etcd roles selected in the node pool.</li> <li>In such scenarios, you can go ahead and perform the steps mentioned in the resolution to remove the worker role.</li> </ul>"},{"location":"kbs/000021557/#resolution","title":"Resolution","text":"<p>STEP 1:</p> <ul> <li>Use the \"Edit Config\" option to edit the cluster via Cluster Management in the Rancher UI.</li> <li>Find the \"MachinePool\" where you would like to make the changes.</li> <li>Uncheck the worker role and save the cluster config.\u00a0This should remove the worker role from the cluster.</li> <li>However, if you still see the values \"workerRole: true\" for the created Machine Pool when you perform \"Edit YAML\" of the cluster via Cluster Management, you can set\u00a0\"workerRole: false\".</li> </ul> <p>STEP 2:</p> <ul> <li>Though the Machine pools were updated using STEP 1, you would see the worker role when you run the \"kubectl get nodes\" command.</li> <li>This is because of the of the label \"node-role.kubernetes.io/worker=true\" still present on that node.</li> <li>You would need to remove this using the below command:</li> </ul> <pre><code>kubectl label node &lt;nodeName&gt; node-role.kubernetes.io/worker-\n</code></pre> <ul> <li>Now, when you run the \"kubectl get nodes\" command, you would not be able to see the worker role on the node.</li> </ul> <p>STEP 3:</p> <ul> <li>When only the Controlplane and etcd roles are selected, the cluster would not allow to deploy regular pods on that nodes besides from the controlplane components.</li> <li>This is due to certain safe taints added to the node by Rancher when selecting the controlplane and etcd roles.</li> <li>We need to apply these taints on the nodes where worker role was removed because these taints are not applied when the worker role is selected.</li> <li>To apply the taint, you can run the below command:</li> </ul> <pre><code>kubectl taint node &lt;nodeName&gt; node-role.kubernetes.io/etcd:NoExecute node-role.kubernetes.io/control-plane:NoSchedule\n</code></pre> <ul> <li>This would help the scheduler to not schedule any regular pods on the Controlplane and etcd nodes.</li> </ul>"},{"location":"kbs/000021557/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021558/","title":"Unable to scale up Rancher node pool nodes in downstream cluster due to mismatch in cluster name at VMware vCenter","text":"<p>This document (000021558) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021558/#environment","title":"Environment","text":"<p>SUSE Rancher 2.8.x and above</p> <p>Rancher Provisioned vSphere Clusters.</p>"},{"location":"kbs/000021558/#situation","title":"Situation","text":"<ul> <li>After upgrading downstream cluster, we notice the below error for the cluster when we click the \"Edit Config\" option from Cluster Management.</li> </ul> <pre><code>pool1: The provided value for hostsystem was not found in the list of expected values. This can happen with clusters provisioned outside of Rancher or when options for the provider have changed\n\npool2: The provided value for hostsystem was not found in the list of expected values. This can happen with clusters provisioned outside of Rancher or when options for the provider have changed\n\npool3: The provided value for hostsystem was not found in the list of expected values. This can happen with clusters provisioned outside of Rancher or when options for the provider have changed\n</code></pre> <ul> <li>Also, when we try to scale up a node, we would be able to see in Rancher that it creates a machine resource and then immediately deletes it.</li> <li>However, checking from vSphere side, you would not notice any VM creation or even an API call for VM creation.</li> <li>Running the \"kubectl get vmwarevspheremachine -n fleet-default\" command will list all the machines of the VMware clusters managed by Rancher and you would be able to see the vmwarevspheremachine object\u00a0that got created recently as part of the scale up process.</li> <li>Thereafter, running the \"kubectl describe\" command for the newly created\u00a0machine\u00a0will show\u00a0why it was not able to provision the VM on vSphere.</li> <li>We were able to see that the VM did not get created on vSphere because of the following error message:</li> </ul> <pre><code>Failed creating server [fleet-default/Prod-infra-xxxxxxxx-xhxxx] of kind (VmwarevsphereMachine) for machine Prod-infra-xxxxxxxx-xbxxx in infrastructure provider:\nCreateError: Running pre-create checks... (Prod-infra-xxxxxxxx-xhxxx) Connecting to vSphere for pre-create checks... (Prod-infra-xxxxxxxx-xhxxx) Using datacenter /Kubernetes\n(Prod-infra-xxxxxxxx-xhxxx) Using Network /Kubernetes/network/Datacenter-01 LB Kubernetes Error with pre-create check: \"host '/Kubernetes/host/K8s Prod/xxvm22.xxxxx.xxxxxxx.xx' not found\"\n</code></pre> <ul> <li>Checking\u00a0on vSphere, we could confirm that\u00a0node ttvm26.xxxxx.xxxxxxx.xx existed.</li> <li>However, the ESXi cluster name in the error messages was \"K8s Prod\" but on vSphere it was changed to a different name by the vSphere admin that triggered this issue on the Rancher's downstream cluster.</li> </ul>"},{"location":"kbs/000021558/#resolution","title":"Resolution","text":"<ul> <li>The only available option is to change the name of the ESXi cluster on vSphere back to \"K8s Prod\" as seen in the error message since we do not have an option to change or select the updated ESXi cluster resource from Rancher side for the existing downstream cluster.</li> </ul>"},{"location":"kbs/000021558/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021559/","title":"Configure GitRepo with Private Key Authentication via Rancher UI","text":"<p>This document (000021559) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021559/#environment","title":"Environment","text":"<p>Rancher with Fleet Continuous Delivery</p>"},{"location":"kbs/000021559/#situation","title":"Situation","text":"<p>Configuring GitHub and Rancher Fleet Continuous Delivery for a GitRepo Using SSH Key Authentication</p>"},{"location":"kbs/000021559/#resolution","title":"Resolution","text":"<p>Create SSH Key Pair:</p> <p>For private repositories, generate an SSH key pair in PEM format using the following command. Please note that the private key should not have a passphrase. It can be either an RSA or EC private key</p> <pre><code>ssh-keygen -t rsa -b 4096 -m pem -C \"user@email.com\"\n</code></pre> <p>Configure Github:</p> <p>Now that you have the private and public key pair, add the public key to your Git repository. For example, adding the public key to GitHub is demonstrated below</p> <p>Github &gt;&gt; Settings &gt;&gt; SSH and GPG keys &gt;&gt; Click button \"New SSH key\" &gt;&gt; Give Title &gt;&gt; Select the \"Authentication Key\" from drop down &gt;&gt; Add the public key in the text box &gt;&gt; \"Add SSH key\" to add it</p> <p>Now you will be able to use the private key to authenticate to your Github private repository</p> <p>Create Gitrepo in Rancher:</p> <p>Go to the Rancher UI &gt;&gt; Continous Delivery &gt;&gt; GitRepos &gt;&gt; Add Repository</p> <p>Enter a name and Repository URL. For example, in the case of github, the repo URL will be similar to git@github.com:username/repository.git. Enter the Branch name corresponding to the repo</p> <p>In Github authentication drop-down, Select \"Create a SSH Key Secret\" and add your Private Key generated in the previous step. Private key alone is sufficient for authentication and you may keep the Public Key field blank.</p> <p>If you already have a secret created using the kubectl command in either the fleet-local or fleet-default namespace on the local cluster, you can select that secret from the dropdown. If you prefer to create the secret manually via the command line, run the following command on the local cluster</p> <pre><code>kubectl create secret generic ssh-key -n fleet-default --from-file=ssh-privatekey=/file/to/private/key  --type=kubernetes.io/ssh-auth\n</code></pre> <p>Note: If you want to create Gitrepo for local cluster, replace fleet-default with fleet-local</p> <p>If you want to verify the identity of the GitHub when you establish an SSH connection, you can collect the SSH host key using the below command and add it to the secret</p> <pre><code>ssh-keyscan -H github.com\n</code></pre> <p>Get the host key line corresponding to ssh-rsa from previous command results and place it another file and create the secret as below</p> <pre><code>kubectl create secret generic ssh-key-known-host  -n fleet-default --from-file=ssh-privatekey=/file/to/private/key --from-file=known_hosts=/file/to/known_hosts  --type=kubernetes.io/ssh-auth\n</code></pre> <p>Fill in any remaining details, then click on 'Create' . The authentication to the GitHub private repository should be completed successfully</p>"},{"location":"kbs/000021559/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021559/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021575/","title":"How to configure an authenticated forward proxy for Alertmanager in rancher-monitoring","text":"<p>This document (000021575) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021575/#environment","title":"Environment","text":"<p>Kubernetes cluster where you have installed the\u00a0Alertmanager as part of the Monitoring Stack.</p>"},{"location":"kbs/000021575/#situation","title":"Situation","text":"<ul> <li>You have the\u00a0Alertmanager installed on the cluster.</li> <li>You need to configure any receiver integration, e.g., Slack, PagerDuty, Opsgenie, etc.,\u00a0where an authenticated proxy is required.</li> </ul>"},{"location":"kbs/000021575/#resolution","title":"Resolution","text":"<p>A proxy configuration, with username and password, can be added to the alertmanager configuration, and the alertmanager UI will automatically obfuscate the password in the Status -&gt; Config view. e.g. proxy_url: http://:xxxxx@: so this is secured from users with UI access. There are two ways you can define this proxy configuration: <ul> <li>A non-persistent implementation, e.g. to test the proxy configuration, can be performed by editing the \"alertmanager-rancher-monitoring-alertmanager\" secret in the \"cattle-monitoring-system\" namespace post-deployment to set the proxy_url including authentication directly in the URL. However, this solution will not be permanent, as the secret could will be overwritten, deleting the changes, if any update is made to the rancher-monitoring chart.</li> <li>As a persistent implementation,\u00a0you can create a copy of this secret (with the proxy_url including the credentials already\u00a0set) in the cattle-monitoring-system namespace. Then, you can\u00a0refer to this new secret in the cattle-monitoring chart alertmanager.alertmanagerSpec.configSecret value to work with your receiver integration.</li> </ul>"},{"location":"kbs/000021575/#additional-information","title":"Additional Information","text":"<p>According to the\u00a0Alertmanager official documentation, the proxy credentials should be facilitated using the following http_config spec:\u00a0https://prometheus.io/docs/alerting/latest/configuration/#http_config. However, in the latest versions of the software, the \"http_config.proxy_from_environment\" field has been removed, avoiding the possibility of setting these credentials using the env variables PROXY_PASS and HTTPS_PROXY. Trying to configure the mentioned credentials using those env variables, will throw the following error message:</p> <p>level=error ts=2024-06-28T14:29:14.657065268Z caller=klog.go:126 component=k8s_client_runtime func=ErrorDepth msg=\"sync \\\"cattle-monitoring-system/rancher-monitoring-alertmanager\\\" failed: provision alertmanager configuration: failed to initialize from secret: yaml: unmarshal errors:\\n line 4: field proxy_from_environment not found in type alertmanager.httpClientConfig\"</p> <p>Then, the solution needs to pass by setting the proxy credentials directly on the URL defined in the http_config.</p>"},{"location":"kbs/000021575/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021577/","title":"After migration of Rancher between different Kubernetes distriubtions, via the rancher-backup operator, the local cluster distribution within Rancher is not updated","text":"<p>This document (000021577) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021577/#environment","title":"Environment","text":"<p>Rancher 2.5+</p>"},{"location":"kbs/000021577/#situation","title":"Situation","text":"<p>Migration of Rancher from one Kubernetes distribution to another, e.g. from K3s to RKE2, using the\u00a0Rancher Backup Operator.</p>"},{"location":"kbs/000021577/#resolution","title":"Resolution","text":"<p>Follow the documented steps at the end of point 2. Restore from backup using a Restore custom resource in the Rancher migration documentation, to update the distribution version of the Rancher local cluster within the Rancher state.</p>"},{"location":"kbs/000021577/#cause","title":"Cause","text":"<p>The rancher-backup operator performs a restore of Rancher configuration resources, without manipulating any of these in the process. As a result, when migrating Rancher between different cluster types, values within the local cluster.management.cattle.io resource need to be manually updated to to reflect the change in distribution. Further details can be found in the GitHub issue at https://github.com/rancher/rancher/issues/42158</p>"},{"location":"kbs/000021577/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021581/","title":"rke2 helm-install job failing with INSTALLATION FAILED: cannot re-use a name that is still in use","text":"<p>This document (000021581) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021581/#environment","title":"Environment","text":"<ul> <li>Rancher v2.7+</li> <li>RKE2 v1.26+</li> </ul>"},{"location":"kbs/000021581/#situation","title":"Situation","text":"<p>During an upgrade of an RKE2 cluster, you may face issues related to helm-install job(s) upgrading internal components such as rke2-ingress-nginx or rke2-metrics-server.</p> <p>By checking the related helm install Job pod logs you can see the following error message:</p> <pre><code>Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n</code></pre> <p>This situation can occur as the result of a previously failed update to or removal of the component.</p> <p>This KB describes how you can solve the above issue.</p>"},{"location":"kbs/000021581/#resolution","title":"Resolution","text":"<p>The following commands (in order) should resolve the issue:</p> <p>- `helm ls -A` to identify which rke2 deployed helm chart is not in a deployed state</p> <p>- `helm -n kube-system history rke2-ingress-nginx` to view the release history for the affected chart (in this example the rke2-ingress-nginx chart in the kube-system Namespace).</p> <p>NOTE: for affected charts the most recent revision will be in a non-deployed status, e.g. the output below where it indicates the chart is uninstalling and deletion is in progress. This example shows that revision number 5 is stuck and not deployed properly</p> <pre><code>REVISION    UPDATED                     STATUS          CHART                           APP VERSION DESCRIPTION\n5           Thu Oct  3 15:32:44 2024    uninstalling    rke2-ingress-nginx-4.10.401 1.10.4          Deletion in progress (or silently failed)\n</code></pre> <p>- `kubectl get secrets -n kube-system | grep rke2-ingress-nginx`</p> <p>NOTE: every X version will have a secret name that looks like:\u00a0sh.helm.release.v1.rke2-ingress-nginx.v X</p> <p>Following the example above, the name should be:\u00a0sh.helm.release.v1.rke2-ingress-nginx.v5</p> <p>Delete that secret</p> <p>- `kubectl delete secrets -n kube-system sh.helm.release.v1.rke2-ingress-nginx.v5` To delete the affected helm release secret</p> <p>- `kubectl delete pods -n kube-system helm-install-rke2-ingress-nginx-xxxxx` To delete the failed helm Job pod</p> <p>The last command will delete the existing helm Job pod in an error state (with CrashLoopBackoff). After the pod deletion, a new Job pod will be scheduled and should run correctly (following the previous helm release secret deletion).</p>"},{"location":"kbs/000021581/#cause","title":"Cause","text":"<p>Helm deploys a version with a revision number for every component (e.g. rke2-ingress-nginx or rke2-metrics-server) in an RKE2 cluster in a certain namespace.</p> <p>Whenever these components get upgraded, helm creates a new secret to indicate that a new release/version has been installed/rolled out.</p> <p>Deleting the secret in question that reports the error message can unblock the situation and get the component upgrade to deploy successfully.</p>"},{"location":"kbs/000021581/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021585/","title":"How to resolve the error \"Secret -admission-configuration-psact does not contain the expected content\" when updating RKE2 clusters via Rancher2 Terraform Provider <p>This document (000021585) is provided subject to the disclaimer at the end of this document.</p>","text":""},{"location":"kbs/000021585/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher v2.6.x - v2.9.x</li> <li>Rancher2 Terraform Provider &lt;= v5.1.x</li> <li>A Rancher-provisioned RKE2 cluster, managed via Terraform, with the default_pod_security_admission_configuration_template_name field set</li> </ul>"},{"location":"kbs/000021585/#situation","title":"Situation","text":"<p>After setting the \" default_pod_security_admission_configuration_template_name\" argument on a rancher2_cluster_v2 resource, which defines cluster configuration under \"defaultPodSecurityAdmissionConfigurationTemplateName\", any\u00a0 further change to the cluster configuration results in a new entry on the cluster.provisioning.cattle.io resource in Rancher, per the example below:</p> <pre><code>machineSelectorFiles:\n    - fileSources:\n      - configMap:\n          name: \"\"\n        secret:\n          items:\n          - hash: LW5oTV9pmpX7+xWMjgC3IgbHtLkSMlCgyaKXG13CihA=\n            key: admission-config-psact\n            path: /etc/rancher/rke2/config/rancher-psact.yaml\n          name: &lt;cluster-name&gt;-admission-configuration-psact\n      machineLabelSelector:\n        matchLabels:\n          rke.cattle.io/control-plane-role: \"true\"\n</code></pre> <p>These blocks are not overwritten, instead, a new one is added each time. Thus, subsequent changes to the cluster result in multiple blocks, per the following example:</p> <pre><code>    - fileSources:\n      - configMap:\n          name: \"\"\n        secret:\n          items:\n          - dynamic: true\n            hash: LW5oTV9pmpX7+xWMjgC3IgbHtLkSMlCgyaKXG13CihA=\n            key: admission-config-psact\n            path: /etc/rancher/rke2/config/rancher-psact.yaml\n          name: &lt;cluster-name&gt;-admission-configuration-psact\n      machineLabelSelector:\n        matchLabels:\n          rke.cattle.io/control-plane-role: \"true\"\n    - fileSources:\n      - configMap:\n          name: \"\"\n        secret:\n          items:\n          - hash: LW5oTV9pmpX7+xWMjgC3IgbHtLkSMlCgyaKXG13CihA=\n            key: admission-config-psact\n            path: /etc/rancher/rke2/config/rancher-psact.yaml\n          name: &lt;cluster-name&gt;-admission-configuration-psact\n      machineLabelSelector:\n        matchLabels:\n          rke.cattle.io/control-plane-role: \"true\"\n</code></pre> <p>The issue arises when there is a change on the \"defaultPodSecurityAdmissionConfigurationTemplateName\" parameter, which changes the hash of this block. Having multiple identical blocks pointing at the same secret under \"machineSelectorFiles\", but with different hashes, causes Rancher to error out and blocks sending the plan to the downstream cluster with the following error:</p> <pre><code>Secret &lt;cluster-name&gt;-admission-configuration-psact does not contain the expected content\n</code></pre>"},{"location":"kbs/000021585/#resolution","title":"Resolution","text":"<p>The issue is fixed in version v5.2.0 and above of the Rancher2 Terraform Provider. Users should upgrade the provider to a version containing the fix and which is compatible with their Rancher instance.</p> <p>The workaround for this issue, in affected versions of the Rancher 2 Terraform Provider, is to delete all of the older admission-config-psact blocks described above, which contain a different hash to the current one, from the <code>cluster.provisioning.cattle.io</code> resource for the affected cluster.</p> <p>A good option is to just leave the last one generated (often the one on the bottom), so there is only one, and it is the most recent, per the example below.</p> <p>The <code>cluster.provisioning.cattle.io</code> resource can be edited by navigating to\u00a0Cluster Management in the Rancher UI, and clicking\u00a0Edit YAML for the relevant RKE2 cluster. After removing the older admission-config-psact blocks click\u00a0Save to save the changes.</p> <pre><code>machineSelectorFiles:\n      - fileSources:\n          - configMap:\n              name: ''\n            secret:\n              items:\n                - hash: OBe4tz0Nne3By2yPn/e6L/FC3QzGlllyoYTPU/b8eeQ=\n                  key: admission-config-psact\n                  path: /etc/rancher/rke2/config/rancher-psact.yaml\n              name: &lt;cluster-name&gt;-admission-configuration-psact\n        machineLabelSelector:\n          matchLabels:\n            rke.cattle.io/control-plane-role: 'true'\n</code></pre>"},{"location":"kbs/000021585/#cause","title":"Cause","text":"<p>This behavior is the result of a bug that duplicates the file reference for the PSACT secret instead of overwriting it, whenever there is a change in the cluster from Terraform. The issue was tracked in\u00a0GitHub issue #1426.</p>"},{"location":"kbs/000021585/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021599/","title":"Azure AD - An error occurred logging in Server error while authenticating","text":"<p>This document (000021599) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021599/#environment","title":"Environment","text":"<ul> <li>Rancher versions: 2.9.1, 2.9.2, 2.9.3</li> <li>Auth provider: Azure AD</li> </ul>"},{"location":"kbs/000021599/#situation","title":"Situation","text":"<p>When upgrading to Rancher 2.9.1 or 2.9.2 and use Azure AD as your main auth provider to login, after a certain amount of time users will be unable to \u00a0login and will receive the following message:</p> <p><code>An error occurred logging in Server error while authenticating</code></p> <p>The reason is because in the local Rancher cluster, there is a secret called `azuread-access-token` in the `cattle-global-data` namespace that appends user login information whenever a user logs in. Over time, the secret will grow in size till eventually reaching Kubernetes max secret size: 1MB or 1048576 bytes.</p> <p>Note: The secret can reach over this limit, and when it does, that's when we start to see users not able to login to Rancher. To verify it's size you can run a couple of commands:</p> <p><code>kubectl get secret azuread-access-token -n &lt;namespace&gt; -o jsonpath=\"{.data}\" | base64 -d | wc -c</code></p> <p>or</p> <p><code>kubectl describe secret azuread-access-token -n cattle-global-data | grep bytes</code></p>"},{"location":"kbs/000021599/#resolution","title":"Resolution","text":"<p>As a workaround, remove the <code>azuread-access-token</code>\u00a0in the\u00a0<code>cattle-global-data</code> namespace. Once deleted, verify that the secret is indeed deleted. The secret will get recreated when a user logs back into Rancher. And the size of the secret should decrease.</p> <p>In the official patch, we changed the behavior to create a new client for every token authentication which doesn't use the cache. This patch will be included in Rancher 2.10 as well as backported to 2.9 more specifically in &gt;=2.9.4:</p> <ul> <li>https://github.com/rancher/rancher/issues/47672 (For 2.10)</li> <li>https://github.com/rancher/rancher/issues/47688 (2.9 backport)</li> </ul>"},{"location":"kbs/000021599/#cause","title":"Cause","text":"<p>The cause is due to the\u00a0<code>azuread-access-token</code> filling up with user login information, till eventually hitting Kubernetes max limit. The Azure client login was using the access token cache, which led to additional tokens being cached.</p>"},{"location":"kbs/000021599/#status","title":"Status","text":"<p>Reported to Engineering</p>"},{"location":"kbs/000021599/#additional-information","title":"Additional Information","text":"<p>In the local Rancher cluster, more specifically the Rancher pods, here is an example of errors that you may encounter in the logs:</p> <pre><code>[ERROR] API error response 500 for POST /v3-public/azureADProviders/azuread?action=login. Cause: getting OID from AuthCode: error updating secret azuread-access-token: Secret \"azuread-access-token\" is invalid: data: Too long: must have at most 1048576 bytes\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000021599/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021604/","title":"User retains active session after its password is changed by an admin","text":"<p>This document (000021604) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021604/#environment","title":"Environment","text":"<ul> <li>Rancher 2.8.x</li> <li>Rancher 2.9.x</li> </ul>"},{"location":"kbs/000021604/#situation","title":"Situation","text":"<p>If an admin changes the password of a user while said user has an active session, the user can continue to use its session without needing to re-login.</p> <p>This scenario results on any active session for that user to remain active, even though the password has changed. If the password change was triggered due to the account being compromised, the administrator has no way to invalidate the active sessions, therefore the malicious user retains access until their session expires.</p>"},{"location":"kbs/000021604/#resolution","title":"Resolution","text":"<p>Admins can delete session tokens for that particular user themselves.</p> <pre><code>kubectl get token  \\\n-o custom-columns=\":metadata.name\" \\\n-l authn.management.cattle.io/kind=session,authn.management.cattle.io/token-userId=&lt;user name&gt; \\\n| xargs kubectl delete token\n</code></pre> <p>The manual remediation above force user sessions to be invalidated, for both local auth and external auth providers.</p>"},{"location":"kbs/000021604/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021608/","title":"Testing network bandwidth between local web browser and Rancher management cluster","text":"<p>This document (000021608) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021608/#environment","title":"Environment","text":"<p>Rancher management cluster</p>"},{"location":"kbs/000021608/#situation","title":"Situation","text":"<p>The Tuning and Best Practices for SUSE\u00ae Rancher Prime at Scale documentation has a Browser Requirements section which has a recommendation for network bandwidth to the Rancher management cluster. How can I collect this data in my environment to compare to the recommendation?</p>"},{"location":"kbs/000021608/#resolution","title":"Resolution","text":""},{"location":"kbs/000021608/#collection","title":"Collection","text":"<ol> <li>install the\u00a0<code>iperf3</code> tool (on macOS, use <code>brew install iperf3</code>)</li> <li>run the following commands from a machine with kubectl access to the upstream Rancher cluster:</li> </ol> <pre><code>kubectl run iperf3-server --image=networkstatic/iperf3 --port=5201 --command -- iperf3 -s\nkubectl port-forward pod/iperf3-server 5201:5201 &amp;\niperf3 -c localhost -p 5201\n</code></pre> <ol> <li>The output will look like this:</li> </ol> <pre><code>Connecting to host localhost, port 5201\n[  7] local ::1 port 57800 connected to ::1 port 5201\n[ ID] Interval           Transfer     Bitrate\n[  7]   0.00-1.00   sec   121 MBytes  1.01 Gbits/sec\n[  7]   1.00-2.00   sec   111 MBytes   933 Mbits/sec\n[  7]   2.00-3.00   sec   110 MBytes   921 Mbits/sec\n[  7]   3.00-4.00   sec   110 MBytes   925 Mbits/sec\n[  7]   4.00-5.00   sec   110 MBytes   924 Mbits/sec\n[  7]   5.00-6.01   sec   110 MBytes   921 Mbits/sec\n[  7]   6.01-7.00   sec   110 MBytes   924 Mbits/sec\n[  7]   7.00-8.00   sec   110 MBytes   925 Mbits/sec\n[  7]   8.00-9.01   sec   110 MBytes   927 Mbits/sec\n[  7]   9.01-10.01  sec   103 MBytes   867 Mbits/sec\n   - - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate\n[  7]   0.00-10.01  sec  1.08 GBytes   928 Mbits/sec                  sender\n[  7]   0.00-10.08  sec  1.07 GBytes   915 Mbits/sec                  receiver\n\niperf Done\n</code></pre>"},{"location":"kbs/000021608/#interpretation","title":"Interpretation","text":"<p>The last two table rows, under the Bitrate column, represent actual raw bandwidth.</p> <p>For optimal Rancher performance at scale,\u00a0SUSE recommends at least 72 Mbits/s \"receiver\" bandwidth (equivalent to a single 802.11n Wi-Fi 4 link stream).</p>"},{"location":"kbs/000021608/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021610/","title":"Installing Istio results with this error: \"Invalid value: 31380: provided port is already allocated\"","text":"<p>This document (000021610) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021610/#environment","title":"Environment","text":"<p>SUSE Rancher 2.7.x+</p> <p>Istio 102.3.1+up1.18.2</p>"},{"location":"kbs/000021610/#situation","title":"Situation","text":"<p>When trying to install Istio from \"Apps\" in the Rancher UI the installation fails with the below error.</p> <pre><code>Ingress gateways encountered an error: failed to update resource with server-side apply for obj Service/istio-system/istio-ingressgateway:\nService \"istio-ingressgateway\" is invalid: spec.ports[1].nodePort: Invalid value: 31380: provided port is already allocated\n</code></pre>"},{"location":"kbs/000021610/#resolution","title":"Resolution","text":"<p>You can use a different port by adding a 'Custom Overlay File' to the install.</p> <p>1. Go to the Istio chart in Apps|Charts.</p> <p>2. Click on 'Update'.</p> <p>3. Check 'Customize Helm options before install'.</p> <p>4. Add the following lines to\u00a0'Custom Overlay File' by adding the following lines starting on line 1.</p> <p>5. Change the nodePort to a different value from 31380 to 31370.</p> <p>6. Once you have added the below lines, you can click on 'Next' and then 'Update'.</p> <p>(I just chose 31370; you can choose any open port on your cluster in the range of 30000-32767)</p> <pre><code>### alter only the status-port as nodeport 31370\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  components:\n    ingressGateways:\n    - enabled: true\n      name: istio-ingressgateway\n      k8s:\n        service:\n          ports:\n            - name: status-port\n              port: 15021\n              targetPort: 15021\n              nodePort: 31370\n              protocol: TCP\n</code></pre> <p>7. Upon completion of the update, you can verify by executing the following command to confirm.</p> <p>(The NodePort below shows 31370)</p> <pre><code>kubectl get svc -n istio-system\n\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                 AGE\nkiali                  ClusterIP   10.43.228.239   &lt;none&gt;        20001/TCP                               3d23h\nistiod                 ClusterIP   10.43.166.131   &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP   3d23h\nistio-ingressgateway   NodePort    10.43.36.43     &lt;none&gt;        15021:31370/TCP                         3d23h\n</code></pre>"},{"location":"kbs/000021610/#cause","title":"Cause","text":"<p>The is caused by another application that is currently using the default port of Istio. Which is causing a conflict with the default port.</p>"},{"location":"kbs/000021610/#additional-information","title":"Additional Information","text":"<p>https://ranchermanager.docs.rancher.com/integrations-in-rancher/istio/configuration-options#overlay-file</p>"},{"location":"kbs/000021610/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021612/","title":"Using Private Registries for Custom and Imported Clusters","text":"<p>This document (000021612) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021612/#environment","title":"Environment","text":"<p>Air-gapped</p> <p>SUSE Rancher 2.9.3</p>"},{"location":"kbs/000021612/#situation","title":"Situation","text":"<p>Rancher fails to import or create a custom cluster node while pulling images when the default registry (Docker Hub) or the configured Global Default Registry is not accessible to this specific cluster.</p>"},{"location":"kbs/000021612/#resolution","title":"Resolution","text":"<p>At the time of writing of this KB (current as of v2.9.3) one of the following workarounds may be useful to you:</p> <ul> <li>In RKE2 Custom Clusters you can add the contents usually added to the `registries.yaml` manifest in RKE2 cluster nodes (see this doc) to the `machineGlobalConfig` in the cluster manifest when creating the cluster per\u00a0RKE2 Cluster Configuration Reference for \"machineGlobalConfig\" (with the minor correction of `registries` instead of `private-registry` per Rancher Product GitHub pull request - PR).</li> <li>In K3s Custom Clusters you can add the contents usually added to the `registries.yaml` manifest in K3s cluster nodes (see this doc) to the `machineGlobalConfig` in the cluster manifest when creating the cluster per K3S Cluster Configuration Reference \"machineGlobalConfig\".</li> <li>You can also manually pull the needed images to each of the nodes and retag them so that the local container management finds them locally (for example, you can use `docker tag repo.url:Port/rancher/rancher-agent:v2.9.1 rancher/rancher-agent:v2.9.1` to retag a `rancher-agent` image pulled from your private repo so your node doesn't attempt to pull from Dockerhub). \u00a0Consult the `rancher-images.txt` file in your Rancher version's release notes for a list of all images and versions Rancher will need in general, or review the logs generated on the node itself when import or creating nodes for a cluster for specific images the node is failing to retrieve. These will include (but are not necessarily limited to): `rancher/rancher-agent:v2.x.x`, `rancher/shell:vx.x.x`, `rancher/fleet-agent:vx.x.x` (see `rancher-images.txt` for your necessary version numbers)</li> <li>Previously there has been a workaround of adding an `agentImageOverride` or `desiredAgentImage` in the cluster manifest `spec` to override the default, but there is currently an issue with this which can be tracked for resolution here:\u00a0GitHub Issue - 47593</li> </ul>"},{"location":"kbs/000021612/#cause","title":"Cause","text":"<p>At this time, when importing a cluster or creating a custom cluster, Rancher images are pulled from the Global configured private registry (or the default of Docker Hub if one is not configured). If you are using a separate private registry for multiple air-gaped downstream clusters, you may find that this is not a viable solution.</p>"},{"location":"kbs/000021612/#status","title":"Status","text":"<p>Reported to Engineering</p>"},{"location":"kbs/000021612/#additional-information","title":"Additional Information","text":"<p>Related GitHub issues:</p> <ul> <li>[RFE] Cluster level private registry for imported Air-gapped setup #35192</li> <li>Need documentation on deploying and managing airgapped clusters #80</li> <li>[BUG] Agent image field not getting updated for imported clusters #47593</li> </ul>"},{"location":"kbs/000021612/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021616/","title":"Checking Browser Health and Resource Consumption for Rancher","text":"<p>This document (000021616) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021616/#environment","title":"Environment","text":"<p>SUSE Rancher 2.9.x+</p>"},{"location":"kbs/000021616/#situation","title":"Situation","text":"<p>Should you need to check the browser health (performance) and resource consumption for Rancher, this article explains the steps to do so.</p>"},{"location":"kbs/000021616/#resolution","title":"Resolution","text":"<p>Check the following aspects:</p> <ul> <li>CPU</li> <li>Go to \u22ee \u2192 <code>More Tools</code> \u00a0\u2192 <code>Task Manager</code></li> <li>Monitor the CPU column. Is it constantly at 100?</li> <li>Memory</li> <li>Go to \u22ee \u2192 <code>More Tools</code> \u00a0\u2192 <code>Task Manager</code><ul> <li>Monitor the <code>Memory footprint</code> column.\u00a0Is it constantly higher than 400 MB? What does it peak at? Does it drop down after a minute?</li> </ul> </li> <li>Go to \u22ee \u2192 <code>More Tools</code> \u00a0\u2192 <code>Developer Tools</code> \u2192 <code>Memory Tab</code><ul> <li>Monitor the Total JS heap size. Is it constantly higher than 100 MB?</li> </ul> </li> <li>Web Socket traffic</li> <li>Go to \u22ee \u2192 <code>More Tools</code> \u00a0\u2192 <code>Developer Tools</code> \u00a0\u2192 <code>Network Tab</code> \u00a0\u2192 <code>WS</code> \u00a0\u2192  \u2192 <code>Messages Tab</code> <li>Are there hundreds of messages per second?</li> <li>Errors or spammed messages</li> <li>Go to <code>More Tools</code> \u00a0\u2192 <code>Developer Tools</code> \u00a0\u2192 <code>Console Tab</code></li> <li>Are there Error entries?</li> <li>Are there lots and lots of messages?</li>"},{"location":"kbs/000021616/#additional-information","title":"Additional Information","text":"<p>You should take a note of all this information and report back to the Rancher Support team via a support ticket if you need any further analysis.</p>"},{"location":"kbs/000021616/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021618/","title":"How to rollback the Kubernetes version of an RKE2 upstream/standalone cluster following an upgrade","text":"<p>This document (000021618) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021618/#environment","title":"Environment","text":"<p>A standalone (i.e. not Rancher-provisioned) RKE2 cluster, such as an upstream Rancher local RKE2 cluster, that has been upgraded</p>"},{"location":"kbs/000021618/#situation","title":"Situation","text":"<p>Following a Kubernetes version upgrade to a standalone RKE2 cluster, it may be necessary to perform a version rollback, due to an issue encountered with the upgrade or new Kubernetes version.</p> <p>This requires:</p> <ul> <li>An RKE2 etcd snapshot of the cluster prior to the version upgrade</li> <li>The previous RKE2 version</li> </ul>"},{"location":"kbs/000021618/#resolution","title":"Resolution","text":"<p>The rollback process is described below, for the purpose of this example, a rollback to RKE2 v1.30.9+rke2r1 is imagined, after an upgrade to a later version.</p> <ol> <li>If the cluster is running, with the Kubernetes API available, and you wish to gracefully stop workloads within the cluster before executing the rollback, first drain all of the nodes: `kubectl drain --ignore-daemonsets --delete-emptydir-data   ...` <li>On each node, run the `rke2-killall.sh` script to stop the rke2-server or rke2-agent service (depending if it is a server or agent node) and all running Pod processes.</li> <li>Manually roll back the RKE2 binary to the previous version.</li> <li>In clusters with internet access, this can be done via the RKE2 installation script, specifying the previous RKE2 Kubernetes version via the variable <code>INSTALL_RKE2_VERSION:</code><ol> <li>On server nodes run `<code>curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"v1.30.9+rke2r1\" sh -</code> `</li> <li>On agent nodes run `<code>curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=\"v1.30.9+rke2r1\" INSTALL_RKE2_TYPE=agent sh -</code> `</li> </ol> </li> <li>In air-gapped clusters, this can be done by downloading the artefacts for the previous RKE2 version on the nodes and invoking the install script locally, per the RKE2-airgapped installation documentation.</li> <li><code>```With the previous RKE2 binary installed on all cluster nodes, the next step is to restore an etcd snapshot taken whilst the cluster was still running this previous RKE2 version (in this example v1.30.9+rke2r1). On the first server node (i.e. the node without a \\</code>server:` defined in its RKE2 config file), initiate the cluster restore (per step 2 of the RKE2 documentation on Restoring a Snapshot to Existing Nodes): ` <code>rke2 server --cluster-reset --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt;</code> `</li> <li>Once the restore process is completed, proceed through steps 3-5 of the RKE2 documentation on Restoring a Snapshot to Existing Nodes):</li> <li>Start the rke2-server service on the first server node as follows: ` <code>systemctl start rke2-server</code> `.</li> <li>Remove the rke2 db directory on the other server nodes as follows: `rm -rf /var/lib/rancher/rke2/server/db`</li> <li>Start the rke2-server service on other server nodes with the following command: `systemctl start rke2-server`</li> <li>Finally, start the rke2-agent service on any agent nodes: `systemctl start rke2-agent`</li>"},{"location":"kbs/000021618/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021631/","title":"How to enable debug logging for the rancher-system-agent","text":"<p>This document (000021631) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021631/#environment","title":"Environment","text":"<p>A Rancher-provisioned RKE2 or K3s cluster</p>"},{"location":"kbs/000021631/#situation","title":"Situation","text":"<p>The Rancher System Agent is a daemon designed to run on a system and apply \"plans\" to the system. This is deployed on Linux nodes in Rancher-provisioned RKE2 and K3s clusters, and used by Rancher for node provisioning and configuration management.</p> <p>During troubleshooting, it may be helpful to enable debug level logging on the rancher-system-agent, and this process is detailed below.</p>"},{"location":"kbs/000021631/#resolution","title":"Resolution","text":""},{"location":"kbs/000021631/#enable-debug-of-custom-clusters-at-provisioning-time","title":"Enable debug of custom clusters at provisioning time","text":"<ol> <li>Add the DEBUG environment variable to the Registration command, by setting CATTLE_AGENT_LOGLEVEL=debug when running the registration command on RKE2 and K3s Linux hosts, e.g.:</li> </ol> <pre><code>curl -fL https://&lt;RANCHER_FQDN&gt;/system-agent-install.sh | sudo CATTLE_AGENT_LOGLEVEL=debug sh -s - --server https://&lt;RANCHER_FQDN&gt; --label 'cattle.io/os=linux' --token &lt;RANCHER_TOEKN&gt; --ca-checksum &lt;RANCHER_CA_CHECKSUM&gt; --etcd --controlplane --worker\n</code></pre>"},{"location":"kbs/000021631/#enable-debug-for-running-clusters","title":"Enable debug for running clusters","text":"<ol> <li>Open a SSH session into the node and modify the rancher-system-agent service configuration file.</li> <li>Run the command `systemctl edit rancher-system-agent.service` to open a systemd unit file override editor for the rancher-system-agent service.</li> <li>Add a [Service] block to the file, below the top-level comment, with the single entry Environment=CATTLE_LOGLEVEL=debug per the example below, and save the file.</li> </ol> <pre><code>### Editing /etc/systemd/system/rancher-system-agent.service.d/override.conf\n### Anything between here and the comment below will become the new contents of the file\n[Service]\nEnvironment=CATTLE_LOGLEVEL=debug\n</code></pre> <p>The changes will be saved to /etc/systemd/system/rancher-system-agent.service.d/override.conf 4. Restart the rancher-system-agent systemd service to apply the changes:</p> <pre><code>systemctl restart rancher-system-agent\n</code></pre> <ol> <li>You can now view and collect rancher-system-agent logs at debug level for troubleshooting.</li> <li>Once you no longer required rancher-system-agent debug logs, you can run `systemctl edit rancher-system-agent.service` to remove the contents previously added, or set Environment=CATTLE_LOGLEVEL=info to return to info level logging.</li> </ol>"},{"location":"kbs/000021631/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021636/","title":"Known issue deleting etcd nodes in RKE clusters managed by Rancher v2.8.6 \u2264 version &lt; v2.9.0 or RKE CLI v1.5.11 \u2265 version &lt; v1.5.14","text":"<p>This document (000021636) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021636/#environment","title":"Environment","text":"<ul> <li>A Rancher-provisioned RKE cluster managed using Rancher v2.8.6 \u2264 version &lt; v2.9.0</li> <li>Or an RKE standalone cluster managed using RKE v1.5.11 \u2265 version &lt; v1.5.14</li> </ul>"},{"location":"kbs/000021636/#situation","title":"Situation","text":"<p>Due to a known bug in these versions of Rancher and RKE, after removing a node with at least the etcd role, it won't get removed from the etcd list, even if from the kubectl perspective, the node is not part of the cluster anymore.</p> <p>The node won't be part of the cluster and won't appear using the Rancher UI, the Rancher API, or the kubectl command line.</p> <p>However, when going into one of the other etcd nodes, and checking the etcd member list, the old etcd node will still appear:</p> <pre><code>docker exec etcd etcdctl member list\n</code></pre> <p>This inconsistent removal may lead you into unexpected scenarios, as the number of etcd nodes running in the cluster won't match the number you expect it to be.</p> <p>Depending on the number of etcd nodes running, and the actions performed on the theoretically removed etcd nodes, you may even lose control over the cluster if it becomes unresponsive due to etcd quorum lost.</p> <pre><code>\n</code></pre>"},{"location":"kbs/000021636/#resolution","title":"Resolution","text":"<p>To workaround this issue, you need to:</p> <ol> <li>Double-check the etcd member list, where the old node/s should still be listed:</li> </ol> <pre><code>docker exec etcd etcdctl member list\n</code></pre> <ol> <li>Manually remove the desired etcd node, even if this node was already deleted via kubectl or editing the configuration YAML file.</li> </ol> <pre><code>docker exec etcdctl member remove &lt;etcd node ID&gt;\n</code></pre> <pre><code>\n</code></pre> <p>This bug won't affect you if you run an earlier or more modern version of Rancher or RKE. Please check the Environment section.</p> <pre><code>\n</code></pre>"},{"location":"kbs/000021636/#cause","title":"Cause","text":"<p>Due to a known bug in these versions of Rancher and RKE, after removing a node with at least the etcd role, it won't get removed from the etcd list, even if the node is not part anymore of the cluster from the kubectl perspective.</p>"},{"location":"kbs/000021636/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021637/","title":"Cannot upgrade Rancher-provisioned RKE clusters using an RKE template","text":"<p>This document (000021637) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021637/#environment","title":"Environment","text":"<ul> <li>Rancher v2.9.x &lt; v2.9.5</li> <li>A Rancher-provisioned RKE cluster using an RKE Template</li> </ul>"},{"location":"kbs/000021637/#situation","title":"Situation","text":"<p>Attempting to upgrade downstream clusters using RKE templates in Rancher v2.9.0 - v2.9.4 may fail. When navigating to\u00a0Cluster Management in the Rancher UI and selecting\u00a0Edit Config for a templated RKE cluster, after changing the template revision to a template with a newer Kubernetes version, sections of the form including Kubernetes options, private registries, and advanced options disappear.</p>"},{"location":"kbs/000021637/#resolution","title":"Resolution","text":"<p>The issue is fixed in Rancher v2.9.5 and above, and you should upgrade to take advantage of the fix.</p> <p>To workaround the issue in Rancher v2.9.0 - v2.9.4, affected clusters can be upgraded using the Rancher v3 API.</p>"},{"location":"kbs/000021637/#workaround-steps","title":"Workaround Steps:","text":"<ol> <li>Access the Rancher v3 API of the downstream cluster where you want to upgrade the Kubernetes version: https://[RANCHER-URL]/v3/clusters/[CLUSTER-ID]</li> <li>To easily find the cluster ID, within the Rancher UI navigate to Cluster Management and select the cluster</li> <li> <p>On the URL bar, you will see something similar to:</p> <p>https://[RANCHER-URL]/dashboard/c/_/manager/provisioning.cattle.io.cluster/fleet-default/ c-xxxxx</p> <p>The last 7 characters (c-xxxxx) are your cluster ID. 2. You will be able to see now the Rancher v3 API. In the top-right corner, you will see an operations window. 3. Click on Edit and you will see all the cluster's configurations. 4. Make use of the Ctrl+F search to find the \"clusterTemplateRevisionId\" value. 5. It should show a select field where you can see all your template revisions. 6. Select the revision to which you wish to upgrade the cluster. 7. Scroll down and click on the Show Request button. 8. Scroll down again and click on the\u00a0Send Request button to apply the changes.</p> </li> </ol>"},{"location":"kbs/000021637/#additional-information","title":"Additional Information","text":""},{"location":"kbs/000021637/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021647/","title":"How to Inquire About CVEs in SUSE Rancher Prime Product","text":"<p>This document (000021647) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021647/#environment","title":"Environment","text":"<p>The information applies to the following SUSE products:</p> <ul> <li> <p>Rancher Prime</p> </li> <li> <p>RKE</p> </li> <li> <p>RKE2</p> </li> <li> <p>K3s</p> </li> <li> <p>Harvester</p> </li> <li> <p>Longhorn</p> </li> <li> <p>NeuVector</p> </li> </ul>"},{"location":"kbs/000021647/#situation","title":"Situation","text":"<p>Customers concerned about security vulnerabilities and CVEs in container images published by SUSE Rancher Prime, and listed in the CVE portal, can contact SUSE to inquire about those CVEs.</p>"},{"location":"kbs/000021647/#scope","title":"Scope","text":"<p>Inquiries about the following types of CVEs are applicable to this document:</p> <ol> <li> <p>CVEs that directly affect source code dependencies used by our application or in third-party upstream binaries that we import in our images. Examples of such CVEs are related to Rancher\u2019s Go dependencies.</p> </li> <li> <p>CVEs that affect OS level packages inside the container images that we use or that we mirror from third-party upstream. Examples of such CVEs are the ones affecting `curl` and `openssl` binaries.</p> </li> </ol> <p>Note: for reporting new security vulnerabilities that are not public yet (aka 0-days) that directly affect the source code of SUSE Rancher Prime and the related products, please follow the responsible disclosure guidelines provided here.</p>"},{"location":"kbs/000021647/#resolution","title":"Resolution","text":""},{"location":"kbs/000021647/#note-about-etas-for-cve-fixes","title":"Note about ETAs for CVE fixes","text":"<p>Before following this process to inquire about CVEs, make sure to read the information provided in the KB \u2018 How to use SUSE Rancher Prime\u2019s CVE Portal \u2019.</p> <p>SUSE doesn\u2019t provide a date (ETA) for when the CVEs listed in the portal will be fixed, because such fixes normally depend on multiple variables:</p> <ol> <li> <p>If a fixed version for the dependency or package is available.</p> </li> <li> <p>If the fixed version is compatible with Rancher.</p> </li> <li> <p>Recompiling the affected dependency or updating the package across all the affected areas.</p> </li> <li> <p>For the cases where the CVE is inside a mirrored image, those that we pull directly and without modifications from its upstream developer, it\u2019s also necessary to wait for a new release of the image that contains the needed fix.</p> </li> <li> <p>QA tests being executed to identify possible regressions.</p> </li> <li> <p>The update being rolled out to all of the possible affected areas while still inside the development window for the next release of Rancher.</p> </li> </ol>"},{"location":"kbs/000021647/#how-to-inquiry-about-a-cve-as-a-suse-customer","title":"How to inquiry about a CVE as a SUSE Customer","text":"<p>The workflow presented below must be observed when inquiring about CVEs in any of the listed products.</p> <ol> <li> <p>Verify if you are using and scanning the latest patch version or development version of the product. Inquiries regarding CVEs in older patch versions will not be evaluated.</p> </li> <li> <p>If the CVE has critical or high severity and is not listed in the portal, that\u2019s because it\u2019s already fixed in one of the development branches or is considered a false-positive. Those will not be evaluated.</p> </li> <li> <p>Verify if the severity of the CVE wasn\u2019t modified, reduced or increased, according to SUSE\u2019s own re-evaluation of CVEs\u2019 original CVSS rating applicability. CVEs that had their original CVSS rating recalculated will have a distinctive tag in the CVE portal.</p> </li> <li> <p>Inquiries for CVEs considered informational or that don\u2019t have a severity defined, i.e., are considered none according to the CVSS rating calculation by SUSE as listed in SUSE\u2019s CVE database, will not be evaluated. CVEs that are eventually not tracked by SUSE will have their CVSS score based on the NVD database.</p> </li> <li> <p>Inquiries for medium and low can be sent and will be analyzed, although with a minor priority given their lower severity. We are currently prioritizing the analysis and fixes of critical and high severity CVEs. Some medium and low CVEs are updated in conjunction with higher severity ones.</p> </li> <li> <p>Submit your request through the SUSE Customer Center (SCC). Consult your designated Support or Sales engineer in case you don\u2019t know how to use the SCC.</p> </li> <li> <p>In the request, provide the following mandatory information:</p> </li> <li> <p>The complete versions (major, minor and patch) of the products that were scanned.</p> </li> <li> <p>Name and version of the scanning tool used.</p> </li> <li> <p>The actual scan report containing the CVEs must be in a CSV file with the following structure:</p> <ol> <li>Each line containing the fields:<ol> <li>\"image:version\",\"package_name\",\"vulnerability_id\",\"severity\".</li> </ol> </li> <li> <p>The fields are described as:</p> <ol> <li> <p>Field 1 - image:version: name and version of the scanned image.</p> </li> <li> <p>Field 2 - package_name: name of the package, with its full path inside the image, where the vulnerability was identified.</p> </li> <li> <p>Field 3 - vulnerability_id: public vulnerability identifier in the CVE format or similar (this can vary between scanning tools).</p> </li> <li> <p>Field 4 - severity: severity level of the vulnerability.</p> <ol> <li>Example of a valid input file scan report:</li> </ol> </li> </ol> </li> </ol> </li> </ol> \"image:version\",\"package_name\",\"vulnerability_id\",\"severity\"rancher/shell:v0.1.18,2.7.3\",/usr/local/bin/helm,CVE-2022-41723,mediumrancher/hardened-kubernetes:v1.23.17-rke2r1-build20230228,container-suseconnect,SUSE-SU-2023:0871-1,mediumrancher/rke2-runtime:v1.23.17-rke2r1,bin/containerd,CVE-2022-27664,medium <ol> <li>Failing to provide the needed information might result in the ticket being rejected or in delayed processing time due to extra information being requested back to the reporter.</li> </ol>"},{"location":"kbs/000021647/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021649/","title":"How to override the cattle-agent image in troubleshooting with SUSE Rancher Support","text":"<p>This document (000021649) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021649/#environment","title":"Environment","text":"<p>SUSE Rancher v2.9+</p>"},{"location":"kbs/000021649/#situation","title":"Situation","text":"<p>During support investigations, SUSE Rancher support and engineering might propose a hotfix image for Rancher, which contains a potential fix for a specific issue, for troubleshooting purposes.</p> <p>The usage of a specific hotfix image will deploy the corresponding rancher-agent images in the cattle-cluster-agent deployments downstream within minutes. It is possible to prevent the change of the cattle-cluster-agent image by using the CATTLE_AGENT_IMAGE environment variable in the rancher deployment to pin the version of the agent.</p>"},{"location":"kbs/000021649/#resolution","title":"Resolution","text":""},{"location":"kbs/000021649/#configuration-instructions","title":"Configuration instructions","text":"<p>Consider that you are about to change your Rancher image from v2.9.3 to a hotfix image v2.9.3-hotfix-XYZ. From a machine with access to the upstream cluster, edit the rancher deployment (to change the rancher image to the test version): ``</p> <pre><code>export AGENT_IMAGE_TAG=v2.9.3\nkubectl -n cattle-system set env deployment/rancher CATTLE_AGENT_IMAGE=registry.rancher.com/rancher/rancher-agent:${AGENT_IMAGE_TAG}\n</code></pre> <p>This method is intended to temporarily change the <code>CATTLE_AGENT_IMAGE</code> between upgrades. Alternatively, you can make this change persistent by setting the <code>extraEnv</code> value in the Rancher Helm chart, which ensures the configuration is retained in the Helm release. However, since this operation is intended for troubleshooting purposes, it is preferable to edit the Rancher deployment directly rather than modifying the Helm chart via the\u00a0<code>values.yaml</code> file. This approach prevents any impact on the <code>cattle-cluster-agent</code> image versioning during a Rancher upgrade.</p> <p>``</p> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000021649/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021666/","title":"rke2-ingress-nginx deployed with watchIngressWithoutClass: false in v1.27.16+rke2r1, v1.28.12+rke2r1, v1.29.7+rke2r1 and v1.30.3+rke2r1","text":"<p>This document (000021666) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021666/#environment","title":"Environment","text":"<p>An RKE2 cluster running v1.27.16+rke2r1, v1.28.12+rke2r1, v1.29.7+rke2r1 or v1.30.3+rke2r1, which was upgraded from an earlier version.</p>"},{"location":"kbs/000021666/#situation","title":"Situation","text":"<p>After an RKE2 upgrade to version v1.27.16+rke2r1, v1.28.12+rke2r1, v1.29.7+rke2r1 or v1.30.3+rke2r1, ingress-nginx will stop watching any ingress resources that do not define spec.ingressClassName: nginx. Newly created ingress resources that do not define an ingressClassName will have this value set by default. However, any existing ingress objects that were created without an ingressClassName will not be watched by ingress-nginx and traffic will not be routed to them.</p>"},{"location":"kbs/000021666/#resolution","title":"Resolution","text":"<p>A fix is available via the rke2-ingress-nginx chart in RKE2 v1.27.16+rke2r2, v1.28.13+rke2r1, v1.29.8+rke2r1, v1.30.4+rke2r1 and above. In these versions, ingress-nginx is deployed with ' --watch-ingress-without-class=true' by default, to ensure traffic continues to be routed to existing ingress resources without an ingressClassName. Users are encouraged to upgrade to take advantage of this fix.</p> <p>For the affected RKE2 versions, three possible workarounds exist:</p> <ol> <li>Delete and recreate affected ingress objects, so the default ingressClassName of nginx is applied</li> <li>Update affected ingress resources to set <code>spec.ingressClassName: nginx\u00a0manually</code></li> <li><code>` Customise the rke2-ingress-nginx helm chart values, [via a HelmChartConfig resource](https://docs.rke2.io/helm#customizing-packaged-components-with-helmchartconfig), to set</code>watchIngressWithoutClass: true`</li> </ol>"},{"location":"kbs/000021666/#cause","title":"Cause","text":"<p>The rke2-ingress-nginx chart version 4.10.102, bundled with the affected RKE2 versions, is deployed with --watch-ingress-without-class=false. As a result, ingress resources created on older RKE2 versions, in which there was no default ingressClassName, and for which the ingressClassName attribute was not explicitly set to nginx, are not watched by ingress-nginx.</p>"},{"location":"kbs/000021666/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021669/","title":"helm-install-rke-calico fails on downstream clusters; cannot re-use a name that is still in use","text":"<p>This document (000021669) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021669/#environment","title":"Environment","text":"<ul> <li>Rancher v2.6+</li> <li>A Rancher-provisioned RKE2 cluster</li> </ul>"},{"location":"kbs/000021669/#situation","title":"Situation","text":"<p>Following an RKE2 upgrade, the helm-install-rke2-calico Job, in the kube-system Namespace, fails with the error ' cannot re-use a name that is still in use', per the following example from the Pod logs:</p> <pre><code>+ echo 'Installing helm chart'\n+ helm install --set-string global.cattle.systemDefaultRegistry=docker.io --set-string global.clusterCIDR=10.42.0.0/16 --set-string global.clusterCIDRv4=10.42.0.0/16 --set-string global.clusterDNS=10.43.0.10 --set-string global.clusterDomain=cluster.local --set-string global.rke2DataDir=/var/lib/rancher/rke2 --set-string global.serviceCIDR=10.43.0.0/16 --set-string global.systemDefaultIngressClass=ingress-nginx --set-string global.systemDefaultRegistry=docker.io rke2-calico /tmp/rke2-calico.tgz --values /config/values-10_HelmChartConfig.yaml\nError: INSTALLATION FAILED: cannot re-use a name that is still in use\n</code></pre>"},{"location":"kbs/000021669/#resolution","title":"Resolution","text":"<ol> <li>Get the list of current rke2-calico helm release secrets from the kube-system Namespace, which will be in a superseded status, e.g.:</li> </ol> <pre><code>kubectl -n kube-system get secrets --field-selector type=helm.sh/release.v1 -o custom-columns='NAME:.metadata.name,STATUS:.metadata.labels.status' -l name=rke2-calico\nNAME                                STATUS\nsh.helm.release.v1.rke2-calico.v1   superseded\nsh.helm.release.v1.rke2-calico.v2   superseded\n</code></pre> <ol> <li>Delete these superseded rke2-calico helm-releases secrets, e.g.:</li> </ol> <pre><code>      kubectl -n kube-system delete secret sh.helm.release.v1.rke2-calico.v1\n      kubectl -n kube-system delete secret sh.helm.release.v1.rke2-calico.v2\n</code></pre> <ol> <li>Delete the rke2-calico helm install Job:</li> </ol> <pre><code>kubectl -n kube-system delete job helm-install-rke2-calico\n</code></pre> <ol> <li>Restart the rke2-server service on a server node, to trigger the helm-install-rke2-calico Job, resolving the issue:</li> </ol> <pre><code>systemctl restart rke2-server\n</code></pre>"},{"location":"kbs/000021669/#cause","title":"Cause","text":"<p>A failed rke2-calico helm install Job during an RKE2 upgrade leaves the cluster with only superseded rke2-calico releases.</p> <pre><code>helm -n kube-system history rke2-calico\nREVISION        UPDATED                         STATUS          CHART                   APP VERSION     DESCRIPTION\n1               Mon Feb  3 09:35:23 2025        superseded      rke2-calico-v3.29.100   v3.29.1         Install complete\n2               Mon Feb  3 09:49:28 2025        superseded      rke2-calico-v3.29.100   v3.29.1         Upgrade complete\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000021669/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021676/","title":"The `rke2 certificate check` command does not check the kube-controller-manager and kube-scheduler certificates in Rancher-provisioned RKE2 cluster","text":"<p>This document (000021676) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021676/#environment","title":"Environment","text":"<ul> <li>Rancher v2.7+</li> <li>A Rancher-provisioned RKE2 cluster</li> </ul>"},{"location":"kbs/000021676/#situation","title":"Situation","text":"<p>When the `rke certificate check` command is run on a server node in an Rancher-provisioned RKE2 cluster, output is missing for both the kube-controller-manager and kube-scheduler certificates, when compared with the output for a standalone RKE2 cluster:</p> <pre><code>$ rke2 certificate check\nINFO[0000] Server detected, checking agent and server certificates\nINFO[0000] Checking certificates for kube-proxy\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-kube-proxy.crt: certificate CN=system:kube-proxy is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-kube-proxy.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/agent/client-kube-proxy.crt: certificate CN=system:kube-proxy is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/agent/client-kube-proxy.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for kubelet\nINFO[0000] /var/lib/rancher/rke2/agent/client-kubelet.crt: certificate CN=system:node:test-rancheragent-rke2-all-0,O=system:nodes is ok, expires at 2026-02-07T09:50:10Z\nINFO[0000] /var/lib/rancher/rke2/agent/client-kubelet.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/agent/serving-kubelet.crt: certificate CN=test-rancheragent-rke2-all-0 is ok, expires at 2026-02-07T09:50:09Z\nINFO[0000] /var/lib/rancher/rke2/agent/serving-kubelet.crt: certificate CN=rke2-server-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for rke2-controller\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-rke2-controller.crt: certificate CN=system:rke2-controller is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-rke2-controller.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/agent/client-rke2-controller.crt: certificate CN=system:rke2-controller is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/agent/client-rke2-controller.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for api-server\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt: certificate CN=system:apiserver,O=system:masters is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt: certificate CN=kube-apiserver is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt: certificate CN=rke2-server-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for cloud-controller\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.crt: certificate CN=rke2-cloud-controller-manager is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-rke2-cloud-controller.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for scheduler\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-scheduler.crt: certificate CN=system:kube-scheduler is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-scheduler.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for supervisor\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-supervisor.crt: certificate CN=system:rke2-supervisor,O=system:masters is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-supervisor.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for admin\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-admin.crt: certificate CN=system:admin,O=system:masters is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-admin.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for auth-proxy\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-auth-proxy.crt: certificate CN=system:auth-proxy is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-auth-proxy.crt: certificate CN=rke2-request-header-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for controller-manager\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-controller.crt: certificate CN=system:kube-controller-manager is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/client-controller.crt: certificate CN=rke2-client-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] Checking certificates for etcd\nINFO[0000] /var/lib/rancher/rke2/server/tls/etcd/client.crt: certificate CN=etcd-client is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/etcd/client.crt: certificate CN=etcd-server-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/etcd/server-client.crt: certificate CN=etcd-server is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/etcd/server-client.crt: certificate CN=etcd-server-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.crt: certificate CN=etcd-peer is ok, expires at 2026-02-07T09:41:47Z\nINFO[0000] /var/lib/rancher/rke2/server/tls/etcd/peer-server-client.crt: certificate CN=etcd-peer-ca@1738921307 is ok, expires at 2035-02-05T09:41:47Z\n</code></pre>"},{"location":"kbs/000021676/#resolution","title":"Resolution","text":"<p>For Rancher-provisioned RKE2 clusters, the cluster certificates should be managed and rotated by Rancher, versus the rke2 command directly. Documentation on rotating RKE2 certificates for Rancher-provisioned RKE2 clusters can be found at https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/manage-clusters/rotate-certificates</p>"},{"location":"kbs/000021676/#cause","title":"Cause","text":"<p>In a Rancher-provisioned RKE2 cluster, there are two new certificates not managed by RKE2 itself:</p> <ul> <li>/var/lib/rancher/rke2/server/tls/kube-scheduler/</li> <li>kube-scheduler.key</li> <li>kube-scheduler.crt</li> <li>/var/lib/rancher/rke2/server/tls/kube-controller-manager/</li> <li>kube-controller-manager.crt</li> <li>kube-controller-manager.key</li> </ul> <p>These certificates are not created and managed by the RKE2 supervisor process. Instead, they are managed by custom kube-controller-manager and kube-scheduler args added by Rancher that instruct the controller-manager and kube-scheduler to create self-signed certs in these locations.</p>"},{"location":"kbs/000021676/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021679/","title":"Only a user logged in via external auth provider can access Rancher user and group attributes from auth provider","text":"<p>This document (000021679) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021679/#environment","title":"Environment","text":"<p>SUSE Rancher configured with an external authentication provider</p>"},{"location":"kbs/000021679/#situation","title":"Situation","text":"<p>When SUSE Rancher is configured to use an external authentication provider, such as Active Directory, Keycloack, or OpenLDAP etc., you may be interested in searching across external authentication users or groups to configure their permissions or add them to the correspondent clusters. However, if this search is made using any local user, including the Rancher local admin, the operation won't retrieve any information.</p>"},{"location":"kbs/000021679/#resolution","title":"Resolution","text":"<p>In order to retrieve this information, you need to log in with a user with the proper external permissions. Thus, the user that needs to be used has to be a properly configured user, with access to the external authentication provider.</p>"},{"location":"kbs/000021679/#cause","title":"Cause","text":"<p>This is the expected behaviour by design. When you are logged in with a local account, this user won't have the necessary permissions in the external authentication provider to search for users/groups. Then, Rancher won't be able to search users/groups, as intended.</p> <p>Since you are trying to access information that does not belong to the user in the external authentication provider, the same information cannot be fetched from Rancher as well. Even if the local user is an administrator user in Rancher, if this user doesn't have permission to access this information in the external authentication provider, then Rancher will not be able to.</p>"},{"location":"kbs/000021679/#additional-information","title":"Additional Information","text":"<p>https://github.com/rancher/rancher/issues/45496#issuecomment-2228581165</p>"},{"location":"kbs/000021679/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021681/","title":"How to configure Spegel (Embedded Registry Mirror) for Rancher-provisioned RKE2/k3s clusters","text":"<p>This document (000021681) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021681/#environment","title":"Environment","text":"<ul> <li>Rancher v2.9+</li> <li>A Rancher-provisioned K3s cluster with v1.29.12+k3s1, v1.30.8+k3s1, v1.31.4+k3s1 or above, or a Rancher-provisioned RKE2 cluster with v1.29.12+rke2r1,v1.30.8+rke2r1, v1.31.4+rke2r1 or above</li> </ul>"},{"location":"kbs/000021681/#situation","title":"Situation","text":"<p>This article details how to enable and monitor the Embedded Registry Mirror, provided by an embedded instance of Spegel, in Rancher-provisioned K3s and RKE2 clusters.</p>"},{"location":"kbs/000021681/#resolution","title":"Resolution","text":""},{"location":"kbs/000021681/#enable-the-embedded-registry-mirror","title":"Enable the Embedded Registry Mirror","text":"<ol> <li>To enable the Embedded Registry Mirror the `embedded-registry: true` option needs to be set on server nodes within the cluster. To enable this on a Rancher-provisioned K3s or RKE2 cluster, navigate to Cluster Management within the Rancher UI and select Edit YAML for the relevant cluster. Define `embedded-registry:true` within a machineSelectorConfig block for controplane nodes, as below. Optionally, set `supervisor-metrics: true` to enable querying of Spegel metrics:</li> </ol> <pre><code>       machineSelectorConfig:\n      - config:\n          embedded-registry: true\n          supervisor-metrics: true\n        machineLabelSelector:\n          matchLabels:\n            rke.cattle.io/control-plane-role: 'true'\n</code></pre> <ol> <li>Per, the K3s and RKE2 documentation, \"Enabling mirroring for a registry allows a node to both pull images from that registry from other nodes, and share the registry's images with other nodes.\" Therefore, if a registry that you wish to use with the Embedded Registry Mirror is not already defined in the cluster's registry mirror configuration, you will need to add it. You can add the registry without any endpoints, per the following example for docker.io (Docker Hub):</li> </ol> <pre><code>       registries:\n         mirrors:\n           docker.io: {}\n</code></pre> <ol> <li>Click Save to update the cluster with these changes, enabling the Embedded Registry Mirror.</li> </ol>"},{"location":"kbs/000021681/#query-metrics-for-the-embedded-registry-mirror","title":"Query metrics for the Embedded Registry Mirror","text":"<p>If you set `supervisor-metrics: true` in step 1 above, you will be able to query the Embedded Registry Mirror (Spegel) metrics on each node within the cluster. Please note that the kubectl queries below will only work to the Kubernetes API endpoint on cluster server nodes directly, and will not work with the Rancher-proxied Kubernetes API endpoint for the cluster, nor via the built-in kubectl shell for the cluster within the Rancher UI.</p>"},{"location":"kbs/000021681/#query-the-spegel-metrics-for-an-rke2-cluster-node","title":"Query the Spegel metrics for an RKE2 cluster node","text":"<p>The Spegel metrics will be exposed via the RKE2 supervisor metrics for each cluster node, on port 9345. The example below shows how to query the spegel metrics for a node from an RKE2 cluster server node:</p> <pre><code>$ export KUBECONFIG=/etc/rancher/rke2/rke2.yaml\n$ alias kubectl=/var/lib/rancher/rke2/bin/kubectl\n$ kubectl get --server https://&lt;node-ip&gt;:9345 --raw /metrics | grep spegel\n# HELP spegel_advertised_image_digests Number of image digests advertised to be available.\n# TYPE spegel_advertised_image_digests gauge\nspegel_advertised_image_digests{registry=\"docker.io\"} 25\n# HELP spegel_advertised_image_tags Number of image tags advertised to be available.\n# TYPE spegel_advertised_image_tags gauge\nspegel_advertised_image_tags{registry=\"docker.io\"} 25\n# HELP spegel_advertised_images Number of images advertised to be available.\n# TYPE spegel_advertised_images gauge\nspegel_advertised_images{registry=\"docker.io\"} 50\n# HELP spegel_advertised_keys Number of keys advertised to be available.\n# TYPE spegel_advertised_keys gauge\nspegel_advertised_keys{registry=\"docker.io\"} 218\n</code></pre>"},{"location":"kbs/000021681/#query-the-spegel-metrics-for-a-k3s-cluster-node","title":"Query the Spegel metrics for a K3s cluster node","text":"<p>The Spegel metrics will be exposed via the K3s supervisor metrics for each cluster node, on port 6443. The example below shows how to query the Spegel metrics for a node from a K3s cluster server node:</p> <pre><code>kubectl get --server https://&lt;node-ip&gt;:6443 --raw /metrics | grep spegel\nlibp2p_rcmgr_streams{dir=\"inbound\",protocol=\"/spegel/kad/1.0.0\",scope=\"protocol\"} 1\nlibp2p_rcmgr_streams{dir=\"outbound\",protocol=\"/spegel/kad/1.0.0\",scope=\"protocol\"} 1\n# HELP spegel_advertised_image_digests Number of image digests advertised to be available.\n# TYPE spegel_advertised_image_digests gauge\nspegel_advertised_image_digests{registry=\"docker.io\"} 9\n# HELP spegel_advertised_image_tags Number of image tags advertised to be available.\n# TYPE spegel_advertised_image_tags gauge\nspegel_advertised_image_tags{registry=\"docker.io\"} 9\n# HELP spegel_advertised_images Number of images advertised to be available.\n# TYPE spegel_advertised_images gauge\nspegel_advertised_images{registry=\"docker.io\"} 18\n# HELP spegel_advertised_keys Number of keys advertised to be available.\n# TYPE spegel_advertised_keys gauge\nspegel_advertised_keys{registry=\"docker.io\"} 92\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"kbs/000021681/#additional-information","title":"Additional Information","text":"<ul> <li>K3s Embedded Registry Mirror documentation</li> <li>RKE2 Embedded Registry Mirror documentation</li> </ul>"},{"location":"kbs/000021681/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021691/","title":"Steps for troubleshooting the error \"failed to create containerd task, failed to create shim task OCI runtime create failed, unable to start container process\"","text":"<p>This document (000021691) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021691/#environment","title":"Environment","text":"<p>Rancher</p> <p>RKE2</p>"},{"location":"kbs/000021691/#situation","title":"Situation","text":"<p>If you encounter these errors while creating the RKE2 custom cluster, adding new nodes to an existing cluster, or running a pod on the new nodes, you may try the steps mentioned in the checklist below.</p> <p>From containerd logs</p> <pre><code>failed, error\" error=\"failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: can't copy bootstrap data to pipe: write init-p: broken\n</code></pre> <p>From Rancher pod logs</p> <pre><code>{\"Code\":\"Forbidden\",\"Status\":403},\"Message\":\"clusters.management.cattle.io \\\"c-m-f5lgdss6\\\" is forbidden: User \\\"u-z72j43l5aq\\\" cannot get resource \\\"clusters\\\" in API group \\\"management.cattle.io\\\" at the cluster scope\",\"Cause\":null,\"FieldName\":\"\"} (get nodes)\n</code></pre>"},{"location":"kbs/000021691/#resolution","title":"Resolution","text":"<p>Troubleshooting Steps:</p> <ol> <li>Verify whether SELinux is enabled on the nodes. If RKE2 SELinux is enabled, it should be configured according to the documentation, including the\u00a0<code>rke2-selinux</code> package. If SELinux is enforcing, it may block access to certain files. Reference:\u00a0https://docs.rke2.io/install/methods#rpm https://ranchermanager.docs.rancher.com/reference-guides/rancher-security/selinux-rpm/about-rke2-selinux</li> <li>Verify whether the operating system firewalld or firewall is enabled or present in the environment. If the firewall is configured, make sure the correct rules are applied.</li> <li>Verify whether the NetworkManager is enabled on the operating system. If so, make sure the correct methods are followed. Reference:\u00a0https://docs.rke2.io/known_issues</li> <li>Any proxy settings added to the environment, if yes, make sure the correct proxy and NO_PROXY values are set</li> <li>Check if any antivirus software is enabled or running on the nodes, it is advisable to disable it during the cluster creation. Container users may lack the necessary permissions to access files or directories to run the runc. Reference: https://www.suse.com/support/kb/doc/?id=000020477</li> <li>Check if security-related software is running which may block the container creation at runtime.</li> <li>Verify K8s PSA is enabled, if yes, make sure the correct policies are set.</li> <li>Review the registry settings, and make sure the nodes can communicate to the registry. If registry mirrors are enabled, make sure the correct endpoints are set. container user lacks the necessary permissions to access files or directories within the container image.</li> <li>Verify CIS profiles are enabled, if yes, make sure correct security contexts are set. Reference:\u00a0https://docs.rke2.io/security/hardening_guide</li> <li>Review the host resource limitations, if the container attempts to use more memory or CPU than allocated on the host, causing the container to fail to start.</li> </ol>"},{"location":"kbs/000021691/#cause","title":"Cause","text":"<p>Concerning 'runc' container/OCI runtime error typically occurs when there is a problem with the configuration or execution of a container using the 'runc' due to not accessing necessary resources, or experiencing permission errors due to incorrect container image settings or host system configurations or incorrect permissions on container files.</p>"},{"location":"kbs/000021691/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021691/#additional-information","title":"Additional Information","text":"<p>Please feel free to contact SUSE support if the issue persists.</p>"},{"location":"kbs/000021691/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021703/","title":"How to configure custom kube-scheduler profiles in a Rancher-provisioned RKE2 cluster","text":"<p>This document (000021703) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021703/#environment","title":"Environment","text":"<ul> <li>Rancher v2.6+</li> <li>A Rancher-provisioned RKE2 cluster</li> </ul>"},{"location":"kbs/000021703/#situation","title":"Situation","text":"<p>This article details how to configure the Kubernetes scheduler by injecting custom kube-scheduler profiles in a Rancher-provisioned RKE2 cluster.</p>"},{"location":"kbs/000021703/#resolution","title":"Resolution","text":"<p>This solution employs a ConfigMap and\u00a0<code>machineSelectorFiles</code> to deploy the necessary kube-scheduler configuration to the control-plane nodes within a Rancher-provisioned RKE2 cluster. The following steps are needed:</p> <ol> <li>ConfigMap for kube-scheduler configuration:</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n\u00a0 name: kube-scheduler-config\n\u00a0 namespace: fleet-default\n\u00a0 annotations:\n\u00a0 \u00a0 rke.cattle.io/object-authorized-for-clusters: rke2custom\ndata:\n\u00a0 config: |\n\u00a0 \u00a0 apiVersion: kubescheduler.config.k8s.io/v1\n\u00a0 \u00a0 kind: KubeSchedulerConfiguration\n\u00a0 \u00a0 clientConnection:\n\u00a0 \u00a0 \u00a0 kubeconfig: /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig\n\u00a0 \u00a0 profiles:\n\u00a0 \u00a0 - schedulerName: default-scheduler\n\u00a0 \u00a0 - schedulerName: no-scoring-scheduler\n\u00a0 \u00a0 \u00a0 plugins:\n\u00a0 \u00a0 \u00a0 \u00a0 preScore:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 disabled:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: '*'\n  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 score:\n</code></pre> <ol> <li>Referencing the ConfigMap in the RKE2 cluster configuration: This is done within the <code>cluster.provisioning.cattle.io rkeConfig</code>.</li> </ol> <pre><code>machineSelectorFiles:\n\u00a0 - fileSources:\n\u00a0 \u00a0 - configMap:\n\u00a0 \u00a0 \u00a0 items:\n\u00a0 \u00a0 \u00a0 \u00a0- key: config\n\u00a0 \u00a0 \u00a0 \u00a0 path: /kube-scheduler-config\n\u00a0 \u00a0 \u00a0 \u00a0 permissions: '644'\n\u00a0 \u00a0 \u00a0 name: kube-scheduler-config\n\u00a0 \u00a0machineLabelSelector:\n\u00a0 \u00a0 matchLabels:\n\u00a0 \u00a0 \u00a0rke.cattle.io/control-plane-role: 'true'\n\u00a0machineSelectorConfig:\n\u00a0 - config:\n\u00a0 \u00a0 kube-scheduler-arg: 'config=/kube-scheduler-config'\n\u00a0 \u00a0machineLabelSelector:\n\u00a0 \u00a0 matchLabels:\n  \u00a0 \u00a0rke.cattle.io/control-plane-role: 'true'\n</code></pre> <p>Note: The\u00a0<code>/kube-scheduler-config</code> path is arbitrary and was used for testing purposes</p> <pre><code>\n</code></pre>"},{"location":"kbs/000021703/#cause","title":"Cause","text":"<p>To customise the Kubernetes scheduler in an RKE2 cluster a custom kube-scheduler configuration can be injected to achieve this desired result.</p>"},{"location":"kbs/000021703/#additional-information","title":"Additional Information","text":"<ul> <li> <p>Key Considerations:</p> </li> <li> <p>The\u00a0<code>clientConnection.kubeconfig</code> configuration is essential, as\u00a0the presence of the <code>--config</code> parameter in the kube-scheduler arguments overrides the <code>--kubeconfig</code> parameter automatically configured by RKE2.</p> </li> <li>Do not manually add the <code>/kube-scheduler-config</code> file as an additional mount via <code>kube-apiserver-extra-mount</code>. RKE2 handles this automatically when referenced in the arguments. Adding it manually will result in a double-mount attempt and kube-scheduler pod startup failures.</li> <li>Configuring custom scheduling policies like\u00a0<code>NodeResourcesBalancedAllocation</code> is possible in RKE2. However, creating and troubleshooting such policies are outside the standard support scope. For tailored guidance, users should contact their CSM for consulting services.</li> <li>References:</li> <li>Kubernetes Scheduler Configuration</li> <li>RKE2 Server Configuration</li> <li>Rancher RKE2 Cluster Configuration - machineSelectorFiles</li> <li>Rancher RKE2 Cluster Configuration - machineSelectorConfig</li> </ul>"},{"location":"kbs/000021703/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021706/","title":"How to change the memlock value for pods in an RKE2 or K3s cluster","text":"<p>This document (000021706) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021706/#environment","title":"Environment","text":"<p>RKE2</p> <p>K3s</p>"},{"location":"kbs/000021706/#resolution","title":"Resolution","text":"<p>This can be set by modifying the RKE2 systemd unit file on the node. Here is an example showing how to change the memlock value to unlimited on an RKE2 worker node:</p> <ol> <li>SSH into the node</li> <li>Run the command:</li> </ol> <pre><code>sudo systemctl edit rke2-agent\n</code></pre> <p>and modify the start of the file so it looks like:</p> <pre><code>### Editing /etc/systemd/system/rke2-agent.service.d/override.conf\n### Anything between here and the comment below will become the new contents of the file\n[Service]\nLimitMEMLOCK=infinity\n</code></pre> <ol> <li>Once you save, it will create an override.conf file with the new setting under /etc/systemd/system/rke2-agent.service.d/</li> <li>run the following commandto apply the changes:</li> </ol> <pre><code>sudo systemctl restart rke2-agent.service\n</code></pre> <p>Now any new pod running on this node will have memlock set to unlimited. Which you can verify if you exec into the pod and run ulimit -l</p>"},{"location":"kbs/000021706/#notes","title":"Notes:","text":"<ul> <li>For a K3s cluster, the steps are the same except the name of the systemd unit file that needs to be modified in step #2 would be k3s-agent and the name of the service in step #4 would be\u00a0k3s-agent.service.</li> </ul>"},{"location":"kbs/000021706/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021708/","title":"Rancher-backup app stuck in uninstalling state","text":"<p>This document (000021708) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021708/#environment","title":"Environment","text":"<p>SUSE Rancher 2.10.x</p>"},{"location":"kbs/000021708/#situation","title":"Situation","text":"<p>Uninstalling rancher-backup app results in the error below:</p> <p><code>Error: failed to delete release: rancher-backup</code></p>"},{"location":"kbs/000021708/#resolution","title":"Resolution","text":"<ul> <li>Always delete the \"rancher-backup\" app first and then delete \"rancher-backup-crd\" when uninstalling the app. Deleting the CRD first might result in this issue.</li> <li>As a workaround delete the relevant secret \"sh.helm.release.v1.rancher-backup.v1\" in \"cattle-resources-system\" namespace which should delete the app stuck in the \"Uninstalling\" state.</li> <li>Then delete \"cattle-resource-system\" namespace:</li> </ul> <p><code>#kubectl delete ns cattle-resources-system</code></p>"},{"location":"kbs/000021708/#cause","title":"Cause","text":"<p>\"rancher-backup-crd\" was deleted before deleting \"rancher-backup\" helm chart.</p>"},{"location":"kbs/000021708/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021709/","title":"Why permissive profile is not seen with CIS benchmark version v1.9?","text":"<p>This document (000021709) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021709/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher 2.10.x</li> <li>RKE2 v1.27.x and above</li> </ul>"},{"location":"kbs/000021709/#situation","title":"Situation","text":"<p>Only one profile \"rke2-cis-1.9-profile\" is seen with CIS Benchmark app versions 1.9 and above, there aren't any additional profiles like the permissive profile.</p>"},{"location":"kbs/000021709/#resolution","title":"Resolution","text":"<p>Starting from CIS-1.9, there will be only one profile called \"rke2-cis-1.9-profile\" which covers all use cases.\u00a0Permissive profiles have been removed, and we now work with a single profile that is meant to be the hardened one. All required checks must be enforced to pass the CIS using the hardening guide.\u00a0Please find the relevant information here: https://github.com/rancher/rancher/issues/46881</p> <p>Please note \"rke2-cis-1.9-profile\" should be used for all hardened/non-hardened RKE2 clusters with version 1.27 and above.</p>"},{"location":"kbs/000021709/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021712/","title":"Enabling ServiceMonitors for fluentd and fluentbit in rancher-logging","text":"<p>This document (000021712) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021712/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher v2.9+</li> <li>rancher-monitoring installed</li> <li>rancher-logging installed</li> </ul>"},{"location":"kbs/000021712/#situation","title":"Situation","text":"<p>There are several ways of enabling ServiceMonitors in the rancher-logging helm chart, for both fluentd and fluent bit. This article details the correct configuration to ensure the ServiceMonitors are enabled for all logging instances.</p>"},{"location":"kbs/000021712/#resolution","title":"Resolution","text":"<p>Rancher v2.10+</p> <p>In Rancher v2.10+, with rancher-logging 105.0.0+up4.8.0 and above, ServiceMonitors can be configured via the fluentd.metrics.serviceMonitor and fluentbit.metrics.serviceMonitor values of the rancher-logging Helm Chart:</p> <pre><code>fluentd:\n  metrics:\n    serviceMonitor: true\nfluentbit:\n  metrics:\n    serviceMonitor: true\n</code></pre> <p>Rancher v2.9</p> <p>In Rancher v2.9 an overlay configuration is needed for it to work correctly and expose the metrics offered by both fluentd and fluent bit. To do so, the following parameters have to be defined in the values of the rancher-logging Helm Chart:</p> <pre><code>loggingOverlay:\n  spec:\n    fluentd:\n      metrics:\n        serviceMonitor: true\n\nfluentbitAgentOverlay:\n  spec:\n    metrics:\n      serviceMonitor: true\n</code></pre> <p>After upgrading the chart with these values, the metrics will start to appear in rancher-monitoring.</p>"},{"location":"kbs/000021712/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021719/","title":"Failed snapshot restore for RKE2 clusters with misplaced mountpoint for the RKE2 binaries/config files","text":"<p>This document (000021719) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021719/#environment","title":"Environment","text":"<p>A standalone or Rancher-provisioned RKE2 cluster, in which a rollback is performed from:</p> <ul> <li>a v1.22 patch version &gt;= v1.22.16+rke2r1,</li> <li>a v1.23 patch version &gt;= v1.23.15+rke2r1,</li> <li>a v1.24 patch version &gt;= v1.24.9,</li> <li>a v1.25 patch version &gt;= v1.25.5+rke2r1,</li> <li>or any RKE2 release &gt;= v1.26.0+rke2r1</li> </ul> <p>to:</p> <ul> <li>a v1.22 patch version &lt; v1.22.16+rke2r1,</li> <li>a v1.23 patch version &lt; v1.23.15+rke2r1,</li> <li>a v1.24 patch version &lt; v1.24.9,</li> <li>a v1.25 patch version &lt; v1.25.5+rke2r1,</li> <li>or any RKE2 release &lt;= v1.21.14+rke2r1</li> </ul>"},{"location":"kbs/000021719/#situation","title":"Situation","text":"<p>The etcd snapshot restore will fail.</p>"},{"location":"kbs/000021719/#resolution","title":"Resolution","text":"<p>Workaround:</p> <p>Execute on all nodes of the cluster before initiating the restore.</p> <pre><code>rm /etc/systemd/system/rke2-*.service\nsystemctl daemon-reload\n</code></pre>"},{"location":"kbs/000021719/#cause","title":"Cause","text":"<ul> <li>The RKE2 install script for v1.24.4 does not install to\u00a0/opt/rke2 if /usr/local is on a separate mount.</li> <li>The RKE2 install script for v1.24.17 does install to /opt/rke2 if /usr/local is on a separate mount.</li> <li>As part of the copy process, the systemd unit files are stored into an override in /etc/system/systemd, with /opt/rke2 defined in the exec path</li> <li>When you rollback from v1.24.17 to v1.24.4, the binaries are written to /usr/local, but the systemd unit file override remains in place pointing to /opt/rke2 and the v1.24.17 binaries. The restore will fail as a result of not finding the right binaries and systemd config files.</li> <li>This behaviour is the result of an earlier bug in the RKE2 install script\u00a0that was fixed in the following script.</li> </ul>"},{"location":"kbs/000021719/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021723/","title":"Fleet agent fails to start with error secret fleet-agent not found","text":"<p>This document (000021723) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021723/#environment","title":"Environment","text":"<p>Rancher\u00a0 2.8, 2.9</p>"},{"location":"kbs/000021723/#situation","title":"Situation","text":"<p>After Rancher upgrade, the fleet agent fails to start with the following error messages:</p> <pre><code>leaderelection.go:248] attempting to acquire leader lease cattle-fleet-system/fleet-agent-lock...\nleaderelection.go:258] successfully acquired lease cattle-fleet-system/fleet-agent-lock\ntime=\"2024-03-21T16:02:37Z\" level=warning msg=\"Cannot find fleet-agent secret, running registration\"\npanic: assignment to entry in nil map\n(*Command).execute\\n\\t/home/runner/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985\\ngithub.com/spf13/cobra.\n(*Command).Execute\\n\\t/home/runner/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041\\ngithub.com/spf13/cobra.\n</code></pre>"},{"location":"kbs/000021723/#resolution","title":"Resolution","text":"<p>In the Rancher UI:</p> <ol> <li> <p>Click\u00a0\u2630 &gt; Continuous Delivery.</p> </li> <li> <p>Select your namespace at the top of the menu, noting the following:</p> </li> <li> <p>By default,\u00a0fleet-default\u00a0is selected which includes all downstream clusters that are registered through Rancher.</p> </li> <li> <p>You may switch to\u00a0fleet-local, which only contains the\u00a0local\u00a0cluster, or you may create your own workspace to which you may assign and move clusters.</p> </li> <li>Click on Clusters on the left navigation bar and\u00a0 Force Update</li> </ol>"},{"location":"kbs/000021723/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021724/","title":"Activating CIS on the Rancher local cluster breaks Rancher related deployments","text":"<p>This document (000021724) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021724/#environment","title":"Environment","text":"<p>Rancher 2.8+</p>"},{"location":"kbs/000021724/#situation","title":"Situation","text":"<p>You have applied a CIS profile to an RKE2 Rancher local cluster, and your Rancher and Fleet deployments are unable to scale pods, displaying this error message:</p> <pre><code>Error creating: pods \"rancher-677d78b948-d7m9s\" is forbidden: violates PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (containers \"rancher\", \"rancher-audit-log\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \"rancher\", \"rancher-audit-log\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or containers \"rancher\", \"rancher-audit-log\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers \"rancher\", \"rancher-audit-log\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\").\n</code></pre> <p>Another detected symptom is the impossibility to connect on the local cluster through the Kubectl UI shell.</p>"},{"location":"kbs/000021724/#resolution","title":"Resolution","text":"<p>Copy this PSA from Rancher security guide to for example <code>/etc/rancher/rke2/rke2-pss_rancher.yaml</code> and add the <code>pod-security-admission-config-file</code> variable pointing to this file to ensure that Kubelet applies it.</p> <p>RKE2 configuration file will then looks like this:</p> <pre><code># cat /etc/rancher/rke2/config.yaml\nprofile: cis\npod-security-admission-config-file: \"/etc/rancher/rke2/rke2-pss_rancher.yaml\"\n</code></pre> <p>Restart <code>rke2-server</code> systemd service for this new configuration to take effect:</p> <pre><code>systemctl restart rke2-server\n</code></pre> <p>To ensure that your new PSA configuration is applied by Kubelet, you have two options:</p> <ul> <li>Check the kubelet process arguments:</li> </ul> <pre><code># ps -ef | grep [a]dmission-control-config-file\nroot        3633    3580  6 09:44 ?        00:02:28 kube-apiserver --admission-control-config-file=/etc/rancher/rke2/rke2-pss.yaml --audit-policy-file=/etc/rancher/rke2/audit-policy.yaml --audit-log-maxage=30 --audit-log-maxbackup=10 --audit-log-maxsize=100 --audit-log-path=/var/lib/rancher/rke2/server/logs/audit.log --admission-control-config-file=/etc/rancher/rke2/config/rancher-psact.yaml --allow-privileged=true ...\n</code></pre> <p>If multiple <code>--admission-control-config-file</code> options are provided, the last one will take precedence and be applied.</p> <ul> <li>Create a dummy pod in the <code>cattle-system</code> namespace:</li> </ul> <pre><code>$ kubectl run dummy-pod --image='busybox' -n cattle-system\npod/dummy-pod created\n</code></pre> <p>If the correct PSA is not applied, pod creation in the <code>cattle-system</code> namespace will be forbidden.</p>"},{"location":"kbs/000021724/#cause","title":"Cause","text":"<p>The PSA exemptions on namespace pushed by RKE2 on file <code>/etc/rancher/rke2/rke2-pss.yaml</code> are not enough to let Rancher components to correctly work.</p> <p>You can find in the Rancher security guide the correct PodSecurityConfiguration containing all the required Rancher namespace exemptions for a rancher-restricted cluster to run properly.</p>"},{"location":"kbs/000021724/#additional-information","title":"Additional Information","text":"<p>Link to PSA Rancher exemption:</p> <p>https://ranchermanager.docs.rancher.com/reference-guides/rancher-security/psa-restricted-exemptions</p>"},{"location":"kbs/000021724/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021736/","title":"How to query Rancher API tokens information via kubectl","text":"<p>This document (000021736) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021736/#environment","title":"Environment","text":"<ul> <li>A Rancher v2.x instance</li> <li>A Rancher admin kubeconfig sourced</li> </ul>"},{"location":"kbs/000021736/#situation","title":"Situation","text":"<p>This article provides detailed information about how to query Rancher API tokens using the kubectl command line.</p>"},{"location":"kbs/000021736/#resolution","title":"Resolution","text":"<p>The API tokens' information can be retrieved via the <code>tokens.management.cattle.io</code> custom resources in the Rancher local cluster.</p> <p>Warning: the \"isDerived: false\" tokens are temporary UI session tokens and are not intended to be manually operated or modified. Please only use derived tokens.</p> <p>For example, if you want to view all the fields that can be accessed in the API tokens via kubectl, you can obtain a detailed list by using the following command:</p> <p>``</p> <pre><code>kubectl get tokens.management.cattle.io -o jsonpath='{.items[]}' | jq keys\n</code></pre> <p>The expected fields are: <code>apiVersion, authProvider, current, description, expired, expiresAt, isDerived, kind, lastUsedAt, metadata, token, ttl, userId, userPrincipal</code>.</p> <p>By consulting or modifying these values, we can get or edit information from the API tokens using the kubectl command line. For example, a possible kubectl command to check the API tokens in our cluster with their description and expiration information could be:</p> <p>``</p> <pre><code>kubectl get tokens.management.cattle.io -o custom-columns=Name:'{.metadata.name},Description:{.description},isDerived:{.isDerived},TTL:{.ttl},expired:{.expired},expiresAt:{.expiresAt}'\n</code></pre>"},{"location":"kbs/000021736/#cause","title":"Cause","text":"<p>The possibility of viewing and modifying API tokens via kubectl offers the opportunity to automate periodic operations or checks in your cluster.</p> <p>For example, as a good security practice, it may be interesting to track and remove certain\u00a0<code>ttl=0</code> tokens if they are not used. API tokens with <code>ttl=0</code> never expire unless you invalidate them.</p>"},{"location":"kbs/000021736/#additional-information","title":"Additional Information","text":"<p>Rancher API Token Documentation</p>"},{"location":"kbs/000021736/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021743/","title":"Resolving \"Error: Failed to install provider\" with Rancher2 Provider (openpgp: unsupported feature: public key algorithm 22)","text":"<p>This document (000021743) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021743/#environment","title":"Environment","text":"<p>Rancher2 Terraform Provider</p>"},{"location":"kbs/000021743/#situation","title":"Situation","text":"<p>Users attempting to initialize or apply Terraform configurations using the <code>rancher2</code> provider (version <code>~&gt; 6.0.0</code>) may encounter the following error:</p> <pre><code>Error: Failed to install provider\n\nError while installing rancher/rancher2 v6.1.4: error decoding signing key:\nopenpgp: unsupported feature: public key algorithm 22\n</code></pre>"},{"location":"kbs/000021743/#resolution","title":"Resolution","text":"<p>The underlying issues have been resolved. The Rancher team has addressed the key management and signing algorithm problems.</p> <p>For users who encountered this issue:</p> <ul> <li>Upgrade Terraform: Ensure you are using Terraform version v1.5.7 or later. This version includes support for the necessary signing algorithm.</li> <li>To check your Terraform version, run: <code>$ terraform --version</code></li> <li>To upgrade Terraform, follow the official HashiCorp documentation.</li> <li>Re-initialize Terraform: After upgrading Terraform, run <code>terraform init</code> to initialise.</li> </ul>"},{"location":"kbs/000021743/#cause","title":"Cause","text":"<p>This issue stems from two related incidents affecting the signing and validation of the <code>rancher2</code> provider:</p> <ol> <li>An initial issue arose due to a key management issue within HashiCorp's systems, combined with a Terraform update that invalidated the existing Rancher signing key. This prevented the provider release from being successfully signed.</li> <li>Subsequently, the Rancher signing key was cross-signed using an algorithm (specifically, algorithm \u00a0ed25519) that older versions of Terraform do not support. This incompatibility caused validation failures when older Terraform versions attempted to verify the provider's signature.</li> </ol>"},{"location":"kbs/000021743/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021743/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021744/","title":"How to enable and query the supervisor loadbalancer metrics in an RKE2 or K3s cluster","text":"<p>This document (000021744) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021744/#environment","title":"Environment","text":"<p>A Rancher-provisioned or standalone RKE2 or K3s cluster; running RKE2 v1.29.12+rke2r1, v1.30.8+rke2r1, v1.31.4+rke2r1 or above; or K3s v1.29.12+k3s1, v1.30.8+k3s1, v1.31.4+k3s1 or above</p>"},{"location":"kbs/000021744/#situation","title":"Situation","text":"<p>The supervisor loadbalancer in RKE2 and K3s clusters loadbalances kube-apiserver, etcd and supervisor traffic between cluster nodes. More information on the supervisor process and load-balancing can be found in the documentation here. This article provides instructions on how to query metrics for this loadbalancer process.</p>"},{"location":"kbs/000021744/#resolution","title":"Resolution","text":"<p>The presence of the loadbalancer depends upon the node's roles, as below:</p> <ul> <li>An all role, or controlplane and etcd role server node will have no loadbalancers, since it has everything locally.</li> <li>A control-plane role only server node will have a loadbalancer to connect to etcd nodes.</li> <li>An etcd role only server node will have a loadbalancer to connect to control-plane nodes.</li> <li>An agent (worker role only) node will have a loadbalancer to connect to control-plane nodes.</li> </ul> <p>The command for querying the loadbalancer metrics depends upon the cluster and node type, as detailed in the following instructions.</p>"},{"location":"kbs/000021744/#rke2","title":"RKE2","text":"<p>In an RKE2 cluster the supervisor's loadbalancer metrics are exposed on nodes via the supervisor port (9345).</p> <p>To query the loadbalancer metrics in an RKE2 cluster:</p> <ol> <li>Enable the supervisor metrics in the RKE2 configuration:</li> <li> <p>For a standalone RKE2 cluster node: Edit the configuration file on the node under /etc/rancher/rke2/config.yaml, add <code>supervisor-metrics: true and restart the rke2-server/rke2-agent process, depending upon the node type (`systemctl restart rke2-server` or `systemctl restart rke2-agent`)</code></p> <pre><code>## sample /etc/rancher/rke2/config.yaml\n[...]\nsupervisor-metrics: true\n[...]\n</code></pre> <p>``</p> </li> <li> <p>For a Rancher-provisioned RKE2 cluster: in the Cluster Management interface of the Rancher UI, click Edit YAML for the applicable cluster, add the configuration \" supervisor-metrics: true\" into the machineGlobalConfig block, and click Save</p> <p><pre><code>[...]\n    machineGlobalConfig:\n      supervisor-metrics: true\n[...]\n</code></pre> 2. Check the metrics are enabled: On an all role or control-plane role only node in the cluster (not via the Rancher-proxied Kubernetes API endpoint or Authorized Cluster Endpoint), query the metrics per the example below. Replace  with the IP of the node you wish to query. N.B. The node from which you are running the command will need to be able to reach port 9345 on the node you are querying. <pre><code>export KUBECONFIG=/etc/rancher/rke2/rke2.yaml\nalias kubectl=/var/lib/rancher/rke2/bin/kubectl\nkubectl get --server https://&lt;node-ip&gt;:9345 --raw /metrics | grep load\n# HELP rke2_loadbalancer_dial_duration_seconds Time taken to dial a connection to a backend server\n# TYPE rke2_loadbalancer_dial_duration_seconds histogram\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.001\"} 13\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.002\"} 22\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.004\"} 51\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.008\"} 79\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.016\"} 123\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.032\"} 154\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.064\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.128\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.256\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"0.512\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"1.024\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"2.048\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"4.096\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"8.192\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"16.384\"} 163\nrke2_loadbalancer_dial_duration_seconds_bucket{name=\"rke2-etcd-server-load-balancer\",status=\"success\",le=\"+Inf\"} 163\nrke2_loadbalancer_dial_duration_seconds_sum{name=\"rke2-etcd-server-load-balancer\",status=\"success\"} 1.7390672159999991\nrke2_loadbalancer_dial_duration_seconds_count{name=\"rke2-etcd-server-load-balancer\",status=\"success\"} 163\n# HELP rke2_loadbalancer_server_connections Count of current connections to loadbalancer server\n# TYPE rke2_loadbalancer_server_connections gauge\nrke2_loadbalancer_server_connections{name=\"rke2-etcd-server-load-balancer\",server=\"24.199.104.66:2379\"} 0\nrke2_loadbalancer_server_connections{name=\"rke2-etcd-server-load-balancer\",server=\"24.199.96.251:2379\"} 0\nrke2_loadbalancer_server_connections{name=\"rke2-etcd-server-load-balancer\",server=\"64.23.213.163:2379\"} 153\n# HELP rke2_loadbalancer_server_health Current health value of loadbalancer server\n# TYPE rke2_loadbalancer_server_health gauge\nrke2_loadbalancer_server_health{name=\"rke2-etcd-server-load-balancer\",server=\"24.199.104.66:2379\"} 5\nrke2_loadbalancer_server_health{name=\"rke2-etcd-server-load-balancer\",server=\"24.199.96.251:2379\"} 5\nrke2_loadbalancer_server_health{name=\"rke2-etcd-server-load-balancer\",server=\"64.23.213.163:2379\"} 7\n</code></pre>"},{"location":"kbs/000021744/#k3s","title":"K3s","text":"<p>In a K3s cluster the loadbalancer metrics are exposed on agent (worker role only) nodes via the kubelet metrics port (10250) and on server nodes via the kube-apiserver port (6443).</p> <p>To query the loadbalancer metrics in a K3s cluster:</p> <ol> <li>Enable the supervisor metrics in the K3s configuration: </li> <li> <p>For a standalone K3s cluster node: Edit the configuration file on the node under /etc/rancher/k3s/config.yaml, add\u00a0<code>supervisor-metrics: true and</code> restart the K3s service <code>(`systemctl restart k3s`)</code></p> <pre><code>## sample /etc/rancher/k3s/config.yaml\n[...]\nsupervisor-metrics: true\n[...]\n</code></pre> </li> <li> <p>For a Rancher-provisioned K3s cluster: in the\u00a0Cluster Management interface of the Rancher UI, click Edit YAML for the applicable cluster, add the configuration \" supervisor-metrics: true\" into the machineGlobalConfig block, and click Save</p> <p><pre><code>[...]\n    machineGlobalConfig:\n      supervisor-metrics: true\n[...]\n</code></pre> 2. Check the metrics are enabled: On an a server node in the cluster (not via the Rancher-proxied Kubernetes API endpoint or Authorized Cluster Endpoint), query the metrics per the example below. Replace  with the IP of the node you wish to query and  with 10250 if querying an agent node, or 6443 if querying a server node. N.B. The node from which you are running the command will need to be able to reach  on the node you are querying. <pre><code>kubectl get --server https://&lt;node-ip&gt;:&lt;port&gt; --raw /metrics |grep -i k3s_load\n# HELP k3s_loadbalancer_dial_duration_seconds Time taken to dial a connection to a backend server\n# TYPE k3s_loadbalancer_dial_duration_seconds histogram\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.001\"} 218\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.002\"} 239\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.004\"} 253\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.008\"} 264\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.016\"} 278\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.032\"} 290\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.064\"} 293\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.128\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.256\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"0.512\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"1.024\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"2.048\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"4.096\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"8.192\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"16.384\"} 294\nk3s_loadbalancer_dial_duration_seconds_bucket{name=\"k3s-etcd-server-load-balancer\",status=\"success\",le=\"+Inf\"} 294\nk3s_loadbalancer_dial_duration_seconds_sum{name=\"k3s-etcd-server-load-balancer\",status=\"success\"} 0.8818124119999996\nk3s_loadbalancer_dial_duration_seconds_count{name=\"k3s-etcd-server-load-balancer\",status=\"success\"} 294\n# HELP k3s_loadbalancer_server_connections Count of current connections to loadbalancer server\n# TYPE k3s_loadbalancer_server_connections gauge\nk3s_loadbalancer_server_connections{name=\"k3s-etcd-server-load-balancer\",server=\"164.92.125.58:2379\"} 102\n# HELP k3s_loadbalancer_server_health Current health value of loadbalancer server\n# TYPE k3s_loadbalancer_server_health gauge\nk3s_loadbalancer_server_health{name=\"k3s-etcd-server-load-balancer\",server=\"164.92.125.58:2379\"} 7\n</code></pre>"},{"location":"kbs/000021744/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021747/","title":"Reasons to avoid PV/PVC as opposed to S3 as a storage location for the Rancher Backup Operator","text":"<p>This document (000021747) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021747/#environment","title":"Environment","text":"<p>Rancher 2.x</p> <p>Rancher Backup Operator</p>"},{"location":"kbs/000021747/#situation","title":"Situation","text":"<p>Choosing a Persistent Volume\u00a0 over a S3 bucket as the storage location for the Rancher Backup Operator.</p>"},{"location":"kbs/000021747/#resolution","title":"Resolution","text":"<p>Using the Rancher Backup Operator with a PVC is not very intuitive from a recovery point of view, as the PVs are tied to the Kubernetes cluster.</p> <p>If that cluster is lost, a detached PV is left, that needs to be attached to another host to copy the Rancher Backup Operator snapshots out of, and then copy into a PV on a freshly created cluster.</p> <p>If a S3 endpoint is not readily available, it would be easier to use the local Rancher clusters own RKE2 etcd snapshots, which can be copied off the nodes to a different location via a simple cronjob on the hosts themselves.</p>"},{"location":"kbs/000021747/#additional-information","title":"Additional Information","text":"<p>Backup Operator:</p> <p>https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/backup-restore-and-disaster-recovery/back-up-restore-usage-guide</p> <p>Cluster Snapshot backup:</p> <p>https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/backup-restore-and-disaster-recovery/back-up-rancher-launched-kubernetes-clusters</p>"},{"location":"kbs/000021747/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021756/","title":"Nginx \"IngressNightmare\"","text":"<p>This document (000021756) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021756/#environment","title":"Environment","text":"<p>The vulnerability affects the ingress-nginx component. If you do not have it installed on your cluster, then you are not affected.</p> <p>If you are using RKE1, you can check if you have ingress-nginx running with:</p> <pre><code>kubectl get pods --all-namespaces --selector app.kubernetes.io/name=ingress-nginx\n</code></pre> <p>If you are using RKE2, then the command will look similar, but the name will change from ingress-nginx to rke2-ingress-nginx:</p> <pre><code>kubectl get pods --all-namespaces --selector app.kubernetes.io/name=rke2-ingress-nginx\n</code></pre> <p>The affected versions of ingress-nginx are:</p> <ul> <li>All versions prior to v1.11.4 (included)</li> <li>v1.12.0</li> </ul> <p>Harvester Users: If you are using Harvester, please refer to CVE-2025-1974: ingress-nginx admission controller RCE escalation for additional information.</p>"},{"location":"kbs/000021756/#situation","title":"Situation","text":"<p>Context The upstream Kubernetes project announced, on March 24th of 2025, five different security vulnerabilities CVEs in the ingress-nginx component.</p> <p>In this KB article, we will address these five CVE\u2019s with mitigation steps and provide information for released Kubernetes versions, containing the latest ingress-nginx version for RKE1 and RKE2.</p> <p>See the additional information section at the end of this document for all the relevant links for the vulnerabilities. Kubernetes also released a\u00a0blog post to provide context about the issues in question.</p>"},{"location":"kbs/000021756/#resolution","title":"Resolution","text":"<p>Common resolution</p> <p>RKE2 users can upgrade to one of the following Kubernetes versions:</p> <ul> <li>v1.32.3+rke2r1 https://github.com/rancher/rke2/releases/tag/v1.32.3%2Brke2r1</li> <li>v1.31.7+rke2r1\u00a0https://github.com/rancher/rke2/releases/tag/v1.31.7%2Brke2r1</li> <li>v1.30.11+rke2r1\u00a0https://github.com/rancher/rke2/releases/tag/v1.30.11%2Brke2r1</li> <li>v1.29.15+rke2r1\u00a0https://github.com/rancher/rke2/releases/tag/v1.29.15%2Brke2r1</li> </ul> <p>For RKE1 CLI users, please find the following versions to include the ingress-nginx patches:</p> <ul> <li>1.6.9: https://github.com/rancher/rke/releases/tag/v1.6.9</li> <li>1.7.5: https://github.com/rancher/rke/releases/tag/v1.7.5</li> <li>1.8.1: https://github.com/rancher/rke/releases/tag/v1.8.1</li> </ul> <p>If you are not using the RKE1 CLI tool (binary), please ensure that you are upgrading to one of these Kubernetes versions to include the latest ingress-nginx version:</p> <ul> <li>v1.32.3-rancher1-1 or higher</li> <li>V1.31.7-rancher1-1</li> <li>v1.30.11-rancher1-1</li> <li>v1.29.15-rancher1-1</li> </ul> <p>Important Note: K3s, in its default configuration, is not affected by these vulnerabilities as it does not utilize ingress-nginx.</p> <p>If you are unable to upgrade now, SUSE strongly advises disabling the ingress-nginx admission webhooks to significantly mitigate the risk associated with CVE-2025-1974, CVE-2025-24513. Disabling these webhooks is a critical, albeit temporary, security measure.</p> <p>Workaround for CVE-2025-1974, CVE-2025-24513</p> <p>Immediate Action Required: Disable Admission Webhooks</p> <p>RKE2 Mitigation Steps:</p> <p>If you are using RKE2 and the built-in chart, a HelmChartConfig can be used to disable the controller admission webhook like this example:</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-ingress-nginx\n  namespace: kube-system\nspec:\n  valuesContent: |\n    controller:\n      admissionWebhooks:\n        enabled: false\n</code></pre> <p>The HelmChartConfig can be added for clusters provisioned by Rancher when editing the cluster in Cluster Management. This may be an option called \"Additional Manifests\" in the left-hand list, or nested under \"Add-On Config\" depending on the Rancher version.</p> <p>For standalone RKE2 clusters, the HelmChartConfig file can be populated on each rke2-server node in the /var/lib/rancher/rke2/server/manifests directory.</p> <p>RKE1 Mitigation Steps:</p> <p>If you are using RKE1 and wish to disable the admission webhook, then you can perform the following:</p> <p>Delete the ValidatingWebhookConfiguration called ingress-nginx-admission</p> <ul> <li>Edit the ingress-nginx-controller DaemonSet, and remove the --validating-webhook argument from the controller container.</li> <li>To further reduce the risk and mitigate CVE-2025-24514, add the --enable-annotation-validation=true argument to the ingress-nginx-controller DaemonSet</li> </ul> <p>Optional:</p> <p>To remove the service associated with the admission webhook, delete the service called ingress-nginx-controller-admission.</p> <p>Please note that the above changes with RKE1 will be overridden the next time the cluster is reconciled or upgraded.</p> <p>Mitigation for Directly Installed ingress-nginx:</p> <p>If you use the upstream ingress-nginx component (not installed by RKE1 or RKE2), then update it to v1.11.5, v1.12.1, or any later version.</p> <p>To upgrade, follow the instructions available at https://kubernetes.github.io/ingress-nginx/deploy/upgrade/ and the specific release notes for v1.12.1 and v1.11.5. Pay attention to the specific post-fix needed for CVE-2025-1974.</p> <p>If you can't upgrade right away, the following steps can be followed to disable the admission webhook:</p> <ul> <li>If you have installed ingress-nginx using Helm</li> <li>Reinstall, setting the Helm value controller.admissionWebhooks.enabled=false</li> <li>If you have installed ingress-nginx manually, apply the equivalent changes to your deployment method, or manually apply the following:</li> <li> <p>Delete the ValidatingWebhookConfiguration called ingress-nginx-admission</p> </li> <li> <p>Edit the ingress-nginx-controller Deployment or Daemonset, removing --validating-webhook from the controller container\u2019s argument list</p> </li> </ul> <p>Workaround for CVE-2025-24514</p> <p>Mitigation steps for RKE1/RKE2</p> <p>Please check if the enable-annotation-validation argument is true by default.</p> <p>For RKE1</p> <pre><code>kubectl -n ingress-nginx exec nginx-ingress-controller-xxxxx -- /nginx-ingress-controller --help 2&gt;&amp;1 | grep enable-annotation-validation\n</code></pre> <p>For RKE2</p> <pre><code>kubectl -n kube-system exec rke2-ingress-nginx-controller-xxxxx -- /nginx-ingress-controller --help 2&gt;&amp;1 | grep enable-annotation-validation\n</code></pre> <p>If the argument is set to true by default, no further change is needed.</p> <pre><code>--enable-annotation-validation\nIf true, will enable the annotation validation feature. Defaults to true (default true)\n</code></pre> <p>If this is true, this issue does not affect you. If it is not true, you can mitigate this issue by setting the enable-annotation-validation argument to true with the below steps:</p> <p>For RKE1:</p> <pre><code>kubectl -n ingress-nginx edit daemonset nginx-ingress-controller\n</code></pre> <p>Add the argument as below:</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n...\n        - --enable-annotation-validation=true # &lt;-- Add this line\n</code></pre> <p>For RKE2:</p> <p>Add a HelmChartConfig to change the argument.</p> <p>The HelmChartConfig can be added for clusters provisioned by Rancher when editing the cluster in Cluster Management. This may be an option called \"Additional Manifests\" in the left-hand list or nested under \"Add-On Config\" depending on the Rancher version.</p> <p>For standalone RKE2 clusters, the HelmChartConfig file can be populated on each rke2-server node in the /var/lib/rancher/rke2/server/manifests directory</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-ingress-nginx\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    controller:\n      enableAnnotationValidations: true\n</code></pre> <p>Workaround for CVE-2025-1097, CVE-2025-1098</p> <p>There is no workaround mentioned for these CVEs. Please simply upgrade your RKE1/RKE2 version.</p>"},{"location":"kbs/000021756/#cause","title":"Cause","text":"<p>Under certain conditions, an unauthenticated attacker with access to the pod network can achieve arbitrary code execution in the context of the ingress-nginx controller. This can lead to disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)</p> <p>Source: CVE-MITRE.org</p>"},{"location":"kbs/000021756/#additional-information","title":"Additional Information","text":"<p>Kubernetes upstream issues for the CVEs:</p> <p>CVE-2025-24513: https://github.com/kubernetes/kubernetes/issues/131005</p> <p>CVE-2025-24514: https://github.com/kubernetes/kubernetes/issues/131006</p> <p>CVE-2025-1097: https://github.com/kubernetes/kubernetes/issues/131007</p> <p>CVE-2025-1098: https://github.com/kubernetes/kubernetes/issues/131008</p> <p>CVE-2025-1974: https://github.com/kubernetes/kubernetes/issues/131009</p>"},{"location":"kbs/000021756/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021759/","title":"How to install Kiali, Istio, and Jaeger","text":"<p>This document (000021759) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021759/#environment","title":"Environment","text":"<p>SUSE Rancher 2.11.x+</p>"},{"location":"kbs/000021759/#situation","title":"Situation","text":"<p>Starting in Rancher v2.12, the\u00a0rancher-istio chart will be deprecated. This article will provide guidance on how to replicate the functionality of the rancher-istio chart in Rancher v2.12 and later using SUSE Application Collection</p>"},{"location":"kbs/000021759/#resolution","title":"Resolution","text":""},{"location":"kbs/000021759/#prerequisites","title":"Prerequisites","text":"<p>This document assumes you are familiar with how authentication works in Application Collection. It also assumes you have a running Kubernetes cluster, and kubectl and helm installed.</p>"},{"location":"kbs/000021759/#namespace-creation","title":"Namespace Creation","text":"<p>We will be installing the applications in the same namespace, in this case `istio-system`. Don't forget to also create the secret containing your Application Collection credentials.</p> <pre><code>kubectl create ns istio-system\nkubectl create secret docker-registry application-collection --docker-server=dp.apps.rancher.io --docker-username=&lt;username&gt; --docker-password=&lt;your_token&gt; -n istio-system\n</code></pre>"},{"location":"kbs/000021759/#installing-prometheus","title":"Installing Prometheus","text":"<p>Prometheus is a dependency of Kiali, so it needs to be installed first. You can check the compatibility here. You can install it using the helm chart provided by Application Collection. I.e:</p> <pre><code>helm install prometheus oci://dp.apps.rancher.io/charts/prometheus --version 25.30.2 --set global.imagePullSecrets={application-collection} -n istio-system --wait\n</code></pre>"},{"location":"kbs/000021759/#installing-kiali","title":"Installing Kiali","text":"<p>The next step is to install Kiali using Helm. You can follow the documentation from upstream, which can be found here, and the compatibility matrix can be found\u00a0here.) You also need to provide the correct value in the external_services field. i.e:</p> <pre><code>helm install kiali oci://dp.apps.rancher.io/charts/kiali --version 2.3.0 --set global.imagePullSecrets={application-collection} -n istio-system --set external_services.prometheus.custom_metrics_url=\"http://prometheus-server.istio-system.svc.cluster.local\" \\\n--set external_services.prometheus.url=\"http://prometheus-server.istio-system.svc.cluster.local\" --wait\n</code></pre>"},{"location":"kbs/000021759/#installing-istio","title":"Installing Istio","text":"<p>Now you can install Istio.</p> <p>In the case you already have an installation of rancher-istio, it should be noted that there is no migration path from it to an installation of Istio provided by SUSE Application Collection. In that case, the recommended way is to uninstall the rancher-istio helm chart first before following the rest of this guide. The Istio CRDs that remain after the uninstallation of rancher-istio should not be removed as they preserve the Istio control plane; deletion can lead to loss of your custom Istio resources.</p> <p>The reference guide can be found at:\u00a0https://docs.apps.rancher.io/reference-guides/istio/ and the compatibility matrix can be found here.</p>"},{"location":"kbs/000021759/#installing-jaeger","title":"Installing Jaeger","text":"<p>To install Jaeger, you need to deploy the Helm Chart which will deploy the operator and then create a new Jaeger resource to deploy the Jaeger instance you want.</p>"},{"location":"kbs/000021759/#installing-cert-manager","title":"Installing cert-manager","text":"<p>Cert manager is a dependency of Jaeger, and as such needs to be installed first. If you already have it installed with a version later than 1.6.1, please proceed to the next step.</p> <pre><code>kubectl create ns cert-manager\nkubectl create secret docker-registry application-collection --docker-server=dp.apps.rancher.io --docker-username=&lt;username&gt; --docker-password=&lt;your_password&gt; -n cert-manager\nhelm install cert-manager oci://dp.apps.rancher.io/charts/cert-manager --namespace cert-manager --version 1.16.0 --set global.imagePullSecrets={application-collection} --set crds.enabled=true --wait\n</code></pre>"},{"location":"kbs/000021759/#deploy-the-helm-chart","title":"Deploy the Helm Chart","text":"<p>You can use the Helm chart provided by Application Collection. To do that, you can do the following:</p> <pre><code>helm install jaeger oci://dp.apps.rancher.io/charts/jaeger-operator --set global.imagePullSecrets={application-collection} --set rbac.clusterRole=true -n istio-system --wait\n</code></pre>"},{"location":"kbs/000021759/#create-a-new-jaeger-resource","title":"Create a New Jaeger Resource","text":"<p>In this example, we are going to use jaeger-all-in-one which is more suitable for testing and development.</p> <p>The first thing needed is to create the service account that will be used.</p> <pre><code>kubectl create sa jaeger-test-sa -n istio-system\n</code></pre> <p>Now you need to add your Application Collection token to the service account:</p> <pre><code>kubectl patch serviceaccount -n istio-system jaeger-test-sa -p '{\"imagePullSecrets\": [{\"name\": \"application-collection\"}]}'\n</code></pre> <p>The next step is to create the Jaeger resource:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n name: jaeger-test\n namespace: istio-system\nspec:\n serviceAccount: jaeger-test-sa\nEOF\n</code></pre>"},{"location":"kbs/000021759/#configuring-istio-and-jaeger","title":"Configuring Istio and Jaeger","text":"<p>The final step is to configure Istio and Jaeger to work together. The documentation can be found at: https://istio.io/latest/docs/tasks/observability/distributed-tracing/jaeger/.</p> <p>First, create a `values.yaml` file like the following:</p> <pre><code>istiod:\n meshConfig:\n   enableTracing: true\n   defaultConfig:\n     tracing: {} # disable legacy MeshConfig tracing options\n   extensionProviders:\n   - name: jaeger\n     opentelemetry:\n       port: 4317\n       service: jaeger-test-collector.istio-system.svc.cluster.local\n</code></pre> <p>Now, update the Istio installation with the new values:</p> <pre><code>helm upgrade istio oci://dp.apps.rancher.io/charts/istio \\\n    --set global.imagePullSecrets={application-collection} \\\n    --values values.yaml -n istio-system --wait\n</code></pre> <p>Lastly, enable telemetry by creating a new telemetry resource:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: telemetry.istio.io/v1\nkind: Telemetry\nmetadata:\n  name: mesh-default\n  namespace: istio-system\nspec:\n  tracing:\n  - providers:\n    - name: jaeger\nEOF\n</code></pre>"},{"location":"kbs/000021759/#note","title":"Note","text":"<p>Istio, Kiali, and Jaeger are third party tools which are not supported by Rancher Support.</p>"},{"location":"kbs/000021759/#cause","title":"Cause","text":"<p>The rancher-istio chart is being deprecated in Rancher v2.12 and later. The\u00a0rancher-istio chart is being replaced by SUSE Application Collection which provides the same functionality as the rancher-istio chart.</p>"},{"location":"kbs/000021759/#status","title":"Status","text":"<p>Reported to Engineering</p>"},{"location":"kbs/000021759/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021764/","title":"Configure quota-backend-bytes to extend etcd keyspace limit in RKE2","text":"<p>This document (000021764) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021764/#environment","title":"Environment","text":"<p>Suse Rancher</p> <p>RKE2</p>"},{"location":"kbs/000021764/#situation","title":"Situation","text":"<p>In a Kubernetes cluster, etcd serves as the primary data store, maintaining the cluster state, configuration, and other critical metadata.</p> <p>Over time, as workloads grow and more resources are created, the etcd database can reach its default size limit, leading to performance degradation or even failures in cluster operations.</p>"},{"location":"kbs/000021764/#resolution","title":"Resolution","text":"<p>For a standalone RKE2 cluster, you can increase the etcd database size by modifying the RKE2 configuration file /etc/rancher/rke2/config.yaml.</p> <pre><code>etcd-arg:\n  - \"quota-backend-bytes=8589934592\"\n</code></pre> <p>and restart rke2-server</p> <pre><code>systemctl restart rke2-server\n</code></pre> <p>For a cluster managed by Rancher, go to Rancher, Cluster Management -&gt; select the cluster and edit the yaml ( which you will get from the 3 dots).</p> <p>Add quota-backend-bytes under machineSelectorConfig.config.etcd-arg.\u00a0 For example:</p> <pre><code> machineSelectorConfig:\n      - config:\n          etcd-arg: quota-backend-bytes=8589934592\n        matchLabels:\n          rke.cattle.io/etcd-role: 'true'\n</code></pre> <p>Increasing the etcd database size can help mitigate etcd storage consumption issues and ensure the cluster remains stable and responsive.</p>"},{"location":"kbs/000021764/#cause","title":"Cause","text":"<p>The etcd database size is primarily influenced by the number of objects stored, frequent updates, and high write activity. If the database size limit is reached, you may experience issues such as slow API responses, failed leader elections, or cluster instability. Additionally, excessive fragmentation due to outdated entries and stale snapshots can contribute to rapid storage consumption.</p>"},{"location":"kbs/000021764/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021764/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021767/","title":"How to Connect to Rancher Provisioned Clusters using CLI When Rancher is Offline","text":"<p>This document (000021767) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021767/#environment","title":"Environment","text":"<p>Rancher Version : Applicable for all Rancher versions.</p> <p>Downstream clusters : RKE1 , RKE2 or K3S</p> <p>Standalone Clusters : Its applicable for Standalone clusters of RKE1 , RKE2 or K3S</p> <p>SSH access to the ControlPlane nodes are mandatory.</p>"},{"location":"kbs/000021767/#situation","title":"Situation","text":"<p>In the event of a Rancher application outage, administrators may need to directly access provisioned downstream clusters via command-line interface (CLI) tools to diagnose and resolve issues.</p>"},{"location":"kbs/000021767/#resolution","title":"Resolution","text":"<p>RKE2 :</p> <ul> <li>SSH to the Server Node ( Privileged access is required )</li> <li>Run the Below Command</li> </ul> <pre><code># export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml KUBECONFIG=/etc/rancher/rke2/rke2.yaml\n# PATH=$PATH:/var/lib/rancher/rke2/bin\n</code></pre> <ul> <li>To verify, check the \"crictl\" and \"kubectl\" access to the cluster from the Server node.</li> </ul> <p>K3S :</p> <ul> <li>SSH to the Server Node ( Privileged access is required )</li> <li>Run the Below Command</li> </ul> <pre><code># export CRI_CONFIG_FILE=/var/lib/rancher/k3s/agent/etc/crictl.yaml KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n# PATH=$PATH:/var/lib/rancher/k3s/agent/containerd/bin/\n</code></pre> <ul> <li>To verify, check the \"crictl\" and \"kubectl\" access to the cluster from the Server node.</li> </ul> <p>RKE1 :</p> <ul> <li>SSH to the Server Node ( Privileged access is required )</li> <li>Follow the GH article to retrieve kubeconfig file</li> <li>To verify, check the \"docker\" and \"kubectl\" access to the cluster from the Server node.</li> </ul>"},{"location":"kbs/000021767/#cause","title":"Cause","text":"<p>NA</p>"},{"location":"kbs/000021767/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021767/#additional-information","title":"Additional Information","text":"<p>Refer for RKE2 : https://docs.rke2.io/cluster_access</p> <p>Refer for K3S : https://docs.k3s.io/cluster-access</p>"},{"location":"kbs/000021767/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021769/","title":"Cattle-cluster-agent flapping between versions","text":"<p>This document (000021769) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021769/#environment","title":"Environment","text":"<p>Any Rancher-managed downstream cluster.</p>"},{"location":"kbs/000021769/#situation","title":"Situation","text":"<p>The cattle-cluster-agent is seen flapping between versions, appearing as pods restarting frequently, alternating between new and old versions. You will also notice that the cattle-cluster-agent deployment will have a high revision count (ex. deployment.kubernetes.io/revision: 31736 ) and you will see multiple replica set with active pods:</p> <pre><code> kubectl get -n cattle-system replicasets -o custom-columns=NAME:.metadata.name,READY_REPLICAS:.status.readyReplicas,TOTAL_REPLICAS:.status.replicas,IMAGE:.spec.template.spec.containers[*].image\nNAME                               READY_REPLICAS   TOTAL_REPLICAS   IMAGE\ncattle-cluster-agent-59854fg9cf    1                 1                registry.rancher.com/rancher/rancher-agent:v2.10.1\ncattle-cluster-agent-673596d4q5    1                 2                registry.rancher.com/rancher/rancher-agent:v2.10.4\n</code></pre>"},{"location":"kbs/000021769/#resolution","title":"Resolution","text":"<p>Enable Audit Logs: Enable audit logs to trace which calls are triggering the update in your cluster [1]. This will help track which calls are updating the cattle-cluster-agent deployment and identify what is making those changes. Use the following REGEX string to find the updates:</p> <pre><code>deployments/cattle-cluster-agent.*patch.*&lt;UNEXPECTED_RANCHER_VERSION&gt;\n</code></pre> <p>Analyze the Logs: you will see results similar to the following:</p> <pre><code>\"{\"\"kind\"\":\"\"Event\"\",\"\"apiVersion\"\":\"\"audit.k8s.io/v1\"\",\"\"level\"\":\"\"RequestResponse\"\",\"\"auditID\"\":\"\"2270172f-4d8e-4a71-b6d0-d8847d1159a5\"\",\"\"stage\"\":\"\"ResponseComplete\"\",\"\"requestURI\"\":\"\"/apis/apps/v1/namespaces/cattle-system/deployments/cattle-cluster-agent?fieldManager=kubectl-client-side-apply\\u0026fieldValidation=Strict\"\",\"\"verb\"\":\"\"patch\"\",\"\"user\"\":{\"\"username\"\":\"\"system:serviceaccount:cattle-impersonation-system:cattle-impersonation-u-ezg219c4qv\"\",\"\"uid\"\":\"\"1f59bb08-6b81-488a-al31-b3439235e3c6\"\",\"\"groups\"\":[\"\"system:serviceaccounts\"\",\"\"system:serviceaccounts:cattle-impersonation-system\"\",\"\"system:authenticated\"\"]},\"\"impersonatedUser\"\":{\"\"username\"\":\"\"u-ezg22196cxl\"\",\"\"groups\"\":[\"\"system:authenticated\"\",\"\"system:cattle:authenticated\"\"],\"\"extra\"\":{\"\"principalid\"\":[\"\"system://c-r7x6s\"\",\"\"local://u-ezg22196cxl\"\"],\"\"username\"\":[\"\"System account for Cluster c-r7x6s\"\"]}},\"\"sourceIPs\"\":[\"\"127.0.0.1\"\",\"\"10.0.41.7\"\"],\"\"userAgent\"\":\"\"kubectl/v1.28.6+k3s2 (linux/amd64) kubernetes/c9f49a3\"\",\"\"objectRef\"\":{\"\"resource\"\":\"\"deployments\"\",\"\"namespace\"\":\"\"cattle-system\"\",\"\"name\"\":\"\"cattle-cluster-agent\"\",\"\"apiGroup\"\":\"\"apps\"\",\"\"apiVersion\"\":\"\"v1\"\"},\"\"responseStatus\"\":{\"\"metadata\"\":{},\"\"code\"\":200},\"\"requestObject\"\":{\"\"metadata\"\":{\"\"annotations\"\":{\"\"kubectl.kubernetes.io/last-applied-configuration\"\":\"\"{\\\"\"apiVersion\\\"\":\\\"\"apps/v1\\\"\",\\\"\"kind\\\"\":\\\"\"Deployment\\\"\",\\\"\"metadata\\\"\":{\\\"\"annotations\\\"\":{\\\"\"management.cattle.io/scale-available\\\"\":\\\"\"2\\\"\"},\\\"\"name\\\"\":\\\"\"cattle-cluster-agent\\\"\",\\\"\"namespace\\\"\":\\\"\"cattle-system\\\"\"},\\\"\"spec\\\"\":{\\\"\"selector\\\"\":{\\\"\"matchLabels\\\"\":{\\\"\"app\\\"\":\\\"\"cattle-cluster-agent\\\"\"}},\\\"\"strategy\\\"\":{\\\"\"rollingUpdate\\\"\":{\\\"\"maxSurge\\\"\":1,\\\"\"maxUnavailable\\\"\":0},\\\"\"type\\\"\":\\\"\"RollingUpdate\\\"\"},\\\"\"template\\\"\":{\\\"\"metadata\\\"\":{\\\"\"labels\\\"\":{\\\"\"app\\\"\":\\\"\"cattle-cluster-agent\\\"\"}},\\\"\"spec\\\"\":{\\\"\"affinity\\\"\":{\\\"\"nodeAffinity\\\"\":{\\\"\"preferredDuringSchedulingIgnoredDuringExecution\\\"\":[{\\\"\"preference\\\"\":{\\\"\"matchExpressions\\\"\":[{\\\"\"key\\\"\":\\\"\"node-role.kubernetes.io/controlplane\\\"\",\\\"\"operator\\\"\":\\\"\"In\\\"\",\\\"\"values\\\"\":[\\\"\"true\\\"\"]}]},\\\"\"weight\\\"\":100},{\\\"\"preference\\\"\":{\\\"\"matchExpressions\\\"\":[{\\\"\"key\\\"\":\\\"\"node-role.kubernetes.io/control-plane\\\"\",\\\"\"operator\\\"\":\\\"\"In\\\"\",\\\"\"values\\\"\":[\\\"\"true\\\"\"]}]},\\\"\"weight\\\"\":100},{\\\"\"preference\\\"\":{\\\"\"matchExpressions\\\"\":[{\\\"\"key\\\"\":\\\"\"node-role.kubernetes.io/master\\\"\",\\\"\"operator\\\"\":\\\"\"In\\\"\",\\\"\"values\\\"\":[\\\"\"true\\\"\"]}]},\\\"\"weight\\\"\":100},{\\\"\"preference\\\"\":{\\\"\"matchExpressions\\\"\":[{\\\"\"key\\\"\":\\\"\"cattle.io/cluster-agent\\\"\",\\\"\"operator\\\"\":\\\"\"In\\\"\",\\\"\"values\\\"\":[\\\"\"true\\\"\"]}]},\\\"\"weight\\\"\":1}],\\\"\"requiredDuringSchedulingIgnoredDuringExecution\\\"\":{\\\"\"nodeSelectorTerms\\\"\":[{\\\"\"matchExpressions\\\"\":[{\\\"\"key\\\"\":\\\"\"beta.kubernetes.io/os\\\"\",\\\"\"operator\\\"\":\\\"\"NotIn\\\"\",\\\"\"values\\\"\":[\\\"\"windows\\\"\"]}]}]}},\\\"\"podAntiAffinity\\\"\":{\\\"\"preferredDuringSchedulingIgnoredDuringExecution\\\"\":[{\\\"\"podAffinityTerm\\\"\":{\\\"\"labelSelector\\\"\":{\\\"\"matchExpressions\\\"\":[{\\\"\"key\\\"\":\\\"\"app\\\"\",\\\"\"operator\\\"\":\\\"\"In\\\"\",\\\"\"values\\\"\":[\\\"\"cattle-cluster-agent\\\"\"]}]},\\\"\"topologyKey\\\"\":\\\"\"kubernetes.io/hostname\\\"\"},\\\"\"weight\\\"\":100}]}},\\\"\"containers\\\"\":[{\\\"\"env\\\"\":[{\\\"\"name\\\"\":\\\"\"CATTLE_FEATURES\\\"\",\\\"\"value\\\"\":\\\"\"embedded-cluster-api=false,fleet=false,monitoringv1=false,multi-cluster-management=false,multi-cluster-management-agent=true,provisioningv2=false,rke2=false\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_IS_RKE\\\"\",\\\"\"value\\\"\":\\\"\"false\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_SERVER\\\"\",\\\"\"value\\\"\":\\\"\"https://k8s.weg.net\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_CA_CHECKSUM\\\"\",\\\"\"value\\\"\":\\\"\"REDACTED\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_CLUSTER\\\"\",\\\"\"value\\\"\":\\\"\"true\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_K8S_MANAGED\\\"\",\\\"\"value\\\"\":\\\"\"true\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_CLUSTER_REGISTRY\\\"\",\\\"\"value\\\"\":\\\"\"registry.rancher.com\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_SERVER_VERSION\\\"\",\\\"\"value\\\"\":\\\"\"v2.10.1\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_INSTALL_UUID\\\"\",\\\"\"value\\\"\":\\\"\"REDACTED\\\"\"},{\\\"\"name\\\"\":\\\"\"CATTLE_INGRESS_IP_DOMAIN\\\"\",\\\"\"value\\\"\":\\\"\"sslip.io\\\"\"}],\\\"\"image\\\"\":\\\"\"registry.rancher.com/rancher/rancher-agent:v2.10.1\\\"\",\\\"\"imagePullPolicy\\\"\":\\\"\"IfNotPresent\\\"\",\\\"\"name\\\"\":\\\"\"cluster-register\\\"\",\\\"\"volumeMounts\\\"\":[{\\\"\"mountPath\\\"\":\\\"\"/cattle-credentials\\\"\",\\\"\"name\\\"\":\\\"\"cattle-credentials\\\"\",\\\"\"readOnly\\\"\":true}]}],\\\"\"serviceAccountName\\\"\":\\\"\"cattle\\\"\",\\\"\"tolerations\\\"\":[{\\\"\"effect\\\"\":\\\"\"NoSchedule\\\"\",\\\"\"key\\\"\":\\\"\"node-role.kubernetes.io/controlplane\\\"\",\\\"\"value\\\"\":\\\"\"true\\\"\"},{\\\"\"effect\\\"\":\\\"\"NoSchedule\\\"\",\\\"\"key\\\"\":\\\"\"node-role.kubernetes.io/control-plane\\\"\",\\\"\"operator\\\"\":\\\"\"Exists\\\"\"},{\\\"\"effect\\\"\":\\\"\"NoSchedule\\\"\",\\\"\"key\\\"\":\\\"\"node-role.kubernetes.io/master\\\"\",\\\"\"operator\\\"\":\\\"\"Exists\\\"\"}],\\\"\"volumes\\\"\":[{\\\"\"name\\\"\":\\\"\"cattle-credentials\\\"\",\\\"\"secret\\\"\":{\\\"\"defaultMode\\\"\":320,\\\"\"secretName\\\"\":\\\"\"cattle-credentials-998877ccbbaa\\\"\"}}]}}}}\n</code></pre> <p>Identify the Source: In the example above, a patch request was made on the cattle-cluster-agent resource, updating the cattle-cluster-agent image version to 2.10.1. The source IP that initiated this call is shown as 10.0.41.7. This IP can be traced to identify the host responsible for making the call.\u00a0If it is not a node that is a part of your local cluster where you made the update to 2.10.4, then that means that this downstream cluster is managed by another cluster as well. You will need to remove the association to the other cluster so that it is only managed by one Rancher instance.</p> <p>[1] https://ranchermanager.docs.rancher.com/how-to-guides/advanced-user-guides/enable-api-audit-log-in-downstream-clusters</p>"},{"location":"kbs/000021769/#cause","title":"Cause","text":"<p>If the cluster is being managed by two Rancher instances and they are running different versions of Rancher, the cattle-cluster-agent will constantly flip between the versions that each Rancher cluster expects.</p>"},{"location":"kbs/000021769/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021778/","title":"403 Forbidden: \"system:unauthenticated\" error message when using a cluster-scoped Rancher API Token on a different cluster","text":"<p>This document (000021778) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021778/#environment","title":"Environment","text":"<p>A Rancher v2.x instance, managing multiple clusters, and a cluster-scoped Rancher API token</p>"},{"location":"kbs/000021778/#situation","title":"Situation","text":"<p>When you try to access a Rancher-managed Kubernetes cluster using <code>kubectl</code>, the command fails with a403 Forbidden error. The error message indicates an authentication failure from <code>User \"system:unauthenticated\":</code></p> <p>``</p> <pre><code>User \\\\\\\"system:unauthenticated\\\\\\\" cannot get resource \\\\\\\"clusters\\\\\\\" in API group \\\\\\\"management.cattle.io\\\\\\\" at the cluster scope\n</code></pre> <p>This typically happens when you use a <code>kubeconfig</code> file downloaded from one cluster (Cluster A) to access a different cluster (Cluster B), where Cluster A has the Authorized Cluster Endpoint (ACE) enabled.</p>"},{"location":"kbs/000021778/#resolution","title":"Resolution","text":"<p>This error is caused by attempting to use a cluster-scoped Rancher API token to query the Kubernetes API endpoint of a different cluster. In order to mitigate this, use a cluster-scoped Rancher API token that is scoped to the correct cluster, if you are connecting to the Authorized Cluster Endpoint; or alternatively, if you are not connecting to the Authorized Cluster Endpoint, you can use a non-scoped token.</p>"},{"location":"kbs/000021778/#cause","title":"Cause","text":"<p>This most regularly occurs, where a user downloads a <code>kubeconfig</code> file from Rancher, for a cluster with Authorized Cluster Endpoint enabled, and then attempts to use the token from that <code>kubeconfig</code> to connect to the Kubernetes API endpoint of a different cluster.</p> <p>There is a difference between the token used in a Rancher-generated\u00a0<code>kubeconfig</code> for clusters with and without Authorized Cluster Endpoint enabled.\u00a0Here's a breakdown of the two\u00a0<code>kubeconfig</code> types:</p> <p>Authorized Cluster Endpoint Disabled (Default)</p> <ul> <li> <p>API Server: The <code>kubeconfig</code> points to the Rancher-proxied Kubernetes API endpoint of the cluster.</p> </li> <li> <p>Authentication: The token is a non-scoped Rancher API token. Rancher acts as a proxy, authenticating the request and then forwarding it to Kubernetes API endpoint of the cluster.</p> </li> <li> <p>Portability: You can use the non-scoped token from this <code>kubeconfig</code> to connect to the Rancher-proxied Kubernetes API endpoint of any cluster on which you are granted permissions.</p> </li> </ul> <p>Authorized Cluster Endpoint Enabled</p> <ul> <li> <p>API Server: The <code>kubeconfig</code> contains the Authorized Cluster Endpoint address, in addition to the Rancher-proxied Kubernetes API endpoint of the cluster.</p> </li> <li> <p>Authentication: The token is a cluster-scoped API token created specifically for your user on that individual cluster. This token is propagated to the downstream cluster, to enable its use when accessing the Authorized Cluster Endpoint, which bypasses the Rancher-proxied endpoint.</p> </li> <li> <p>Portability: The token in this <code>kubeconfig</code> is valid only for the\u00a0Kubernetes API server of the cluster it was generated for. Using it against any other cluster will result in an <code>system:unauthenticated</code> error.</p> </li> </ul>"},{"location":"kbs/000021778/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021805/","title":"RKE2 certificate rotation failing on Windows worker nodes","text":"<p>This document (000021805) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021805/#environment","title":"Environment","text":"<p>Rancher &lt; v2.9.8 and Rancher v2.10 &lt; v2.10.4</p>"},{"location":"kbs/000021805/#situation","title":"Situation","text":"<p>The cluster is stuck in an Updating state, with a status message of the format \"waiting for [rke2-win-pnm94-pqhb5] certificate rotation\", after the execution of Certification Rotation, waiting for the certificates to be rotated in Windows nodes.</p> <p>The rancher-wins service (rancher-system-agent) on the affected Windows node(s) fails with the error: \" {error executing instruction 0: exec:/bin/sh\": \u00a0executable file not found in %PATH%}</p> <pre><code>PS C:\\Windows&gt; Get-EventLog -LogName Application -Source 'rancher-wins' -Newest 50 | format-table -Property TimeGenerated, ReplacementStrings -Wrap\n\nTimeGenerated         ReplacementStrings\n-------------         ------------------\n4/25/2025 10:22:46 AM {[K8s] updated plan secret fleet-default/rke2-win-pnm94-pqhb5-machine-plan with feedback}\n4/25/2025 10:22:46 AM  {error executing instruction 0: exec: \"/bin/sh\":  executable file not found in %PATH%}\n4/25/2025 10:22:46 AM {[Applyinator] Running command: /bin/sh [-x /var/lib/rancher/capr/idempotence/idempotent.sh\n                      certificate-rotation/restart-reset-failed\n                      6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b\n                      77bafa9e3a8a092afc4f1dd84e6e1cc58b68f42eb934bf0f6b73deb2518f78a3 /bin/sh /var/lib/rancher/capr\n                      -c if [ $(systemctl is-failed rke2-agent) = failed ]; then systemctl reset-failed rke2-agent;\n                      fi]}\n4/25/2025 10:22:46 AM {[Applyinator] No image provided, creating empty working directory C:\\var\\lib\\rancher\\agent\\work\\\n                      20250425-102246\\e6cc55ca9f306482c21ae53e9090c85730d4509b6dd5aa7882538cbc703acb2b_0}\n4/25/2025 10:22:46 AM {[Applyinator] Applying one-time instructions for plan with checksum\n                      e6cc55ca9f306482c21ae53e9090c85730d4509b6dd5aa7882538cbc703acb2b}\n4/25/2025 10:22:16 AM {[K8s] updated plan secret fleet-default/rke2-win-pnm94-pqhb5-machine-plan with feedback}\n4/25/2025 10:22:16 AM {error executing instruction 0: exec: \"/bin/sh\": executable file not found in %PATH%}\n4/25/2025 10:22:16 AM {[Applyinator] Running command: /bin/sh [-x /var/lib/rancher/capr/idempotence/idempotent.sh\n                      certificate-rotation/restart-reset-failed\n                      6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b\n                      77bafa9e3a8a092afc4f1dd84e6e1cc58b68f42eb934bf0f6b73deb2518f78a3 /bin/sh /var/lib/rancher/capr\n                      -c if [ $(systemctl is-failed rke2-agent) = failed ]; then systemctl reset-failed rke2-agent;\n</code></pre> <p>Requirements:</p> <ul> <li>Powershell access to Windows worker nodes</li> <li>.NET installed in Windows worker nodes.</li> </ul>"},{"location":"kbs/000021805/#resolution","title":"Resolution","text":"<p>The issue occurs in affected versions due to the attempt to invoke a script via the sh binary (/bin/sh) on Windows worker nodes, that is not present and can only be invoked on Linux nodes.</p> <p>The following steps are intended to mitigate the issue, by creating an empty sh binary in the required location on Windows nodes. The binary created does not rotate the Windows certificates and this is a temporary workaround. We recommend upgrading Rancher to Rancher v2.9.8+, v2.10.4+, or 2.11+ to fix the issue permanently.</p> <p>Create an sh.exe binary with an empty function, that will return without performing any action, in all the Windows worker nodes within the cluster. The script must be created in the C:/bin folder:</p> <ol> <li>Create a folder \"bin\" in C: and change directory to the newly created folder to execute step 2.</li> </ol> <pre><code>mkdir C:/bin\ncd C:/bin\n</code></pre> <ol> <li>Create the C# source file with the name rancher-windowsnode-sh-exe.cs and the contents below. This file will be complied in step 3.</li> </ol> <pre><code>class Program\n\n{\n\nstatic void Main(string[] args){}\n\n}\n</code></pre> <pre><code>\n</code></pre> <pre><code>PS C:\\bin&gt; type .\\rancher-windowsnode-sh-exe.cs\nclass Program\n\n{\n\nstatic void Main(string[] args){}\n\n}\nPS C:\\bin&gt;\n</code></pre> <pre><code>\n</code></pre> <ol> <li>Run the command below to compile the binary from the source file:</li> </ol> <pre><code>    c:\\Windows\\microsoft.net\\Framework64\\v4.0.30319\\csc.exe /target:exe /out:c:\\bin\\sh.exe rancher-windowsnode-sh-exe.cs\n</code></pre> <p>The output should be similar to:</p> <pre><code>PS C:\\bin&gt; c:\\Windows\\microsoft.net\\Framework64\\v4.0.30319\\csc.exe /target:exe /out:c:\\bin\\sh.exe rancher-windowsnode-sh-exe.cs\nMicrosoft (R) Visual C# Compiler version 4.8.4161.0\nfor C# 5\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nThis compiler is provided as part of the Microsoft (R) .NET Framework, but only supports language versions up to C# 5, which is no longer the latest version. For compilers that support newer versions of the C# programming language, see http://go.microsoft.com/fwlink/?LinkID=533240\n</code></pre> <ol> <li>You can now confirm the presence of the sh.exe binary within the C:/bin folder:</li> </ol> <pre><code>PS C:\\bin&gt; dir\n\n\n       Directory: C:\\bin\n\n\nMode                 LastWriteTime         Length Name\n   ----                 -------------         ------ ----\n   -a----         4/25/2025   1:57 PM             60 rancher-windowsnode-sh-exe.cs\n   -a----         4/25/2025   2:02 PM           3584 sh.exe\n</code></pre> <p>If the cluster remains in an Updating status, verify if the cluster status is paused and proceed to unpause by following the Resolution steps described at\u00a0https://www.suse.com/support/kb/doc/?id=000021399</p>"},{"location":"kbs/000021805/#additional-information","title":"Additional Information","text":"<ul> <li>GitHub issue tracking the issue: https://github.com/rancher/rancher/issues/44979</li> <li>Certificate Rotation documentation for Rancher-provisioned clusters: https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/manage-clusters/rotate-certificates#certificate-rotation</li> </ul>"},{"location":"kbs/000021805/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021824/","title":"Pods stuck in terminating state due to PodDisruptionBudget","text":"<p>This document (000021824) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021824/#environment","title":"Environment","text":"<p>Rancher Prime: v2.9.2.</p> <p>RKE2: v1.28.11+rke2r1</p>"},{"location":"kbs/000021824/#situation","title":"Situation","text":"<p>With PodDisruptionBudget configured, restarting the workload deployments or draining/shutting down the node puts the pods in Terminating state.</p>"},{"location":"kbs/000021824/#resolution","title":"Resolution","text":"<p>The permanent fix would be to upgrade to v1.30.9+rke2r1.</p> <p>Temporary fix would be to disable PodDisruptionBudget or adjust the minAvailable/ maxUnavailable values in PDB to get it working without disabling PDBs.</p>"},{"location":"kbs/000021824/#cause","title":"Cause","text":"<p>When a PodDisruptionBudget (PDB) is configured, restarting workload deployments or draining/shutting down a node may result in pods entering a Terminating state without completing cleanup.</p> <p>We observed this behavior on Kubernetes v1.28.11+rke2r1, particularly under the following edge case:</p> <ul> <li>Assume a cluster with two worker nodes.</li> <li>A deployment has three pod replicas, and the PDB is configured with\u00a0<code>minAvailable: 50%</code>.</li> <li>Suppose one replica is scheduled on one node and the remaining two replicas on the second node.</li> <li>If the node hosting the two replicas is drained or rebooted, multiple pods may become unavailable simultaneously.</li> </ul> <p>Given the PDB setting of\u00a0<code>minAvailable: 50%</code>\u00a0and a total of three replicas, only one pod can be voluntarily disrupted at any time. Draining the node with two replicas causes more than one pod to be unavailable, thereby violating the PDB policy. As a result, the remaining pods may be stuck in the Terminating state and not clean up properly.</p> <p>This behaviour is not reflected in the pod or deployment logs. The only observable clue is found in the <code>kube-controller-manager</code>\u00a0logs, where the following error message typically appears:</p> <pre><code>kube-system-kube-controller-manager-k00m01.nve90.rpt.idia:E0131 10:37:43.721229 1 disruption.go:626] Error syncing PodDisruptionBudget &lt;namespace&gt;/&lt;deployment&gt;, requeuing: Operation cannot be fulfilled on poddisruptionbudgets.policy \"&lt;deployment&gt;\": the object has been modified; please apply your changes to the latest version and try again\n</code></pre>"},{"location":"kbs/000021824/#additional-information","title":"Additional Information","text":"<p>https://kubernetes.io/docs/tasks/run-application/configure-pdb/#think-about-how-your-application-reacts-to-disruptions</p>"},{"location":"kbs/000021824/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021826/","title":"Adding a node to RKE1 cluster fails due to SSH tunneling issues.","text":"<p>This document (000021826) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021826/#environment","title":"Environment","text":"<p>Rancher version: 2.9.x</p>"},{"location":"kbs/000021826/#situation","title":"Situation","text":"<p>Attempts to add a node to the existing RKE1 cluster are failing due to a restriction on SSH tunneling. Specifically, the <code>AllowTcpForwarding</code> directive is likely disabled on the node, preventing the necessary port forwarding for successful node registration.</p>"},{"location":"kbs/000021826/#resolution","title":"Resolution","text":"<p>To permit TCP forwarding and enable the creation of SSH tunnels, it is necessary to modify the SSH daemon configuration file located at <code>/etc/ssh/sshd_config</code>. Within this file, locate the <code>AllowTcpForwarding</code> parameter and set its value to <code>yes</code>.</p>"},{"location":"kbs/000021826/#cause","title":"Cause","text":"<p>When attempting to add a node to your RKE1 cluster using the\u00a0rke up command, a failure in establishing the necessary SSH tunnel will prevent the node from being added. This scenario is often indicated by the following error messages in the output:</p> <p>Error Messages Indicating SSH Tunneling Failure:</p> <pre><code>time=\"2025-04-10T11:34:52+05:30\" level=warning msg=\"Failed to set up SSH tunneling for host [10.232.xxx.xxx]: Can't retrieve Docker Info: error during connect: Get \\\"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/info\\\": Unable to access the s\nUnable to access the service on /var/run/docker.sock. The service might be still starting up. Error: ssh: rejected: connect failed (open failed)\"\ntime=\"2025-04-10T11:34:52+05:30\" level=warning msg=\"Removing host [10.232.xxx.xxx] from node lists\"\ntime=\"2025-04-10T11:36:46+05:30\" level=fatal msg=\"cannot proceed with upgrade of controlplane since 1 host(s) cannot be reached prior to upgrade\u201d\n</code></pre>"},{"location":"kbs/000021826/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021826/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021827/","title":"Fleet Git Repos stuck in \"Git Updating\" state due to NTP sync issues","text":"<p>This document (000021827) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021827/#environment","title":"Environment","text":"<p>Rancher version: 2.10.x</p>"},{"location":"kbs/000021827/#situation","title":"Situation","text":"<p>Consistent time synchronization between the upstream Rancher cluster and its downstream clusters, ideally through a reliable NTP server, is crucial for stable operation. Time discrepancies can lead to unforeseen issues, including the inability of Fleet to synchronize with deployed resources on downstream clusters.</p>"},{"location":"kbs/000021827/#resolution","title":"Resolution","text":"<p>Addressing the identified time synchronization discrepancies between the upstream and downstream clusters by implementing NTP or chrony on all nodes resolved the Git Repo synchronization failures. Subsequently, the Git Repo status returned to \"Ready.\"</p>"},{"location":"kbs/000021827/#cause","title":"Cause","text":"<p>Fleet Git Repositories in a Continuous Delivery setup are experiencing a recurring issue where they transition to a \"Git Updating\" state and fail to synchronize automatically. Manual \"Force Update\" resolves the issue temporarily, indicating a breakdown in auto-synchronization. Notably, the <code>fleet-controller</code> and <code>fleet-agent</code> logs do not show any explicit error messages.\u00a0</p> <p>Observed Error in Rancher UI (After enabling Fleet Debug Log):</p> <p></p> <pre><code>Job Failed. failed: 1/1\ntime=\"2025-04-30T09:58:25.679769+0000\" level=info msg=\"Using in-cluster namespace\" namespace=fleet-default\ntime=\"2025-04-30T09:58:25.680059+0000\" level=info msg=\"Using in-cluster configuration\"\ntime=\"2025-04-30T09:58:25.681524+0000\" level=debug msg=\"Request Body\" body=\"{\\\"kind\\\":\\\"DeleteOptions\\\",\\\"apiVersion\\\":\\\"fleet.cattle.io/v1alpha1\\\"}\"\ntime=\"2025-04-30T09:58:25.681699+0000\" level=debug msg=\"curl -v -XDELETE -H \\\"Accept: application/json,*/*\\\" -H \\\"Content-Type: application/json\\\" -H \\\"Authorization: Bearer &lt;masked&gt;\\\" -H \\\"User-Agent: fleet/v0.0.0 (linux/amd64) kubernetes/$Format\\\" https://10.43.0.1:443/api/v1/namespaces/fleet-default/secrets/ocx-xx-xxxx-001-xx-xxx-t-001-xxxxxxxx-web-ui\"\ntime=\"2025-04-30T09:58:25.682309+0000\" level=debug msg=\"HTTP Trace: Dial to tcp: 10.43.0.1:443 succeed\"\ntime=\"2025-04-30T09:58:25.686097+0000\" level=debug msg=\"DELETE https://10.43.0.1:443/api/v1/namespaces/fleet-default/secrets/ocx-xx-xxxx-001-xx-xxx-t-001-xxxxxxxx-web-ui 401 Unauthorized in 4 milliseconds\"\ntime=\"2025-04-30T09:58:25.686143+0000\" level=debug msg=\"HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 2 ms ServerProcessing 1 ms Duration 4 ms\"\ntime=\"2025-04-30T09:58:25.686149+0000\" level=debug msg=\"Response Headers:\"\ntime=\"2025-04-30T09:58:25.686156+0000\" level=debug msg=\"Audit-Id: fe86d238-783-4232-a646-8bb165dbfa3\"\ntime=\"2025-04-30T09:58:25.686161+0000\" level=debug msg=\"Cache-Control: no-cache, private\"\ntime=\"2025-04-30T09:58:25.686165+0000\" level=debug msg=\"Content-Type: application/json\"\ntime=\"2025-04-30T09:58:25.686173+0000\" level=debug msg=\"Content-Length: 129\"\ntime=\"2025-04-30T09:58:25.686182+0000\" level=debug msg=\"Date: Wed, 30 Apr 2025 09:56:58 GMT\"\ntime=\"2025-04-30T09:58:25.686226+0000\" level=debug msg=\"Response Body\" body=\"{\\\"kind\\\":\\\"Status\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"metadata\\\":{},\\\"status\\\":\\\"Failure\\\",\\\"message\\\":\\\"Unauthorized\\\",\\\"reason\\\":\\\"Unauthorized\\\",\\\"code\\\":401}\"\ntime=\"2025-04-30T09:58:25Z\" level=fatal msg=Unauthorized\n</code></pre> <p>Observations:</p> <ul> <li> <p>Bundles, resources, and associated clusters are reported as healthy.</p> </li> <li> <p>The debug log indicates a successful initial connection to the Kubernetes API server (10.43.0.1:443)</p> </li> </ul> <p>Error in Gitjob Pod (Triggered by Force Update):</p> <pre><code>{\"level\":\"error\",\"ts\":\"2025-03-27T07:22:02Z\",\"logger\":\"gitops-status\",\"msg\":\"Reconcile failed update to git repo status\",\"controller\":\"GitRepoStatus\",\"controllerGroup\":\"fleet.cattle.io\",\"controllerKind\":\"GitRepo\",\"GitRepo\":{\"name\":\"xxx-xx-test-001\",\"namespace\":\"fleet-default\"},\"namespace\":\"fleet-default\",\"name\":\"xxx-xx-test-001\",\"reconcileID\":\"8e6da18a-e838-4be8-8855-692aec99eb89\",\"generation\":15,\"commit\":\"2ce6239c21fa08a9d8746dd6c812f45xxxxxxxxx\",\"conditions\":[{\"type\":\"Ready\",\"status\":\"True\",\"lastUpdateTime\":\"2025-03-27T06:26:03Z\"},{\"type\":\"GitPolling\",\"status\":\"True\",\"lastUpdateTime\":\"2025-03-25T09:35:15Z\"},{\"type\":\"Reconciling\",\"status\":\"False\",\"lastUpdateTime\":\"2025-02-21T20:44:49Z\"},{\"type\":\"Stalled\",\"status\":\"True\",\"lastUpdateTime\":\"2025-03-27T07:20:05Z\",\"reason\":\"Stalled\",\"message\":\"Job Failed. failed: 1/1time=\\\"2025-03-27T07:20:02Z\\\" level=fatal msg=Unauthorized\\n\"}]}\n</code></pre> <p>kube-apiserver Logs (During Force Update):</p> <pre><code>E0430 10:07:19.141522       1 authentication.go:73] \"Unable to authenticate the request\" err=\"[invalid bearer token, service account token is not valid yet]\"\n</code></pre> <p>The \" Unauthorized\" error in Rancher UI for the Git Repo and the kube-apiserver logs, specifically the message \"service account token is not valid yet\", strongly suggests an issue with token validity, likely due to time synchronisation problems between the upstream Rancher cluster and the downstream clusters managed by Fleet.</p> <p>Verification:</p> <p>Checking the time across the upstream and downstream clusters confirmed the presence of time synchronisation discrepancies.</p> <pre><code>\n</code></pre>"},{"location":"kbs/000021827/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021834/","title":"Finding Expired SSL Certificates","text":"<p>This document (000021834) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021834/#environment","title":"Environment","text":"<p>Any Kubernetes environment using TLS certificates</p>"},{"location":"kbs/000021834/#situation","title":"Situation","text":"<p>There can be many SSL certificates in a Kubernetes cluster. When a certificate expires and is left in the system, it can cause monitoring alerts or issues. To help find expired certificates that may be affecting your Kubernetes clusters you can use the following scripts.</p>"},{"location":"kbs/000021834/#resolution","title":"Resolution","text":"<p>Kubernetes Secrets</p> <p>You can run the following command to check your Kubernetes secrets for any expired TLS certificates.</p> <pre><code>for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do\n\u00a0 for sec in $(kubectl get secrets -n $ns -o json | jq -r '.items[] | select(.type==\"kubernetes.io/tls\") | .metadata.name'); do\n\u00a0 \u00a0 exp=$(kubectl get secret $sec -n $ns -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d | openssl x509 -noout -enddate | cut -d= -f2)\n\u00a0 \u00a0 if [[ $(date -d \"$exp\" +%s) -lt $(date +%s) ]]; then\n\u00a0 \u00a0 \u00a0 echo \"Expired certificate: $sec (namespace: $ns, expiry date: $exp)\"\n\u00a0 \u00a0 fi\n\u00a0 done\ndone\n</code></pre> <p>If you want to check for certificates expiring within a certain period of time, for example by 30 days, you can increment the date like this:\u00a0<code>'-d +30 days'</code>. Please note that this is wrapped in single quotes .</p> <p>See the example below for reference.</p> <pre><code>for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do\n  for sec in $(kubectl get secrets -n $ns -o json | jq -r '.items[] | select(.type==\"kubernetes.io/tls\") | .metadata.name'); do\n    exp=$(kubectl get secret $sec -n $ns -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d | openssl x509 -noout -enddate | cut -d= -f2)\n    if [[ $(date -d \"$exp\" +%s) -lt $(date -d '+30 days' +%s) ]]; then\n\u00a0 \u00a0 \u00a0 echo \"Expired certificate: $sec (namespace: $ns, expiry date: $exp)\"\n\u00a0 \u00a0 fi\n  done\ndone\n</code></pre> <p>Local Machines</p> <p>You can run the following command on your nodes to check for expiring certificates located on the host machine:</p> <pre><code>sudo find / -type f -name \"*.crt\" -exec sh -c '\n\u00a0 for cert; do\n\u00a0 \u00a0 exp=$(openssl x509 -in \"$cert\" -noout -enddate 2&gt;/dev/null | cut -d= -f2)\n\u00a0 \u00a0 if [ -n \"$exp\" ] &amp;&amp; [ \"$(date -d \"$exp\" +%s)\" -lt \"$(date +%s)\" ]; then\n      echo \"Expired certificate: $cert (validity period: $exp)\"\n\u00a0 \u00a0 fi\n\u00a0 done\n' sh {} + 2&gt;/dev/null\n</code></pre> <p>If you want to check for certificates expiring within a certain time frame, for example by 30 days, you can increment the date like this: <code>-d +30 days</code>. Note that single quotes ( ' ) are not needed in this case.</p> <p>See the example below for reference.</p> <pre><code>sudo find / -type f -name \"*.crt\" -exec sh -c '\n\u00a0 for cert; do\n\u00a0 \u00a0 exp=$(openssl x509 -in \"$cert\" -noout -enddate 2&gt;/dev/null | cut -d= -f2)\n\u00a0 \u00a0 if [ -n \"$exp\" ] &amp;&amp; [ \"$(date -d \"$exp\" +%s)\" -lt \"$(date -d +30 days +%s)\" ]; then\n      echo \"Expired certificate: $cert (validity period: $exp)\"\n\u00a0 \u00a0 fi\n\u00a0 done\n' sh {} + 2&gt;/dev/null\n</code></pre>"},{"location":"kbs/000021834/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021839/","title":"rancher-logging-root-fluentd-0 pod keeps restarting continuously with exit code 137 even after increasing memory","text":"<p>This document (000021839) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021839/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher 2.9.x</li> <li>Rancher-logging 104.1.x+</li> </ul>"},{"location":"kbs/000021839/#situation","title":"Situation","text":"<ul> <li> <p>A cluster with rancher-logging installed causing rancher-logging-root-fluentd-0 pod to restart continuously with error code '137'. But the same issue persists even after increasing the memory significantly.</p> </li> <li> <p>The rancher-logging-root-fluentd-0 pod only shows below error:</p> </li> </ul> <pre><code>Last status: Exited with 137: Error, Started: Fri Feb 28, 2025 4:50:53 PM, Exited: Fri Feb 28, 2025 4:57:07 PM\n</code></pre> <ul> <li>Upon further investigation, rancher-logging-root-fluentbit pod shows below errors:</li> </ul> <pre><code>[2025/03/04 11:01:38] [error] [net] TCP connection failed: rancher-logging-root-fluentd.cattle-logging-system.svc.cluster.local:24240 (Connection refused)\n[2025/03/04 11:01:38] [error] [output:forward:forward.0] no upstream connections available\n[2025/03/04 11:01:38] [ warn] [engine] failed to flush chunk '1-1741004097.135890300.flb', retry in 320 seconds: task_id=147, input=tail.0 &gt; output=forward.0 (out_id=0)\n</code></pre>"},{"location":"kbs/000021839/#resolution","title":"Resolution","text":"<ul> <li>Configure the output buffer to use the type 'file' instead of 'memory'.</li> <li>Below is an example output snippet for elasticsearch:</li> </ul> <pre><code>apiVersion: logging.banzaicloud.io/v1beta1\nkind: Output\nmetadata:\n  name: efk\n  namespace: cattle-logging-system\nspec:\n  elasticsearch:\n    buffer:\n      flush_interval: 30s\n      flush_mode: interval\n      flush_thread_count: 4\n      queued_chunks_limit_size: 300\n      type: file                          &lt;&lt;========================\n</code></pre> <ul> <li>Furthermore, login to Rancher &gt;&gt; explore the desired cluster &gt;&gt; Apps &gt;&gt; Installed Apps &gt;&gt; Rancher-Logging &gt;&gt; Click on \"Edit/Upgrade\" and review if the 'Buffer_Chunk_Size' and 'Buffer_Max_Size' mentioned below can be tuned further with a value that best suits the cluster needs as per\u00a0https://github.com/rancher/rancher-docs/issues/90</li> </ul> <pre><code>inputTail:\n    Buffer_Chunk_Size: ''\n    Buffer_Max_Size: ''\n</code></pre> <ul> <li>Observe that the pod rancher-logging-root-fluentd-0 does not restart anymore and logs are sent successfully.</li> </ul>"},{"location":"kbs/000021839/#cause","title":"Cause","text":"<ul> <li>By default when Fluent Bit processes data, it uses Memory as a primary and temporary place to store the records. There are scenarios where it would be ideal to have a persistent buffering mechanism based in the filesystem to provide aggregation and data safety capabilities.</li> <li>Fluentbit can lead to these issues when destination is slow or the cluster is producing large volumes of data.</li> <li>It is important to understand the correct configuration in case of slow destinations or large backpressure.</li> <li>More information can be found here: https://docs.fluentbit.io/manual/administration/buffering-and-storage.</li> </ul>"},{"location":"kbs/000021839/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021852/","title":"How to remove user addons or addons_include without deleting the resources","text":"<p>This document (000021852) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021852/#environment","title":"Environment","text":"<ul> <li>A Rancher Kubernetes Engine (RKE) Kubernetes cluster provisioned by the RKE CLI or Rancher v2.x</li> <li>kubectl access to the cluster with a kubeconfig sourced for a global admin or cluster owner user</li> </ul>"},{"location":"kbs/000021852/#situation","title":"Situation","text":"<p>You may want to update the <code>user_addons</code> or <code>addons_include</code> specifications in an RKE1 cluster.yaml definition without deleting the resources created during the initial deployment. For example, users who hardened an RKE1 cluster using Rancher versions 2.0 to 2.4 might have included the <code>cattle-system</code> namespace as a user addon (see [1]). However, an upgraded version of the hardened <code>cluster.yaml</code> file (see [2]) may no longer include this specification. If the <code>cattle-system</code> namespace is removed from the <code>user_addons</code> section, running the <code>rke up</code> operation will delete the namespace\u2014and with it, Rancher itself.</p> <p>This article explains how to update the <code>cluster.yaml</code> file while preserving the resources defined under <code>user_addons</code> or <code>addons_include</code> in a Rancher Kubernetes Engine (RKE) cluster.</p> <p>[1] - https://ranchermanager.docs.rancher.com/v2.0-v2.4/reference-guides/rancher-security/rancher-v2.4-hardening-guides/hardening-guide-with-cis-v1.5-benchmark</p> <p>[2] - https://ranchermanager.docs.rancher.com/v2.7/reference-guides/rancher-security/hardening-guides/rke1-hardening-guide#reference-hardened-rke-clusteryml-configuration</p>"},{"location":"kbs/000021852/#resolution","title":"Resolution","text":"<ol> <li>Perform a backup of the User Addon Jobs and related ConfigMaps</li> </ol> <pre><code>kubectl -n kube-system get job rke-user-addon-deploy-job -o yaml &gt; rke-user-addon-deploy-job-backup.yaml\nkubectl -n kube-system get job rke-user-includes-addons-deploy-job -o yaml &gt; rke-user-includes-addons-deploy-job-backup.yaml\nkubectl -n kube-system get configmap rke-user-addon -o yaml &gt; rke-user-addon-backup.yaml\nkubectl -n kube-system get configmap rke-user-includes-addons &gt; rke-user-includes-addons-backup.yaml\n</code></pre> <ol> <li>Delete the User Addon Jobs</li> </ol> <p>These Jobs are responsible for deploying user-defined addons. Removing them prevents re-application of outdated configurations.</p> <pre><code>kubectl -n kube-system delete job rke-user-addon-deploy-job rke-user-includes-addons-deploy-job\n</code></pre> <ol> <li>Delete the Related ConfigMaps</li> </ol> <p>Clean up the associated ConfigMaps for good measure. These contain the actual addon definitions that were applied by the Jobs.</p> <pre><code>kubectl -n kube-system delete configmap rke-user-addon rke-user-includes-addons\n</code></pre> <ol> <li>Perform the rke up operation Update the cluster with the new configuration. For RKE provisioned clusters, invoke <code>rke up --cluster.yml</code>\u00a0( ensure the cluster.rkestate file is present in the working directory when invoking\u00a0<code>rke up</code>). For Rancher provisioned clusters, click\u00a0<code>Save</code>\u00a0in the Rancher UI\u00a0<code>Edit as YAML</code>\u00a0view.</li> </ol>"},{"location":"kbs/000021852/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021853/","title":"system-reserved and kube-reserved resource reservations","text":"<p>This document (000021853) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021853/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher 2.x</li> <li>Downstream or Standalone/Custom RKE1/RKE2/K3s cluster</li> </ul>"},{"location":"kbs/000021853/#situation","title":"Situation","text":"<ul> <li>Kubernetes nodes can be scheduled to\u00a0<code>Capacity</code>. Pods can consume all the available capacity on a node by default. This is an issue because nodes typically run quite a few system daemons that power the OS and Kubernetes itself. Unless resources are set aside for these system daemons, pods and system daemons compete for resources and lead to resource starvation issues on the node.</li> <li>The scheduler treats 'Allocatable' as available <code>capacity</code> for pods</li> <li>The <code>kubelet</code> exposes a feature named ' Node Allocatable' that helps to reserve compute resources for system daemons. Kubernetes recommends that cluster administrators configure 'Node Allocatable' based on their workload density on each node.</li> <li>' Allocatable' on a Kubernetes node is defined as the amount of compute resources that are available for pods (Allocatable = Total node capacity - kube-reserved - system-reserved). . The scheduler does not over-subscribe 'Allocatable'. 'CPU', 'memory', and 'ephemeral-storage' are supported values.</li> </ul>"},{"location":"kbs/000021853/#kube-reserved","title":"Kube Reserved:","text":"<ul> <li>KubeletConfiguration Setting: <code>kubeReserved: {}</code>. Example value <code>{cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}</code> ``</li> <li><code>kubeReserved</code> is meant to capture resource reservation for Kubernetes system daemons like the <code>kubelet</code>, <code>container runtime</code>, etc.</li> <li>In addition to\u00a0<code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be specified to reserve the specified number of process IDs for Kubernetes system daemons.</li> </ul>"},{"location":"kbs/000021853/#system-reserved","title":"System Reserved","text":"<ul> <li>KubeletConfiguration Setting: <code>systemReserved: {}</code>. Example value <code>{cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}</code> ``</li> <li><code>systemReserved</code> is meant to capture resource reservation for OS system daemons like <code>sshd</code>, <code>udev</code>, etc. <code>systemReserved</code> should reserve <code>memory</code> for the <code>kernel</code> too since <code>kernel</code> memory is not accounted to pods in Kubernetes at this time. Reserving resources for user login sessions is also recommended ( <code>user.slice</code> in systemd world).</li> <li>In addition to\u00a0<code>cpu</code>, <code>memory</code>, and <code>ephemeral-storage</code>, <code>pid</code> may be specified to reserve the specified number of process IDs for OS system daemons.</li> <li>Be careful while enforcing\u00a0<code>system-reserved</code> reservation, since it can lead to critical system services being CPU starved, OOM killed, or unable to fork on the node. The recommendation is to enforce\u00a0<code>system-reserved</code>\u00a0only if a user has profiled their nodes exhaustively to come up with precise estimates and is confident in their ability to recover if any process in that group is oom-killed</li> </ul>"},{"location":"kbs/000021853/#resolution","title":"Resolution","text":"<p>How to configure system-reserved and kube-reserved reservations:</p> <p>Rancher-provisioned cluster:</p> <ul> <li>RKE2 cluster: In the Rancher UI go to Cluster management -&gt; Select your RKE2 cluster -&gt; Edit Config -&gt; Advanced, then specify the parameters as 'Additional Kubelet Args'. Add the following lines (sample), click on Save to apply.</li> </ul> <p><code>kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=5Gi</code></p> <p><code>system-reserved=cpu=1,memory=1548Mi,ephemeral-storage=30Gi</code></p> <p></p> <ul> <li>K3s cluster: \u00a0In the Rancher UI go to Cluster management -&gt; Select your K3s cluster -&gt; Edit Config -&gt; Advanced, then specify the parameters as 'Additional Kubelet Args'. Add the following lines ( these are a sample), click on Save to apply.</li> </ul> <p><code>kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=5Gi</code></p> <p><code>system-reserved=cpu=1,memory=1548Mi,ephemeral-storage=30Gi</code></p> <ul> <li>RKE1 cluster: In the Rancher UI go to Cluster management -&gt; Select your RKE cluster -&gt; Edit Config -&gt; Edit as YAML, and under the 'Services' -&gt; 'Kubelet' -&gt; 'extra_args',\u00a0 add the following lines (these are a sample), click on Save to apply.</li> </ul> <pre><code>    kubelet:\n      extra_args:\n        kube-reserved: \"cpu=1,memory=1Gi,ephemeral-storage=1Gi\"\n        system-reserved: \"cpu=500m,memory=1Gi,ephemeral-storage=1Gi\"\n</code></pre> <p>Standalone clusters:</p> <ul> <li>RKE2 cluster: Specify the system-reserved and kube-reserved parameters under the configuration file available in the path /etc/rancher/rke2/config.yaml. Here is a sample /etc/rancher/rke2/config.yaml config for a worker (agent) node:</li> </ul> <pre><code>token:  rASpGladPp\nnode-name: wk1.cluster.local\nnode-ip: 10.10.24.1\ncluster-domain: cluster.local\ntls-san:\n  - cluster.local\ncluster-cidr: 10.10.16.0/21\nservice-cidr: 10.10.0.0/20\ncluster-dns: 10.10.0.10\nservice-node-port-range: 30000-32767\nkube-apiserver-arg:\n  - request-timeout=2m\nkubelet-arg:\n  - kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=5Gi\n  - system-reserved=cpu=1,memory=1548Mi,ephemeral-storage=30Gi\n  - eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;4%\ncni: calico\n</code></pre> <ul> <li>K3s cluster:\u00a0 Specify the system-reserved and kube-reserved settings under the configuration file available in /etc/rancher/h3s/config.yaml. Here is a sample /etc/rancher/k3s/config.yaml config for a worker(agent) node.</li> </ul> <p><code>token:  xxxxxxxxx   cluster-domain: k3s.local   tls-san:   - k3s.local kube-apiserver-arg:   - request-timeout=2m kubelet-arg:   - kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=5Gi   - system-reserved=cpu=1,memory=1548Mi,ephemeral-storage=30Gi   - eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;4% <pre><code>- **RKE1 cluster**:\u00a0 Speficy the system-reserved and kube-reserved settings as 'extra\\_args' for the kubelet in the cluster configuration file 'cluster.yml'. Here is a sample cluster.yml config file:\n</code></pre> nodes:   - address: 192.168.100.41     internal_address: 192.168.100.41     user: root     role: [controlplane, etcd]   - address: 192.168.100.42     internal_address: 192.168.100.42     user: root     role: [worker]   - address: 192.168.100.43     internal_address: 192.168.100.43     user: root     role: [worker]   - address: 192.168.100.44     internal_address: 192.168.100.44     user: root     role: [worker] cluster_name: rke-sample kubernetes_version: v1.26.15-rancher1-1 services: etcd:     backup_config:       interval_hours: 6       retention: 30 kube-api:     service_cluster_ip_range: 10.41.0.0/16 kube-controller:     cluster_cidr: 10.40.0.0/16     service_cluster_ip_range: 10.41.0.0/16 kubelet:     cluster_dns_server: 10.41.0.10     extra_args:       enforce-node-allocatable: \"pods,kube-reserved,system-reserved\"       kube-reserved: \"cpu=1,memory=1Gi,ephemeral-storage=1Gi\"       system-reserved: \"cpu=500m,memory=1Gi,ephemeral-storage=1Gi\"       kube-reserved-cgroup: /kube.slice       system-reserved-cgroup: /system.slice       eviction-hard: \"memory.available&lt;500Mi,imagefs.available&lt;10%,nodefs.available&lt;10%,nodefs.inodesFree&lt;5%\"</code></p>"},{"location":"kbs/000021853/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021855/","title":"Managing Nginx Ingress Removal and Reinstallation on Rancher-Managed RKE Clusters","text":"<p>This document (000021855) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021855/#environment","title":"Environment","text":"<p>Rancher : Any</p> <p>Downstream clusters : RKE1</p>"},{"location":"kbs/000021855/#situation","title":"Situation","text":"<p>Incorrect modifications to the default Nginx Ingress can cause pods to enter a CrashLoop.</p>"},{"location":"kbs/000021855/#resolution","title":"Resolution","text":"<p>To Remove</p> <pre><code>Go to Rancher UI &gt; Edit config of DS cluster -&gt; Advance options -&gt; Nginx Ingress -&gt; Click Disabled &gt;  Save ( Cluster will go for a reconcile)\n\nOnce completed, make sure all Ingress components are removed from the Ingress namespace.\n</code></pre> <p>To Reinstall</p> <pre><code>Go to Rancher UI &gt; Edit config of the cluster -&gt; Advance options -&gt; Nginx Ingress -&gt; Click Enabled &gt;  Save ( Cluster will go for a reconcile)\n</code></pre> <p>Restart DeamonSets</p> <pre><code>Go to the cluster explorer &gt; Workloads &gt; Click on DeamonSets &gt; Selet the Ingress Controller &gt; Restart\n</code></pre> <p>Downgrade the Nginx Version</p> <pre><code>Go to the cluster explorer &gt; Workloads &gt; Click on DeamonSets &gt; Selet the Ingress Controller &gt; Edit as YAML &gt; Change the Image version &gt; Save\n</code></pre>"},{"location":"kbs/000021855/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021855/#additional-information","title":"Additional Information","text":"<p>Please take all the necessary backups</p>"},{"location":"kbs/000021855/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021861/","title":"Pod level DNS resolution failed after modifying the node's dns servers on /etc/resolv.conf","text":"<p>This document (000021861) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021861/#environment","title":"Environment","text":"<p>RKE2</p>"},{"location":"kbs/000021861/#situation","title":"Situation","text":"<p>The newly created pods are unable to communicate with core-dns after updating the node's DNS server details in /etc/resolv.conf. The new pods fail to resolve DNS.</p>"},{"location":"kbs/000021861/#resolution","title":"Resolution","text":"<p>Flush the dns cache in Linux node to ensure, the system or the application stop using stale dns resolution that was cached before the changes.</p> <p>The following steps need to be executed.</p> <p>Restart the rke2-server service if it is a control plane+worker node</p> <p><code>systemctl restart rke2-server.service</code></p> <p>Restart the rke2-agent service if the node is only a worker</p> <p><code>systemctl restart rke2-agent.service</code></p> <p>To flush the dns cache, different commands can be used, depends on the Linux distro.</p> <p>If using nscd</p> <p><code>service nscd restart</code> or <code>systemctl restart nscd</code></p> <p>If using dnsmaq</p> <p><code>systemctl restart dnsmaq</code></p> <p>If using systemd-resolvd</p> <p><code>systemd-resolve --flush-caches</code></p>"},{"location":"kbs/000021861/#cause","title":"Cause","text":"<p>On Linux, some dns services like systemd-resolved, nscd, dnsmaq etc. caches dns queries in memory and may not immediately pick up changes to /etc/resolv.conf unless restarted or flushed. If a dns query was previously resolved and cached, the system might continue using old IP address even after updating the /etc/resolv.conf</p>"},{"location":"kbs/000021861/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021861/#additional-information","title":"Additional Information","text":"<p>To flush the dns cache, different commands can be used, depends on the Linux distro, and the dns flushing many not be necessary for all the Linux distributions.</p>"},{"location":"kbs/000021861/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021862/","title":"RKE2 how to set the timezone","text":"<p>This document (000021862) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021862/#environment","title":"Environment","text":"<p>RKE2</p>"},{"location":"kbs/000021862/#situation","title":"Situation","text":"<p>Containers do not inherit timezones from host machines and have only accessed to the clock from the kernel - which is always UTC. The default timezone for most images is UTC, yet it is not guaranteed and may be different from container to container since it can be changed on a pod or image level.</p>"},{"location":"kbs/000021862/#resolution","title":"Resolution","text":"<p>You can use the following environment variables to set the desired timezone in the various pods by editing the cluster YAML:</p> <p>``</p> <pre><code>\u00a0 \u00a0 machineSelectorConfig:\n\u00a0 \u00a0 \u00a0 - config:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kube-apiserver-extra-env: \"TZ=&lt;timezone&gt;\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kube-scheduler-extra-env: \"TZ=&lt;timezone&gt;\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kube-controller-manager-extra-env: \"TZ=&lt;timezone&gt;\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kube-proxy-extra-env: \"TZ=&lt;timezone&gt;\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 etcd-extra-env: \"TZ=&lt;timezone&gt;\"\n  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud-controller-manager-extra-env: \"TZ=&lt;timezone&gt;\"\n</code></pre> <p>This assumes the tzdb package is present on the nodes. Rancher started including this in their k8s releases from January 2025 onwards.</p> <p>For other non-static pods and workloads you may have, you can consider using k8tz that creates a admission controller that injects the timezones into any pods created.</p> <p>See here: https://github.com/k8tz/k8tz</p>"},{"location":"kbs/000021862/#cause","title":"Cause","text":"<p>Rancher started including the tzdb package in k8s releases from January 2025 onwards.</p> <p>Prior to this it as not possible to set the Timezone.</p> <p>See this issue:</p> <p>https://github.com/rancher/rke2/issues/7331</p>"},{"location":"kbs/000021862/#additional-information","title":"Additional Information","text":"<p>Check the k8tz controller to manage it globally in your cluster.</p> <p>See:\u00a0 https://github.com/k8tz/k8tz</p>"},{"location":"kbs/000021862/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021868/","title":"Longhorn Node Disk Space Not Updating on UI After Expansion","text":"<p>This document (000021868) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021868/#environment","title":"Environment","text":"<p>SUSE Enterprise Storage - Longhorn</p>"},{"location":"kbs/000021868/#situation","title":"Situation","text":"<p>After expanding the disk space on a Kubernetes node, Longhorn fails to recognize and update the new disk capacity. The <code>node.longhorn.io</code> object for the affected node does not reflect the expanded size, even though the disk expansion was successful at the operating system level.</p> <p>Upon reviewing the <code>longhorn-manager</code> pod logs, the following recurring error is observed:</p> <pre><code>2025-05-24T10:27:50.565235147+02:00 time=\"2025-05-24T08:27:50Z\" level=error msg=\"Dropping Longhorn node out of the queue\" func=controller.handleReconcileErrorLogging file=\"utils.go:79\" LonghornNode=longhorn-system/node01 controller=longhorn-node error=\"failed to sync node for longhorn-system/node01: no node name provided to check node down or deleted\" node=node01\n</code></pre> <p>This error indicates that the <code>node.spec.name</code> field is missing from the respective <code>node.longhorn.io</code> object, preventing the Longhorn node controller from properly syncing the node's status, including disk information.</p>"},{"location":"kbs/000021868/#resolution","title":"Resolution","text":"<p>To resolve this issue, you must manually add the <code>node.spec.name</code> field to the affected <code>node.longhorn.io</code> object.</p> <ol> <li>Identify the Longhorn Node Object(s) : First, list all <code>node.longhorn.io</code> objects in the <code>longhorn-system</code> namespace to identify the affected node(s).</li> </ol> <pre><code>kubectl get node.longhorn.io -n longhorn-system\n</code></pre> <p>Example Output:</p> <pre><code>NAME      READY   ALLOWSCHEDULING   SCHEDULABLE   AGE\nnode01    True    true              True          46m\nnode02    True    true              True          46m\nnode03    True    true              True          46m\n</code></pre> <ol> <li>Backup the Affected Node Object (Optional but Recommended) :\u00a0Before making any changes, it's good practice to take a YAML backup of the specific\u00a0<code>node.longhorn.io</code>\u00a0object you intend to modify. Replace\u00a0\u00a0with the actual name of your Longhorn node (e.g.,\u00a0<code>node01</code>). <pre><code>kubectl get node.longhorn.io &lt;NAME&gt; -n longhorn-system -o yaml &gt; &lt;NAME&gt;_longhorn_node_backup.yaml\n</code></pre> <ol> <li>Edit the Longhorn Node Object :\u00a0Edit the\u00a0<code>node.longhorn.io</code> resource for the problematic node. Replace\u00a0\u00a0with the actual name of your Longhorn node. <pre><code>kubectl edit node.longhorn.io &lt;NAME&gt; -n longhorn-system\n</code></pre> <ol> <li>Add\u00a0<code>` node.spec.na</code>me<code>**:**\u00a0 Locate the</code>spec<code>section within the YAML. Add or modify the</code>name<code>field under</code>spec<code>to match the</code>metadata.name`\u00a0of the object.</li> </ol> <p>Example:</p> <p>Change highlighted by\u00a0<code># This is the change made</code></p> <pre><code>apiVersion: longhorn.io/v1beta2\nkind: Node\nmetadata:\n     finalizers:\n  - longhorn.io\ngeneration: 1\nname: node01\nnamespace: longhorn-system\nresourceVersion: \"12713\"\nuid: 3c11b55d-e0633-47a2f-a525-a2f98e96df66\nspec:\nallowScheduling: true\ndisks:\n    default-disk-697367c0672dc2af:\n      allowScheduling: true\n      diskDriver: \"\"\n      diskType: filesystem\n      evictionRequested: false\n      path: /var/lib/longhorn/\n      storageReserved: 150265232486\n      tags: []\nevictionRequested: false\ninstanceManagerCPURequest: 0\nname: node01 # This is the change made\ntags: []\n</code></pre> <ol> <li>Save and Verify: Save the changes to the <code>node.longhorn.io</code>\u00a0object. Once the\u00a0<code>node.spec.name</code> field is correctly added, the Longhorn manager should automatically begin syncing the node. You should observe that the disk status on the\u00a0<code>node.longhorn.io</code>  `` object is updated to reflect the expanded disk space.</li> </ol> <p>``</p>"},{"location":"kbs/000021868/#cause","title":"Cause","text":"<p>The Longhorn <code>node-controller</code> includes a disk monitor that periodically queries the node's disk statistics and updates the <code>node.status.diskStatus</code> field of the <code>node.longhorn.io</code> object. However, if the <code>node.spec.name</code> field is absent from the <code>node.longhorn.io</code> definition, the controller cannot correctly identify and process the node, leading to sync failures and the inability to reflect updated disk sizes.</p>"},{"location":"kbs/000021868/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021868/#additional-information","title":"Additional Information","text":"<p>Ideally, the <code>node.spec.name</code> field should not be allowed to be missing or removed from <code>node.longhorn.io</code> objects. An enhancement request is open with Longhorn (refer to longhorn/longhorn#6793) to implement stricter validation of resource <code>spec</code> and <code>status</code> fields within the validating webhook, which would prevent such issues in the future.</p>"},{"location":"kbs/000021868/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021869/","title":"RBAC issues in Fleet's GitRepos after upgrade","text":"<p>This document (000021869) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021869/#environment","title":"Environment","text":"<p>Rancher 2.9.9+</p> <p>Rancher 2.10.5+</p> <p>Up to\u00a0 Rancher 2.11.1.</p>"},{"location":"kbs/000021869/#situation","title":"Situation","text":"<pre><code>After upgrading to Rancher 2.9.9+ or  2.10.5+, there are insufficient permissions on the Role for the ServiceAccount in charge of updating each GitRepo.\nYou might see errors like the following one:\n</code></pre> <pre><code>Job Failed. failed: 1/1time=\"YYYY-MM-DDThh:mm:ssZ\" level=fatal msg=\"secrets \\\"sample-git-repo-sa-secret\\\" is forbidden: User \\\"system:serviceaccount:fleet-default:git-repo-sample-name\\\" cannot delete resource \\\"secrets\\\" in API group \\\"\\\" in the namespace \\\"fleet-default\\\"\"\n</code></pre> <pre><code>This issue affects anyone with GitRepos that are upgrading to Rancher version v2.9.9+ or Rancher version v2.10.5+.\nRancher v2.11.1+ is not affected since the newer Fleet versions automatically resolves the Role's permissions.\n</code></pre>"},{"location":"kbs/000021869/#resolution","title":"Resolution","text":"<p>The fix is to manually add the permissions missing to the Role. These steps are to be run on the 'local' cluster (Rancher):</p> <ul> <li> <p>Create a backup for the Role\u00a0of an affected\u00a0GitRepo.\u00a0It will be\u00a0named 'git-REPONAME' in the ' <code>fleet-default'</code> namespace (replace \"REPONAME\" by your GitRepo name)</p> </li> <li> <p>For example for a repo named 'app-test': ' <code>kubectl get role -n fleet-default git-app-test -o yaml &gt; role-git-app-test-backup.yaml</code>'</p> </li> <li>Edit the role for the affected\u00a0repo.</li> <li>For example, for a repo named 'app-test': ' <code>kubectl edit role -n fleet-default git-app-test'</code></li> <li>Add the following section at the bottom of the role, under the rules section:</li> </ul> <pre><code>- apiGroups:\n  - \"\"\n  resources:\n  - secrets\n  verbs:\n  - get\n  - create\n  - update\n  - delete\n</code></pre> <pre><code>\n</code></pre> <ul> <li>Save the changes</li> <li>Run a \"Force Update\" on the GitRepo in the Continuous Delivery section in the Rancher UI (three-dots menu next to the GitRepo)</li> </ul>"},{"location":"kbs/000021869/#cause","title":"Cause","text":"<pre><code>There are insufficient permissions on the Role for the ServiceAccount in charge of updating each GitRepo.\n</code></pre>"},{"location":"kbs/000021869/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021870/","title":"Node Removal After Changing Cloud Provider to vSphere in an Existing RKE2 Cluster","text":"<p>This document (000021870) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021870/#environment","title":"Environment","text":"<p>Applicable for all versions of:</p> <ul> <li>RKE2</li> <li>K3s</li> </ul>"},{"location":"kbs/000021870/#situation","title":"Situation","text":"<p>After updating the cloud provider configuration in an existing RKE2 cluster from the default (no explicit cloud provider) to <code>vsphere</code>, some nodes became unavailable in the Rancher UI and appeared in the <code>nodenotfound</code> state.</p> <p>The following log entries were recorded in the vSphere Cloud Provider Interface (CPI) logs:</p> <pre><code>E0514 06:31:06.975972   1 datacenter.go:128] Unable to find VM by UUID. VM UUID: rke2://&lt;NODE_IDENTIFIER&gt;\nE0514 06:31:06.975992   1 search.go:181] Error while looking for vm=rke2://&lt;NODE_IDENTIFIER&gt;(byUUID) in vc=&lt;VSPHERE_VCENTER&gt; and datacenter=&lt;VSPHERE_DATACENTER&gt;: No VM found\nI0514 06:31:06.976000   1 search.go:186] Did not find node rke2://&lt;NODE_IDENTIFIER&gt; in vc=&lt;VSPHERE_VCENTER&gt; and datacenter=&lt;VSPHERE_DATACENTER&gt;\nI0514 06:31:06.983134   1 instances.go:177] instances.InstanceExistsByProviderID() NOT CACHED for node uid \"rke2://&lt;NODE_IDENTIFIER&gt;\"\nI0514 06:31:06.983150   1 node_lifecycle_controller.go:164] deleting node since it is no longer present in cloud provider: &lt;NODE_IDENTIFIER&gt;\nI0514 06:31:06.983268   1 event.go:389] \"Event occurred\" object=\"&lt;NODE_IDENTIFIER&gt;\" fieldPath=\"\" kind=\"Node\" apiVersion=\"\" type=\"Normal\" reason=\"DeletingNode\" message=\"Deleting node &lt;NODE_IDENTIFIER&gt; because it does not exist in the cloud provider\"\n</code></pre> <p>These messages indicate that the vSphere CPI attempted to locate virtual machines based on the\u00a0<code>providerID</code> format, and nodes with the <code>rke2://</code> prefix could not be matched to any virtual machines in the vSphere environment.</p>"},{"location":"kbs/000021870/#resolution","title":"Resolution","text":"<p>Changing the cloud provider on an existing RKE2 cluster is not supported. The <code>cloud-provider</code> parameter must be defined during the initial provisioning of the cluster and cannot be altered afterward without consequences.</p> <p>When the cloud provider is changed to <code>vsphere</code> post-deployment:</p> <ul> <li> <p>The vSphere CPI only recognizes and manages nodes with a <code>providerID</code> that begins with <code>vsphere://</code>.</p> </li> <li> <p>Nodes previously registered with <code>providerID: rke2://</code> are not recognized by the vSphere CPI.</p> </li> <li> <p>The Kubernetes Node Lifecycle Controller, informed by CPI, interprets these nodes as non-existent in the cloud infrastructure and proceeds to remove them from the cluster.</p> </li> </ul> <p>To restore cluster health, affected nodes must be replaced with newly provisioned nodes that are configured with the correct cloud provider from the outset. These new nodes will be registered with the correct\u00a0<code>providerID</code> format and managed successfully by the vSphere CPI.</p>"},{"location":"kbs/000021870/#cause","title":"Cause","text":"<p>The vSphere CPI expects nodes to be registered with a <code>providerID</code> in the format <code>vsphere://</code>.\u00a0Nodes originally registered with the RKE2 default behavior (i.e., without a cloud provider) have a <code>providerID</code> format of <code>rke2://</code>. These identifiers do not match any VM in vSphere. As a result, the CPI reports them as non-existent, and the Kubernetes Node Lifecycle Controller initiates their deletion from the cluster.</p>"},{"location":"kbs/000021870/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021871/","title":"Unable to delete any Projects in Rancher &gt;= 2.9 if the Legacy Feature Flag was active when the project was created in Rancher &lt; 2.9","text":"<p>This document (000021871) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021871/#environment","title":"Environment","text":"<ul> <li>Rancher 2.9+</li> <li>Rancher projects created for the local or Downstream cluster, on a previous version of Rancher</li> <li>The \"Legacy feature flag\" is enabled under Global Settings in Rancher.</li> </ul>"},{"location":"kbs/000021871/#situation","title":"Situation","text":"<ul> <li>A project with the finalizer \" clusterscoped.controller.cattle.io/project-precan-alert-controller\" cannot be removed, either from Rancher UI or executing this kubectl conmand in the Rancher local cluster:\u00a0 ' <code>kubectl delete projects.management.cattle.io p-xxxx</code>' and it remains in \"terminating\" status due to the finalizer.</li> <li>This project has been created on a Rancher version &lt;2.9, with the \"legacy\" feature flag enabled under Global Settings=&gt; Feature Flags =&gt; Legacy</li> </ul>"},{"location":"kbs/000021871/#resolution","title":"Resolution","text":"<ul> <li>You can manually edit the project hanging in Terminating status, and remove the finalizer clusterscoped.controller.cattle.io/project-precan-alert-controller.</li> <li>There is a sample script that can help locating projects with the finalizer\u00a0clusterscoped.controller.cattle.io/project-precan-alert-controller, and delete them (to be executed in the\u00a0 Rancher local cluster), but please use it with caution:</li> </ul> <pre><code>namespaces=$(kubectl get ns --no-headers -o custom-columns=NAME:.metadata.name | grep \"c-\" | grep -v cluster-fleet)\n\nfor n in $namespaces; do\n    echo \"Namespace: $n\"\n    projects=$(kubectl get project -n $n --no-headers -o name)\n\n    for p in $projects; do\n      echo \"Project: $p\"\n      finalizers=$(kubectl get $p -n $n -o json | jq -r '.metadata.finalizers[]')\n\n      for i in $finalizers; do\n        if [[ $i == clusterscoped.controller.cattle.io/project-precan-alert-controller* ]]; then\n          echo \"Removing finalizer from $p: $i\"\n          index=$(kubectl get $p -n $n -o json | jq -r \".metadata.finalizers | index(\\\"$i\\\")\")\n          kubectl patch $p -n $n --type='json' -p=\"[{\\\"op\\\": \\\"remove\\\", \\\"path\\\": \\\"/metadata/finalizers/$index\\\"}]\"\n        fi\n      done\n    done\ndone\n</code></pre>"},{"location":"kbs/000021871/#cause","title":"Cause","text":"<ul> <li>The \"legacy\" feature flag is a remanent from Rancher 2.5.x and per the docs: https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/installation-references/feature-flags, this flag is disabled by default on new Rancher installations.</li> <li>It is meant for backwards compatibility of clusters that started in the 2.5 era and needed time to remove usage of those features.</li> <li>The finalizer\u00a0\" clusterscoped.controller.cattle.io/project-precan-alert-controller\" is from the deprecated and removed Monitoring/Logging/Alerting V1.</li> <li>In Rancher 2.9 and later, none of the legacy code related to this exists. It is advisable not to enable \"Legacy feature flag\", unless strictly required.</li> </ul>"},{"location":"kbs/000021871/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021878/","title":"How to configure custom attributes for a Rancher-provisioned vSphere cluster?","text":"<p>This document (000021878) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021878/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher Prime - All Versions.</li> <li>Rancher-provisioned vSphere cluster.</li> </ul>"},{"location":"kbs/000021878/#situation","title":"Situation","text":"<ul> <li>``` Set-PowerCLIConfiguration -InvalidCertificateAction Ignore <pre><code>Custom attributes in vSphere are user-defined fields that allow users to store metadata for objects like VMs, hosts,etc.\n- Custom attributes are a legacy feature in vSphere for attaching metadata to objects, while tags are the modern alternative. A tag is a label that can be applied to an object in the vSphere inventory.\n- Users may need custom attributes for existing workflows or integrations that rely on this metadata for the vSphere clusters provisioned through Rancher.\n\n### Resolution\n\n- Make sure the attributes are already created from vSphere client side: vSphere client &gt;&gt; Menu &gt;&gt; Tags &amp; Custom Attributes.\n\n- As custom attribute is stored in key-value format and each key has its own identifier as `ID`. To fetch the ID of the key, first verify that PowerCLI is installed. If not, run the command below from PowerShell to install:\n</code></pre> Install-Module -Name VMware.PowerCLI <pre><code>- Now, connect to the desired vSphere cluster using the below command in Power CLI:\n</code></pre> Connect-VIServer -Server server.example.com -User  -Password  <pre><code>- If it fails with SSL error, then you can try the workaround below &amp; re-run the above command:\n</code></pre> Set-PowerCLIConfiguration -InvalidCertificateAction Ignore <pre><code>- Now fetch the customAttributes from the environment, below is an example shown for custom attribute\u00a0`Owner`:\n</code></pre> Get-CustomAttribute <pre><code>- Run the command below to get a specific attribute. Replace\u00a0`Owner ` with any custom attribute defined in your vSphere environment:\n</code></pre> Get-CustomAttribute -Name \"Owner\" <pre><code>- Note the ID mentioned in\u00a0`Key`\u00a0column for the attribute.\n- Edit the resource\u00a0`vmwarevsphereconfigs.rke-machine-config.cattle.io`\u00a0in the Rancher management cluster (local cluster) for the specific downstream cluster. Run the first command to get the resource name for the desired downstream cluster.\u00a0Below is an example shown for a customAttribute\u00a0`Owner`:\n</code></pre> kubectl -n fleet-default get vmwarevsphereconfigs.rke-machine-config.cattle.io kubectl -n fleet-default edit vmwarevsphereconfigs.rke-machine-config.cattle.io  <pre><code>- In the example YAML output given below 1234 is the id for the attribute\u00a0`Owner`. Replace the id with the one defined in your vSphere environment for the desired attribute:\n</code></pre> customAttribute: <li> <p>1234=user1 ```</p> </li> <li> <p>Verify the custom attribute in the vSphere client under 'Tags &amp; Custom Attributes' or run the below command from PowerCLI:</p> </li> <p><code>Get-VM -Name \"&lt;VM_NAME&gt;\" | Get-Annotation</code></p>"},{"location":"kbs/000021878/#cause","title":"Cause","text":"<ul> <li>According to the\u00a0documentation, Custom attributes are a legacy feature that will eventually be removed from vSphere. However, it can be configured through a custom resource definition <code>vmwarevsphereconfigs.rke-machine-config.cattle.io</code> on a Rancher-provisioned vsphere cluster.</li> <li>Since custom attributes will be removed in future vSphere versions, users should consider migrating to tags or other solutions for long-term use.</li> </ul>"},{"location":"kbs/000021878/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021884/","title":"RKE2 server fails to start due to etcd metrics port conflict","text":"<p>This document (000021884) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021884/#environment","title":"Environment","text":"<p>RKE2 - v1.32.5+rke2r1</p>"},{"location":"kbs/000021884/#situation","title":"Situation","text":"<p>In an RKE2 setup, the <code>rke2-server</code> service intermittently fails to start with the following error message in the journal logs:</p> <pre><code>jun 20 09:04:13 &lt;node&gt; rke2[2524118]: time=\"2025-06-20T09:04:13+02:00\" level=fatal msg=\"Failed to reconcile with temporary etcd: listen tcp 127.0.0.1:2381: bind: address already in use\"\njun 20 09:04:13 &lt;node&gt; systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE.\n</code></pre> <p>The <code>config.yaml</code> includes the following custom etcd configuration:</p> <pre><code>etcd-arg:\n  - --listen-metrics-urls=http://127.0.0.1:2381,http://&lt;nodeip&gt;:2381\netcd-expose-metrics: true\n</code></pre>"},{"location":"kbs/000021884/#resolution","title":"Resolution","text":"<p>To avoid the port binding conflict, remove the manual override of the <code>--listen-metrics-urls</code> parameter. Instead, rely on the below `` setting alone:</p> <pre><code>etcd-expose-metrics: true\n</code></pre> <p>This instructs RKE2 to expose metrics in a compatible manner without causing a conflict between temporary and cluster etcd instances.</p>"},{"location":"kbs/000021884/#cause","title":"Cause","text":"<p>Overriding the <code>--listen-metrics-urls</code> argument with\u00a0<code>127.0.0.1:2381</code> and <code>&lt;nodeip&gt;:2381</code> causes both the temporary and cluster etcd instances to attempt to bind to the same local port ( <code>127.0.0.1:2381</code>). This results in a port conflict, preventing the <code>rke2-server</code> from starting.</p>"},{"location":"kbs/000021884/#additional-information","title":"Additional Information","text":"<p>Reference: Github issue #4479</p>"},{"location":"kbs/000021884/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021891/","title":"Removing 'server: istio-envoy' Header from Istio Responses","text":"<p>This document (000021891) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021891/#environment","title":"Environment","text":"<ul> <li>SUSE Rancher 2.x</li> <li>RKE2</li> <li>Rancher-istio application</li> </ul>"},{"location":"kbs/000021891/#situation","title":"Situation","text":"<p>When routing a web application through Istio, the HTTP responses may include the header \" <code>server: istio-envoy</code>\". This may be flagged by security or compliance teams during audits.</p>"},{"location":"kbs/000021891/#resolution","title":"Resolution","text":"<p>To remove the <code>server</code> header from the HTTP responses, apply the following <code>EnvoyFilter</code> configuration on the affected cluster:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: EnvoyFilter\nmetadata:\n  name: ef-removeserver\n  namespace: istio-system\nspec:\n  configPatches:\n  - applyTo: NETWORK_FILTER\n    match:\n      listener:\n        filterChain:\n          filter:\n            name: \"envoy.filters.network.http_connection_manager\"\n    patch:\n      operation: MERGE\n      value:\n        typed_config:\n          \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\"\n          server_header_transformation: PASS_THROUGH\n  - applyTo: ROUTE_CONFIGURATION\n    patch:\n      operation: MERGE\n      value:\n        response_headers_to_remove:\n        - \"server\"\n</code></pre> <ul> <li>Ensure this filter is applied in the <code>istio-system</code> namespace or where your ingress gateway is running, depending on your specific deployment setup.</li> <li>This solution uses an EnvoyFilter to remove the <code>server</code> header. The approach is based on community guidance shared in the following GitHub issue:\u00a0GitHub Issue #13861 - Remove Server Header</li> <li>The configuration uses <code>server_header_transformation: PASS_THROUGH</code> to avoid setting the default <code>istio-envoy</code> value, and explicitly removes the <code>server</code> header from response headers.</li> </ul>"},{"location":"kbs/000021891/#additional-information","title":"Additional Information","text":"<p>The rancher-istio application is planned for deprecation in upcoming Rancher releases. It is recommended to review the official deprecation notice and plan for migration accordingly.</p> <p>SUSE Blog - Rancher Istio Will Be Deprecated</p> <p>SUSE Announcement - Deprecation of Rancher Istio</p>"},{"location":"kbs/000021891/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021901/","title":"How to configure Pod Topology Spread Constraints in RKE2?","text":"<p>This document (000021901) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021901/#environment","title":"Environment","text":"<p>RKE2</p>"},{"location":"kbs/000021901/#situation","title":"Situation","text":"<p>This article explains how to configure pod topology spread constraints in RKE2 to ensure better distribution of pods and improve fault tolerance.</p>"},{"location":"kbs/000021901/#resolution","title":"Resolution","text":"<p>Note: Ensure to back up any of the files mentioned below if they already exist, especially if they contain other customized configurations.</p> <p>1]\u00a0 Create a file named <code>scheduler.config</code>\u00a0in the <code>/var/lib/rancher/rke2/server/manifests/</code> directory with the following content:</p> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nclientConnection:\n  kubeconfig: /var/lib/rancher/rke2/server/cred/scheduler.kubeconfig\nprofiles:\n  - schedulerName: default-scheduler\n    pluginConfig:\n      - name: PodTopologySpread\n        args:\n          defaultConstraints:\n            - maxSkew: 1\n              topologyKey: topology.kubernetes.io/region\n              whenUnsatisfiable: DoNotSchedule\n          defaultingType: List\n</code></pre> <p>This configuration sets default constraints\u00a0for the <code>PodTopologySpread</code> plugin, ensuring pods are spread across different <code>topology.kubernetes.io/region</code> with a maximum skew of 1. The configuration can be modified as per requirement.</p> <p>2] Edit the RKE2 configuration file, located at <code>/etc/rancher/rke2/config.yaml</code>, and add the following argument under <code>kube-scheduler-arg:</code></p> <pre><code>kube-scheduler-arg:\n  - \"config=/var/lib/rancher/rke2/server/manifests/scheduler.config\"\n</code></pre> <p>This tells the <code>kube-scheduler</code> to use the custom configuration file.</p> <p>3] Apply the changes by restarting the <code>rke2-server</code> service:</p> <pre><code>systemctl restart rke2-server\n</code></pre> <p>4] Finally, deploy some test pods and observe their distribution to ensure the custom topology spread constraints are being applied as expected.</p> <p>5] Below is an example deployment used for testing PodTopologySpread at a region level:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-dev\n  labels:\n    app: nginx-dev\nspec:\n  replicas: 6 # Adjust this based on your desired total Pod count\n  selector:\n    matchLabels:\n      app: nginx-dev\n  template:\n    metadata:\n      labels:\n        app: nginx-dev\n    spec:\n      # --- PodTopologySpread Configuration Starts Here ---\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/region # This is the key: spread across regions\n        whenUnsatisfiable: DoNotSchedule # If the constraint cannot be met, do not schedule the Pod\n        labelSelector:\n          matchLabels:\n            app: nginx-dev # Only count Pods belonging to this Deployment\n      # --- PodTopologySpread Configuration Ends Here ---\n\n      containers:\n      - name: nginx-container\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"100Mi\"\n</code></pre> <p>7] Topology spread constraints rely on node labels to identify the topology domains that each node is in. Hence, its important to label the nodes accordingly.</p>"},{"location":"kbs/000021901/#additional-information","title":"Additional Information","text":"<p>In RKE2 clusters, pods may get scheduled unevenly across nodes, regions or zones, which can lead to poor availability or resource usage. To fix this, Kubernetes offers Pod Topology Spread Constraints, which help spread pods evenly across nodes, regions, or other topology domains.</p>"},{"location":"kbs/000021901/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021906/","title":"New downstream clusters are picking up the old default registry, even though the system default registry has been modified.","text":"<p>This document (000021906) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021906/#environment","title":"Environment","text":"<p>RKE2</p> <p>RKE</p>"},{"location":"kbs/000021906/#situation","title":"Situation","text":"<p>The system default registry, configured via the Rancher UI's Global Settings, is not being applied to new downstream clusters. These clusters are still attempting to pull images from the old registry</p>"},{"location":"kbs/000021906/#resolution","title":"Resolution","text":"<p>The system default registry, configured via the Rancher UI's Global Settings, is not being applied to new downstream clusters.</p> <p>These clusters are still attempting to pull images from the old registry</p> <pre><code>Rancher UI &gt; Explore Local cluster &gt; Rancher deployment &gt; Edit YML\n</code></pre> <p>Take a backup of the below YAML before the edit</p> <pre><code>Rancher UI &gt; Explore Local cluster &gt; Rancher deployment &gt; Edit YML &gt; Update and Save\n</code></pre> <p>Perform the Rancher pod rollout restart.</p> <pre><code>Rancher UI &gt; Explore Local cluster &gt; Rancher deployment &gt; Redeploy\n</code></pre>"},{"location":"kbs/000021906/#cause","title":"Cause","text":"<p>While creating a new downstream cluster through Rancher, Rancher acts as an orchestrator. It uses its internal configuration, including the default registry, to provision the cluster's control plane and worker nodes. If Rancher tries to pull cattle-cluster-agent, Kubernetes components, CNI plugins, or other essential images from the wrong registry, the cluster creation will fail or result in an unhealthy state.</p>"},{"location":"kbs/000021906/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021906/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021913/","title":"How to disable Rancher System Upgrade Controller for Imported K3s and RKE2 Clusters","text":"<p>This document (000021913) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021913/#environment","title":"Environment","text":"<p>Rancher v2.10.6 and later.</p> <p>This KB should NOT implemented in Rancher environment if you have a mix of Custom, Node Provisioned as Imported clusters and disabling the Rancher System Upgrade Controller via the feature flag would cause unexpected consequences on Rancher Provisioned clusters.</p>"},{"location":"kbs/000021913/#situation","title":"Situation","text":"<p>The System Upgrade Controller can be installed on standalone K3S/RKE2 clusters to manage the cluster upgrades. However, when the standalone cluster is imported and managed by Rancher, an additional system upgrade controller is installed on these clusters by Rancher when Rancher is upgraded to v2.10.6.</p>"},{"location":"kbs/000021913/#resolution","title":"Resolution","text":"<p>The procedure to disable/manage the Rancher System Upgrade Controller for imported clusters is a bit different for Rancher\u00a0v2.10.x and 2.11.x.</p> <p>Rancher v2.10.x:</p> <p>DISCLAIMER: This method should only be followed for a Rancher setup if all the clusters are imported. If there is a mix of Custom, Node Provisioned and imported clusters, then its recommended to first upgrade Rancher to v2.11.x and follow the steps outlined in the below Rancher v2.11\u00a0section.</p> <p>Users can disable the System Upgrade Controller deployed by Rancher for new and existing imported clusters (this includes the local cluster aka Rancher cluster since its considered as an imported cluster) by updating the Rancher helm chart values. Doing this would disable the Rancher System Upgrade Controller for all the imported clusters. To proceed:</p> <ul> <li>Open the values.yaml file using your preferred editor. If you do not have this file, you can generate it for an existing Rancher installation using the below command:</li> </ul> <pre><code>helm get values rancher -n cattle-system -o yaml &gt; values.yaml\n</code></pre> <ul> <li>Now you can update \"features: managed-system-upgrade-controller=false\" in the file. Below is a reference of the file after adding the flag:</li> </ul> <pre><code>bootstrapPassword: P@ssw2rd\nfeatures: managed-system-upgrade-controller=false\nhostname: rancher.foo.bar\nreplicas: 1\n</code></pre> <ul> <li>Now you can perform the Rancher upgrade using the below command by passing the values.yaml file.</li> </ul> <p>NOTE: Ensure that you add other parameters to the below command which was used to install Rancher initially.</p> <pre><code> helm upgrade rancher rancher-prime/rancher \\\n  --namespace cattle-system \\\n  -f values.yaml \\\n  --version &lt;DEPLOYED_RANCHER_VERSION&gt;\n</code></pre> <ul> <li>Once upgraded, Rancher will not install any system-upgrade-controller for new clusters that you import.</li> <li>An additional step needs to be performed for already existing imported clusters. Even after updating the Rancher helm release with the above helm values, the Rancher\u00a0system-upgrade-controller would not be automatically removed. For existing clusters, you would have to remove the rancher system-upgrade-controller manually and to do that, you can run the below command on the existing cluster.</li> </ul> <pre><code>kubectl delete apps -n cattle-system system-upgrade-controller\n</code></pre> <p>NOTE: Going forward, any Kubernetes upgrades for the imported clusters where Rancher system-upgrade-controller is disabled should not be performed from Rancher UI. If performed, this would not upgrade the cluster and it would keep the cluster in\u00a0\"Upgrading\" state with a message \"Cluster is being upgraded\" and you would notice the following error message in the Rancher pod logs:</p> <pre><code>2025/07/23 09:33:59 [ERROR] error syncing 'c-m-zz2sgb6k': handler k3s-upgrade-controller: the server could not find the requested resource (get plans.upgrade.cattle.io), requeuing\n</code></pre> <ul> <li>Hence, its recommended that you either perform manual or automated K3S/RKE2 upgrades outside of Rancher.</li> </ul> <p>Rancher v2.11:</p> <p>In Rancher v2.11 onwards, users can control the per-cluster behaviour of the Rancher System Upgrade Controller that should be deployed while importing the cluster. You can choose one of the values while importing the K3S/RKE2 cluster from the Rancher UI in the\u00a0K8s version management section. This feature is called Version Management and more details about this can be found here.</p> <ul> <li>Global default: This is the default value. Inherits behaviour from the global\u00a0imported-cluster-version-management setting in Rancher UI &gt; Global Settings &gt; Settings.</li> <li>True: Enables version management, allowing users to control the Kubernetes version and upgrade strategy of the cluster through Rancher. This would install Rancher system-upgrade-controller on the imported clusters.</li> <li>False: Disables version management, enabling users to manage the cluster\u2019s Kubernetes version independently, outside of Rancher. This option would NOT install Rancher system-upgrade-controller on the imported clusters.</li> </ul>"},{"location":"kbs/000021913/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021913/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021922/","title":"How to configure Calico Node IP Autodetection with Specific CIDRs","text":"<p>This document (000021922) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021922/#environment","title":"Environment","text":"<p>Rancher 2.x</p> <p>RKE2</p>"},{"location":"kbs/000021922/#situation","title":"Situation","text":"<p>In environments where Kubernetes nodes (e.g., RKE2) have multiple network interfaces or IP addresses, it's often necessary to explicitly tell the Container Network Interface (CNI), such as Calico, which IP address to use for its internal communication and pod networking. If Calico automatically selects an incorrect interface (e.g., the interface of a management network instead of the intended data network), it can lead to network connectivity issues for pods or between nodes.</p>"},{"location":"kbs/000021922/#resolution","title":"Resolution","text":"<p>Add the below config in the downstream cluster under the Additional Manifest tab in the Rancher UI during the creation to adjust the auto detection method for Calico:</p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: rke2-calico\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    installation:\n      calicoNetwork:\n        nodeAddressAutodetectionV4:\n          firstFound: false\n          cidrs:\n            - \"192.168.1.0/24\" # Replace with your target CIDR\n</code></pre>"},{"location":"kbs/000021922/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021922/#additional-information","title":"Additional Information","text":"<p>Configure IP autodetection | Calico Documentation</p>"},{"location":"kbs/000021922/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021930/","title":"Is it possible to share a single S3 bucket and folder for etcd backups across multiple different clusters?","text":"<p>This document (000021930) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021930/#environment","title":"Environment","text":"<p>Rancher-provisioned or standalone RKE, RKE2 or K3s clusters, with etcd backups configured to an S3 bucket</p>"},{"location":"kbs/000021930/#situation","title":"Situation","text":"<p>When configuring etcd backups to an S3 bucket for RKE, RKE2 or K3s clusters, can the same S3 bucket and folder be used?</p>"},{"location":"kbs/000021930/#resolution","title":"Resolution","text":"<ul> <li>No, each cluster must use a different S3 bucket and folder combination.</li> <li>Thus, whilst an S3 bucket can be shared between clusters, each cluster should be configured to use a unique folder within the bucket.</li> <li>The same bucket and folder should not be used for different clusters, as this might lead to issues:</li> <li>Mixed etcd snapshots from different clusters</li> <li>Unavailability to recover a cluster by choosing the wrong etcd snapshot</li> <li>Etcd snapshots pruned by another cluster</li> </ul>"},{"location":"kbs/000021930/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021931/","title":"Newly created namespaces not visible due to malformed ResourceQuota annotations","text":"<p>This document (000021931) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021931/#environment","title":"Environment","text":"<p>A Rancher v2.x managed cluster, using Project Resource Quotas</p>"},{"location":"kbs/000021931/#situation","title":"Situation","text":"<p>When a non-admin user creates a new namespace within an affected project, the namespace is not visible to them; however, the namespace object is successfully created and can be viewed by an admin user.</p> <p>Inspecting the manifest of the affected namespace, after creation, the following annotation is missing:</p> <p><code>lifecycle.cattle.io/create.namespace-auth: \"true\"</code></p> <p>Another important symptom is that the Rancher logs show errors similar to the following:</p> <p><code>[ERROR] error syncing 'test-suse': handler namespace-auth: invalid character 'm' after object key:value pair, handler resourceQuotaSyncController: invalid character 'm' after object key:value pair, requeuing</code></p>"},{"location":"kbs/000021931/#resolution","title":"Resolution","text":"<p>To resolve this issue, you must identify and correct the malformed <code>field.cattle.io/resourceQuota</code> annotation on the problematic namespace(s) within the affected project. Look for syntax errors such as incorrect or extra quotes, missing commas, or other typos, and once you've found the namespace with the invalid annotation, use <code>kubectl edit</code> to correct the JSON syntax.</p> <p>For example, correct this annotation with extra single quotes:\u00a0<code>field.cattle.io/resourceQuota: '''{\"limit\":...}'''</code></p> <p>To this: <code>field.cattle.io/resourceQuota: '{\"limit\":...}'</code></p>"},{"location":"kbs/000021931/#cause","title":"Cause","text":"<p>The problem is not with the newly created namespace, but with an existing namespacewithin the same project that has a malformed JSON string in the\u00a0<code>field.cattle.io/resourceQuota</code> annotation.</p> <p>When a new namespace is created, Rancher controllers\u00a0process resources within that project. If these controllers encounter a namespace with a syntax error in the <code>field.cattle.io/resourceQuota</code> annotation, this process will fail. This behaviour is related to an induced failure in Go's <code>json.Unmarshal</code> function within Rancher's resource quota controllers. As a result, the expected RBAC is not configured on the new namespace, indicated by the absence of the\u00a0<code>lifecycle.cattle.io/create.namespace-auth: \"true\"</code> annotation, and it remains hidden to non-admin users.</p> <p>This issue is not introduced by Rancher itself and namespaces where the resource quota is only defined via the Rancher UI will contain correctly formatted JSON. The issue occurs where the <code>field.cattle.io/resourceQuota</code> annotation is modified manually via\u00a0<code>kubectl edit</code> or through GitOps tools (such as ArgoCD), and a syntax error is introduced.</p> <p>Examples of malformed JSON:</p> <ul> <li>Incorrect quoting: Using triple single quotes instead of one.</li> </ul> <pre><code>field.cattle.io/resourceQuota: '''{\"limit\":{\"pods\":\"30\", ... }}'''\n</code></pre> <ul> <li>Missing characters: A missing double quote on a value.</li> </ul> <pre><code>field.cattle.io/resourceQuota: '{\"limit\":{ ... \"limitsCpu\":1600m\", ... }}'\n</code></pre> <p>(Note the missing quote before <code>1600m</code>)</p>"},{"location":"kbs/000021931/#additional-information","title":"Additional Information","text":"<p>The issue is not a Rancher product bug. It is caused by auser-defined configuration error in an existing Kubernetes namespace object.</p>"},{"location":"kbs/000021931/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021943/","title":"Namespaces stuck in a terminating state in RKE2 v1.32.5+rke2r1 clusters with the Calico API Server enabled","text":"<p>This document (000021943) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021943/#environment","title":"Environment","text":"<p>A standalone or Rancher-provisioned RKE2 v1.32.5+rke2r1, with the Calico CNI, in which the Calico API Server (disabled by default) is enabled</p>"},{"location":"kbs/000021943/#situation","title":"Situation","text":"<p>Namespaces are stuck in a terminating state when attempting to delete the, but disabling the Calico API Server allows the namespaces to be deleted successfully.</p>"},{"location":"kbs/000021943/#resolution","title":"Resolution","text":"<p>This upstream Calico issue is fixed in Calico v3.30.1 which is shipped in RKE2 v1.32.6+rke2r1. Affected clusters should be upgraded to resolve the issue.</p>"},{"location":"kbs/000021943/#cause","title":"Cause","text":"<p>This is a result of an upstream Calico issue affecting the Calico API Server on Calico v3.30.0 when running in a Kubernetes v1.32 cluster. Calico v3.30.0 was shipped in RKE2 v1.32.5+rke2r1.</p>"},{"location":"kbs/000021943/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021951/","title":"How to recover a Rancher-provisioned RKE2 or K3s cluster after misconfiguring agent proxy variables","text":"<p>This document (000021951) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021951/#environment","title":"Environment","text":"<p>A Rancher-provisioned RKE2 or K3s cluster, in which a proxy is required for the downstream cluster to connect to Rancher</p>"},{"location":"kbs/000021951/#situation","title":"Situation","text":"<p>In a Rancher-provisioned RKE2 or K3s cluster, in which a proxy is required for the downstream cluster to connect to Rancher, the rancher-system-agent relies on proxy environment variables to communicate with the Rancher management server. These variables are typically configured via the \"Agent Environment Variables\" section in the Rancher UI Cluster Configuration for a given cluster.</p> <p>If anincorrect or unreachable proxy is configured in this section after the cluster is already registered and operational, the communication between Rancher and the downstream cluster breaks. Even if the correct proxy settings are later re-applied in the Rancher UI, the cluster remains disconnected and unmanageable through Rancher, as the the rancher-system-agents are disconnected and unable to apply the update.</p>"},{"location":"kbs/000021951/#resolution","title":"Resolution","text":"<p>To restore communication between Rancher and the downstream cluster:</p>"},{"location":"kbs/000021951/#1-update-proxy-settings-in-the-rancher-ui","title":"1. Update Proxy Settings in the Rancher UI","text":"<p>Navigate to the affected cluster in the Cluster Management section of the Rancher UI and ensure that the correct proxy settings are configured under:</p> <p>Cluster \u2192 Edit Config \u2192 Agent Environment Variables</p> <p>Ensure the following variables are correctly defined:</p> <pre><code>HTTP_PROXY=http://&lt;your-proxy&gt;:&lt;port&gt;\nHTTPS_PROXY=https://&lt;your-proxy&gt;:&lt;port&gt;\nNO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local,0.0.0.0,cattle-system.svc\n</code></pre>"},{"location":"kbs/000021951/#2-manually-fix-proxy-settings-on-one-control-plane-node","title":"2. Manually Fix Proxy Settings on one Control Plane Node","text":"<p>SSH into one of the control plane nodes of the affected downstream cluster, and update the environment file used by the rancher-system-agent.</p> <p>a. Edit the file:</p> <pre><code>sudo vi /etc/systemd/system/rancher-system-agent.env\n</code></pre> <p>b. Add or update the following lines:</p> <pre><code>http_proxy=http://&lt;your-correct-proxy&gt;:&lt;port&gt;\nhttps_proxy=http://&lt;your-correct-proxy&gt;:&lt;port&gt;\nNO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local,0.0.0.0,cattle-system.svc\n</code></pre> <p>c. Restart the agent:</p> <pre><code>sudo systemctl restart rancher-system-agent\n</code></pre> <p>Once the agent on this node can communicate with Rancher, it will allow Rancher to re-establish connectivity and trigger the appropriate upgrade plan via the system-upgrade-controller.</p>"},{"location":"kbs/000021951/#3-automatic-rollout-to-remaining-nodes","title":"3. Automatic Rollout to Remaining Nodes","text":"<p>Once Rancher regains contact with one control plane node, it can deploy a system-upgrade-controller job to roll out the updated configuration to the remaining nodes. This happens even if the other nodes are temporarily disconnected, as Rancher can now orchestrate the changes via Kubernetes Jobs.</p>"},{"location":"kbs/000021951/#cause","title":"Cause","text":"<p>The root cause of this issue is that once a non-functional or invalid proxy is applied through the Agent Environment Variables setting, it is persisted in the local system environment of the rancher-system-agent on each node.</p> <p>Even if the correct proxy settings are later configured in the Rancher UI, the rancher-system-agent processes cannot automatically reload or overwrite their existing environment file (/etc/systemd/system/rancher-system-agent.env), because they are disconnected from Rancher due to the incorrect proxy settings.</p>"},{"location":"kbs/000021951/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021954/","title":"System Upgrade pods fail on Windows nodes with 'service does not exist' error","text":"<p>This document (000021954) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021954/#environment","title":"Environment","text":"<p>Rancher with managed rke2 downstream clusters that have Windows nodes.</p>"},{"location":"kbs/000021954/#situation","title":"Situation","text":"<p>Pods created by the <code>system-upgrade-controller</code> on Windows nodes to manage the <code>rancher-wins</code> service fail and enter an error state with the error:</p> <pre><code>could not build initial state for rancher-wins: could not open rancher-wins service while building initial state: rancher-wins service does not exist\n</code></pre> <p>The affected pods will be missing important securityContext.runAsUserName:</p> <pre><code>    securityContext:\n      capabilities:\n        add:\n        - CAP_SYS_BOOT\n      privileged: true\n</code></pre> <p>The correct securityContext for a system-upgrade pod for Windows is:</p> <pre><code>    securityContext:\n      windowsOptions:\n        hostProcess: true\n        runAsUserName: NT AUTHORITY\\SYSTEM\n</code></pre> <p>This missing context prevents the pod from accessing the Windows service information it needs, causing the failure.</p>"},{"location":"kbs/000021954/#resolution","title":"Resolution","text":"<p>The issue is resolved by forcing Rancher to recreate the upgrade plan for the Windows nodes.</p> <ul> <li>Delete the existing plan using <code>kubectl</code>:</li> </ul> <p><code>kubectl -n cattle-system delete plan system-agent-upgrader-windows</code></p> <p>Rancher\u00a0will detect the missing plan and automatically regenerate it. The new plan will create pods with the correct <code>securityContext</code>, allowing the <code>rancher-wins</code> upgrade to proceed successfully.</p>"},{"location":"kbs/000021954/#cause","title":"Cause","text":"<p>The root cause for the plan being generated incorrectly is currently unknown. This issue has only been observed in a small number of environments and has not been successfully reproduced in testing.</p>"},{"location":"kbs/000021954/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021954/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021955/","title":"RKE2 etcd Restore Fails with \"Permission Denied\" Error Due to umask Value","text":"<p>This document (000021955) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021955/#environment","title":"Environment","text":"<p>Suse Rancher 2.x</p> <p>RKE2 imported cluster</p>"},{"location":"kbs/000021955/#situation","title":"Situation","text":"<p>When attempting to restore <code>etcd</code> on an imported RKE2 cluster using the command \" <code>rke2 server --cluster-reset --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt;\"</code>, as the root user, the operation fails with the following error messages from the etcd container:</p> <pre><code>failed to verify flags\", \"error\":\"open /var/lib/rancher/rke2/server/db/etcd/config: permission denied\"\n</code></pre>"},{"location":"kbs/000021955/#resolution","title":"Resolution","text":"<p>Change the operating system's <code>umask</code> value to <code>0022</code> before performing the <code>etcd</code> restore. A <code>umask</code> of <code>0022</code> sets the default permissions to <code>rwxr-xr-x</code>, which grants read and execute permissions to the group and others, while maintaining write permissions for the owner.</p> <p>Stop the RKE2 service</p> <pre><code>systemctl stop rke2-server\n</code></pre> <p>Change the umask value temporarily</p> <pre><code>umask 0022\n</code></pre> <p>Execute the restore command</p> <pre><code>rke2 server --cluster-reset --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt;\n</code></pre> <p>Start the rke2 service</p> <pre><code>systemctl start rke2-server\n</code></pre>"},{"location":"kbs/000021955/#cause","title":"Cause","text":"<p>A more restrictive <code>umask</code> of <code>0027</code> sets the default permissions to <code>rwxr-x---</code>, which means files and directories are not writable by the group or others, which is required by etcd to be able to read/write it's config and data.</p> <p>During the <code>etcd</code> restore process, RKE2 attempts to access or create files within the <code>/var/lib/rancher/rke2/server/db/etcd/</code> directory. The restrictive <code>umask</code> of <code>0027</code> prevents the necessary read/write access for the process, resulting in the \"permission denied\" error.</p>"},{"location":"kbs/000021955/#status","title":"Status","text":"<p>Top Issue</p>"},{"location":"kbs/000021955/#additional-information","title":"Additional Information","text":"<p>https://github.com/rancher/rke2/issues/4679#issuecomment-1692870875</p>"},{"location":"kbs/000021955/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021961/","title":"Why is my LoadBalancer service stuck in Provisioning?","text":"<p>This document (000021961) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021961/#environment","title":"Environment","text":"<p>A standalone or Rancher-managed Kubernetes cluster, with a Service of type Loadbalancer</p>"},{"location":"kbs/000021961/#situation","title":"Situation","text":"<p>When attempting to deploy a LoadBalancer\u00a0service in a Rancher or Rancher-managed cluster, the service remains in a Pending or Provisioning state, with the following message:</p> <pre><code>Service is ready: LoadBalancer is being provisioned\n</code></pre>"},{"location":"kbs/000021961/#resolution","title":"Resolution","text":"<p>When deploying a LoadBalancer service, Kubernetes relies on an external controller to provision the external IP address. If such a controller is not present or is misconfigured, the service will remain in the Pending\u00a0state indefinitely.</p> <p>This commonly happens in the following scenarios:</p>"},{"location":"kbs/000021961/#1-missing-or-misconfigured-cloud-provider-integration","title":"1. Missing or misconfigured Cloud Provider Integration","text":"<p>In cloud environments (e.g., GCP, AWS, Azure), Kubernetes depends on a Cloud Controller Manager (CCM) to provision LoadBalancer services. If the CCM is not properly integrated or the cloud credentials lack the required permissions, provisioning will fail.</p>"},{"location":"kbs/000021961/#2-no-external-lb-controller-on-bare-metal","title":"2. No External LB Controller on Bare Metal","text":"<p>On bare-metal clusters or self-hosted environments, the option of a Cloud Controller Manager with which to provision LoadBalancer services is absent. In these cases, you must manually install a LoadBalancer controller, such as MetalLB:</p> <p>https://metallb.universe.tf/</p> <p>Note:\u00a0MetalLB is a third-party project. SUSE Rancher Support does not cover configuration or troubleshooting of third-party tools.</p>"},{"location":"kbs/000021961/#3-insufficient-cloud-iam-permissions","title":"3. Insufficient Cloud IAM Permissions","text":"<p>Ensure that the cloud credentials used by the Cloud Controller Manager have sufficient privileges to create Load Balancers in the infrastructure provider (e.g., appropriate IAM roles or policies).</p> <p>Tips for further investigation</p> <p>To further investigate the issue, run the following commands:</p> <p>```whitespace-pre! bash</p> <p> kubectl get svc  -n  -o wide kubectl describe svc  -n  <p>```</p> <p>Pay attention to the following fields in the output:</p> <ul> <li> <p>Events: Look for any warnings or errors related to provisioning</p> </li> <li> <p>External IP: Should show an allocated IP once provisioning succeeds</p> </li> <li> <p>Annotations: Review for any cloud-provider or controller-specific settings</p> </li> </ul>"},{"location":"kbs/000021961/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021968/","title":"How to configure the Authorized Cluster Endpoint with a Layer 4 Loadbalancer for a Rancher-provisioned RKE2 or K3s cluster","text":"<p>This document (000021968) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021968/#environment","title":"Environment","text":"<ul> <li>A Rancher-provisioned RKE2 or K3s cluster with Authorized Cluster Endpoint (ACE) enabled</li> </ul>"},{"location":"kbs/000021968/#situation","title":"Situation","text":"<p>You are trying to configure the Authorized Cluster Endpoint (ACE) to provide direct <code>kubectl</code> access to a downstream cluster through a Layer 4 load balancer. You have enabled ACE and provided the FQDN of your load balancer, with the intention of using the cluster's default self-signed certificates.</p> <p>However, when you use the generated <code>kubeconfig</code> file to connect to the cluster's FQDN, the kubeconfig generated does not contain a CA certificate in <code>certificate-authority-data</code> ( ``) and the connection fails with a TLS certificate validation error, such as:</p> <p><code>Unable to connect to the server: tls: failed to verify certificate: x509: certificate signed by unknown authority</code></p>"},{"location":"kbs/000021968/#resolution","title":"Resolution","text":"<p>To resolve this, you must perform a two-step configuration. The first step can be done during cluster provisioning, and the second is done after the cluster is active.</p> <p>Step 1: Add the FQDN as a TLS Alternate Name (SAN)</p> <p>Pass the FQDN of the external loadbalancer as a TLS Alternate Name to ensure that the Kuberentes API Server certificates generated by RKE2 are valid for this FQDN.</p> <ul> <li>Via Rancher UI:</li> </ul> <p>When creating or editing the cluster, navigate to the Networking tab and add your load balancer's FQDN (e.g., kube-api.my-company.com) to the TLS Alternate Names field. - Via YAML:</p> <p>Add the tls-san parameter to your cluster configuration YAML.</p> <pre><code>YAML\n\nspec:\n rkeConfig:\n   machineSelectorConfig:\n     - config:\n         tls-san:\n           - kube-api.my-company.com\n</code></pre> <p>Step 2: Provide the Cluster CA Certificate to the ACE Configuration</p> <p>After initial cluster provisioning retrieve the Kubernetes API Server CA Certificate and add this to the CA Certificates field of the ACE configuration. This step tells Rancher which Certificate Authority to use to validate the endpoint.</p> <ol> <li>Retrieve the CA Certificate from a control plane node in your downstream cluster. You will need SSH access to the node.</li> </ol> <p>- For RKE2: /var/lib/rancher/rke2/server/tls/server-ca.crt</p> <p>- For K3s: /var/lib/rancher/k3s/server/tls/server-ca.crt 2. Copy the entire content of this server-ca.crt file. 3. Update the ACE Configuration in the Rancher UI:</p> <p>- Navigate to your cluster and click Edit Config.</p> <p>- In the Cluster Configuration Networking section, scroll to Authorized Cluster Endpoint.</p> <p>- Paste the copied certificate content into the CA Certificates field.</p> <p>- Save the changes.</p> <p>Download a new kubeconfig file. It will now contain the correct certificate-authority-data and will connect successfully.</p>"},{"location":"kbs/000021968/#cause","title":"Cause","text":"<p>The Authorized Cluster Endpoint (ACE) functionality is currently designed for TLS termination at the Loadbalancer (Layer 7 load balancers), in which a user configures a separate CA-signed certificate for this Loadbalancer endpoint (that signs the TLS certificate used on the load balancer itself), and then defines this CA in the ACE configuration for the cluster. As a result, it is not currently possible to automate configuration of the ACE endpoint using the Kubernetes API Server CA Certificate at cluster provisioning time.</p> <p>When using a Layer 4 load balancer, TLS traffic is passed through directly to the cluster's API server. This creates two requirements that are not automatically configured:</p> <ul> <li>The API server's own TLS certificate must include the load balancer's FQDN as a valid Subject Alternative Name (SAN). Without this, the certificate is not considered valid for that hostname.</li> <li>Rancher's kubeconfig generator needs to be explicitly told to trust the cluster's internal, self-signed CA for that FQDN.</li> </ul>"},{"location":"kbs/000021968/#additional-information","title":"Additional Information","text":"<p>Enhancement Request: This manual, multi-step process is a known limitation. An enhancement request exists to streamline this workflow for Layer 4 load balancers, which would automate the steps outlined above.</p>"},{"location":"kbs/000021968/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021976/","title":"Jobs apply-system-agent-upgrader on nodes keep failing","text":"<p>This document (000021976) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021976/#environment","title":"Environment","text":"<p>Rancher-provisioned RKE2 \u00a0downstream clusters</p>"},{"location":"kbs/000021976/#situation","title":"Situation","text":"<p>Jobs apply-system-agent-upgrader-on every node are failing in newly Rancher-provisioned RKE2 downstream clusters with an error</p> <pre><code>+ CATTLE_AGENT_VAR_DIR=/var/lib/rancher/agent\n+ TMPDIRBASE=/var/lib/rancher/agent/tmp\n+ mkdir -p /host/var/lib/rancher/agent/tmp\nmkdir: cannot create directory '/host/var/lib/rancher': File exists\n</code></pre> <p>It indicates that the directory /host/var/lib/rancher cannot be created because the file already exists. This occurs because /var/lib/rancher is a symlink on the host.</p>"},{"location":"kbs/000021976/#resolution","title":"Resolution","text":"<p>Delete the downstream cluster and recreate it without the symlink on nodes. Alternatively, remove the /var/lib/rancher symlink before creating the cluster.</p>"},{"location":"kbs/000021976/#cause","title":"Cause","text":"<p>The issue is caused by a pre-existing symlink for /var/lib/rancher that prevents the RKE2 cluster from creating the directory.</p> <p>A symlink (symbolic link) is a pointer to another file or directory on the same filesystem. Because the pod's filesystem is separate from the host's, a symlink created on the host will point to a location on the host's filesystem. When the pod tries to follow that link, it can't find the target because the path doesn't exist within the pod's isolated filesystem. This is a security and isolation feature of Kubernetes and containerization in general.</p>"},{"location":"kbs/000021976/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021992/","title":"RKE2/K3S Cluster Support for Mixed Operating Systems","text":"<p>This document (000021992) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021992/#environment","title":"Environment","text":"<p>Rancher 2.x</p> <p>RKE2</p> <p>K3S</p>"},{"location":"kbs/000021992/#situation","title":"Situation","text":"<p>Creation of RKE2/K3S cluster by using a mix of Linux OS. For instance, using Red Hat OS for master nodes and Ubuntu for worker nodes.</p>"},{"location":"kbs/000021992/#resolution","title":"Resolution","text":"<p>For a stable and supported deployment, a single, consistent operating system should be used across all master and worker nodes in a cluster.</p> <ul> <li> <p>Unsupported Configuration: The SUSE engineering\u00a0team doesn't test or validate mixed-OS setups for RKE2 or K3S. \u00a0This means that any problems encountered may be challenging to diagnose and fix, as the root cause could be related to the OS differences rather than a software bug.</p> </li> <li> <p>Recommendation: To ensure cluster stability, simplify management, and receive reliable support, always use the same OS for every node in your cluster.</p> </li> </ul>"},{"location":"kbs/000021992/#cause","title":"Cause","text":"<p>Using a mixed-OS configuration within a single cluster is not officially supported. This is due to potential issues that can arise from the differences between operating systems. Variations in kernel versions, system libraries, and networking can lead to unpredictable behaviour and make troubleshooting difficult.</p>"},{"location":"kbs/000021992/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"kbs/000021994/","title":"Tigera-operator Pod operations failing due to rancher-webhook unauthorized errors in a Rancher-managed RKE2 cluster","text":"<p>This document (000021994) is provided subject to the disclaimer at the end of this document.</p>"},{"location":"kbs/000021994/#environment","title":"Environment","text":"<p>An RKE2 cluster, with the Calico CNI, managed by a Rancher v2.7.4+ instance</p>"},{"location":"kbs/000021994/#situation","title":"Situation","text":"<p>The tigera-operator fails when attempting to apply Pod Security Admission (PSA) labels to the calico-system Namespace, with the requests denied by the Rancher admission webhook (rancher-webhook). The tigera-operator Pod in the tigera-operator Namespace logs errors of the following format:</p> <pre><code>{\"level\":\"info\",\"ts\":\"2025-08-22T09:57:08Z\",\"logger\":\"controller_installation\",\"msg\":\"Failed to update object.\",\"Name\":\"calico-system\",\"Namespace\":\"\",\"Kind\":\"Namespace\",\"key\":{\"name\":\"calico-system\"}}\n{\"level\":\"error\",\"ts\":\"2025-08-22T09:57:08Z\",\"logger\":\"controller_installation\",\"msg\":\"Failed to create or update object\",\"component\":\"*render.namespaceComponent\",\"key\":{\"name\":\"calico-system\"},\"error\":\"admission webhook \\\"rancher.cattle.io.namespaces\\\" denied the request: Unauthorized\",\"stacktrace\":\"github.com/tigera/operator/pkg/controller/utils.(*componentHandler).CreateOrUpdateOrDelete\\n\\t/go/src/github.com/tigera/operator/pkg/controller/utils/component.go:347\\ngithub.com/tigera/operator/pkg/controller/installation.(*ReconcileInstallation).Reconcile\\n\\t/go/src/github.com/tigera/operator/pkg/controller/installation/core_controller.go:1499\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:118\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:328\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:288\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:249\"}\n{\"level\":\"error\",\"ts\":\"2025-08-22T09:57:08Z\",\"logger\":\"controller_installation\",\"msg\":\"Error creating / updating resource\",\"Request.Namespace\":\"calico-system\",\"Request.Name\":\"active-operator\",\"reason\":\"ResourceUpdateError\",\"error\":\"admission webhook \\\"rancher.cattle.io.namespaces\\\" denied the request: Unauthorized\",\"stacktrace\":\"github.com/tigera/operator/pkg/controller/status.(*statusManager).SetDegraded\\n\\t/go/src/github.com/tigera/operator/pkg/controller/status/status.go:356\\ngithub.com/tigera/operator/pkg/controller/installation.(*ReconcileInstallation).Reconcile\\n\\t/go/src/github.com/tigera/operator/pkg/controller/installation/core_controller.go:1500\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Reconcile\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:118\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:328\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:288\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:249\"}\n{\"level\":\"error\",\"ts\":\"2025-08-22T09:57:08Z\",\"msg\":\"Reconciler error\",\"controller\":\"tigera-installation-controller\",\"object\":{\"name\":\"active-operator\",\"namespace\":\"calico-system\"},\"namespace\":\"calico-system\",\"name\":\"active-operator\",\"reconcileID\":\"66cddc2d-4337-4c12-949c-22122d342ded\",\"error\":\"admission webhook \\\"rancher.cattle.io.namespaces\\\" denied the request: Unauthorized\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:341\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:288\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.2/pkg/internal/controller/controller.go:249\"}\n</code></pre> <p>The rancher-webhook admission controller rejections can also be seen in the Kubernetes API server logs for the cluster:</p> <pre><code>W0822 10:00:54.333194       1 dispatcher.go:225] rejected by webhook \"rancher.cattle.io.namespaces\": &amp;errors.StatusError{ErrStatus:v1.Status{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ListMeta:v1.ListMeta{SelfLink:\"\", ResourceVersion:\"\", Continue:\"\", RemainingItemCount:(*int64)(nil)}, Status:\"Failure\", Message:\"admission webhook \\\"rancher.cattle.io.namespaces\\\" denied the request: Unauthorized\", Reason:\"Unauthorized\", Details:(*v1.StatusDetails)(nil), Code:403}}\n</code></pre> <p>If the typha-certs in the calico-system Namespace have expired, the inability of the tigera-operator to add these PSA labels to the calico-system Namespace, blocks it from successfully rotating the certificates. As a result, errors of the following format may be observed in the calico-node Pods, and cluster Pod networking will not function correctly:</p> <pre><code>2025-08-022 08:14:02.163 [WARNING][16782] tunnel-ip-allocator/sync_client.go 158: error connecting to typha endpoint (3 of 3) 10.43.9.174:5473 connID=0x0 error=x509: certificate has expired or is not yet valid: current time 2025-08-22T08:14:02Z is after 2025-06-11T06:15:14Z type=\"tunnel-ip-allocation\"\n</code></pre>"},{"location":"kbs/000021994/#resolution","title":"Resolution","text":"<p>Create the following ClusterRole and ClusterRolebinding, in the affected cluster, to provide the tigera-operator with permission to update PSA labels:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: tigera-operator-psa\nrules:\n- apiGroups:\n  - management.cattle.io\n  resources:\n  - projects\n  verbs:\n  - updatepsa\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: tigera-operator-psa\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: tigera-operator-psa\nsubjects:\n- kind: ServiceAccount\n  name: tigera-operator\n  namespace: tigera-operator\n</code></pre>"},{"location":"kbs/000021994/#cause","title":"Cause","text":"<p>The tigera-operator requires the updatepsa permission to apply the PSA labels \"pod-security.kubernetes.io/enforce: privileged\" and \"pod-security.kubernetes.io/enforce-version: latest\" to the calico-system Namespace, in a Rancher-managed RKE2 cluster (where the rancher-webhook is deployed by Rancher).</p> <p>By default, these labels are successfully applied to the calico-system Namespace by the installation of the rke2-calico chart, during initial cluster provisioning, before the subsequent installation of the rancher-webhook.</p> <p>If the cluster was initially provisioned with a much older version of RKE2, using a version of calico that did not apply the labels (e.g. v1.22.4+rke2r2), or the labels are manually removed from the calico-system Namespace after initial cluster provisioning, this issue is encountered. In this instance, applying the ClusterRole and ClusterRoleBinding, as documented here, is required to permit the tigera-operator to successfully apply the PSA labels.</p>"},{"location":"kbs/000021994/#additional-information","title":"Additional Information","text":"<ul> <li>Tigera documentation on the installation of Calico Enterprise within an RKE2 cluster</li> <li>Rancher admission webhook (rancher-webhook) documentation on PSAs</li> </ul>"},{"location":"kbs/000021994/#disclaimer","title":"Disclaimer","text":"<p>This Support Knowledgebase provides a valuable tool for SUSE customers and parties interested in our products and solutions to acquire information, ideas and learn from one another. Materials are provided for informational, personal or non-commercial use within your organization and are presented \"AS IS\" WITHOUT WARRANTY OF ANY KIND.</p>"}]}